1.13.1 

num_attention_heads: 80, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x1x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x1x2048): 0.659
Elapsed time for attention_prob_times_values (320x2048x2048x1): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x1): 1.377

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 0.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x2x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x2x2048): 0.870
Elapsed time for attention_prob_times_values (320x2048x2048x2): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x2): 2.780

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x3x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x3x2048): 2.719
Elapsed time for attention_prob_times_values (320x2048x2048x3): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x3): 3.246

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 3.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x4x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x4x2048): 2.421
Elapsed time for attention_prob_times_values (320x2048x2048x4): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x4): 5.423

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 4.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x5x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x5x2048): 4.550
Elapsed time for attention_prob_times_values (320x2048x2048x5): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x5): 5.177

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 6.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x6x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x6x2048): 3.567
Elapsed time for attention_prob_times_values (320x2048x2048x6): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x6): 8.025

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 7.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x7x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x7x2048): 6.498
Elapsed time for attention_prob_times_values (320x2048x2048x7): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x7): 6.483

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 10.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x8x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x8x2048): 8.710
Elapsed time for attention_prob_times_values (320x2048x2048x8): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x8): 9.736

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 14.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x9x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x9x2048): 6.221
Elapsed time for attention_prob_times_values (320x2048x2048x9): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x9): 8.792

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 12.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x10x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x10x2048): 6.069
Elapsed time for attention_prob_times_values (320x2048x2048x10): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x10): 13.505

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 14.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x11x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x11x2048): 7.482
Elapsed time for attention_prob_times_values (320x2048x2048x11): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x11): 10.701

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 16.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x12x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x12x2048): 7.311
Elapsed time for attention_prob_times_values (320x2048x2048x12): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x12): 15.039

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 19.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x13x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x13x2048): 8.601
Elapsed time for attention_prob_times_values (320x2048x2048x13): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x13): 12.029

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 20.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x14x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x14x2048): 8.529
Elapsed time for attention_prob_times_values (320x2048x2048x14): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x14): 17.851

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 24.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x15x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x15x2048): 10.253
Elapsed time for attention_prob_times_values (320x2048x2048x15): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x15): 13.432

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 25.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x16x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x16x2048): 12.798
Elapsed time for attention_prob_times_values (320x2048x2048x16): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x16): 21.670

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 36.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x17x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x17x2048): 11.692
Elapsed time for attention_prob_times_values (320x2048x2048x17): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x17): 15.285

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 30.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x18x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x18x2048): 10.746
Elapsed time for attention_prob_times_values (320x2048x2048x18): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x18): 22.513

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 35.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x19x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x19x2048): 12.703
Elapsed time for attention_prob_times_values (320x2048x2048x19): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x19): 16.952

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 36.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x20x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x20x2048): 12.097
Elapsed time for attention_prob_times_values (320x2048x2048x20): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x20): 24.786

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 41.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x21x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x21x2048): 14.337
Elapsed time for attention_prob_times_values (320x2048x2048x21): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x21): 19.114

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 43.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x22x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x22x2048): 13.055
Elapsed time for attention_prob_times_values (320x2048x2048x22): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x22): 26.720

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 47.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x23x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x23x2048): 15.151
Elapsed time for attention_prob_times_values (320x2048x2048x23): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x23): 21.019

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 49.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x24x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x24x2048): 27.013
Elapsed time for attention_prob_times_values (320x2048x2048x24): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x24): 30.731

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 82.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x25x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x25x2048): 16.236
Elapsed time for attention_prob_times_values (320x2048x2048x25): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x25): 18.628

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 51.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x26x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x26x2048): 15.756
Elapsed time for attention_prob_times_values (320x2048x2048x26): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x26): 27.877

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 61.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x27x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x27x2048): 17.569
Elapsed time for attention_prob_times_values (320x2048x2048x27): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x27): 22.483

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 61.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x28x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x28x2048): 16.029
Elapsed time for attention_prob_times_values (320x2048x2048x28): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x28): 31.607

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 67.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x29x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x29x2048): 19.043
Elapsed time for attention_prob_times_values (320x2048x2048x29): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x29): 25.058

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 70.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x30x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x30x2048): 17.755
Elapsed time for attention_prob_times_values (320x2048x2048x30): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x30): 32.860

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 77.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x31x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x31x2048): 20.707
Elapsed time for attention_prob_times_values (320x2048x2048x31): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x31): 26.793

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 79.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x32x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x32x2048): 36.885
Elapsed time for attention_prob_times_values (320x2048x2048x32): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x32): 41.441

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 136.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x33x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x33x2048): 18.426
Elapsed time for attention_prob_times_values (320x2048x2048x33): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x33): 23.421

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 73.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x34x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x34x2048): 14.630
Elapsed time for attention_prob_times_values (320x2048x2048x34): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x34): 37.504

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 76.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x35x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x35x2048): 19.475
Elapsed time for attention_prob_times_values (320x2048x2048x35): 0.0191
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x35): 4.920

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 29.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x36x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x36x2048): 14.999
Elapsed time for attention_prob_times_values (320x2048x2048x36): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x36): 39.941

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 83.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x37x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x37x2048): 20.082
Elapsed time for attention_prob_times_values (320x2048x2048x37): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x37): 19.065

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 76.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x38x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x38x2048): 15.891
Elapsed time for attention_prob_times_values (320x2048x2048x38): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x38): 37.236

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 88.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x39x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x39x2048): 20.476
Elapsed time for attention_prob_times_values (320x2048x2048x39): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x39): 26.713

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 93.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x40x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x40x2048): 38.757
Elapsed time for attention_prob_times_values (320x2048x2048x40): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x40): 50.034

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 180.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x41x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x41x2048): 21.993
Elapsed time for attention_prob_times_values (320x2048x2048x41): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x41): 27.563

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 102.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x42x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x42x2048): 17.270
Elapsed time for attention_prob_times_values (320x2048x2048x42): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x42): 43.989

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 106.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x43x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x43x2048): 22.709
Elapsed time for attention_prob_times_values (320x2048x2048x43): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x43): 29.817

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 112.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x44x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x44x2048): 16.858
Elapsed time for attention_prob_times_values (320x2048x2048x44): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x44): 45.734

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 109.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x45x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x45x2048): 23.584
Elapsed time for attention_prob_times_values (320x2048x2048x45): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x45): 31.450

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 121.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x46x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x46x2048): 18.190
Elapsed time for attention_prob_times_values (320x2048x2048x46): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x46): 50.215

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 122.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x47x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x47x2048): 24.481
Elapsed time for attention_prob_times_values (320x2048x2048x47): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x47): 31.233

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 128.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x48x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x48x2048): 44.575
Elapsed time for attention_prob_times_values (320x2048x2048x48): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x48): 60.853

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 244.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x49x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x49x2048): 24.924
Elapsed time for attention_prob_times_values (320x2048x2048x49): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x49): 32.706

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 136.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x50x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x50x2048): 19.426
Elapsed time for attention_prob_times_values (320x2048x2048x50): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x50): 53.492

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 139.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x51x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x51x2048): 25.601
Elapsed time for attention_prob_times_values (320x2048x2048x51): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x51): 34.913

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 147.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x52x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x52x2048): 19.600
Elapsed time for attention_prob_times_values (320x2048x2048x52): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x52): 56.485

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 147.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x53x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x53x2048): 26.788
Elapsed time for attention_prob_times_values (320x2048x2048x53): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x53): 36.876

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 159.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x54x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x54x2048): 20.599
Elapsed time for attention_prob_times_values (320x2048x2048x54): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x54): 56.763

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 157.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x55x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x55x2048): 26.469
Elapsed time for attention_prob_times_values (320x2048x2048x55): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x55): 35.944

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 161.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x56x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x56x2048): 51.382
Elapsed time for attention_prob_times_values (320x2048x2048x56): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x56): 70.220

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 318.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x57x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x57x2048): 27.487
Elapsed time for attention_prob_times_values (320x2048x2048x57): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x57): 17.629

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 117.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x58x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x58x2048): 19.796
Elapsed time for attention_prob_times_values (320x2048x2048x58): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x58): 62.039

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 166.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x59x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x59x2048): 29.373
Elapsed time for attention_prob_times_values (320x2048x2048x59): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x59): 39.649

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 189.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x60x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x60x2048): 20.773
Elapsed time for attention_prob_times_values (320x2048x2048x60): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x60): 57.371

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 173.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x61x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x61x2048): 29.619
Elapsed time for attention_prob_times_values (320x2048x2048x61): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x61): 41.047

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 198.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x62x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x62x2048): 21.539
Elapsed time for attention_prob_times_values (320x2048x2048x62): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x62): 66.267

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 189.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x63x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x63x2048): 29.250
Elapsed time for attention_prob_times_values (320x2048x2048x63): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x63): 40.895

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 201.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x64x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x64x2048): 58.717
Elapsed time for attention_prob_times_values (320x2048x2048x64): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x64): 77.959

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 401.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x65x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x65x2048): 27.044
Elapsed time for attention_prob_times_values (320x2048x2048x65): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x65): 39.989

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 196.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x66x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x66x2048): 19.368
Elapsed time for attention_prob_times_values (320x2048x2048x66): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x66): 69.912

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 186.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x67x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x67x2048): 27.447
Elapsed time for attention_prob_times_values (320x2048x2048x67): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x67): 39.688

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 202.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x68x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x68x2048): 19.080
Elapsed time for attention_prob_times_values (320x2048x2048x68): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x68): 71.828

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 190.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x69x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x69x2048): 27.449
Elapsed time for attention_prob_times_values (320x2048x2048x69): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x69): 43.294

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 214.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x70x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x70x2048): 19.925
Elapsed time for attention_prob_times_values (320x2048x2048x70): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x70): 72.989

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 202.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x71x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x71x2048): 27.404
Elapsed time for attention_prob_times_values (320x2048x2048x71): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x71): 8.966

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 88.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x72x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x72x2048): 65.912
Elapsed time for attention_prob_times_values (320x2048x2048x72): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x72): 78.643

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 475.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x73x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x73x2048): 28.233
Elapsed time for attention_prob_times_values (320x2048x2048x73): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x73): 44.608

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 231.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x74x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x74x2048): 49.890
Elapsed time for attention_prob_times_values (320x2048x2048x74): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x74): 68.125

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 390.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x75x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x75x2048): 28.996
Elapsed time for attention_prob_times_values (320x2048x2048x75): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x75): 46.257

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 244.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x76x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x76x2048): 45.886
Elapsed time for attention_prob_times_values (320x2048x2048x76): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x76): 76.867

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 398.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x77x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x77x2048): 29.517
Elapsed time for attention_prob_times_values (320x2048x2048x77): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x77): 46.237

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 252.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x78x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x78x2048): 52.464
Elapsed time for attention_prob_times_values (320x2048x2048x78): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x78): 77.758

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 444.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x79x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x79x2048): 29.163
Elapsed time for attention_prob_times_values (320x2048x2048x79): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x79): 47.565

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 259.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x80x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x80x2048): 74.600
Elapsed time for attention_prob_times_values (320x2048x2048x80): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x80): 89.393

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 589.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x81x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x81x2048): 29.902
Elapsed time for attention_prob_times_values (320x2048x2048x81): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x81): 45.081

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 263.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x82x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x82x2048): 54.536
Elapsed time for attention_prob_times_values (320x2048x2048x82): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x82): 83.257

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 488.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x83x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x83x2048): 29.904
Elapsed time for attention_prob_times_values (320x2048x2048x83): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x83): 50.088

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 280.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x84x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x84x2048): 56.809
Elapsed time for attention_prob_times_values (320x2048x2048x84): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x84): 83.518

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 511.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x85x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x85x2048): 31.031
Elapsed time for attention_prob_times_values (320x2048x2048x85): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x85): 51.112

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 295.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x86x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x86x2048): 58.386
Elapsed time for attention_prob_times_values (320x2048x2048x86): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x86): 85.220

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 534.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x87x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x87x2048): 31.364
Elapsed time for attention_prob_times_values (320x2048x2048x87): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x87): 51.910

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 304.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x88x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x88x2048): 75.294
Elapsed time for attention_prob_times_values (320x2048x2048x88): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x88): 100.957

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 679.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x89x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x89x2048): 32.075
Elapsed time for attention_prob_times_values (320x2048x2048x89): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x89): 53.221

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 318.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x90x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x90x2048): 60.790
Elapsed time for attention_prob_times_values (320x2048x2048x90): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x90): 88.237

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 578.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x91x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x91x2048): 32.366
Elapsed time for attention_prob_times_values (320x2048x2048x91): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x91): 55.379

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 331.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x92x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x92x2048): 60.735
Elapsed time for attention_prob_times_values (320x2048x2048x92): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x92): 91.951

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 598.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x93x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x93x2048): 32.920
Elapsed time for attention_prob_times_values (320x2048x2048x93): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x93): 56.618

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 344.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x94x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x94x2048): 61.776
Elapsed time for attention_prob_times_values (320x2048x2048x94): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x94): 94.352

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 622.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x95x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x95x2048): 34.548
Elapsed time for attention_prob_times_values (320x2048x2048x95): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x95): 55.518

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 358.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x96x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x96x2048): 91.428
Elapsed time for attention_prob_times_values (320x2048x2048x96): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x96): 106.019

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 834.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x97x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x97x2048): 31.187
Elapsed time for attention_prob_times_values (320x2048x2048x97): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x97): 56.458

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 344.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x98x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x98x2048): 59.613
Elapsed time for attention_prob_times_values (320x2048x2048x98): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x98): 99.047

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 644.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x99x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x99x2048): 29.391
Elapsed time for attention_prob_times_values (320x2048x2048x99): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x99): 58.471

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 341.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x100x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x100x2048): 59.603
Elapsed time for attention_prob_times_values (320x2048x2048x100): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x100): 101.095

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 660.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x101x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x101x2048): 30.632
Elapsed time for attention_prob_times_values (320x2048x2048x101): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x101): 59.775

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 360.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x102x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x102x2048): 60.924
Elapsed time for attention_prob_times_values (320x2048x2048x102): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x102): 99.353

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 677.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x103x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x103x2048): 30.543
Elapsed time for attention_prob_times_values (320x2048x2048x103): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x103): 60.697

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 367.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x104x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x104x2048): 85.277
Elapsed time for attention_prob_times_values (320x2048x2048x104): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x104): 116.604

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 898.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x105x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x105x2048): 30.656
Elapsed time for attention_prob_times_values (320x2048x2048x105): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x105): 61.889

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 377.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x106x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x106x2048): 62.576
Elapsed time for attention_prob_times_values (320x2048x2048x106): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x106): 105.991

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 730.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x107x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x107x2048): 31.620
Elapsed time for attention_prob_times_values (320x2048x2048x107): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x107): 63.674

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 395.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x108x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x108x2048): 64.624
Elapsed time for attention_prob_times_values (320x2048x2048x108): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x108): 105.700

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 756.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x109x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x109x2048): 31.992
Elapsed time for attention_prob_times_values (320x2048x2048x109): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x109): 64.330

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 406.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x110x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x110x2048): 64.094
Elapsed time for attention_prob_times_values (320x2048x2048x110): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x110): 110.422

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 778.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x111x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x111x2048): 32.259
Elapsed time for attention_prob_times_values (320x2048x2048x111): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x111): 66.360

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 419.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x112x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x112x2048): 97.315
Elapsed time for attention_prob_times_values (320x2048x2048x112): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x112): 126.013

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1070.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x113x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x113x2048): 31.808
Elapsed time for attention_prob_times_values (320x2048x2048x113): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x113): 65.737

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 421.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x114x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x114x2048): 66.015
Elapsed time for attention_prob_times_values (320x2048x2048x114): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x114): 112.828

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 825.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x115x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x115x2048): 18.047
Elapsed time for attention_prob_times_values (320x2048x2048x115): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x115): 65.672

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 282.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x116x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x116x2048): 66.700
Elapsed time for attention_prob_times_values (320x2048x2048x116): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x116): 114.746

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 848.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x117x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x117x2048): 34.454
Elapsed time for attention_prob_times_values (320x2048x2048x117): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x117): 69.303

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 466.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x118x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x118x2048): 66.668
Elapsed time for attention_prob_times_values (320x2048x2048x118): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x118): 117.229

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 868.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x119x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x119x2048): 33.440
Elapsed time for attention_prob_times_values (320x2048x2048x119): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x119): 70.561

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 467.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x120x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x120x2048): 90.792
Elapsed time for attention_prob_times_values (320x2048x2048x120): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x120): 131.492

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1114.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x121x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x121x2048): 34.078
Elapsed time for attention_prob_times_values (320x2048x2048x121): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x121): 69.910

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 478.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x122x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x122x2048): 72.078
Elapsed time for attention_prob_times_values (320x2048x2048x122): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x122): 117.432

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 940.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x123x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x123x2048): 35.720
Elapsed time for attention_prob_times_values (320x2048x2048x123): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x123): 73.463

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 509.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x124x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x124x2048): 72.658
Elapsed time for attention_prob_times_values (320x2048x2048x124): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x124): 118.768

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 963.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x125x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x125x2048): 36.541
Elapsed time for attention_prob_times_values (320x2048x2048x125): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x125): 73.305

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 525.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x126x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x126x2048): 73.527
Elapsed time for attention_prob_times_values (320x2048x2048x126): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x126): 122.402

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 996.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x127x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x127x2048): 36.350
Elapsed time for attention_prob_times_values (320x2048x2048x127): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x127): 73.927

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 532.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x128x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x128x2048): 112.204
Elapsed time for attention_prob_times_values (320x2048x2048x128): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x128): 135.791

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1351.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x129x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x129x2048): 33.688
Elapsed time for attention_prob_times_values (320x2048x2048x129): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x129): 51.811

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 452.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x130x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x130x2048): 69.665
Elapsed time for attention_prob_times_values (320x2048x2048x130): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x130): 92.631

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 887.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x131x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x131x2048): 34.087
Elapsed time for attention_prob_times_values (320x2048x2048x131): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x131): 56.759

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 478.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x132x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x132x2048): 70.439
Elapsed time for attention_prob_times_values (320x2048x2048x132): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x132): 91.996

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 902.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x133x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x133x2048): 34.142
Elapsed time for attention_prob_times_values (320x2048x2048x133): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x133): 58.547

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 491.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x134x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x134x2048): 70.421
Elapsed time for attention_prob_times_values (320x2048x2048x134): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x134): 93.954

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 923.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x135x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x135x2048): 33.099
Elapsed time for attention_prob_times_values (320x2048x2048x135): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x135): 56.743

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 482.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x136x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x136x2048): 97.338
Elapsed time for attention_prob_times_values (320x2048x2048x136): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x136): 127.104

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1281.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x137x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x137x2048): 33.141
Elapsed time for attention_prob_times_values (320x2048x2048x137): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x137): 57.583

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 492.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x138x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x138x2048): 71.427
Elapsed time for attention_prob_times_values (320x2048x2048x138): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x138): 92.496

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 949.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x139x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x139x2048): 35.299
Elapsed time for attention_prob_times_values (320x2048x2048x139): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x139): 59.235

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 524.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x140x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x140x2048): 73.926
Elapsed time for attention_prob_times_values (320x2048x2048x140): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x140): 93.730

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 986.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x141x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x141x2048): 34.579
Elapsed time for attention_prob_times_values (320x2048x2048x141): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x141): 57.799

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 519.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x142x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x142x2048): 74.441
Elapsed time for attention_prob_times_values (320x2048x2048x142): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x142): 93.600

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1002.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x143x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x143x2048): 34.513
Elapsed time for attention_prob_times_values (320x2048x2048x143): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x143): 58.278

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 527.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x144x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x144x2048): 109.956
Elapsed time for attention_prob_times_values (320x2048x2048x144): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x144): 132.416

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 1471.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x145x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x145x2048): 55.346
Elapsed time for attention_prob_times_values (320x2048x2048x145): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x145): 58.218

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 699.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x146x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x146x2048): 76.254
Elapsed time for attention_prob_times_values (320x2048x2048x146): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x146): 95.249

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1050.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x147x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x147x2048): 56.491
Elapsed time for attention_prob_times_values (320x2048x2048x147): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x147): 58.479

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 717.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x148x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x148x2048): 78.165
Elapsed time for attention_prob_times_values (320x2048x2048x148): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x148): 97.094

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1088.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x149x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x149x2048): 56.650
Elapsed time for attention_prob_times_values (320x2048x2048x149): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x149): 58.929

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 730.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x150x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x150x2048): 78.679
Elapsed time for attention_prob_times_values (320x2048x2048x150): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x150): 97.396

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1107.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x151x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x151x2048): 56.763
Elapsed time for attention_prob_times_values (320x2048x2048x151): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x151): 58.151

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 735.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x152x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x152x2048): 92.426
Elapsed time for attention_prob_times_values (320x2048x2048x152): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x152): 142.507

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1443.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x153x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x153x2048): 57.940
Elapsed time for attention_prob_times_values (320x2048x2048x153): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x153): 59.417

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 759.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x154x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x154x2048): 80.366
Elapsed time for attention_prob_times_values (320x2048x2048x154): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x154): 98.640

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1154.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x155x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x155x2048): 59.123
Elapsed time for attention_prob_times_values (320x2048x2048x155): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x155): 61.077

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 787.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x156x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x156x2048): 82.553
Elapsed time for attention_prob_times_values (320x2048x2048x156): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x156): 101.071

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1198.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x157x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x157x2048): 59.220
Elapsed time for attention_prob_times_values (320x2048x2048x157): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x157): 61.450

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 800.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x158x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x158x2048): 82.384
Elapsed time for attention_prob_times_values (320x2048x2048x158): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x158): 101.961

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1216.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x159x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x159x2048): 60.514
Elapsed time for attention_prob_times_values (320x2048x2048x159): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x159): 61.369

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 817.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x160x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x160x2048): 119.348
Elapsed time for attention_prob_times_values (320x2048x2048x160): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x160): 150.320

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 1796.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x161x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x161x2048): 56.650
Elapsed time for attention_prob_times_values (320x2048x2048x161): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x161): 61.581

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 801.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x162x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x162x2048): 78.302
Elapsed time for attention_prob_times_values (320x2048x2048x162): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x162): 104.316

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1221.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x163x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x163x2048): 57.695
Elapsed time for attention_prob_times_values (320x2048x2048x163): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x163): 63.373

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 829.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x164x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x164x2048): 79.238
Elapsed time for attention_prob_times_values (320x2048x2048x164): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x164): 105.724

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1251.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x165x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x165x2048): 57.334
Elapsed time for attention_prob_times_values (320x2048x2048x165): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x165): 64.518

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 843.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x166x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x166x2048): 79.616
Elapsed time for attention_prob_times_values (320x2048x2048x166): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x166): 95.557

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1213.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x167x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x167x2048): 58.218
Elapsed time for attention_prob_times_values (320x2048x2048x167): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x167): 63.108

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 850.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x168x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x168x2048): 108.409
Elapsed time for attention_prob_times_values (320x2048x2048x168): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x168): 150.574

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1780.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x169x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x169x2048): 58.504
Elapsed time for attention_prob_times_values (320x2048x2048x169): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x169): 63.774

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 866.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x170x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x170x2048): 83.659
Elapsed time for attention_prob_times_values (320x2048x2048x170): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x170): 108.981

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1351.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x171x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x171x2048): 59.948
Elapsed time for attention_prob_times_values (320x2048x2048x171): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x171): 66.610

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 906.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x172x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x172x2048): 73.072
Elapsed time for attention_prob_times_values (320x2048x2048x172): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x172): 110.932

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1272.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x173x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x173x2048): 59.170
Elapsed time for attention_prob_times_values (320x2048x2048x173): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x173): 67.412

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 914.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x174x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x174x2048): 82.689
Elapsed time for attention_prob_times_values (320x2048x2048x174): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x174): 86.780

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1235.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x175x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x175x2048): 59.523
Elapsed time for attention_prob_times_values (320x2048x2048x175): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x175): 65.726

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 916.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x176x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x176x2048): 117.518
Elapsed time for attention_prob_times_values (320x2048x2048x176): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x176): 159.548

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 1996.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x177x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x177x2048): 60.966
Elapsed time for attention_prob_times_values (320x2048x2048x177): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x177): 66.128

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 940.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x178x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x178x2048): 84.401
Elapsed time for attention_prob_times_values (320x2048x2048x178): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x178): 110.963

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1429.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x179x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x179x2048): 60.443
Elapsed time for attention_prob_times_values (320x2048x2048x179): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x179): 68.984

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 965.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x180x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x180x2048): 84.949
Elapsed time for attention_prob_times_values (320x2048x2048x180): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x180): 109.636

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1441.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x181x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x181x2048): 61.949
Elapsed time for attention_prob_times_values (320x2048x2048x181): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x181): 65.565

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 964.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x182x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x182x2048): 86.854
Elapsed time for attention_prob_times_values (320x2048x2048x182): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x182): 113.892

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1499.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x183x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x183x2048): 60.548
Elapsed time for attention_prob_times_values (320x2048x2048x183): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x183): 67.463

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 976.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x184x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x184x2048): 111.331
Elapsed time for attention_prob_times_values (320x2048x2048x184): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x184): 167.542

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 2056.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x185x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x185x2048): 61.043
Elapsed time for attention_prob_times_values (320x2048x2048x185): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x185): 68.094

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 994.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x186x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x186x2048): 84.658
Elapsed time for attention_prob_times_values (320x2048x2048x186): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x186): 113.647

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1507.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x187x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x187x2048): 62.199
Elapsed time for attention_prob_times_values (320x2048x2048x187): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x187): 72.052

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1042.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x188x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x188x2048): 89.120
Elapsed time for attention_prob_times_values (320x2048x2048x188): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x188): 118.729

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1597.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x189x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x189x2048): 62.352
Elapsed time for attention_prob_times_values (320x2048x2048x189): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x189): 72.389

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1056.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x190x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x190x2048): 89.966
Elapsed time for attention_prob_times_values (320x2048x2048x190): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x190): 119.196

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1624.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x191x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x191x2048): 63.014
Elapsed time for attention_prob_times_values (320x2048x2048x191): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x191): 71.288

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1065.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x192x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x192x2048): 137.427
Elapsed time for attention_prob_times_values (320x2048x2048x192): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x192): 174.756

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 2461.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x193x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x193x2048): 60.581
Elapsed time for attention_prob_times_values (320x2048x2048x193): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x193): 71.501

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1054.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x194x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x194x2048): 85.519
Elapsed time for attention_prob_times_values (320x2048x2048x194): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x194): 117.496

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1599.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x195x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x195x2048): 60.263
Elapsed time for attention_prob_times_values (320x2048x2048x195): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x195): 74.361

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1080.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x196x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x196x2048): 88.271
Elapsed time for attention_prob_times_values (320x2048x2048x196): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x196): 117.933

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1647.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x197x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x197x2048): 60.898
Elapsed time for attention_prob_times_values (320x2048x2048x197): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x197): 53.277

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 931.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x198x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x198x2048): 87.112
Elapsed time for attention_prob_times_values (320x2048x2048x198): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x198): 118.283

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1652.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x199x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x199x2048): 61.936
Elapsed time for attention_prob_times_values (320x2048x2048x199): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x199): 71.596

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1098.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x200x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x200x2048): 117.982
Elapsed time for attention_prob_times_values (320x2048x2048x200): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x200): 177.601

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 2357.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x201x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x201x2048): 61.468
Elapsed time for attention_prob_times_values (320x2048x2048x201): 0.0282
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x201): 19.137

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 487.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x202x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x202x2048): 82.739
Elapsed time for attention_prob_times_values (320x2048x2048x202): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x202): 112.146

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1597.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x203x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x203x2048): 61.874
Elapsed time for attention_prob_times_values (320x2048x2048x203): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x203): 73.874

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1135.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x204x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x204x2048): 88.272
Elapsed time for attention_prob_times_values (320x2048x2048x204): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x204): 119.378

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1719.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x205x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x205x2048): 62.911
Elapsed time for attention_prob_times_values (320x2048x2048x205): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x205): 73.866

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1156.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x206x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x206x2048): 89.168
Elapsed time for attention_prob_times_values (320x2048x2048x206): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x206): 121.330

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1757.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x207x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x207x2048): 64.158
Elapsed time for attention_prob_times_values (320x2048x2048x207): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x207): 74.704

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1185.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x208x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x208x2048): 128.364
Elapsed time for attention_prob_times_values (320x2048x2048x208): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x208): 185.524

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 2617.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x209x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x209x2048): 60.943
Elapsed time for attention_prob_times_values (320x2048x2048x209): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x209): 73.588

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1155.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x210x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x210x2048): 91.803
Elapsed time for attention_prob_times_values (320x2048x2048x210): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x210): 120.368

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1813.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x211x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x211x2048): 63.523
Elapsed time for attention_prob_times_values (320x2048x2048x211): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x211): 75.396

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1205.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x212x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x212x2048): 91.055
Elapsed time for attention_prob_times_values (320x2048x2048x212): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x212): 120.850

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1823.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x213x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x213x2048): 64.218
Elapsed time for attention_prob_times_values (320x2048x2048x213): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x213): 73.744

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1211.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x214x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x214x2048): 92.992
Elapsed time for attention_prob_times_values (320x2048x2048x214): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x214): 125.132

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1890.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x215x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x215x2048): 65.463
Elapsed time for attention_prob_times_values (320x2048x2048x215): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x215): 75.303

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1246.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x216x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x216x2048): 119.665
Elapsed time for attention_prob_times_values (320x2048x2048x216): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x216): 186.699

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2607.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x217x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x217x2048): 66.059
Elapsed time for attention_prob_times_values (320x2048x2048x217): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x217): 75.977

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1268.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x218x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x218x2048): 94.562
Elapsed time for attention_prob_times_values (320x2048x2048x218): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x218): 125.221

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1942.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x219x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x219x2048): 65.840
Elapsed time for attention_prob_times_values (320x2048x2048x219): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x219): 79.645

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1305.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x220x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x220x2048): 95.013
Elapsed time for attention_prob_times_values (320x2048x2048x220): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x220): 127.253

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1978.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x221x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x221x2048): 66.536
Elapsed time for attention_prob_times_values (320x2048x2048x221): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x221): 79.902

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1326.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x222x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x222x2048): 98.755
Elapsed time for attention_prob_times_values (320x2048x2048x222): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x222): 126.872

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 2037.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x223x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x223x2048): 67.140
Elapsed time for attention_prob_times_values (320x2048x2048x223): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x223): 78.794

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1335.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x224x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x224x2048): 144.379
Elapsed time for attention_prob_times_values (320x2048x2048x224): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x224): 193.897

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 3062.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x225x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x225x2048): 65.297
Elapsed time for attention_prob_times_values (320x2048x2048x225): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x225): 80.516

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1339.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x226x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x226x2048): 92.505
Elapsed time for attention_prob_times_values (320x2048x2048x226): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x226): 129.059

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 2010.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x227x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x227x2048): 65.117
Elapsed time for attention_prob_times_values (320x2048x2048x227): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x227): 80.449

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1348.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x228x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x228x2048): 94.530
Elapsed time for attention_prob_times_values (320x2048x2048x228): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x228): 131.092

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 2066.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x229x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x229x2048): 65.977
Elapsed time for attention_prob_times_values (320x2048x2048x229): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x229): 81.843

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1380.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x230x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x230x2048): 93.954
Elapsed time for attention_prob_times_values (320x2048x2048x230): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x230): 132.330

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2084.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x231x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x231x2048): 66.100
Elapsed time for attention_prob_times_values (320x2048x2048x231): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x231): 80.907

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1385.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x232x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x232x2048): 126.567
Elapsed time for attention_prob_times_values (320x2048x2048x232): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x232): 199.838

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2963.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x233x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x233x2048): 66.298
Elapsed time for attention_prob_times_values (320x2048x2048x233): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x233): 80.915

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1399.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x234x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x234x2048): 95.613
Elapsed time for attention_prob_times_values (320x2048x2048x234): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x234): 132.960

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 2144.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x235x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x235x2048): 65.332
Elapsed time for attention_prob_times_values (320x2048x2048x235): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x235): 83.873

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1421.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x236x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x236x2048): 98.190
Elapsed time for attention_prob_times_values (320x2048x2048x236): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x236): 135.161

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 2210.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x237x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x237x2048): 67.575
Elapsed time for attention_prob_times_values (320x2048x2048x237): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x237): 84.942

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1468.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x238x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x238x2048): 96.825
Elapsed time for attention_prob_times_values (320x2048x2048x238): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x238): 137.066

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 2223.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x239x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x239x2048): 67.668
Elapsed time for attention_prob_times_values (320x2048x2048x239): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x239): 84.151

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1475.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x240x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x240x2048): 142.196
Elapsed time for attention_prob_times_values (320x2048x2048x240): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x240): 145.496

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 2840.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x241x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x241x2048): 69.319
Elapsed time for attention_prob_times_values (320x2048x2048x241): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x241): 85.033

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1514.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x242x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x242x2048): 100.087
Elapsed time for attention_prob_times_values (320x2048x2048x242): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x242): 138.512

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2313.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x243x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x243x2048): 40.059
Elapsed time for attention_prob_times_values (320x2048x2048x243): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x243): 86.863

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1095.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x244x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x244x2048): 101.831
Elapsed time for attention_prob_times_values (320x2048x2048x244): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x244): 138.168

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2352.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x245x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x245x2048): 69.686
Elapsed time for attention_prob_times_values (320x2048x2048x245): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x245): 89.639

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1579.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x246x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x246x2048): 101.044
Elapsed time for attention_prob_times_values (320x2048x2048x246): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x246): 140.068

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2373.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x247x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x247x2048): 70.344
Elapsed time for attention_prob_times_values (320x2048x2048x247): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x247): 74.773

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1471.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x248x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x248x2048): 127.099
Elapsed time for attention_prob_times_values (320x2048x2048x248): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x248): 214.358

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 3251.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x249x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x249x2048): 70.019
Elapsed time for attention_prob_times_values (320x2048x2048x249): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x249): 90.066

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1611.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x250x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x250x2048): 102.605
Elapsed time for attention_prob_times_values (320x2048x2048x250): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x250): 142.541

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2449.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x251x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x251x2048): 72.107
Elapsed time for attention_prob_times_values (320x2048x2048x251): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x251): 91.079

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1658.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x252x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x252x2048): 101.367
Elapsed time for attention_prob_times_values (320x2048x2048x252): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x252): 146.357

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 2477.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x253x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x253x2048): 71.460
Elapsed time for attention_prob_times_values (320x2048x2048x253): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x253): 94.191

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1687.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x254x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x254x2048): 104.973
Elapsed time for attention_prob_times_values (320x2048x2048x254): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x254): 146.671

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 2550.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x255x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x255x2048): 72.237
Elapsed time for attention_prob_times_values (320x2048x2048x255): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x255): 91.502

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1689.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x256x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x256x2048): 148.756
Elapsed time for attention_prob_times_values (320x2048x2048x256): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x256): 218.026

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 3713.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x257x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x257x2048): 70.042
Elapsed time for attention_prob_times_values (320x2048x2048x257): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x257): 55.278

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1302.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x258x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x258x2048): 101.083
Elapsed time for attention_prob_times_values (320x2048x2048x258): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x258): 94.325

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 2064.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x259x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x259x2048): 69.923
Elapsed time for attention_prob_times_values (320x2048x2048x259): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x259): 56.171

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1322.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x260x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x260x2048): 100.327
Elapsed time for attention_prob_times_values (320x2048x2048x260): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x260): 95.915

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 2090.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x261x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x261x2048): 70.197
Elapsed time for attention_prob_times_values (320x2048x2048x261): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x261): 57.017

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1345.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x262x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x262x2048): 102.071
Elapsed time for attention_prob_times_values (320x2048x2048x262): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x262): 96.171

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 2126.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x263x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x263x2048): 68.927
Elapsed time for attention_prob_times_values (320x2048x2048x263): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x263): 55.978

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1331.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x264x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x264x2048): 131.086
Elapsed time for attention_prob_times_values (320x2048x2048x264): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x264): 132.217

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 2846.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x265x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x265x2048): 69.138
Elapsed time for attention_prob_times_values (320x2048x2048x265): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x265): 56.679

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1351.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x266x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x266x2048): 102.229
Elapsed time for attention_prob_times_values (320x2048x2048x266): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x266): 96.838

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 2166.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x267x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x267x2048): 70.056
Elapsed time for attention_prob_times_values (320x2048x2048x267): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x267): 57.457

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1380.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x268x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x268x2048): 104.311
Elapsed time for attention_prob_times_values (320x2048x2048x268): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x268): 97.110

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 2206.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x269x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x269x2048): 70.230
Elapsed time for attention_prob_times_values (320x2048x2048x269): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x269): 58.180

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1401.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x270x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x270x2048): 101.980
Elapsed time for attention_prob_times_values (320x2048x2048x270): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x270): 97.212

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 2199.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x271x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x271x2048): 70.062
Elapsed time for attention_prob_times_values (320x2048x2048x271): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x271): 56.764

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1390.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x272x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x272x2048): 144.968
Elapsed time for attention_prob_times_values (320x2048x2048x272): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x272): 136.826

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 3132.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x273x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x273x2048): 71.156
Elapsed time for attention_prob_times_values (320x2048x2048x273): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x273): 56.927

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1412.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x274x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x274x2048): 104.728
Elapsed time for attention_prob_times_values (320x2048x2048x274): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x274): 97.737

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 2265.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x275x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x275x2048): 71.348
Elapsed time for attention_prob_times_values (320x2048x2048x275): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x275): 58.217

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1441.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x276x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x276x2048): 104.065
Elapsed time for attention_prob_times_values (320x2048x2048x276): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x276): 96.800

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 2263.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x277x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x277x2048): 72.026
Elapsed time for attention_prob_times_values (320x2048x2048x277): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x277): 58.195

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1457.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x278x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x278x2048): 105.854
Elapsed time for attention_prob_times_values (320x2048x2048x278): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x278): 98.033

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 2312.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x279x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x279x2048): 72.635
Elapsed time for attention_prob_times_values (320x2048x2048x279): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x279): 57.684

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1465.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x280x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x280x2048): 131.186
Elapsed time for attention_prob_times_values (320x2048x2048x280): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x280): 135.447

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3048.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x281x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x281x2048): 72.091
Elapsed time for attention_prob_times_values (320x2048x2048x281): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x281): 57.987

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1475.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x282x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x282x2048): 102.404
Elapsed time for attention_prob_times_values (320x2048x2048x282): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x282): 98.250

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 2309.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x283x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x283x2048): 71.765
Elapsed time for attention_prob_times_values (320x2048x2048x283): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x283): 59.537

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1503.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x284x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x284x2048): 106.881
Elapsed time for attention_prob_times_values (320x2048x2048x284): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x284): 98.401

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 2375.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x285x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x285x2048): 73.135
Elapsed time for attention_prob_times_values (320x2048x2048x285): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x285): 59.316

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1524.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x286x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x286x2048): 108.132
Elapsed time for attention_prob_times_values (320x2048x2048x286): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x286): 101.426

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 2443.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x287x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x287x2048): 72.435
Elapsed time for attention_prob_times_values (320x2048x2048x287): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x287): 59.146

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1525.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x288x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x288x2048): 148.192
Elapsed time for attention_prob_times_values (320x2048x2048x288): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x288): 144.907

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3443.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x289x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x289x2048): 71.595
Elapsed time for attention_prob_times_values (320x2048x2048x289): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x289): 59.263

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1528.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x290x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x290x2048): 103.826
Elapsed time for attention_prob_times_values (320x2048x2048x290): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x290): 101.828

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 2432.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x291x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x291x2048): 71.066
Elapsed time for attention_prob_times_values (320x2048x2048x291): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x291): 60.640

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1553.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x292x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x292x2048): 103.655
Elapsed time for attention_prob_times_values (320x2048x2048x292): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x292): 100.379

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 2428.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x293x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x293x2048): 70.769
Elapsed time for attention_prob_times_values (320x2048x2048x293): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x293): 60.569

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1559.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x294x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x294x2048): 105.449
Elapsed time for attention_prob_times_values (320x2048x2048x294): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x294): 101.597

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 2480.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x295x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x295x2048): 69.799
Elapsed time for attention_prob_times_values (320x2048x2048x295): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x295): 59.758

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1548.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x296x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x296x2048): 135.778
Elapsed time for attention_prob_times_values (320x2048x2048x296): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x296): 142.922

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3359.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x297x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x297x2048): 70.724
Elapsed time for attention_prob_times_values (320x2048x2048x297): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x297): 59.977

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1571.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x298x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x298x2048): 107.638
Elapsed time for attention_prob_times_values (320x2048x2048x298): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x298): 102.882

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 2554.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x299x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x299x2048): 70.114
Elapsed time for attention_prob_times_values (320x2048x2048x299): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x299): 61.680

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1598.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x300x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x300x2048): 107.621
Elapsed time for attention_prob_times_values (320x2048x2048x300): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x300): 102.379

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 2564.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x301x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x301x2048): 70.832
Elapsed time for attention_prob_times_values (320x2048x2048x301): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x301): 61.578

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1615.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x302x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x302x2048): 104.964
Elapsed time for attention_prob_times_values (320x2048x2048x302): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x302): 103.697

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2565.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x303x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x303x2048): 70.612
Elapsed time for attention_prob_times_values (320x2048x2048x303): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x303): 61.104

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1616.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x304x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x304x2048): 151.475
Elapsed time for attention_prob_times_values (320x2048x2048x304): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x304): 147.614

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3700.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x305x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x305x2048): 70.221
Elapsed time for attention_prob_times_values (320x2048x2048x305): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x305): 61.492

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1627.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x306x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x306x2048): 106.204
Elapsed time for attention_prob_times_values (320x2048x2048x306): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x306): 105.534

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2636.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x307x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x307x2048): 69.789
Elapsed time for attention_prob_times_values (320x2048x2048x307): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x307): 63.138

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1656.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x308x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x308x2048): 106.674
Elapsed time for attention_prob_times_values (320x2048x2048x308): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x308): 104.470

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 2645.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x309x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x309x2048): 70.650
Elapsed time for attention_prob_times_values (320x2048x2048x309): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x309): 62.812

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1671.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x310x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x310x2048): 108.179
Elapsed time for attention_prob_times_values (320x2048x2048x310): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x310): 106.551

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2707.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x311x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x311x2048): 70.510
Elapsed time for attention_prob_times_values (320x2048x2048x311): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x311): 61.939

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1668.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x312x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x312x2048): 136.499
Elapsed time for attention_prob_times_values (320x2048x2048x312): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x312): 139.171

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 3497.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x313x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x313x2048): 70.040
Elapsed time for attention_prob_times_values (320x2048x2048x313): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x313): 62.138

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1676.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x314x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x314x2048): 110.839
Elapsed time for attention_prob_times_values (320x2048x2048x314): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x314): 106.306

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2770.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x315x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x315x2048): 70.590
Elapsed time for attention_prob_times_values (320x2048x2048x315): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x315): 63.559

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1713.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x316x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x316x2048): 110.454
Elapsed time for attention_prob_times_values (320x2048x2048x316): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x316): 107.478

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 2798.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x317x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x317x2048): 70.689
Elapsed time for attention_prob_times_values (320x2048x2048x317): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x317): 60.606

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1681.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x318x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x318x2048): 111.500
Elapsed time for attention_prob_times_values (320x2048x2048x318): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x318): 108.912

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2847.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x319x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x319x2048): 72.308
Elapsed time for attention_prob_times_values (320x2048x2048x319): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x319): 62.118

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1732.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x320x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x320x2048): 157.269
Elapsed time for attention_prob_times_values (320x2048x2048x320): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x320): 153.458

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 4038.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x321x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x321x2048): 69.671
Elapsed time for attention_prob_times_values (320x2048x2048x321): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x321): 62.677

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1720.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x322x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x322x2048): 106.931
Elapsed time for attention_prob_times_values (320x2048x2048x322): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x322): 109.811

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 2834.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x323x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x323x2048): 69.321
Elapsed time for attention_prob_times_values (320x2048x2048x323): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x323): 64.548

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1753.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x324x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x324x2048): 107.914
Elapsed time for attention_prob_times_values (320x2048x2048x324): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x324): 109.794

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 2864.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x325x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x325x2048): 70.032
Elapsed time for attention_prob_times_values (320x2048x2048x325): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x325): 60.991

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1720.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x326x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x326x2048): 107.301
Elapsed time for attention_prob_times_values (320x2048x2048x326): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x326): 110.154

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 2877.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x327x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x327x2048): 69.875
Elapsed time for attention_prob_times_values (320x2048x2048x327): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x327): 63.690

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1769.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x328x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x328x2048): 133.185
Elapsed time for attention_prob_times_values (320x2048x2048x328): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x328): 154.672

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3810.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x329x2048): 0.0270
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x329x2048): 32.728
Elapsed time for attention_prob_times_values (320x2048x2048x329): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x329): 64.409

Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 1158.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0407
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x330x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x330x2048): 108.744
Elapsed time for attention_prob_times_values (320x2048x2048x330): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x330): 110.016

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 2929.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x331x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x331x2048): 72.028
Elapsed time for attention_prob_times_values (320x2048x2048x331): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x331): 66.215

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1853.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x332x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x332x2048): 109.596
Elapsed time for attention_prob_times_values (320x2048x2048x332): 0.0233
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x332): 38.192

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1525.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x333x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x333x2048): 72.396
Elapsed time for attention_prob_times_values (320x2048x2048x333): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x333): 66.247

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1869.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x334x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x334x2048): 110.024
Elapsed time for attention_prob_times_values (320x2048x2048x334): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x334): 96.923

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2792.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x335x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x335x2048): 73.598
Elapsed time for attention_prob_times_values (320x2048x2048x335): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x335): 65.727

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1886.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x336x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x336x2048): 151.984
Elapsed time for attention_prob_times_values (320x2048x2048x336): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x336): 159.851

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 4246.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x337x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x337x2048): 73.634
Elapsed time for attention_prob_times_values (320x2048x2048x337): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x337): 60.603

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1816.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x338x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x338x2048): 100.889
Elapsed time for attention_prob_times_values (320x2048x2048x338): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x338): 112.460

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2914.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x339x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x339x2048): 74.407
Elapsed time for attention_prob_times_values (320x2048x2048x339): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x339): 67.345

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1943.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x340x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x340x2048): 113.056
Elapsed time for attention_prob_times_values (320x2048x2048x340): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x340): 113.069

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 3116.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x341x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x341x2048): 75.107
Elapsed time for attention_prob_times_values (320x2048x2048x341): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x341): 67.924

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1971.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x342x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x342x2048): 113.006
Elapsed time for attention_prob_times_values (320x2048x2048x342): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x342): 114.311

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 3150.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x343x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x343x2048): 56.890
Elapsed time for attention_prob_times_values (320x2048x2048x343): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x343): 67.877

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1720.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x344x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x344x2048): 168.579
Elapsed time for attention_prob_times_values (320x2048x2048x344): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x344): 160.145

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 4578.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x345x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x345x2048): 75.179
Elapsed time for attention_prob_times_values (320x2048x2048x345): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x345): 65.235

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1952.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x346x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x346x2048): 110.189
Elapsed time for attention_prob_times_values (320x2048x2048x346): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x346): 114.214

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 3144.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x347x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x347x2048): 74.902
Elapsed time for attention_prob_times_values (320x2048x2048x347): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x347): 68.923

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2017.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x348x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x348x2048): 110.920
Elapsed time for attention_prob_times_values (320x2048x2048x348): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x348): 115.613

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 3191.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x349x2048): 0.0336
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x349x2048): 27.866
Elapsed time for attention_prob_times_values (320x2048x2048x349): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x349): 68.810

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 1121.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x350x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x350x2048): 108.274
Elapsed time for attention_prob_times_values (320x2048x2048x350): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x350): 103.256

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2996.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x351x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x351x2048): 76.933
Elapsed time for attention_prob_times_values (320x2048x2048x351): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x351): 69.338

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2073.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x352x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x352x2048): 186.977
Elapsed time for attention_prob_times_values (320x2048x2048x352): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x352): 169.099

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 5061.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x353x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x353x2048): 72.245
Elapsed time for attention_prob_times_values (320x2048x2048x353): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x353): 64.872

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1953.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x354x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x354x2048): 111.171
Elapsed time for attention_prob_times_values (320x2048x2048x354): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x354): 115.866

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 3251.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x355x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x355x2048): 72.474
Elapsed time for attention_prob_times_values (320x2048x2048x355): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x355): 70.436

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2052.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x356x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x356x2048): 110.908
Elapsed time for attention_prob_times_values (320x2048x2048x356): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x356): 117.233

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 3284.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x357x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x357x2048): 73.122
Elapsed time for attention_prob_times_values (320x2048x2048x357): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x357): 70.408

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2072.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x358x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x358x2048): 110.222
Elapsed time for attention_prob_times_values (320x2048x2048x358): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x358): 45.176

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1856.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x359x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x359x2048): 69.707
Elapsed time for attention_prob_times_values (320x2048x2048x359): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x359): 69.281

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2018.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x360x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x360x2048): 174.310
Elapsed time for attention_prob_times_values (320x2048x2048x360): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x360): 171.508

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 5035.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x361x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x361x2048): 71.772
Elapsed time for attention_prob_times_values (320x2048x2048x361): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x361): 70.020

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2070.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x362x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x362x2048): 112.277
Elapsed time for attention_prob_times_values (320x2048x2048x362): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x362): 118.896

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 3381.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x363x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x363x2048): 71.111
Elapsed time for attention_prob_times_values (320x2048x2048x363): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x363): 71.298

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2090.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x364x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x364x2048): 112.211
Elapsed time for attention_prob_times_values (320x2048x2048x364): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x364): 120.230

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 3417.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x365x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x365x2048): 73.333
Elapsed time for attention_prob_times_values (320x2048x2048x365): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x365): 72.098

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2146.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x366x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x366x2048): 112.957
Elapsed time for attention_prob_times_values (320x2048x2048x366): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x366): 120.590

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 3452.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x367x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x367x2048): 73.101
Elapsed time for attention_prob_times_values (320x2048x2048x367): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x367): 71.459

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2144.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x368x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x368x2048): 177.893
Elapsed time for attention_prob_times_values (320x2048x2048x368): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x368): 171.800

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 5200.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x369x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x369x2048): 74.060
Elapsed time for attention_prob_times_values (320x2048x2048x369): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x369): 72.756

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2189.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x370x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x370x2048): 112.411
Elapsed time for attention_prob_times_values (320x2048x2048x370): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x370): 123.461

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 3519.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x371x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x371x2048): 73.296
Elapsed time for attention_prob_times_values (320x2048x2048x371): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x371): 73.504

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2200.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x372x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x372x2048): 114.646
Elapsed time for attention_prob_times_values (320x2048x2048x372): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x372): 122.934

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 3566.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x373x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x373x2048): 74.412
Elapsed time for attention_prob_times_values (320x2048x2048x373): 0.0249
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x373): 40.163

Attention duration (in seconds): 0.0384
Attention throughput (in TFLOP/s): 1572.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0384
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x374x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x374x2048): 114.283
Elapsed time for attention_prob_times_values (320x2048x2048x374): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x374): 124.431

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 3600.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x375x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x375x2048): 73.949
Elapsed time for attention_prob_times_values (320x2048x2048x375): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x375): 73.336

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2231.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x376x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x376x2048): 173.991
Elapsed time for attention_prob_times_values (320x2048x2048x376): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x376): 170.433

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 5230.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x377x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x377x2048): 74.119
Elapsed time for attention_prob_times_values (320x2048x2048x377): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x377): 74.267

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2259.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x378x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x378x2048): 115.411
Elapsed time for attention_prob_times_values (320x2048x2048x378): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x378): 124.434

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 3656.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x379x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x379x2048): 75.668
Elapsed time for attention_prob_times_values (320x2048x2048x379): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x379): 76.640

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2330.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x380x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x380x2048): 114.088
Elapsed time for attention_prob_times_values (320x2048x2048x380): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x380): 126.524

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 3682.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x381x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x381x2048): 75.076
Elapsed time for attention_prob_times_values (320x2048x2048x381): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x381): 75.388

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2314.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x382x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x382x2048): 115.319
Elapsed time for attention_prob_times_values (320x2048x2048x382): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x382): 125.209

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 3703.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x383x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x383x2048): 77.372
Elapsed time for attention_prob_times_values (320x2048x2048x383): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x383): 74.527

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2347.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x384x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x384x2048): 178.958
Elapsed time for attention_prob_times_values (320x2048x2048x384): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x384): 179.822

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 5561.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x385x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x385x2048): 74.354
Elapsed time for attention_prob_times_values (320x2048x2048x385): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x385): 75.222

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2324.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x386x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x386x2048): 108.576
Elapsed time for attention_prob_times_values (320x2048x2048x386): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x386): 127.504

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3654.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x387x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x387x2048): 74.317
Elapsed time for attention_prob_times_values (320x2048x2048x387): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x387): 76.023

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2347.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x388x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x388x2048): 108.872
Elapsed time for attention_prob_times_values (320x2048x2048x388): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x388): 127.932

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3683.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x389x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x389x2048): 74.680
Elapsed time for attention_prob_times_values (320x2048x2048x389): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x389): 75.299

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2353.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x390x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x390x2048): 107.615
Elapsed time for attention_prob_times_values (320x2048x2048x390): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x390): 126.106

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 3654.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x391x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x391x2048): 74.535
Elapsed time for attention_prob_times_values (320x2048x2048x391): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x391): 74.003

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2342.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x392x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x392x2048): 171.127
Elapsed time for attention_prob_times_values (320x2048x2048x392): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x392): 178.714

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 5529.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x393x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x393x2048): 74.768
Elapsed time for attention_prob_times_values (320x2048x2048x393): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x393): 74.129

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2360.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x394x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x394x2048): 108.703
Elapsed time for attention_prob_times_values (320x2048x2048x394): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x394): 123.206

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 3670.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x395x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x395x2048): 75.707
Elapsed time for attention_prob_times_values (320x2048x2048x395): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x395): 75.935

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2415.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x396x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x396x2048): 111.103
Elapsed time for attention_prob_times_values (320x2048x2048x396): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x396): 126.248

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 3774.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x397x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x397x2048): 76.537
Elapsed time for attention_prob_times_values (320x2048x2048x397): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x397): 76.814

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2454.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x398x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x398x2048): 109.691
Elapsed time for attention_prob_times_values (320x2048x2048x398): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x398): 125.598

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 3758.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x399x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x399x2048): 76.672
Elapsed time for attention_prob_times_values (320x2048x2048x399): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x399): 75.932

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2454.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x400x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x400x2048): 187.221
Elapsed time for attention_prob_times_values (320x2048x2048x400): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x400): 188.941

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 6065.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x401x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x401x2048): 78.454
Elapsed time for attention_prob_times_values (320x2048x2048x401): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x401): 76.971

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2512.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x402x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x402x2048): 110.696
Elapsed time for attention_prob_times_values (320x2048x2048x402): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x402): 123.378

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 3781.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x403x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x403x2048): 77.022
Elapsed time for attention_prob_times_values (320x2048x2048x403): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x403): 77.412

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2508.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x404x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x404x2048): 111.040
Elapsed time for attention_prob_times_values (320x2048x2048x404): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x404): 122.078

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 3786.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x405x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x405x2048): 79.227
Elapsed time for attention_prob_times_values (320x2048x2048x405): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x405): 77.422

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2556.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x406x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x406x2048): 110.961
Elapsed time for attention_prob_times_values (320x2048x2048x406): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x406): 128.194

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 3892.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x407x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x407x2048): 78.769
Elapsed time for attention_prob_times_values (320x2048x2048x407): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x407): 76.408

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 2544.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x408x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x408x2048): 179.077
Elapsed time for attention_prob_times_values (320x2048x2048x408): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x408): 177.918

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 5868.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x409x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x409x2048): 78.390
Elapsed time for attention_prob_times_values (320x2048x2048x409): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x409): 75.683

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2537.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x410x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x410x2048): 112.730
Elapsed time for attention_prob_times_values (320x2048x2048x410): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x410): 127.865

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 3957.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
