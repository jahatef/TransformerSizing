num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1

MLP duration (in seconds): 0.1395
MLP throughput (in TFLOP/s): 252.273
LN1: 0.0008985996246337891
QKV Transform: 0.051587581634521484
Flash: 0.014302492141723633
Attention linproj: 0.016538619995117188
Post-attention Dropout: 0.04186058044433594
Post-attention residual: 0.0006575584411621094
LN2: 0.0006897449493408203
MLP_h_4h: 0.06825518608093262
MLP_4h_h: 0.06908631324768066
Post-MLP residual: 0.003466367721557617
Attention layer time: 0.26836609840393066
LN1: 0.0007171630859375
QKV Transform: 0.05159473419189453
Flash: 0.016736268997192383
Attention linproj: 0.01636528968811035
Post-attention Dropout: 0.0018148422241210938
Post-attention residual: 0.0006079673767089844
LN2: 0.0006756782531738281
MLP_h_4h: 0.07060813903808594
MLP_4h_h: 0.06989479064941406
Post-MLP residual: 0.0018453598022460938
Attention layer time: 0.2317202091217041
LN1: 0.0007145404815673828
QKV Transform: 0.05150461196899414
Flash: 0.01753544807434082
Attention linproj: 0.016402721405029297
Post-attention Dropout: 0.00186920166015625
Post-attention residual: 0.0006234645843505859
LN2: 0.0007066726684570312
MLP_h_4h: 0.07079672813415527
MLP_4h_h: 0.06874299049377441
Post-MLP residual: 0.0018510818481445312
Attention layer time: 0.23161554336547852
LN1: 0.000713348388671875
QKV Transform: 0.051482200622558594
Flash: 0.017618179321289062
Attention linproj: 0.01636338233947754
Post-attention Dropout: 0.0018486976623535156
Post-attention residual: 0.0006198883056640625
LN2: 0.0006954669952392578
MLP_h_4h: 0.07185626029968262
MLP_4h_h: 0.06906342506408691
Post-MLP residual: 0.0018470287322998047
Attention layer time: 0.23298168182373047
LN1: 0.0007205009460449219
QKV Transform: 0.05250716209411621
Flash: 0.017354488372802734
Attention linproj: 0.016510486602783203
Post-attention Dropout: 0.0018448829650878906
Post-attention residual: 0.0006227493286132812
LN2: 0.0006921291351318359
MLP_h_4h: 0.07216525077819824
MLP_4h_h: 0.06975722312927246
Post-MLP residual: 0.0018398761749267578
Attention layer time: 0.23488759994506836
LN1: 0.0007131099700927734
QKV Transform: 0.05178022384643555
Flash: 0.017436981201171875
Attention linproj: 0.0167849063873291
Post-attention Dropout: 0.0018532276153564453
Post-attention residual: 0.0006213188171386719
LN2: 0.0006947517395019531
MLP_h_4h: 0.07131767272949219
MLP_4h_h: 0.06919717788696289
Post-MLP residual: 0.0018329620361328125
Attention layer time: 0.2331221103668213
LN1: 0.000713348388671875
QKV Transform: 0.05246734619140625
Flash: 0.017369985580444336
Attention linproj: 0.016731739044189453
Post-attention Dropout: 0.0018393993377685547
Post-attention residual: 0.0006234645843505859
LN2: 0.0006918907165527344
MLP_h_4h: 0.07131528854370117
MLP_4h_h: 0.06877851486206055
Post-MLP residual: 0.0018453598022460938
Attention layer time: 0.23325657844543457
LN1: 0.0007305145263671875
QKV Transform: 0.05236530303955078
Flash: 0.017464399337768555
Attention linproj: 0.016624927520751953
Post-attention Dropout: 0.0018630027770996094
Post-attention residual: 0.0006206035614013672
LN2: 0.0006921291351318359
MLP_h_4h: 0.07148981094360352
MLP_4h_h: 0.06926679611206055
Post-MLP residual: 0.0018625259399414062
Attention layer time: 0.2338416576385498
LN1: 0.0007107257843017578
QKV Transform: 0.05102133750915527
Flash: 0.017930269241333008
Attention linproj: 0.016574859619140625
Post-attention Dropout: 0.0018393993377685547
Post-attention residual: 0.0006241798400878906
LN2: 0.0006923675537109375
MLP_h_4h: 0.07211470603942871
MLP_4h_h: 0.06939530372619629
Post-MLP residual: 0.0018427371978759766
Attention layer time: 0.23360395431518555
LN1: 0.0007076263427734375
QKV Transform: 0.05202794075012207
Flash: 0.016969680786132812
Attention linproj: 0.016448259353637695
Post-attention Dropout: 0.0018608570098876953
Post-attention residual: 0.0006213188171386719
LN2: 0.00069427490234375
MLP_h_4h: 0.07175946235656738
MLP_4h_h: 0.06984734535217285
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.2336595058441162
LN1: 0.0007126331329345703
QKV Transform: 0.0509335994720459
Flash: 0.017038345336914062
Attention linproj: 0.01636481285095215
Post-attention Dropout: 0.0018553733825683594
Post-attention residual: 0.0006239414215087891
LN2: 0.0006940364837646484
MLP_h_4h: 0.07160377502441406
MLP_4h_h: 0.0687718391418457
Post-MLP residual: 0.001837015151977539
Attention layer time: 0.23129701614379883
LN1: 0.0007128715515136719
QKV Transform: 0.05143332481384277
Flash: 0.01766490936279297
Attention linproj: 0.016376256942749023
Post-attention Dropout: 0.0018649101257324219
Post-attention residual: 0.0006220340728759766
LN2: 0.0006949901580810547
MLP_h_4h: 0.07218027114868164
MLP_4h_h: 0.06896281242370605
Post-MLP residual: 0.001842498779296875
Attention layer time: 0.23320460319519043
LN1: 0.0007154941558837891
QKV Transform: 0.05246686935424805
Flash: 0.018407344818115234
Attention linproj: 0.016381263732910156
Post-attention Dropout: 0.0018534660339355469
Post-attention residual: 0.000621795654296875
LN2: 0.0006911754608154297
MLP_h_4h: 0.07226085662841797
MLP_4h_h: 0.06926369667053223
Post-MLP residual: 0.0018436908721923828
Attention layer time: 0.2353661060333252
LN1: 0.0007123947143554688
QKV Transform: 0.05194497108459473
Flash: 0.01722884178161621
Attention linproj: 0.016517162322998047
Post-attention Dropout: 0.0018651485443115234
Post-attention residual: 0.0006191730499267578
LN2: 0.0006933212280273438
MLP_h_4h: 0.07200789451599121
MLP_4h_h: 0.06900143623352051
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23331379890441895
LN1: 0.0007145404815673828
QKV Transform: 0.05209708213806152
Flash: 0.017719268798828125
Attention linproj: 0.016524314880371094
Post-attention Dropout: 0.0018506050109863281
Post-attention residual: 0.0006248950958251953
LN2: 0.00069427490234375
MLP_h_4h: 0.07206869125366211
MLP_4h_h: 0.06912922859191895
Post-MLP residual: 0.0018298625946044922
Attention layer time: 0.23412704467773438
LN1: 0.0007271766662597656
QKV Transform: 0.05269742012023926
Flash: 0.01709294319152832
Attention linproj: 0.01681971549987793
Post-attention Dropout: 0.0018458366394042969
Post-attention residual: 0.0006248950958251953
LN2: 0.0006916522979736328
MLP_h_4h: 0.07176709175109863
MLP_4h_h: 0.0691826343536377
Post-MLP residual: 0.001850128173828125
Attention layer time: 0.23418092727661133
LN1: 0.0007154941558837891
QKV Transform: 0.051577091217041016
Flash: 0.01871967315673828
Attention linproj: 0.016614198684692383
Post-attention Dropout: 0.001870870590209961
Post-attention residual: 0.0006220340728759766
LN2: 0.0006926059722900391
MLP_h_4h: 0.07199954986572266
MLP_4h_h: 0.0691070556640625
Post-MLP residual: 0.0018367767333984375
Attention layer time: 0.23462891578674316
LN1: 0.0007145404815673828
QKV Transform: 0.052269697189331055
Flash: 0.017247438430786133
Attention linproj: 0.0168609619140625
Post-attention Dropout: 0.0018482208251953125
Post-attention residual: 0.0006198883056640625
LN2: 0.0006966590881347656
MLP_h_4h: 0.0715632438659668
MLP_4h_h: 0.06942629814147949
Post-MLP residual: 0.0018491744995117188
Attention layer time: 0.23399782180786133
LN1: 0.0007143020629882812
QKV Transform: 0.05167794227600098
Flash: 0.017567873001098633
Attention linproj: 0.01668548583984375
Post-attention Dropout: 0.0018427371978759766
Post-attention residual: 0.0006229877471923828
LN2: 0.0006902217864990234
MLP_h_4h: 0.07169914245605469
MLP_4h_h: 0.06928801536560059
Post-MLP residual: 0.0018439292907714844
Attention layer time: 0.2335205078125
LN1: 0.0007116794586181641
QKV Transform: 0.0514073371887207
Flash: 0.017690181732177734
Attention linproj: 0.016723155975341797
Post-attention Dropout: 0.0018644332885742188
Post-attention residual: 0.0006382465362548828
LN2: 0.0006940364837646484
MLP_h_4h: 0.07148146629333496
MLP_4h_h: 0.06926441192626953
Post-MLP residual: 0.001842498779296875
Attention layer time: 0.2331855297088623
LN1: 0.0007078647613525391
QKV Transform: 0.05111837387084961
Flash: 0.016822099685668945
Attention linproj: 0.016449451446533203
Post-attention Dropout: 0.0018491744995117188
Post-attention residual: 0.000621795654296875
LN2: 0.0006928443908691406
MLP_h_4h: 0.07224559783935547
MLP_4h_h: 0.07017827033996582
Post-MLP residual: 0.001848459243774414
Attention layer time: 0.23342275619506836
LN1: 0.0007140636444091797
QKV Transform: 0.05094742774963379
Flash: 0.016963958740234375
Attention linproj: 0.01646709442138672
Post-attention Dropout: 0.0018453598022460938
Post-attention residual: 0.0006208419799804688
LN2: 0.0006906986236572266
MLP_h_4h: 0.07183575630187988
MLP_4h_h: 0.06906366348266602
Post-MLP residual: 0.0018448829650878906
Attention layer time: 0.23186421394348145
LN1: 0.0007112026214599609
QKV Transform: 0.05162334442138672
Flash: 0.017486095428466797
Attention linproj: 0.016470670700073242
Post-attention Dropout: 0.0018460750579833984
Post-attention residual: 0.0006186962127685547
LN2: 0.0006935596466064453
MLP_h_4h: 0.07207083702087402
MLP_4h_h: 0.06895589828491211
Post-MLP residual: 0.0018475055694580078
Attention layer time: 0.2331836223602295
LN1: 0.0007145404815673828
QKV Transform: 0.05225062370300293
Flash: 0.01764965057373047
Attention linproj: 0.016364574432373047
Post-attention Dropout: 0.0018415451049804688
Post-attention residual: 0.0010612010955810547
LN2: 0.0008647441864013672
MLP_h_4h: 0.07120084762573242
MLP_4h_h: 0.0688638687133789
Post-MLP residual: 0.0019059181213378906
Attention layer time: 0.23369526863098145
LN1: 0.0007233619689941406
QKV Transform: 0.05141139030456543
Flash: 0.018485307693481445
Attention linproj: 0.016344547271728516
Post-attention Dropout: 0.0018544197082519531
Post-attention residual: 0.0006258487701416016
LN2: 0.0006921291351318359
MLP_h_4h: 0.07219934463500977
MLP_4h_h: 0.06903886795043945
Post-MLP residual: 0.001844644546508789
Attention layer time: 0.23416948318481445
LN1: 0.0007154941558837891
QKV Transform: 0.05147743225097656
Flash: 0.01794123649597168
Attention linproj: 0.016411304473876953
Post-attention Dropout: 0.001855611801147461
Post-attention residual: 0.0006260871887207031
LN2: 0.00069427490234375
MLP_h_4h: 0.07190680503845215
MLP_4h_h: 0.06930875778198242
Post-MLP residual: 0.0018532276153564453
Attention layer time: 0.23365497589111328
LN1: 0.0007138252258300781
QKV Transform: 0.05232572555541992
Flash: 0.01753997802734375
Attention linproj: 0.016660213470458984
Post-attention Dropout: 0.0018434524536132812
Post-attention residual: 0.0006232261657714844
LN2: 0.0006911754608154297
MLP_h_4h: 0.07201242446899414
MLP_4h_h: 0.06925201416015625
Post-MLP residual: 0.0018434524536132812
Attention layer time: 0.23437952995300293
LN1: 0.0007150173187255859
QKV Transform: 0.052374839782714844
Flash: 0.017279624938964844
Attention linproj: 0.016759157180786133
Post-attention Dropout: 0.001840353012084961
Post-attention residual: 0.000621795654296875
LN2: 0.000698089599609375
MLP_h_4h: 0.07216167449951172
MLP_4h_h: 0.06927990913391113
Post-MLP residual: 0.0018434524536132812
Attention layer time: 0.23446297645568848
LN1: 0.0007112026214599609
QKV Transform: 0.051743507385253906
Flash: 0.01745462417602539
Attention linproj: 0.016899824142456055
Post-attention Dropout: 0.001863241195678711
Post-attention residual: 0.0006239414215087891
LN2: 0.0006926059722900391
MLP_h_4h: 0.07173943519592285
MLP_4h_h: 0.06939935684204102
Post-MLP residual: 0.0018279552459716797
Attention layer time: 0.23384380340576172
LN1: 0.0007126331329345703
QKV Transform: 0.05113935470581055
Flash: 0.017843961715698242
Attention linproj: 0.016368389129638672
Post-attention Dropout: 0.0018498897552490234
Post-attention residual: 0.0006246566772460938
LN2: 0.0006909370422363281
MLP_h_4h: 0.07239747047424316
MLP_4h_h: 0.06972122192382812
Post-MLP residual: 0.001848459243774414
Attention layer time: 0.23404526710510254
LN1: 0.0007073879241943359
QKV Transform: 0.05110311508178711
Flash: 0.017790555953979492
Attention linproj: 0.016475439071655273
Post-attention Dropout: 0.001842498779296875
Post-attention residual: 0.0006237030029296875
LN2: 0.0006959438323974609
MLP_h_4h: 0.07452535629272461
MLP_4h_h: 0.06928634643554688
Post-MLP residual: 0.0018436908721923828
Attention layer time: 0.23574614524841309
LN1: 0.0007312297821044922
QKV Transform: 0.05218672752380371
Flash: 0.017813444137573242
Attention linproj: 0.01658797264099121
Post-attention Dropout: 0.0018434524536132812
Post-attention residual: 0.0006213188171386719
LN2: 0.0006911754608154297
MLP_h_4h: 0.07192564010620117
MLP_4h_h: 0.06920146942138672
Post-MLP residual: 0.001850128173828125
Attention layer time: 0.2343130111694336
LN1: 0.0007128715515136719
QKV Transform: 0.052034616470336914
Flash: 0.017766237258911133
Attention linproj: 0.016387462615966797
Post-attention Dropout: 0.0018374919891357422
Post-attention residual: 0.0006346702575683594
LN2: 0.0006940364837646484
MLP_h_4h: 0.07236623764038086
MLP_4h_h: 0.0688943862915039
Post-MLP residual: 0.0018398761749267578
Attention layer time: 0.23401856422424316
LN1: 0.0007140636444091797
QKV Transform: 0.05138587951660156
Flash: 0.017641782760620117
Attention linproj: 0.016347885131835938
Post-attention Dropout: 0.001844644546508789
Post-attention residual: 0.0006237030029296875
LN2: 0.0006957054138183594
MLP_h_4h: 0.07194375991821289
MLP_4h_h: 0.06895232200622559
Post-MLP residual: 0.0018563270568847656
Attention layer time: 0.23285245895385742
LN1: 0.0007123947143554688
QKV Transform: 0.051837921142578125
Flash: 0.017250776290893555
Attention linproj: 0.016480445861816406
Post-attention Dropout: 0.0018510818481445312
Post-attention residual: 0.0006222724914550781
LN2: 0.0006947517395019531
MLP_h_4h: 0.07216405868530273
MLP_4h_h: 0.06863951683044434
Post-MLP residual: 0.0018315315246582031
Attention layer time: 0.23295092582702637
LN1: 0.0007159709930419922
QKV Transform: 0.051033735275268555
Flash: 0.019165515899658203
Attention linproj: 0.0163724422454834
Post-attention Dropout: 0.0018858909606933594
Post-attention residual: 0.0006251335144042969
LN2: 0.0006997585296630859
MLP_h_4h: 0.07215309143066406
MLP_4h_h: 0.06920409202575684
Post-MLP residual: 0.001847982406616211
Attention layer time: 0.23456954956054688
LN1: 0.0007164478302001953
QKV Transform: 0.05213284492492676
Flash: 0.017188310623168945
Attention linproj: 0.016462326049804688
Post-attention Dropout: 0.0018510818481445312
Post-attention residual: 0.0006203651428222656
LN2: 0.0006947517395019531
MLP_h_4h: 0.07225871086120605
MLP_4h_h: 0.06918883323669434
Post-MLP residual: 0.0018384456634521484
Attention layer time: 0.23384523391723633
LN1: 0.0007174015045166016
QKV Transform: 0.05268573760986328
Flash: 0.017999649047851562
Attention linproj: 0.01636338233947754
Post-attention Dropout: 0.0018439292907714844
Post-attention residual: 0.000621795654296875
LN2: 0.0007159709930419922
MLP_h_4h: 0.0722036361694336
MLP_4h_h: 0.06916522979736328
Post-MLP residual: 0.0018398761749267578
Attention layer time: 0.23502492904663086
LN1: 0.0007145404815673828
QKV Transform: 0.05199122428894043
Flash: 0.017841100692749023
Attention linproj: 0.01643657684326172
Post-attention Dropout: 0.0018551349639892578
Post-attention residual: 0.0006227493286132812
LN2: 0.0006928443908691406
MLP_h_4h: 0.07217669486999512
MLP_4h_h: 0.06928038597106934
Post-MLP residual: 0.0018496513366699219
Attention layer time: 0.23433208465576172
LN1: 0.0007150173187255859
QKV Transform: 0.05205059051513672
Flash: 0.01773667335510254
Attention linproj: 0.016632795333862305
Post-attention Dropout: 0.001836538314819336
Post-attention residual: 0.0006229877471923828
LN2: 0.0006914138793945312
MLP_h_4h: 0.07186746597290039
MLP_4h_h: 0.06903338432312012
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23389554023742676
LN1: 0.0007128715515136719
QKV Transform: 0.051544189453125
Flash: 0.017470598220825195
Attention linproj: 0.01675248146057129
Post-attention Dropout: 0.001840829849243164
Post-attention residual: 0.0006208419799804688
LN2: 0.0006992816925048828
MLP_h_4h: 0.07186245918273926
MLP_4h_h: 0.06938576698303223
Post-MLP residual: 0.0018353462219238281
Attention layer time: 0.2335987091064453
LN1: 0.0007145404815673828
QKV Transform: 0.051614999771118164
Flash: 0.01851487159729004
Attention linproj: 0.01672220230102539
Post-attention Dropout: 0.0018541812896728516
Post-attention residual: 0.0006213188171386719
LN2: 0.0006911754608154297
MLP_h_4h: 0.07226777076721191
MLP_4h_h: 0.06994318962097168
Post-MLP residual: 0.001847982406616211
Attention layer time: 0.2357006072998047
LN1: 0.0007119178771972656
QKV Transform: 0.052289485931396484
Flash: 0.017450332641601562
Attention linproj: 0.01673269271850586
Post-attention Dropout: 0.0018472671508789062
Post-attention residual: 0.0006220340728759766
LN2: 0.0006918907165527344
MLP_h_4h: 0.07200241088867188
MLP_4h_h: 0.06934952735900879
Post-MLP residual: 0.001837015151977539
Attention layer time: 0.23441362380981445
LN1: 0.0007138252258300781
QKV Transform: 0.05121731758117676
Flash: 0.017714738845825195
Attention linproj: 0.016440153121948242
Post-attention Dropout: 0.001840353012084961
Post-attention residual: 0.0006253719329833984
LN2: 0.0006949901580810547
MLP_h_4h: 0.07198357582092285
MLP_4h_h: 0.06927084922790527
Post-MLP residual: 0.0018486976623535156
Attention layer time: 0.23321008682250977
LN1: 0.0007123947143554688
QKV Transform: 0.0508427619934082
Flash: 0.01699995994567871
Attention linproj: 0.016469955444335938
Post-attention Dropout: 0.0018455982208251953
Post-attention residual: 0.0006208419799804688
LN2: 0.0006928443908691406
MLP_h_4h: 0.07260680198669434
MLP_4h_h: 0.06984233856201172
Post-MLP residual: 0.001844167709350586
Attention layer time: 0.23333144187927246
LN1: 0.0007109642028808594
QKV Transform: 0.05128645896911621
Flash: 0.017660140991210938
Attention linproj: 0.016361236572265625
Post-attention Dropout: 0.0018420219421386719
Post-attention residual: 0.0006234645843505859
LN2: 0.0006945133209228516
MLP_h_4h: 0.07239079475402832
MLP_4h_h: 0.068817138671875
Post-MLP residual: 0.001859903335571289
Attention layer time: 0.23311352729797363
LN1: 0.0007109642028808594
QKV Transform: 0.05152249336242676
Flash: 0.017552852630615234
Attention linproj: 0.01639866828918457
Post-attention Dropout: 0.0018439292907714844
Post-attention residual: 0.0006234645843505859
LN2: 0.0006921291351318359
MLP_h_4h: 0.0722348690032959
MLP_4h_h: 0.06906247138977051
Post-MLP residual: 0.0018489360809326172
Attention layer time: 0.2333383560180664
LN1: 0.0007154941558837891
QKV Transform: 0.05249190330505371
Flash: 0.017368078231811523
Attention linproj: 0.0164029598236084
Post-attention Dropout: 0.0018510818481445312
Post-attention residual: 0.0006220340728759766
LN2: 0.0007026195526123047
MLP_h_4h: 0.0720527172088623
MLP_4h_h: 0.06919312477111816
Post-MLP residual: 0.0018610954284667969
Attention layer time: 0.23412251472473145
LN1: 0.0007154941558837891
QKV Transform: 0.05199718475341797
Flash: 0.017223596572875977
Attention linproj: 0.01650238037109375
Post-attention Dropout: 0.001852273941040039
Post-attention residual: 0.0006222724914550781
LN2: 0.0006949901580810547
MLP_h_4h: 0.07214665412902832
MLP_4h_h: 0.06891369819641113
Post-MLP residual: 0.0018463134765625
Attention layer time: 0.23337912559509277
LN1: 0.0007138252258300781
QKV Transform: 0.05154132843017578
Flash: 0.017531156539916992
Attention linproj: 0.016467809677124023
Post-attention Dropout: 0.0018470287322998047
Post-attention residual: 0.0006241798400878906
LN2: 0.0006978511810302734
MLP_h_4h: 0.07224154472351074
MLP_4h_h: 0.06895756721496582
Post-MLP residual: 0.0018448829650878906
Attention layer time: 0.23334360122680664
LN1: 0.0007128715515136719
QKV Transform: 0.051054954528808594
Flash: 0.019046783447265625
Attention linproj: 0.01659870147705078
Post-attention Dropout: 0.0018429756164550781
Post-attention residual: 0.0006265640258789062
LN2: 0.000690460205078125
MLP_h_4h: 0.07438874244689941
MLP_4h_h: 0.06920528411865234
Post-MLP residual: 0.0018413066864013672
Attention layer time: 0.23688721656799316
LN1: 0.0007174015045166016
QKV Transform: 0.0516810417175293
Flash: 0.017351388931274414
Attention linproj: 0.016501188278198242
Post-attention Dropout: 0.0018613338470458984
Post-attention residual: 0.0006237030029296875
LN2: 0.0006914138793945312
MLP_h_4h: 0.07219505310058594
MLP_4h_h: 0.06902647018432617
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.23336529731750488
LN1: 0.0007123947143554688
QKV Transform: 0.05270838737487793
Flash: 0.017251968383789062
Attention linproj: 0.016698122024536133
Post-attention Dropout: 0.0018701553344726562
Post-attention residual: 0.0006206035614013672
LN2: 0.0006937980651855469
MLP_h_4h: 0.07205033302307129
MLP_4h_h: 0.06935358047485352
Post-MLP residual: 0.001840829849243164
Attention layer time: 0.23468255996704102
LN1: 0.0007157325744628906
QKV Transform: 0.05176377296447754
Flash: 0.017211198806762695
Attention linproj: 0.01661539077758789
Post-attention Dropout: 0.0018506050109863281
Post-attention residual: 0.0006203651428222656
LN2: 0.0006906986236572266
MLP_h_4h: 0.07222509384155273
MLP_4h_h: 0.06968426704406738
Post-MLP residual: 0.0018405914306640625
Attention layer time: 0.23409414291381836
LN1: 0.000713348388671875
QKV Transform: 0.052800655364990234
Flash: 0.01714777946472168
Attention linproj: 0.016528606414794922
Post-attention Dropout: 0.0018563270568847656
Post-attention residual: 0.0006237030029296875
LN2: 0.0006937980651855469
MLP_h_4h: 0.0721578598022461
MLP_4h_h: 0.06915712356567383
Post-MLP residual: 0.0018417835235595703
Attention layer time: 0.2344050407409668
LN1: 0.0007185935974121094
QKV Transform: 0.05134916305541992
Flash: 0.01798415184020996
Attention linproj: 0.016541242599487305
Post-attention Dropout: 0.0018467903137207031
Post-attention residual: 0.0006184577941894531
LN2: 0.0006914138793945312
MLP_h_4h: 0.07203006744384766
MLP_4h_h: 0.06909656524658203
Post-MLP residual: 0.0018415451049804688
Attention layer time: 0.23359107971191406
LN1: 0.0007147789001464844
QKV Transform: 0.05267047882080078
Flash: 0.017359495162963867
Attention linproj: 0.01648116111755371
Post-attention Dropout: 0.0018472671508789062
Post-attention residual: 0.0006229877471923828
LN2: 0.0006918907165527344
MLP_h_4h: 0.07239103317260742
MLP_4h_h: 0.06975436210632324
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23525571823120117
LN1: 0.0007121562957763672
QKV Transform: 0.05167722702026367
Flash: 0.01842331886291504
Attention linproj: 0.01644611358642578
Post-attention Dropout: 0.001844644546508789
Post-attention residual: 0.0006222724914550781
LN2: 0.0006918907165527344
MLP_h_4h: 0.07223224639892578
MLP_4h_h: 0.06903648376464844
Post-MLP residual: 0.001844167709350586
Attention layer time: 0.234405517578125
LN1: 0.0007157325744628906
QKV Transform: 0.05205059051513672
Flash: 0.01829051971435547
Attention linproj: 0.016536474227905273
Post-attention Dropout: 0.0018565654754638672
Post-attention residual: 0.0006229877471923828
LN2: 0.0006918907165527344
MLP_h_4h: 0.07211017608642578
MLP_4h_h: 0.06937146186828613
Post-MLP residual: 0.0018534660339355469
Attention layer time: 0.23500943183898926
LN1: 0.0007197856903076172
QKV Transform: 0.052004337310791016
Flash: 0.017198562622070312
Attention linproj: 0.016491174697875977
Post-attention Dropout: 0.0018515586853027344
Post-attention residual: 0.0006196498870849609
LN2: 0.0006918907165527344
MLP_h_4h: 0.0717477798461914
MLP_4h_h: 0.068634033203125
Post-MLP residual: 0.0018529891967773438
Attention layer time: 0.23269248008728027
LN1: 0.0007128715515136719
QKV Transform: 0.05194401741027832
Flash: 0.01841115951538086
Attention linproj: 0.016598939895629883
Post-attention Dropout: 0.0018496513366699219
Post-attention residual: 0.0006194114685058594
LN2: 0.0006954669952392578
MLP_h_4h: 0.07176065444946289
MLP_4h_h: 0.06932449340820312
Post-MLP residual: 0.0018417835235595703
Attention layer time: 0.234635591506958
LN1: 0.0007171630859375
QKV Transform: 0.052385568618774414
Flash: 0.017396926879882812
Attention linproj: 0.01654195785522461
Post-attention Dropout: 0.001861572265625
Post-attention residual: 0.0006225109100341797
LN2: 0.0006921291351318359
MLP_h_4h: 0.07211565971374512
MLP_4h_h: 0.06921958923339844
Post-MLP residual: 0.0018498897552490234
Attention layer time: 0.23428964614868164
LN1: 0.0007135868072509766
QKV Transform: 0.05196094512939453
Flash: 0.017493247985839844
Attention linproj: 0.016754150390625
Post-attention Dropout: 0.0018434524536132812
Post-attention residual: 0.0006227493286132812
LN2: 0.0006926059722900391
MLP_h_4h: 0.07217884063720703
MLP_4h_h: 0.06926274299621582
Post-MLP residual: 0.0018374919891357422
Attention layer time: 0.2342529296875
LN1: 0.000728607177734375
QKV Transform: 0.05217266082763672
Flash: 0.018117427825927734
Attention linproj: 0.016814708709716797
Post-attention Dropout: 0.001847982406616211
Post-attention residual: 0.0006222724914550781
LN2: 0.0006933212280273438
MLP_h_4h: 0.07170557975769043
MLP_4h_h: 0.06932497024536133
Post-MLP residual: 0.001848459243774414
Attention layer time: 0.2347850799560547
LN1: 0.0007140636444091797
QKV Transform: 0.05204606056213379
Flash: 0.01714920997619629
Attention linproj: 0.016861915588378906
Post-attention Dropout: 0.0018558502197265625
Post-attention residual: 0.0006210803985595703
LN2: 0.0006921291351318359
MLP_h_4h: 0.07160115242004395
MLP_4h_h: 0.06914687156677246
Post-MLP residual: 0.0018413066864013672
Attention layer time: 0.23341155052185059
LN1: 0.000713348388671875
QKV Transform: 0.051448822021484375
Flash: 0.01753067970275879
Attention linproj: 0.016653060913085938
Post-attention Dropout: 0.0018436908721923828
Post-attention residual: 0.0006229877471923828
LN2: 0.0006926059722900391
MLP_h_4h: 0.07193541526794434
MLP_4h_h: 0.06916284561157227
Post-MLP residual: 0.001848459243774414
Attention layer time: 0.23331999778747559
LN1: 0.0007083415985107422
QKV Transform: 0.05115151405334473
Flash: 0.017763614654541016
Attention linproj: 0.016451358795166016
Post-attention Dropout: 0.0018641948699951172
Post-attention residual: 0.0006227493286132812
LN2: 0.0006916522979736328
MLP_h_4h: 0.07225251197814941
MLP_4h_h: 0.0694737434387207
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23367953300476074
LN1: 0.0007114410400390625
QKV Transform: 0.05097675323486328
Flash: 0.016990184783935547
Attention linproj: 0.016371488571166992
Post-attention Dropout: 0.0018498897552490234
Post-attention residual: 0.0006210803985595703
LN2: 0.0006926059722900391
MLP_h_4h: 0.07200098037719727
MLP_4h_h: 0.0688772201538086
Post-MLP residual: 0.0018503665924072266
Attention layer time: 0.23178911209106445
LN1: 0.0007114410400390625
QKV Transform: 0.051612138748168945
Flash: 0.01845526695251465
Attention linproj: 0.01636528968811035
Post-attention Dropout: 0.0018396377563476562
Post-attention residual: 0.0006210803985595703
LN2: 0.0006928443908691406
MLP_h_4h: 0.07230949401855469
MLP_4h_h: 0.06915855407714844
Post-MLP residual: 0.0018458366394042969
Attention layer time: 0.23446202278137207
LN1: 0.0007154941558837891
QKV Transform: 0.052724599838256836
Flash: 0.01719212532043457
Attention linproj: 0.016476154327392578
Post-attention Dropout: 0.001852273941040039
Post-attention residual: 0.0006203651428222656
LN2: 0.0006945133209228516
MLP_h_4h: 0.07247138023376465
MLP_4h_h: 0.06932878494262695
Post-MLP residual: 0.0018758773803710938
Attention layer time: 0.23480820655822754
LN1: 0.0007236003875732422
QKV Transform: 0.052489280700683594
Flash: 0.017312288284301758
Attention linproj: 0.01639080047607422
Post-attention Dropout: 0.0018496513366699219
Post-attention residual: 0.000621795654296875
LN2: 0.0006966590881347656
MLP_h_4h: 0.07244157791137695
MLP_4h_h: 0.06918740272521973
Post-MLP residual: 0.0018601417541503906
Attention layer time: 0.23444867134094238
LN1: 0.00072479248046875
QKV Transform: 0.05258965492248535
Flash: 0.017424821853637695
Attention linproj: 0.0163726806640625
Post-attention Dropout: 0.0018551349639892578
Post-attention residual: 0.0006220340728759766
LN2: 0.0006973743438720703
MLP_h_4h: 0.07238221168518066
MLP_4h_h: 0.06901907920837402
Post-MLP residual: 0.0018548965454101562
Attention layer time: 0.23439693450927734
LN1: 0.000713348388671875
QKV Transform: 0.05218982696533203
Flash: 0.01714468002319336
Attention linproj: 0.016497373580932617
Post-attention Dropout: 0.0018544197082519531
Post-attention residual: 0.0006198883056640625
LN2: 0.0006928443908691406
MLP_h_4h: 0.07224082946777344
MLP_4h_h: 0.06925225257873535
Post-MLP residual: 0.0018458366394042969
Attention layer time: 0.23394465446472168
LN1: 0.0007171630859375
QKV Transform: 0.05199170112609863
Flash: 0.0172269344329834
Attention linproj: 0.016520023345947266
Post-attention Dropout: 0.001865386962890625
Post-attention residual: 0.0006203651428222656
LN2: 0.0006926059722900391
MLP_h_4h: 0.07190227508544922
MLP_4h_h: 0.0691678524017334
Post-MLP residual: 0.0018372535705566406
Attention layer time: 0.23342180252075195
LN1: 0.0007154941558837891
QKV Transform: 0.05263876914978027
Flash: 0.017139434814453125
Attention linproj: 0.016690969467163086
Post-attention Dropout: 0.0018513202667236328
Post-attention residual: 0.00061798095703125
LN2: 0.0006923675537109375
MLP_h_4h: 0.07202768325805664
MLP_4h_h: 0.06934571266174316
Post-MLP residual: 0.0018384456634521484
Attention layer time: 0.23444080352783203
LN1: 0.0007166862487792969
QKV Transform: 0.05231285095214844
Flash: 0.017129182815551758
Attention linproj: 0.016706466674804688
Post-attention Dropout: 0.0018460750579833984
Post-attention residual: 0.0006246566772460938
LN2: 0.0006897449493408203
MLP_h_4h: 0.07191753387451172
MLP_4h_h: 0.0693204402923584
Post-MLP residual: 0.0018439292907714844
Attention layer time: 0.23399615287780762
LN1: 0.0007126331329345703
QKV Transform: 0.051647186279296875
Flash: 0.017330408096313477
Attention linproj: 0.016805171966552734
Post-attention Dropout: 0.0018477439880371094
Post-attention residual: 0.0006215572357177734
LN2: 0.0006926059722900391
MLP_h_4h: 0.07156944274902344
MLP_4h_h: 0.06924152374267578
Post-MLP residual: 0.0018544197082519531
Attention layer time: 0.2332005500793457
LN1: 0.0007100105285644531
QKV Transform: 0.05145621299743652
Flash: 0.01756882667541504
Attention linproj: 0.016594648361206055
Post-attention Dropout: 0.001859426498413086
Post-attention residual: 0.0006227493286132812
LN2: 0.0006930828094482422
MLP_h_4h: 0.07214760780334473
MLP_4h_h: 0.06971073150634766
Post-MLP residual: 0.0018467903137207031
Attention layer time: 0.2340836524963379
LN1: 0.00070953369140625
QKV Transform: 0.052489519119262695
Flash: 0.017542123794555664
Attention linproj: 0.016601085662841797
Post-attention Dropout: 0.0018489360809326172
Post-attention residual: 0.0006227493286132812
LN2: 0.0006918907165527344
MLP_h_4h: 0.07238316535949707
MLP_4h_h: 0.06928467750549316
Post-MLP residual: 0.0018434524536132812
Attention layer time: 0.2348802089691162
LN1: 0.0007121562957763672
QKV Transform: 0.05110311508178711
Flash: 0.017859458923339844
Attention linproj: 0.016442537307739258
Post-attention Dropout: 0.001861572265625
Post-attention residual: 0.0006229877471923828
LN2: 0.0006921291351318359
MLP_h_4h: 0.07218289375305176
MLP_4h_h: 0.069732666015625
Post-MLP residual: 0.0018401145935058594
Attention layer time: 0.2339024543762207
LN1: 0.0007245540618896484
QKV Transform: 0.05160856246948242
Flash: 0.017427921295166016
Attention linproj: 0.01648712158203125
Post-attention Dropout: 0.0018620491027832031
Post-attention residual: 0.0006237030029296875
LN2: 0.0006937980651855469
MLP_h_4h: 0.07256412506103516
MLP_4h_h: 0.07020163536071777
Post-MLP residual: 0.001844167709350586
Attention layer time: 0.2348928451538086
LN1: 0.0007140636444091797
QKV Transform: 0.05165863037109375
Flash: 0.017331600189208984
Attention linproj: 0.01650834083557129
Post-attention Dropout: 0.0018520355224609375
Post-attention residual: 0.000621795654296875
LN2: 0.0006906986236572266
MLP_h_4h: 0.07189774513244629
MLP_4h_h: 0.06901431083679199
Post-MLP residual: 0.0018470287322998047
Attention layer time: 0.2329864501953125
LN1: 0.000713348388671875
QKV Transform: 0.05158233642578125
Flash: 0.017429828643798828
Attention linproj: 0.016376972198486328
Post-attention Dropout: 0.0018498897552490234
Post-attention residual: 0.0006196498870849609
LN2: 0.0006909370422363281
MLP_h_4h: 0.07225275039672852
MLP_4h_h: 0.06902647018432617
Post-MLP residual: 0.0018544197082519531
Attention layer time: 0.2332606315612793
LN1: 0.0007154941558837891
QKV Transform: 0.051895856857299805
Flash: 0.0172269344329834
Attention linproj: 0.01675105094909668
Post-attention Dropout: 0.0018551349639892578
Post-attention residual: 0.0006206035614013672
LN2: 0.0006945133209228516
MLP_h_4h: 0.07179450988769531
MLP_4h_h: 0.06922388076782227
Post-MLP residual: 0.0018491744995117188
Attention layer time: 0.2335529327392578
LN1: 0.0007154941558837891
QKV Transform: 0.05154561996459961
Flash: 0.019918203353881836
Attention linproj: 0.01636052131652832
Post-attention Dropout: 0.0018405914306640625
Post-attention residual: 0.0006215572357177734
LN2: 0.0007083415985107422
MLP_h_4h: 0.0722663402557373
MLP_4h_h: 0.06923675537109375
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.2359309196472168
LN1: 0.0007150173187255859
QKV Transform: 0.05194401741027832
Flash: 0.017230987548828125
Attention linproj: 0.016463279724121094
Post-attention Dropout: 0.0018463134765625
Post-attention residual: 0.0006182193756103516
LN2: 0.0006930828094482422
MLP_h_4h: 0.07225441932678223
MLP_4h_h: 0.06903672218322754
Post-MLP residual: 0.0018520355224609375
Attention layer time: 0.2335350513458252
LN1: 0.0007131099700927734
QKV Transform: 0.052370548248291016
Flash: 0.018285751342773438
Attention linproj: 0.016579151153564453
Post-attention Dropout: 0.0018503665924072266
Post-attention residual: 0.0006232261657714844
LN2: 0.0006945133209228516
MLP_h_4h: 0.0717923641204834
MLP_4h_h: 0.06949305534362793
Post-MLP residual: 0.0018346309661865234
Attention layer time: 0.23513126373291016
LN1: 0.0007166862487792969
QKV Transform: 0.05181288719177246
Flash: 0.017362594604492188
Attention linproj: 0.016483306884765625
Post-attention Dropout: 0.0018463134765625
Post-attention residual: 0.0006198883056640625
LN2: 0.0006921291351318359
MLP_h_4h: 0.07219147682189941
MLP_4h_h: 0.06928634643554688
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23372435569763184
LN1: 0.0007145404815673828
QKV Transform: 0.051969051361083984
Flash: 0.017493247985839844
Attention linproj: 0.016691207885742188
Post-attention Dropout: 0.0018582344055175781
Post-attention residual: 0.0006210803985595703
LN2: 0.0006928443908691406
MLP_h_4h: 0.07184052467346191
MLP_4h_h: 0.06943750381469727
Post-MLP residual: 0.0018417835235595703
Attention layer time: 0.2340254783630371
LN1: 0.0007169246673583984
QKV Transform: 0.0519716739654541
Flash: 0.01747274398803711
Attention linproj: 0.01671147346496582
Post-attention Dropout: 0.001844644546508789
Post-attention residual: 0.0006210803985595703
LN2: 0.00069427490234375
MLP_h_4h: 0.07382583618164062
MLP_4h_h: 0.06896281242370605
Post-MLP residual: 0.0018439292907714844
Attention layer time: 0.2355349063873291
LN1: 0.0007145404815673828
QKV Transform: 0.052408456802368164
Flash: 0.01716303825378418
Attention linproj: 0.016712188720703125
Post-attention Dropout: 0.001852273941040039
Post-attention residual: 0.0006227493286132812
LN2: 0.0006892681121826172
MLP_h_4h: 0.07213187217712402
MLP_4h_h: 0.06944537162780762
Post-MLP residual: 0.0018324851989746094
Attention layer time: 0.23444795608520508
LN1: 0.0007114410400390625
QKV Transform: 0.05250692367553711
Flash: 0.017535686492919922
Attention linproj: 0.016810894012451172
Post-attention Dropout: 0.0018453598022460938
Post-attention residual: 0.0006222724914550781
LN2: 0.0006940364837646484
MLP_h_4h: 0.07228684425354004
MLP_4h_h: 0.06928801536560059
Post-MLP residual: 0.0018477439880371094
Attention layer time: 0.23502779006958008
LN1: 0.0007112026214599609
QKV Transform: 0.051573991775512695
Flash: 0.017400741577148438
Attention linproj: 0.01660466194152832
Post-attention Dropout: 0.0018532276153564453
Post-attention residual: 0.0006220340728759766
LN2: 0.0006909370422363281
MLP_h_4h: 0.07239603996276855
MLP_4h_h: 0.07000422477722168
Post-MLP residual: 0.0018467903137207031
Attention layer time: 0.23457837104797363
LN1: 0.0007109642028808594
QKV Transform: 0.05119657516479492
Flash: 0.017734527587890625
Attention linproj: 0.01645064353942871
Post-attention Dropout: 0.0018396377563476562
Post-attention residual: 0.0006225109100341797
LN2: 0.0006906986236572266
MLP_h_4h: 0.07272505760192871
MLP_4h_h: 0.07004809379577637
Post-MLP residual: 0.0018506050109863281
Attention layer time: 0.2347266674041748
LN1: 0.0007107257843017578
QKV Transform: 0.05268406867980957
Flash: 0.017281532287597656
Attention linproj: 0.016613483428955078
Post-attention Dropout: 0.0018453598022460938
Post-attention residual: 0.0006215572357177734
LN2: 0.0006921291351318359
MLP_h_4h: 0.0726165771484375
MLP_4h_h: 0.06951093673706055
Post-MLP residual: 0.0018463134765625
Attention layer time: 0.23529481887817383
LN1: 0.0007143020629882812
QKV Transform: 0.0514531135559082
Flash: 0.017569780349731445
Attention linproj: 0.01648545265197754
Post-attention Dropout: 0.0018460750579833984
Post-attention residual: 0.0006225109100341797
LN2: 0.0006930828094482422
MLP_h_4h: 0.0724332332611084
MLP_4h_h: 0.06941056251525879
Post-MLP residual: 0.0018508434295654297
Attention layer time: 0.2339332103729248
LN1: 0.0007147789001464844
QKV Transform: 0.0518183708190918
Flash: 0.017926454544067383
Attention linproj: 0.016370296478271484
Post-attention Dropout: 0.0018434524536132812
Post-attention residual: 0.0006206035614013672
LN2: 0.0006923675537109375
MLP_h_4h: 0.07258033752441406
MLP_4h_h: 0.06930923461914062
Post-MLP residual: 0.0018472671508789062
Attention layer time: 0.2345867156982422
LN1: 0.0007114410400390625
QKV Transform: 0.05173754692077637
Flash: 0.01732921600341797
Attention linproj: 0.016472578048706055
Post-attention Dropout: 0.0018472671508789062
Post-attention residual: 0.0006203651428222656
LN2: 0.0007112026214599609
MLP_h_4h: 0.07218289375305176
MLP_4h_h: 0.06892037391662598
Post-MLP residual: 0.0018565654754638672
Attention layer time: 0.23324275016784668
LN1: 0.0007126331329345703
QKV Transform: 0.051950693130493164
Flash: 0.01720118522644043
Attention linproj: 0.016480684280395508
Post-attention Dropout: 0.0018537044525146484
Post-attention residual: 0.0006196498870849609
LN2: 0.00069427490234375
MLP_h_4h: 0.07222604751586914
MLP_4h_h: 0.06897997856140137
Post-MLP residual: 0.0018520355224609375
Attention layer time: 0.2334427833557129
LN1: 0.0007150173187255859
QKV Transform: 0.050914764404296875
Flash: 0.018133163452148438
Attention linproj: 0.016336679458618164
Post-attention Dropout: 0.001837015151977539
Post-attention residual: 0.0006213188171386719
LN2: 0.0006952285766601562
MLP_h_4h: 0.07351469993591309
MLP_4h_h: 0.06908726692199707
Post-MLP residual: 0.0018639564514160156
Attention layer time: 0.23458266258239746
LN1: 0.0007145404815673828
QKV Transform: 0.05211901664733887
Flash: 0.018028974533081055
Attention linproj: 0.016418933868408203
Post-attention Dropout: 0.0018491744995117188
Post-attention residual: 0.0006215572357177734
LN2: 0.0006947517395019531
MLP_h_4h: 0.07214832305908203
MLP_4h_h: 0.06926965713500977
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.234588623046875
LN1: 0.0007157325744628906
QKV Transform: 0.052474021911621094
Flash: 0.017311573028564453
Attention linproj: 0.016759872436523438
Post-attention Dropout: 0.0018477439880371094
Post-attention residual: 0.0006213188171386719
LN2: 0.0006911754608154297
MLP_h_4h: 0.07225251197814941
MLP_4h_h: 0.06925177574157715
Post-MLP residual: 0.0018532276153564453
Attention layer time: 0.23465943336486816
LN1: 0.0007147789001464844
QKV Transform: 0.05243396759033203
Flash: 0.017403602600097656
Attention linproj: 0.016505718231201172
Post-attention Dropout: 0.0018503665924072266
Post-attention residual: 0.0006227493286132812
LN2: 0.0006935596466064453
MLP_h_4h: 0.07210636138916016
MLP_4h_h: 0.06907105445861816
Post-MLP residual: 0.0018413066864013672
Attention layer time: 0.23412108421325684
LN1: 0.0007166862487792969
QKV Transform: 0.05266237258911133
Flash: 0.0172274112701416
Attention linproj: 0.016704320907592773
Post-attention Dropout: 0.0018453598022460938
Post-attention residual: 0.0006239414215087891
LN2: 0.0006911754608154297
MLP_h_4h: 0.07188200950622559
MLP_4h_h: 0.06927347183227539
Post-MLP residual: 0.0018453598022460938
Attention layer time: 0.23435354232788086
LN1: 0.0007140636444091797
QKV Transform: 0.05236244201660156
Flash: 0.017501354217529297
Attention linproj: 0.016739368438720703
Post-attention Dropout: 0.0018467903137207031
Post-attention residual: 0.0006227493286132812
LN2: 0.0006961822509765625
MLP_h_4h: 0.07207775115966797
MLP_4h_h: 0.06918454170227051
Post-MLP residual: 0.0018410682678222656
Attention layer time: 0.23449349403381348
LN1: 0.0007152557373046875
QKV Transform: 0.05281233787536621
Flash: 0.01804208755493164
Attention linproj: 0.0166778564453125
Post-attention Dropout: 0.0018630027770996094
Post-attention residual: 0.0006222724914550781
LN2: 0.0006940364837646484
MLP_h_4h: 0.07210612297058105
MLP_4h_h: 0.06908011436462402
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.2353367805480957
LN1: 0.0007112026214599609
QKV Transform: 0.051795244216918945
Flash: 0.01735544204711914
Attention linproj: 0.01716470718383789
Post-attention Dropout: 0.0018498897552490234
Post-attention residual: 0.0006237030029296875
LN2: 0.0006952285766601562
MLP_h_4h: 0.07244706153869629
MLP_4h_h: 0.06926155090332031
Post-MLP residual: 0.0018444061279296875
Attention layer time: 0.23463916778564453
LN1: 0.0007107257843017578
QKV Transform: 0.05132269859313965
Flash: 0.017666101455688477
Attention linproj: 0.01661372184753418
Post-attention Dropout: 0.0018389225006103516
Post-attention residual: 0.0006215572357177734
LN2: 0.0006921291351318359
MLP_h_4h: 0.07182025909423828
MLP_4h_h: 0.06936788558959961
Post-MLP residual: 0.0018453598022460938
Attention layer time: 0.23337674140930176
LN1: 0.0007112026214599609
QKV Transform: 0.051540374755859375
Flash: 0.01751565933227539
Attention linproj: 0.016476154327392578
Post-attention Dropout: 0.0018432140350341797
Post-attention residual: 0.0006213188171386719
LN2: 0.0006923675537109375
MLP_h_4h: 0.07246708869934082
MLP_4h_h: 0.07025384902954102
Post-MLP residual: 0.0018453598022460938
Attention layer time: 0.234818696975708
LN1: 0.0007159709930419922
QKV Transform: 0.050887346267700195
Flash: 0.017053604125976562
Attention linproj: 0.01638960838317871
Post-attention Dropout: 0.001842498779296875
Post-attention residual: 0.0006234645843505859
LN2: 0.00069427490234375
MLP_h_4h: 0.0720052719116211
MLP_4h_h: 0.06890201568603516
Post-MLP residual: 0.0018455982208251953
Attention layer time: 0.2318129539489746
LN1: 0.0007147789001464844
QKV Transform: 0.05172371864318848
Flash: 0.01733255386352539
Attention linproj: 0.01648426055908203
Post-attention Dropout: 0.0018508434295654297
Post-attention residual: 0.0006227493286132812
LN2: 0.0006933212280273438
MLP_h_4h: 0.07223176956176758
MLP_4h_h: 0.06913375854492188
Post-MLP residual: 0.0018427371978759766
Attention layer time: 0.2334890365600586
LN1: 0.0007123947143554688
QKV Transform: 0.05222749710083008
Flash: 0.017412185668945312
Attention linproj: 0.016373395919799805
Post-attention Dropout: 0.0018503665924072266
Post-attention residual: 0.0006208419799804688
LN2: 0.0006928443908691406
MLP_h_4h: 0.07157063484191895
MLP_4h_h: 0.06827545166015625
Post-MLP residual: 0.0018568038940429688
Attention layer time: 0.23244833946228027
LN1: 0.0007150173187255859
QKV Transform: 0.05238485336303711
Flash: 0.018451213836669922
Attention linproj: 0.016399383544921875
Post-attention Dropout: 0.0018537044525146484
Post-attention residual: 0.0006210803985595703
LN2: 0.0006945133209228516
MLP_h_4h: 0.07187581062316895
MLP_4h_h: 0.06920051574707031
Post-MLP residual: 0.0018458366394042969
Attention layer time: 0.23492693901062012
LN1: 0.0007164478302001953
QKV Transform: 0.05235767364501953
Flash: 0.01748204231262207
Attention linproj: 0.016475200653076172
Post-attention Dropout: 0.0018460750579833984
Post-attention residual: 0.0006234645843505859
LN2: 0.0006926059722900391
MLP_h_4h: 0.07162117958068848
MLP_4h_h: 0.0687718391418457
Post-MLP residual: 0.0018520355224609375
Attention layer time: 0.23329639434814453
LN1: 0.0007131099700927734
QKV Transform: 0.051371097564697266
Flash: 0.017626523971557617
Attention linproj: 0.016860485076904297
Post-attention Dropout: 0.0018429756164550781
Post-attention residual: 0.0006258487701416016
LN2: 0.0006909370422363281
MLP_h_4h: 0.0714871883392334
MLP_4h_h: 0.06923246383666992
Post-MLP residual: 0.0018310546875
Attention layer time: 0.23315048217773438
LN1: 0.0007097721099853516
QKV Transform: 0.052100419998168945
Flash: 0.016844511032104492
Attention linproj: 0.01660633087158203
Post-attention Dropout: 0.0018568038940429688
Post-attention residual: 0.0006227493286132812
LN2: 0.0006909370422363281
MLP_h_4h: 0.07149934768676758
MLP_4h_h: 0.06940031051635742
Post-MLP residual: 0.0018434524536132812
Attention layer time: 0.23302817344665527
LN1: 0.0007126331329345703
QKV Transform: 0.05250668525695801
Flash: 0.01743769645690918
Attention linproj: 0.016386032104492188
Post-attention Dropout: 0.001840829849243164
Post-attention residual: 0.0006244182586669922
LN2: 0.0006914138793945312
MLP_h_4h: 0.07196927070617676
MLP_4h_h: 0.0695500373840332
Post-MLP residual: 0.0018427371978759766
Attention layer time: 0.23441076278686523
LN1: 0.0007123947143554688
QKV Transform: 0.05128026008605957
Flash: 0.017833709716796875
Attention linproj: 0.01667475700378418
Post-attention Dropout: 0.0018579959869384766
Post-attention residual: 0.0006203651428222656
LN2: 0.0006940364837646484
MLP_h_4h: 0.07145547866821289
MLP_4h_h: 0.06875395774841309
Post-MLP residual: 0.0018553733825683594
Attention layer time: 0.23260498046875
LN1: 0.0007295608520507812
QKV Transform: 0.05182027816772461
Flash: 0.01739978790283203
Attention linproj: 0.01654839515686035
Post-attention Dropout: 0.0020093917846679688
Post-attention residual: 0.0006301403045654297
LN2: 0.0007741451263427734
MLP_h_4h: 0.07132196426391602
MLP_4h_h: 0.06864380836486816
Post-MLP residual: 0.0018625259399414062
Attention layer time: 0.23282241821289062
LN1: 0.0007233619689941406
QKV Transform: 0.051782846450805664
Flash: 0.017319202423095703
Attention linproj: 0.016512393951416016
Post-attention Dropout: 0.0018687248229980469
Post-attention residual: 0.0006213188171386719
LN2: 0.0006911754608154297
MLP_h_4h: 0.07147693634033203
MLP_4h_h: 0.06896138191223145
Post-MLP residual: 0.0018622875213623047
Attention layer time: 0.23276233673095703
LN1: 0.0007162094116210938
QKV Transform: 0.05262565612792969
Flash: 0.017149925231933594
Attention linproj: 0.016707181930541992
Post-attention Dropout: 0.0018570423126220703
Post-attention residual: 0.0006201267242431641
LN2: 0.0006914138793945312
MLP_h_4h: 0.07150864601135254
MLP_4h_h: 0.06882905960083008
Post-MLP residual: 0.0018360614776611328
Attention layer time: 0.2334156036376953
LN1: 0.0007126331329345703
QKV Transform: 0.05249381065368652
Flash: 0.018062829971313477
Attention linproj: 0.016507387161254883
Post-attention Dropout: 0.0018486976623535156
Post-attention residual: 0.0006229877471923828
LN2: 0.0006954669952392578
MLP_h_4h: 0.07192516326904297
MLP_4h_h: 0.06935477256774902
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.23493385314941406
LN1: 0.0007143020629882812
QKV Transform: 0.051239967346191406
Flash: 0.018768310546875
Attention linproj: 0.016707420349121094
Post-attention Dropout: 0.0018427371978759766
Post-attention residual: 0.0006210803985595703
LN2: 0.0006923675537109375
MLP_h_4h: 0.07146024703979492
MLP_4h_h: 0.06882119178771973
Post-MLP residual: 0.0018367767333984375
Attention layer time: 0.23357415199279785
LN1: 0.0007092952728271484
QKV Transform: 0.05105304718017578
Flash: 0.01787734031677246
Attention linproj: 0.01645660400390625
Post-attention Dropout: 0.0018618106842041016
Post-attention residual: 0.0006234645843505859
LN2: 0.0006926059722900391
MLP_h_4h: 0.07143163681030273
MLP_4h_h: 0.06878137588500977
Post-MLP residual: 0.0018496513366699219
Attention layer time: 0.23218512535095215
LN1: 0.0007112026214599609
QKV Transform: 0.05143332481384277
Flash: 0.01749706268310547
Attention linproj: 0.016359806060791016
Post-attention Dropout: 0.0018491744995117188
Post-attention residual: 0.0006234645843505859
LN2: 0.0006952285766601562
MLP_h_4h: 0.07187795639038086
MLP_4h_h: 0.06904363632202148
Post-MLP residual: 0.0018460750579833984
Attention layer time: 0.23279356956481934
LN1: 0.000713348388671875
QKV Transform: 0.05161595344543457
Flash: 0.017485618591308594
Attention linproj: 0.01677393913269043
Post-attention Dropout: 0.0018489360809326172
Post-attention residual: 0.0006215572357177734
LN2: 0.0006966590881347656
MLP_h_4h: 0.07157588005065918
MLP_4h_h: 0.0688323974609375
Post-MLP residual: 0.001850128173828125
Attention layer time: 0.23293495178222656
LN1: 0.0007143020629882812
QKV Transform: 0.05233573913574219
Flash: 0.017398834228515625
Attention linproj: 0.016378402709960938
Post-attention Dropout: 0.0018477439880371094
Post-attention residual: 0.0006322860717773438
LN2: 0.0006911754608154297
MLP_h_4h: 0.07208633422851562
MLP_4h_h: 0.06919741630554199
Post-MLP residual: 0.0018410682678222656
Attention layer time: 0.2340106964111328
LN1: 0.0007166862487792969
QKV Transform: 0.05245518684387207
Flash: 0.017378807067871094
Attention linproj: 0.016568422317504883
Post-attention Dropout: 0.0018534660339355469
Post-attention residual: 0.0006206035614013672
LN2: 0.0006923675537109375
MLP_h_4h: 0.0714418888092041
MLP_4h_h: 0.06893253326416016
Post-MLP residual: 0.0018460750579833984
Attention layer time: 0.23337602615356445
LN1: 0.0007138252258300781
QKV Transform: 0.05195355415344238
Flash: 0.017724037170410156
Attention linproj: 0.016672372817993164
Post-attention Dropout: 0.0018372535705566406
Post-attention residual: 0.0006198883056640625
LN2: 0.0006973743438720703
MLP_h_4h: 0.07172417640686035
MLP_4h_h: 0.06910085678100586
Post-MLP residual: 0.0018367767333984375
Attention layer time: 0.2337510585784912
LN1: 0.0007128715515136719
QKV Transform: 0.051589012145996094
Flash: 0.017371177673339844
Attention linproj: 0.0164186954498291
Post-attention Dropout: 0.0018372535705566406
Post-attention residual: 0.0006225109100341797
LN2: 0.0006928443908691406
MLP_h_4h: 0.07169008255004883
MLP_4h_h: 0.06920456886291504
Post-MLP residual: 0.0018472671508789062
Attention layer time: 0.2328352928161621
LN1: 0.0007119178771972656
QKV Transform: 0.0509488582611084
Flash: 0.017024993896484375
Attention linproj: 0.016489267349243164
Post-attention Dropout: 0.0018548965454101562
Post-attention residual: 0.0006196498870849609
LN2: 0.0006926059722900391
MLP_h_4h: 0.07218122482299805
MLP_4h_h: 0.06937718391418457
Post-MLP residual: 0.0018429756164550781
Attention layer time: 0.23260068893432617
LN1: 0.0007147789001464844
QKV Transform: 0.05213212966918945
Flash: 0.01770639419555664
Attention linproj: 0.016350269317626953
Post-attention Dropout: 0.0018544197082519531
Post-attention residual: 0.0006227493286132812
LN2: 0.0006930828094482422
MLP_h_4h: 0.07211065292358398
MLP_4h_h: 0.06925058364868164
Post-MLP residual: 0.0018432140350341797
Attention layer time: 0.23412203788757324
LN1: 0.0007190704345703125
QKV Transform: 0.05148935317993164
Flash: 0.01746368408203125
Attention linproj: 0.016360044479370117
Post-attention Dropout: 0.0018489360809326172
Post-attention residual: 0.0006213188171386719
LN2: 0.0006933212280273438
MLP_h_4h: 0.07205510139465332
MLP_4h_h: 0.06871533393859863
Post-MLP residual: 0.0018358230590820312
Attention layer time: 0.23267650604248047
LN1: 0.0007152557373046875
QKV Transform: 0.05272364616394043
Flash: 0.01742839813232422
Attention linproj: 0.016927242279052734
Post-attention Dropout: 0.0018572807312011719
Post-attention residual: 0.0006220340728759766
LN2: 0.0006911754608154297
MLP_h_4h: 0.071624755859375
MLP_4h_h: 0.06940484046936035
Post-MLP residual: 0.0019102096557617188
Attention layer time: 0.234879732131958
LN1: 0.0007350444793701172
QKV Transform: 0.05239748954772949
Flash: 0.01748180389404297
Attention linproj: 0.016408920288085938
Post-attention Dropout: 0.0018415451049804688
Post-attention residual: 0.0006251335144042969
LN2: 0.0006914138793945312
MLP_h_4h: 0.07181787490844727
MLP_4h_h: 0.0688169002532959
Post-MLP residual: 0.0018448829650878906
Attention layer time: 0.23356842994689941
LN1: 0.0007200241088867188
QKV Transform: 0.05143594741821289
Flash: 0.01756739616394043
Attention linproj: 0.016666173934936523
Post-attention Dropout: 0.001842498779296875
Post-attention residual: 0.0006227493286132812
LN2: 0.0006902217864990234
MLP_h_4h: 0.07179641723632812
MLP_4h_h: 0.0692133903503418
Post-MLP residual: 0.001863241195678711
Attention layer time: 0.2332911491394043
LN1: 0.0007135868072509766
QKV Transform: 0.0524289608001709
Flash: 0.01736927032470703
Attention linproj: 0.01672053337097168
Post-attention Dropout: 0.0018417835235595703
Post-attention residual: 0.0006239414215087891
LN2: 0.0006935596466064453
MLP_h_4h: 0.07151532173156738
MLP_4h_h: 0.06935977935791016
Post-MLP residual: 0.0018367767333984375
Attention layer time: 0.23398709297180176
LN1: 0.0007100105285644531
QKV Transform: 0.051137447357177734
Flash: 0.018889188766479492
Attention linproj: 0.016351938247680664
Post-attention Dropout: 0.0018460750579833984
Post-attention residual: 0.000621795654296875
LN2: 0.0006933212280273438
MLP_h_4h: 0.07170915603637695
MLP_4h_h: 0.06938362121582031
Post-MLP residual: 0.0018391609191894531
Attention layer time: 0.23402953147888184
LN1: 0.0007116794586181641
QKV Transform: 0.05087566375732422
Flash: 0.017059803009033203
Attention linproj: 0.016353368759155273
Post-attention Dropout: 0.0018467903137207031
Post-attention residual: 0.0006206035614013672
LN2: 0.0006933212280273438
MLP_h_4h: 0.07203865051269531
MLP_4h_h: 0.06920146942138672
Post-MLP residual: 0.001844644546508789
Attention layer time: 0.23209309577941895
LN1: 0.0007121562957763672
QKV Transform: 0.0515596866607666
Flash: 0.017541885375976562
Attention linproj: 0.016367435455322266
Post-attention Dropout: 0.0018489360809326172
Post-attention residual: 0.0006222724914550781
LN2: 0.0006978511810302734
MLP_h_4h: 0.07187795639038086
MLP_4h_h: 0.0687406063079834
Post-MLP residual: 0.0018420219421386719
Attention layer time: 0.23265838623046875
LN1: 0.0007131099700927734
QKV Transform: 0.0518345832824707
Flash: 0.017286062240600586
Attention linproj: 0.01642131805419922
Post-attention Dropout: 0.0018510818481445312
Post-attention residual: 0.0006246566772460938
LN2: 0.0006954669952392578
MLP_h_4h: 0.07230138778686523
MLP_4h_h: 0.06910443305969238
Post-MLP residual: 0.0018401145935058594
Attention layer time: 0.23356890678405762
LN1: 0.0007319450378417969
QKV Transform: 0.05173349380493164
Flash: 0.017425537109375
Attention linproj: 0.016803503036499023
Post-attention Dropout: 0.0018453598022460938
Post-attention residual: 0.0006222724914550781
LN2: 0.0006906986236572266
MLP_h_4h: 0.07182097434997559
MLP_4h_h: 0.06944012641906738
Post-MLP residual: 0.0018720626831054688
Attention layer time: 0.2338712215423584
LN1: 0.000713348388671875
QKV Transform: 0.052000999450683594
Flash: 0.017728805541992188
Attention linproj: 0.01652240753173828
Post-attention Dropout: 0.0018467903137207031
Post-attention residual: 0.000621795654296875
LN2: 0.0006926059722900391
MLP_h_4h: 0.07240128517150879
MLP_4h_h: 0.0698850154876709
Post-MLP residual: 0.0018320083618164062
Attention layer time: 0.23511481285095215
LN1: 0.0007128715515136719
QKV Transform: 0.052347421646118164
Flash: 0.017445802688598633
Attention linproj: 0.016817569732666016
Post-attention Dropout: 0.0018467903137207031
Post-attention residual: 0.0006201267242431641
LN2: 0.0006973743438720703
MLP_h_4h: 0.07147598266601562
MLP_4h_h: 0.06924939155578613
Post-MLP residual: 0.0018486976623535156
Attention layer time: 0.23394465446472168
LN1: 0.0007097721099853516
QKV Transform: 0.05151939392089844
Flash: 0.017495393753051758
Attention linproj: 0.016553163528442383
Post-attention Dropout: 0.0018362998962402344
Post-attention residual: 0.0006229877471923828
LN2: 0.0006909370422363281
MLP_h_4h: 0.07225823402404785
MLP_4h_h: 0.06952476501464844
Post-MLP residual: 0.0018391609191894531
Attention layer time: 0.2339153289794922
LN1: 0.0007195472717285156
QKV Transform: 0.05130171775817871
Flash: 0.017737388610839844
Attention linproj: 0.016416072845458984
Post-attention Dropout: 0.00183868408203125
Post-attention residual: 0.0006237030029296875
LN2: 0.0006930828094482422
MLP_h_4h: 0.07225441932678223
MLP_4h_h: 0.07009339332580566
Post-MLP residual: 0.0018541812896728516
Attention layer time: 0.23440003395080566
LN1: 0.0007166862487792969
QKV Transform: 0.05189871788024902
Flash: 0.017849206924438477
Attention linproj: 0.016478538513183594
Post-attention Dropout: 0.0018389225006103516
Post-attention residual: 0.0006227493286132812
LN2: 0.0006926059722900391
MLP_h_4h: 0.07195234298706055
MLP_4h_h: 0.06955838203430176
Post-MLP residual: 0.0018451213836669922
Attention layer time: 0.23433995246887207
LN1: 0.0007126331329345703
QKV Transform: 0.05106210708618164
Flash: 0.016969919204711914
Attention linproj: 0.016373872756958008
Post-attention Dropout: 0.001844644546508789
Post-attention residual: 0.0006198883056640625
LN2: 0.0006895065307617188
MLP_h_4h: 0.07202863693237305
MLP_4h_h: 0.06900787353515625
Post-MLP residual: 0.0018422603607177734
Attention layer time: 0.2319962978363037
LN1: 0.0007140636444091797
QKV Transform: 0.05176281929016113
Flash: 0.01725912094116211
Attention linproj: 0.01640939712524414
Post-attention Dropout: 0.0018508434295654297
Post-attention residual: 0.000621795654296875
LN2: 0.0006964206695556641
MLP_h_4h: 0.07222485542297363
MLP_4h_h: 0.06926774978637695
Post-MLP residual: 0.0018582344055175781
Attention layer time: 0.23351049423217773
LN1: 0.0007152557373046875
QKV Transform: 0.052860260009765625
Flash: 0.017168521881103516
Attention linproj: 0.0166168212890625
Post-attention Dropout: 0.0018703937530517578
Post-attention residual: 0.0006220340728759766
LN2: 0.0007004737854003906
MLP_h_4h: 0.07210636138916016
MLP_4h_h: 0.06896138191223145
Post-MLP residual: 0.0018508434295654297
Attention layer time: 0.23435521125793457
Transformer duration (in seconds): 0.2416
Transformer throughput (in TFLOP/s): 222.954
========================================================================================================================
[2023-11-26 21:55:50,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-26 22:00:34,059] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.0a0+32f93b1 

[2023-11-26 22:00:36,174] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-26 22:00:36,174] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-11-26 22:07:39,514] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.0a0+32f93b1 

[2023-11-26 22:07:41,299] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-26 22:07:41,299] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-11-26 22:07:42,664] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.3, master_port=6000
[2023-11-26 22:07:42,665] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-26 22:07:42,671] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
[2023-11-26 22:09:57,553] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.0a0+32f93b1 

[2023-11-26 22:09:59,141] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-26 22:09:59,141] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-11-26 22:09:59,603] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.3, master_port=6000
[2023-11-26 22:09:59,604] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-26 22:09:59,608] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
[2023-11-26 22:10:55,805] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.0a0+32f93b1 

[2023-11-26 22:10:57,645] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-26 22:10:57,645] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-11-26 22:10:58,128] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.3, master_port=6000
[2023-11-26 22:10:58,128] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-26 22:10:58,134] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.2639
Attention throughput (in TFLOP/s): 70.830
MLP duration (in seconds): 0.4627
MLP throughput (in TFLOP/s): 76.048
Transformer duration (in seconds): 0.7594
Transformer throughput (in TFLOP/s): 70.946
========================================================================================================================
