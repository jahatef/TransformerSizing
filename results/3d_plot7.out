1.13.1 

num_attention_heads: 96, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x1x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x1x2048): 0.516
Elapsed time for attention_prob_times_values (384x2048x2048x1): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x1): 1.429

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 0.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x2x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x2x2048): 1.041
Elapsed time for attention_prob_times_values (384x2048x2048x2): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x2): 2.784

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 1.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x3x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x3x2048): 2.719
Elapsed time for attention_prob_times_values (384x2048x2048x3): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x3): 3.324

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 3.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x4x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x4x2048): 2.376
Elapsed time for attention_prob_times_values (384x2048x2048x4): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x4): 4.791

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 4.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x5x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x5x2048): 3.550
Elapsed time for attention_prob_times_values (384x2048x2048x5): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x5): 4.991

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 6.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x6x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x6x2048): 3.616
Elapsed time for attention_prob_times_values (384x2048x2048x6): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x6): 8.316

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 7.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x7x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x7x2048): 6.352
Elapsed time for attention_prob_times_values (384x2048x2048x7): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x7): 7.131

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 11.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x8x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x8x2048): 9.066
Elapsed time for attention_prob_times_values (384x2048x2048x8): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x8): 10.540

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 17.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x9x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x9x2048): 6.183
Elapsed time for attention_prob_times_values (384x2048x2048x9): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x9): 8.576

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 13.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x10x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x10x2048): 6.007
Elapsed time for attention_prob_times_values (384x2048x2048x10): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x10): 12.885

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 15.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x11x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x11x2048): 7.655
Elapsed time for attention_prob_times_values (384x2048x2048x11): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x11): 10.767

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 18.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x12x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x12x2048): 7.093
Elapsed time for attention_prob_times_values (384x2048x2048x12): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x12): 15.412

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 20.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x13x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x13x2048): 9.016
Elapsed time for attention_prob_times_values (384x2048x2048x13): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x13): 11.516

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 22.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x14x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x14x2048): 8.143
Elapsed time for attention_prob_times_values (384x2048x2048x14): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x14): 16.686

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 25.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x15x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x15x2048): 9.537
Elapsed time for attention_prob_times_values (384x2048x2048x15): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x15): 12.833

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 26.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x16x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x16x2048): 18.387
Elapsed time for attention_prob_times_values (384x2048x2048x16): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x16): 20.655

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 48.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x17x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x17x2048): 11.063
Elapsed time for attention_prob_times_values (384x2048x2048x17): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x17): 15.652

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 33.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x18x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x18x2048): 10.270
Elapsed time for attention_prob_times_values (384x2048x2048x18): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x18): 17.447

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 34.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x19x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x19x2048): 12.786
Elapsed time for attention_prob_times_values (384x2048x2048x19): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x19): 16.912

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 40.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x20x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x20x2048): 11.002
Elapsed time for attention_prob_times_values (384x2048x2048x20): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x20): 24.163

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 43.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x21x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x21x2048): 13.977
Elapsed time for attention_prob_times_values (384x2048x2048x21): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x21): 16.901

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 45.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x22x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x22x2048): 10.948
Elapsed time for attention_prob_times_values (384x2048x2048x22): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x22): 27.891

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 48.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x23x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x23x2048): 15.536
Elapsed time for attention_prob_times_values (384x2048x2048x23): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x23): 21.967

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 57.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x24x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x24x2048): 28.557
Elapsed time for attention_prob_times_values (384x2048x2048x24): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x24): 29.770

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 94.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x25x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x25x2048): 16.687
Elapsed time for attention_prob_times_values (384x2048x2048x25): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x25): 22.413

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 63.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x26x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x26x2048): 15.313
Elapsed time for attention_prob_times_values (384x2048x2048x26): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x26): 26.478

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 66.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x27x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x27x2048): 17.384
Elapsed time for attention_prob_times_values (384x2048x2048x27): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x27): 23.299

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 70.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x28x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x28x2048): 16.147
Elapsed time for attention_prob_times_values (384x2048x2048x28): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x28): 31.167

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 77.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x29x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x29x2048): 18.970
Elapsed time for attention_prob_times_values (384x2048x2048x29): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x29): 24.498

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 79.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x30x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x30x2048): 16.791
Elapsed time for attention_prob_times_values (384x2048x2048x30): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x30): 32.090

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 84.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x31x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x31x2048): 19.227
Elapsed time for attention_prob_times_values (384x2048x2048x31): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x31): 27.179

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 87.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x32x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x32x2048): 36.258
Elapsed time for attention_prob_times_values (384x2048x2048x32): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x32): 40.395

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 152.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x33x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x33x2048): 18.892
Elapsed time for attention_prob_times_values (384x2048x2048x33): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x33): 23.469

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 85.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x34x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x34x2048): 14.929
Elapsed time for attention_prob_times_values (384x2048x2048x34): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x34): 33.029

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 86.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x35x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x35x2048): 19.438
Elapsed time for attention_prob_times_values (384x2048x2048x35): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x35): 24.179

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 92.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x36x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x36x2048): 14.854
Elapsed time for attention_prob_times_values (384x2048x2048x36): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x36): 37.940

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 93.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x37x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x37x2048): 20.394
Elapsed time for attention_prob_times_values (384x2048x2048x37): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x37): 25.620

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 101.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x38x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x38x2048): 16.002
Elapsed time for attention_prob_times_values (384x2048x2048x38): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x38): 41.030

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 105.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x39x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x39x2048): 21.088
Elapsed time for attention_prob_times_values (384x2048x2048x39): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x39): 26.516

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 109.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x40x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x40x2048): 37.880
Elapsed time for attention_prob_times_values (384x2048x2048x40): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x40): 48.482

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 202.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x41x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x41x2048): 20.534
Elapsed time for attention_prob_times_values (384x2048x2048x41): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x41): 28.192

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 115.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x42x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x42x2048): 17.496
Elapsed time for attention_prob_times_values (384x2048x2048x42): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x42): 46.353

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 125.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x43x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x43x2048): 22.458
Elapsed time for attention_prob_times_values (384x2048x2048x43): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x43): 29.146

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 127.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x44x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x44x2048): 14.306
Elapsed time for attention_prob_times_values (384x2048x2048x44): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x44): 48.390

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 113.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x45x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x45x2048): 23.380
Elapsed time for attention_prob_times_values (384x2048x2048x45): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x45): 30.627

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 138.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x46x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x46x2048): 18.321
Elapsed time for attention_prob_times_values (384x2048x2048x46): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x46): 47.661

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 140.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x47x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x47x2048): 24.283
Elapsed time for attention_prob_times_values (384x2048x2048x47): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x47): 31.235

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 147.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x48x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x48x2048): 43.712
Elapsed time for attention_prob_times_values (384x2048x2048x48): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x48): 59.563

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 277.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x49x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x49x2048): 25.084
Elapsed time for attention_prob_times_values (384x2048x2048x49): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x49): 32.189

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 157.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x50x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x50x2048): 19.407
Elapsed time for attention_prob_times_values (384x2048x2048x50): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x50): 53.411

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 161.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x51x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x51x2048): 23.799
Elapsed time for attention_prob_times_values (384x2048x2048x51): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x51): 34.718

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 163.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x52x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x52x2048): 19.735
Elapsed time for attention_prob_times_values (384x2048x2048x52): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x52): 48.124

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 164.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x53x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x53x2048): 26.610
Elapsed time for attention_prob_times_values (384x2048x2048x53): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x53): 36.264

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 183.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x54x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x54x2048): 21.043
Elapsed time for attention_prob_times_values (384x2048x2048x54): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x54): 57.438

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 186.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x55x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x55x2048): 26.591
Elapsed time for attention_prob_times_values (384x2048x2048x55): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x55): 35.353

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 186.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x56x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x56x2048): 51.233
Elapsed time for attention_prob_times_values (384x2048x2048x56): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x56): 66.726

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 362.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x57x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x57x2048): 27.141
Elapsed time for attention_prob_times_values (384x2048x2048x57): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x57): 34.969

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 193.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x58x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x58x2048): 21.947
Elapsed time for attention_prob_times_values (384x2048x2048x58): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x58): 61.202

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 207.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x59x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x59x2048): 28.853
Elapsed time for attention_prob_times_values (384x2048x2048x59): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x59): 40.406

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 219.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x60x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x60x2048): 20.742
Elapsed time for attention_prob_times_values (384x2048x2048x60): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x60): 61.287

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 205.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x61x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x61x2048): 29.556
Elapsed time for attention_prob_times_values (384x2048x2048x61): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x61): 40.617

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 229.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x62x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x62x2048): 22.408
Elapsed time for attention_prob_times_values (384x2048x2048x62): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x62): 66.487

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 228.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x63x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x63x2048): 28.497
Elapsed time for attention_prob_times_values (384x2048x2048x63): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x63): 40.384

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 230.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x64x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x64x2048): 61.478
Elapsed time for attention_prob_times_values (384x2048x2048x64): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x64): 80.317

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 487.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x65x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x65x2048): 26.465
Elapsed time for attention_prob_times_values (384x2048x2048x65): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x65): 39.808

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 225.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x66x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x66x2048): 19.620
Elapsed time for attention_prob_times_values (384x2048x2048x66): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x66): 63.968

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 215.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x67x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x67x2048): 27.441
Elapsed time for attention_prob_times_values (384x2048x2048x67): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x67): 41.263

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 240.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x68x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x68x2048): 18.908
Elapsed time for attention_prob_times_values (384x2048x2048x68): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x68): 69.213

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 219.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x69x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x69x2048): 27.177
Elapsed time for attention_prob_times_values (384x2048x2048x69): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x69): 42.206

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 246.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x70x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x70x2048): 19.517
Elapsed time for attention_prob_times_values (384x2048x2048x70): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x70): 71.915

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 232.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x71x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x71x2048): 26.945
Elapsed time for attention_prob_times_values (384x2048x2048x71): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x71): 42.813

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 253.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x72x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x72x2048): 65.256
Elapsed time for attention_prob_times_values (384x2048x2048x72): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x72): 80.742

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 559.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x73x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x73x2048): 27.774
Elapsed time for attention_prob_times_values (384x2048x2048x73): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x73): 43.522

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 265.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x74x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x74x2048): 49.868
Elapsed time for attention_prob_times_values (384x2048x2048x74): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x74): 72.371

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 468.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x75x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x75x2048): 29.389
Elapsed time for attention_prob_times_values (384x2048x2048x75): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x75): 45.255

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 286.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x76x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x76x2048): 51.831
Elapsed time for attention_prob_times_values (384x2048x2048x76): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x76): 75.385

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 499.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x77x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x77x2048): 30.015
Elapsed time for attention_prob_times_values (384x2048x2048x77): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x77): 17.492

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 181.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x78x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x78x2048): 52.160
Elapsed time for attention_prob_times_values (384x2048x2048x78): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x78): 79.988

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 524.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x79x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x79x2048): 29.615
Elapsed time for attention_prob_times_values (384x2048x2048x79): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x79): 47.134

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 305.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x80x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x80x2048): 76.913
Elapsed time for attention_prob_times_values (384x2048x2048x80): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x80): 84.561

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 684.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x81x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x81x2048): 30.133
Elapsed time for attention_prob_times_values (384x2048x2048x81): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x81): 49.497

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 321.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x82x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x82x2048): 22.501
Elapsed time for attention_prob_times_values (384x2048x2048x82): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x82): 81.835

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 306.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x83x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x83x2048): 30.508
Elapsed time for attention_prob_times_values (384x2048x2048x83): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x83): 50.047

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 332.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x84x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x84x2048): 15.038
Elapsed time for attention_prob_times_values (384x2048x2048x84): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x84): 84.367

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 226.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x85x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x85x2048): 31.478
Elapsed time for attention_prob_times_values (384x2048x2048x85): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x85): 51.107

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 349.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x86x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x86x2048): 58.262
Elapsed time for attention_prob_times_values (384x2048x2048x86): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x86): 85.354

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 627.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x87x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x87x2048): 31.550
Elapsed time for attention_prob_times_values (384x2048x2048x87): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x87): 51.843

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 359.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x88x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x88x2048): 73.958
Elapsed time for attention_prob_times_values (384x2048x2048x88): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x88): 101.136

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 790.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x89x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x89x2048): 31.800
Elapsed time for attention_prob_times_values (384x2048x2048x89): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x89): 52.211

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 369.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x90x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x90x2048): 59.350
Elapsed time for attention_prob_times_values (384x2048x2048x90): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x90): 91.043

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 678.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x91x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x91x2048): 32.606
Elapsed time for attention_prob_times_values (384x2048x2048x91): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x91): 53.701

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 386.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x92x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x92x2048): 60.622
Elapsed time for attention_prob_times_values (384x2048x2048x92): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x92): 93.178

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 706.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x93x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x93x2048): 33.091
Elapsed time for attention_prob_times_values (384x2048x2048x93): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x93): 56.735

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 406.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x94x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x94x2048): 62.195
Elapsed time for attention_prob_times_values (384x2048x2048x94): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x94): 93.192

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 732.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x95x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x95x2048): 34.683
Elapsed time for attention_prob_times_values (384x2048x2048x95): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x95): 56.833

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 426.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x96x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x96x2048): 92.342
Elapsed time for attention_prob_times_values (384x2048x2048x96): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x96): 106.358

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 988.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x97x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x97x2048): 31.480
Elapsed time for attention_prob_times_values (384x2048x2048x97): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x97): 57.909

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 411.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x98x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x98x2048): 56.547
Elapsed time for attention_prob_times_values (384x2048x2048x98): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x98): 85.225

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 692.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x99x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x99x2048): 21.382
Elapsed time for attention_prob_times_values (384x2048x2048x99): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x99): 59.054

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 322.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x100x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x100x2048): 59.585
Elapsed time for attention_prob_times_values (384x2048x2048x100): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x100): 92.293

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 751.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x101x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x101x2048): 31.110
Elapsed time for attention_prob_times_values (384x2048x2048x101): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x101): 59.477

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 427.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x102x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x102x2048): 60.555
Elapsed time for attention_prob_times_values (384x2048x2048x102): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x102): 99.610

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 795.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x103x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x103x2048): 30.394
Elapsed time for attention_prob_times_values (384x2048x2048x103): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x103): 59.629

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 429.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x104x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x104x2048): 82.615
Elapsed time for attention_prob_times_values (384x2048x2048x104): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x104): 115.278

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 1034.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x105x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x105x2048): 31.065
Elapsed time for attention_prob_times_values (384x2048x2048x105): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x105): 61.090

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 446.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x106x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x106x2048): 63.252
Elapsed time for attention_prob_times_values (384x2048x2048x106): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x106): 105.444

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 864.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x107x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x107x2048): 31.745
Elapsed time for attention_prob_times_values (384x2048x2048x107): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x107): 63.162

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 466.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x108x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x108x2048): 62.808
Elapsed time for attention_prob_times_values (384x2048x2048x108): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x108): 104.562

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 873.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x109x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x109x2048): 32.211
Elapsed time for attention_prob_times_values (384x2048x2048x109): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x109): 65.068

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 483.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x110x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x110x2048): 64.608
Elapsed time for attention_prob_times_values (384x2048x2048x110): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x110): 108.624

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 916.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x111x2048): 0.0268
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x111x2048): 13.319
Elapsed time for attention_prob_times_values (384x2048x2048x111): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x111): 64.265

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 251.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0324
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x112x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x112x2048): 94.967
Elapsed time for attention_prob_times_values (384x2048x2048x112): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x112): 122.220

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 1229.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x113x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x113x2048): 33.125
Elapsed time for attention_prob_times_values (384x2048x2048x113): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x113): 67.129

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 514.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x114x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x114x2048): 66.099
Elapsed time for attention_prob_times_values (384x2048x2048x114): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x114): 109.820

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 964.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x115x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x115x2048): 28.601
Elapsed time for attention_prob_times_values (384x2048x2048x115): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x115): 60.405

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 457.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x116x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x116x2048): 68.257
Elapsed time for attention_prob_times_values (384x2048x2048x116): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x116): 115.290

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1018.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x117x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x117x2048): 34.629
Elapsed time for attention_prob_times_values (384x2048x2048x117): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x117): 69.065

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 552.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x118x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x118x2048): 69.172
Elapsed time for attention_prob_times_values (384x2048x2048x118): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x118): 114.564

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1040.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x119x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x119x2048): 33.694
Elapsed time for attention_prob_times_values (384x2048x2048x119): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x119): 68.460

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 548.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x120x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x120x2048): 89.195
Elapsed time for attention_prob_times_values (384x2048x2048x120): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x120): 120.194

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1254.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x121x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x121x2048): 34.127
Elapsed time for attention_prob_times_values (384x2048x2048x121): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x121): 70.820

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 568.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x122x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x122x2048): 70.877
Elapsed time for attention_prob_times_values (384x2048x2048x122): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x122): 114.403

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1088.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x123x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x123x2048): 35.976
Elapsed time for attention_prob_times_values (384x2048x2048x123): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x123): 73.751

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 606.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x124x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x124x2048): 72.949
Elapsed time for attention_prob_times_values (384x2048x2048x124): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x124): 118.743

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1140.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x125x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x125x2048): 36.793
Elapsed time for attention_prob_times_values (384x2048x2048x125): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x125): 73.733

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 624.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x126x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x126x2048): 71.576
Elapsed time for attention_prob_times_values (384x2048x2048x126): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x126): 121.354

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1153.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x127x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x127x2048): 36.440
Elapsed time for attention_prob_times_values (384x2048x2048x127): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x127): 72.923

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 627.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x128x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x128x2048): 109.649
Elapsed time for attention_prob_times_values (384x2048x2048x128): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x128): 139.974

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 1598.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x129x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x129x2048): 33.568
Elapsed time for attention_prob_times_values (384x2048x2048x129): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x129): 54.882

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 545.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x130x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x130x2048): 66.972
Elapsed time for attention_prob_times_values (384x2048x2048x130): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x130): 93.079

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1027.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x131x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x131x2048): 34.301
Elapsed time for attention_prob_times_values (384x2048x2048x131): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x131): 57.588

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 571.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x132x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x132x2048): 71.073
Elapsed time for attention_prob_times_values (384x2048x2048x132): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x132): 91.561

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1070.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x133x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x133x2048): 34.301
Elapsed time for attention_prob_times_values (384x2048x2048x133): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x133): 38.251

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 487.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x134x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x134x2048): 58.758
Elapsed time for attention_prob_times_values (384x2048x2048x134): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x134): 90.840

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 967.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x135x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x135x2048): 32.570
Elapsed time for attention_prob_times_values (384x2048x2048x135): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x135): 49.204

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 535.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x136x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x136x2048): 91.901
Elapsed time for attention_prob_times_values (384x2048x2048x136): 0.0544
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x136): 8.053

Attention duration (in seconds): 0.0592
Attention throughput (in TFLOP/s): 203.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0592
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x137x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x137x2048): 33.338
Elapsed time for attention_prob_times_values (384x2048x2048x137): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x137): 57.836

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 585.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x138x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x138x2048): 74.398
Elapsed time for attention_prob_times_values (384x2048x2048x138): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x138): 92.119

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1147.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x139x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x139x2048): 35.225
Elapsed time for attention_prob_times_values (384x2048x2048x139): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x139): 58.103

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 615.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x140x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x140x2048): 74.148
Elapsed time for attention_prob_times_values (384x2048x2048x140): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x140): 92.382

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1162.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x141x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x141x2048): 34.269
Elapsed time for attention_prob_times_values (384x2048x2048x141): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x141): 58.533

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 614.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x142x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x142x2048): 74.660
Elapsed time for attention_prob_times_values (384x2048x2048x142): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x142): 92.768

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1184.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x143x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x143x2048): 34.506
Elapsed time for attention_prob_times_values (384x2048x2048x143): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x143): 58.597

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 625.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x144x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x144x2048): 107.531
Elapsed time for attention_prob_times_values (384x2048x2048x144): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x144): 132.189

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1719.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x145x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x145x2048): 55.458
Elapsed time for attention_prob_times_values (384x2048x2048x145): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x145): 58.029

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 827.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x146x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x146x2048): 72.277
Elapsed time for attention_prob_times_values (384x2048x2048x146): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x146): 97.147

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1217.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x147x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x147x2048): 54.506
Elapsed time for attention_prob_times_values (384x2048x2048x147): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x147): 58.809

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 836.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x148x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x148x2048): 77.787
Elapsed time for attention_prob_times_values (384x2048x2048x148): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x148): 77.984

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1158.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x149x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x149x2048): 56.429
Elapsed time for attention_prob_times_values (384x2048x2048x149): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x149): 57.891

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 855.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x150x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x150x2048): 78.206
Elapsed time for attention_prob_times_values (384x2048x2048x150): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x150): 96.907

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1303.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x151x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x151x2048): 55.814
Elapsed time for attention_prob_times_values (384x2048x2048x151): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x151): 58.465

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 865.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x152x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x152x2048): 102.790
Elapsed time for attention_prob_times_values (384x2048x2048x152): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x152): 141.231

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1814.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x153x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x153x2048): 57.934
Elapsed time for attention_prob_times_values (384x2048x2048x153): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x153): 58.981

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 896.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x154x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x154x2048): 80.211
Elapsed time for attention_prob_times_values (384x2048x2048x154): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x154): 100.133

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1375.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x155x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x155x2048): 58.083
Elapsed time for attention_prob_times_values (384x2048x2048x155): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x155): 61.817

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 930.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x156x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x156x2048): 80.134
Elapsed time for attention_prob_times_values (384x2048x2048x156): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x156): 100.476

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1393.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x157x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x157x2048): 58.927
Elapsed time for attention_prob_times_values (384x2048x2048x157): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x157): 62.306

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 952.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x158x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x158x2048): 83.724
Elapsed time for attention_prob_times_values (384x2048x2048x158): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x158): 81.452

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1305.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x159x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x159x2048): 54.417
Elapsed time for attention_prob_times_values (384x2048x2048x159): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x159): 60.939

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 914.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x160x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x160x2048): 121.980
Elapsed time for attention_prob_times_values (384x2048x2048x160): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x160): 147.056

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 2133.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x161x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x161x2048): 55.950
Elapsed time for attention_prob_times_values (384x2048x2048x161): 0.0239
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x161): 21.695

Attention duration (in seconds): 0.0332
Attention throughput (in TFLOP/s): 503.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x162x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x162x2048): 78.382
Elapsed time for attention_prob_times_values (384x2048x2048x162): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x162): 104.700

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1451.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x163x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x163x2048): 54.795
Elapsed time for attention_prob_times_values (384x2048x2048x163): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x163): 64.875

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 967.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x164x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x164x2048): 78.031
Elapsed time for attention_prob_times_values (384x2048x2048x164): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x164): 105.197

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1467.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x165x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x165x2048): 54.312
Elapsed time for attention_prob_times_values (384x2048x2048x165): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x165): 64.776

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 973.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x166x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x166x2048): 79.430
Elapsed time for attention_prob_times_values (384x2048x2048x166): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x166): 97.746

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1451.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x167x2048): 0.0221
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x167x2048): 24.347
Elapsed time for attention_prob_times_values (384x2048x2048x167): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x167): 64.112

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 587.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x168x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x168x2048): 106.474
Elapsed time for attention_prob_times_values (384x2048x2048x168): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x168): 155.212

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 2115.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x169x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x169x2048): 58.646
Elapsed time for attention_prob_times_values (384x2048x2048x169): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x169): 64.004

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1030.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x170x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x170x2048): 83.567
Elapsed time for attention_prob_times_values (384x2048x2048x170): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x170): 105.548

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1579.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x171x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x171x2048): 59.246
Elapsed time for attention_prob_times_values (384x2048x2048x171): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x171): 67.629

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1075.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x172x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x172x2048): 82.726
Elapsed time for attention_prob_times_values (384x2048x2048x172): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x172): 104.330

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1580.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x173x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x173x2048): 58.852
Elapsed time for attention_prob_times_values (384x2048x2048x173): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x173): 67.777

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1084.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x174x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x174x2048): 84.131
Elapsed time for attention_prob_times_values (384x2048x2048x174): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x174): 110.901

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1656.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x175x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x175x2048): 59.943
Elapsed time for attention_prob_times_values (384x2048x2048x175): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x175): 66.672

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1098.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x176x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x176x2048): 115.309
Elapsed time for attention_prob_times_values (384x2048x2048x176): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x176): 145.714

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2252.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x177x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x177x2048): 61.082
Elapsed time for attention_prob_times_values (384x2048x2048x177): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x177): 66.487

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1120.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x178x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x178x2048): 83.985
Elapsed time for attention_prob_times_values (384x2048x2048x178): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x178): 112.317

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1699.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x179x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x179x2048): 60.000
Elapsed time for attention_prob_times_values (384x2048x2048x179): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x179): 68.980

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1141.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x180x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x180x2048): 86.230
Elapsed time for attention_prob_times_values (384x2048x2048x180): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x180): 67.703

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1355.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x181x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x181x2048): 61.152
Elapsed time for attention_prob_times_values (384x2048x2048x181): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x181): 65.887

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1139.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x182x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x182x2048): 87.077
Elapsed time for attention_prob_times_values (384x2048x2048x182): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x182): 107.873

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1740.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x183x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x183x2048): 61.430
Elapsed time for attention_prob_times_values (384x2048x2048x183): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x183): 68.467

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1175.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x184x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x184x2048): 111.016
Elapsed time for attention_prob_times_values (384x2048x2048x184): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x184): 162.589

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 2407.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x185x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x185x2048): 62.105
Elapsed time for attention_prob_times_values (384x2048x2048x185): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x185): 67.396

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1185.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x186x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x186x2048): 86.465
Elapsed time for attention_prob_times_values (384x2048x2048x186): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x186): 113.347

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1808.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x187x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x187x2048): 61.894
Elapsed time for attention_prob_times_values (384x2048x2048x187): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x187): 72.267

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1235.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x188x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x188x2048): 89.491
Elapsed time for attention_prob_times_values (384x2048x2048x188): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x188): 118.494

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1899.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x189x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x189x2048): 62.244
Elapsed time for attention_prob_times_values (384x2048x2048x189): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x189): 73.445

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1261.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x190x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x190x2048): 89.776
Elapsed time for attention_prob_times_values (384x2048x2048x190): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x190): 118.900

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1924.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x191x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x191x2048): 63.328
Elapsed time for attention_prob_times_values (384x2048x2048x191): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x191): 72.494

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1278.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x192x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x192x2048): 132.432
Elapsed time for attention_prob_times_values (384x2048x2048x192): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x192): 148.914

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2663.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x193x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x193x2048): 61.053
Elapsed time for attention_prob_times_values (384x2048x2048x193): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x193): 71.649

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1258.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x194x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x194x2048): 85.780
Elapsed time for attention_prob_times_values (384x2048x2048x194): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x194): 119.887

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1918.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x195x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x195x2048): 60.882
Elapsed time for attention_prob_times_values (384x2048x2048x195): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x195): 74.764

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1294.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x196x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x196x2048): 86.574
Elapsed time for attention_prob_times_values (384x2048x2048x196): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x196): 118.931

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1941.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x197x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x197x2048): 61.242
Elapsed time for attention_prob_times_values (384x2048x2048x197): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x197): 75.247

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1314.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x198x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x198x2048): 84.047
Elapsed time for attention_prob_times_values (384x2048x2048x198): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x198): 117.899

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1919.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x199x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x199x2048): 61.750
Elapsed time for attention_prob_times_values (384x2048x2048x199): 0.0249
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x199): 25.719

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 713.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0353
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x200x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x200x2048): 96.299
Elapsed time for attention_prob_times_values (384x2048x2048x200): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x200): 176.866

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2462.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x201x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x201x2048): 61.471
Elapsed time for attention_prob_times_values (384x2048x2048x201): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x201): 71.620

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1312.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x202x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x202x2048): 84.375
Elapsed time for attention_prob_times_values (384x2048x2048x202): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x202): 119.350

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1971.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x203x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x203x2048): 62.253
Elapsed time for attention_prob_times_values (384x2048x2048x203): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x203): 73.477

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1350.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x204x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x204x2048): 89.624
Elapsed time for attention_prob_times_values (384x2048x2048x204): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x204): 103.438

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1932.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x205x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x205x2048): 63.139
Elapsed time for attention_prob_times_values (384x2048x2048x205): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x205): 74.275

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1380.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x206x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x206x2048): 88.672
Elapsed time for attention_prob_times_values (384x2048x2048x206): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x206): 113.397

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 2021.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x207x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x207x2048): 61.579
Elapsed time for attention_prob_times_values (384x2048x2048x207): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x207): 74.693

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1377.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x208x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x208x2048): 129.582
Elapsed time for attention_prob_times_values (384x2048x2048x208): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x208): 181.846

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 3102.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x209x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x209x2048): 60.907
Elapsed time for attention_prob_times_values (384x2048x2048x209): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x209): 74.503

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1380.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x210x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x210x2048): 91.486
Elapsed time for attention_prob_times_values (384x2048x2048x210): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x210): 120.987

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 2155.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x211x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x211x2048): 63.534
Elapsed time for attention_prob_times_values (384x2048x2048x211): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x211): 73.443

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1415.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x212x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x212x2048): 93.687
Elapsed time for attention_prob_times_values (384x2048x2048x212): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x212): 122.862

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 2219.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x213x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x213x2048): 63.992
Elapsed time for attention_prob_times_values (384x2048x2048x213): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x213): 76.928

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1465.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x214x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x214x2048): 93.862
Elapsed time for attention_prob_times_values (384x2048x2048x214): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x214): 122.534

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 2238.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x215x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x215x2048): 65.109
Elapsed time for attention_prob_times_values (384x2048x2048x215): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x215): 76.904

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1491.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x216x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x216x2048): 120.817
Elapsed time for attention_prob_times_values (384x2048x2048x216): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x216): 183.312

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 3094.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x217x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x217x2048): 65.074
Elapsed time for attention_prob_times_values (384x2048x2048x217): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x217): 76.085

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1497.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x218x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x218x2048): 92.984
Elapsed time for attention_prob_times_values (384x2048x2048x218): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x218): 124.418

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 2281.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x219x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x219x2048): 66.005
Elapsed time for attention_prob_times_values (384x2048x2048x219): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x219): 79.379

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1551.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x220x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x220x2048): 95.661
Elapsed time for attention_prob_times_values (384x2048x2048x220): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x220): 128.059

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2368.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x221x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x221x2048): 66.517
Elapsed time for attention_prob_times_values (384x2048x2048x221): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x221): 80.569

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1582.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x222x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x222x2048): 98.396
Elapsed time for attention_prob_times_values (384x2048x2048x222): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x222): 128.097

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2427.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x223x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x223x2048): 68.081
Elapsed time for attention_prob_times_values (384x2048x2048x223): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x223): 79.650

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1608.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x224x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x224x2048): 140.008
Elapsed time for attention_prob_times_values (384x2048x2048x224): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x224): 200.917

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 3630.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x225x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x225x2048): 65.818
Elapsed time for attention_prob_times_values (384x2048x2048x225): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x225): 81.686

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1610.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x226x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x226x2048): 93.184
Elapsed time for attention_prob_times_values (384x2048x2048x226): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x226): 129.460

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 2404.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x227x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x227x2048): 66.027
Elapsed time for attention_prob_times_values (384x2048x2048x227): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x227): 82.683

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1635.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x228x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x228x2048): 93.172
Elapsed time for attention_prob_times_values (384x2048x2048x228): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x228): 131.425

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2439.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x229x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x229x2048): 66.116
Elapsed time for attention_prob_times_values (384x2048x2048x229): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x229): 82.926

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1653.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x230x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x230x2048): 94.009
Elapsed time for attention_prob_times_values (384x2048x2048x230): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x230): 129.608

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 2458.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x231x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x231x2048): 66.451
Elapsed time for attention_prob_times_values (384x2048x2048x231): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x231): 82.783

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1670.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x232x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x232x2048): 121.960
Elapsed time for attention_prob_times_values (384x2048x2048x232): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x232): 203.539

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 3469.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x233x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x233x2048): 66.650
Elapsed time for attention_prob_times_values (384x2048x2048x233): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x233): 82.164

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1681.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x234x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x234x2048): 96.209
Elapsed time for attention_prob_times_values (384x2048x2048x234): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x234): 134.048

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2569.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x235x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x235x2048): 67.879
Elapsed time for attention_prob_times_values (384x2048x2048x235): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x235): 86.504

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1751.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x236x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x236x2048): 97.706
Elapsed time for attention_prob_times_values (384x2048x2048x236): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x236): 138.212

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 2647.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x237x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x237x2048): 68.448
Elapsed time for attention_prob_times_values (384x2048x2048x237): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x237): 86.130

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1771.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x238x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x238x2048): 97.244
Elapsed time for attention_prob_times_values (384x2048x2048x238): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x238): 136.594

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2648.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x239x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x239x2048): 68.337
Elapsed time for attention_prob_times_values (384x2048x2048x239): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x239): 85.214

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1775.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x240x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x240x2048): 136.112
Elapsed time for attention_prob_times_values (384x2048x2048x240): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x240): 199.938

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 3806.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x241x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x241x2048): 69.072
Elapsed time for attention_prob_times_values (384x2048x2048x241): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x241): 85.954

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1807.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x242x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x242x2048): 98.497
Elapsed time for attention_prob_times_values (384x2048x2048x242): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x242): 140.092

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2739.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x243x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x243x2048): 69.367
Elapsed time for attention_prob_times_values (384x2048x2048x243): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x243): 89.092

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1854.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x244x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x244x2048): 99.236
Elapsed time for attention_prob_times_values (384x2048x2048x244): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x244): 140.408

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2776.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x245x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x245x2048): 70.318
Elapsed time for attention_prob_times_values (384x2048x2048x245): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x245): 75.140

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1741.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x246x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x246x2048): 100.799
Elapsed time for attention_prob_times_values (384x2048x2048x246): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x246): 142.665

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 2842.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x247x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x247x2048): 69.923
Elapsed time for attention_prob_times_values (384x2048x2048x247): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x247): 88.922

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1891.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x248x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x248x2048): 125.456
Elapsed time for attention_prob_times_values (384x2048x2048x248): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x248): 197.633

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 3721.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x249x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x249x2048): 69.958
Elapsed time for attention_prob_times_values (384x2048x2048x249): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x249): 87.148

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1889.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x250x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x250x2048): 97.867
Elapsed time for attention_prob_times_values (384x2048x2048x250): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x250): 144.088

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 2848.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x251x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x251x2048): 72.012
Elapsed time for attention_prob_times_values (384x2048x2048x251): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x251): 89.688

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1959.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x252x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x252x2048): 104.161
Elapsed time for attention_prob_times_values (384x2048x2048x252): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x252): 145.092

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 2986.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x253x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x253x2048): 73.172
Elapsed time for attention_prob_times_values (384x2048x2048x253): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x253): 93.681

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2031.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x254x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x254x2048): 104.988
Elapsed time for attention_prob_times_values (384x2048x2048x254): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x254): 146.530

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3035.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x255x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x255x2048): 72.308
Elapsed time for attention_prob_times_values (384x2048x2048x255): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x255): 93.203

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2028.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x256x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x256x2048): 148.038
Elapsed time for attention_prob_times_values (384x2048x2048x256): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x256): 221.460

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 4436.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x257x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x257x2048): 70.039
Elapsed time for attention_prob_times_values (384x2048x2048x257): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x257): 55.443

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1553.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x258x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x258x2048): 100.909
Elapsed time for attention_prob_times_values (384x2048x2048x258): 0.0284
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x258): 29.251

Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 1142.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x259x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x259x2048): 70.284
Elapsed time for attention_prob_times_values (384x2048x2048x259): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x259): 56.694

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1586.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x260x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x260x2048): 99.606
Elapsed time for attention_prob_times_values (384x2048x2048x260): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x260): 94.296

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2458.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x261x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x261x2048): 70.097
Elapsed time for attention_prob_times_values (384x2048x2048x261): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x261): 56.485

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1593.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x262x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x262x2048): 102.747
Elapsed time for attention_prob_times_values (384x2048x2048x262): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x262): 94.154

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2511.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x263x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x263x2048): 42.195
Elapsed time for attention_prob_times_values (384x2048x2048x263): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x263): 56.089

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1235.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x264x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x264x2048): 125.815
Elapsed time for attention_prob_times_values (384x2048x2048x264): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x264): 130.101

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3293.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x265x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x265x2048): 69.144
Elapsed time for attention_prob_times_values (384x2048x2048x265): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x265): 56.023

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1599.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x266x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x266x2048): 101.841
Elapsed time for attention_prob_times_values (384x2048x2048x266): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x266): 95.429

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2555.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x267x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x267x2048): 69.047
Elapsed time for attention_prob_times_values (384x2048x2048x267): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x267): 57.488

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1633.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x268x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x268x2048): 103.466
Elapsed time for attention_prob_times_values (384x2048x2048x268): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x268): 97.140

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2617.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x269x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x269x2048): 70.684
Elapsed time for attention_prob_times_values (384x2048x2048x269): 0.0215
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x269): 40.233

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1344.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x270x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x270x2048): 102.822
Elapsed time for attention_prob_times_values (384x2048x2048x270): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x270): 96.618

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2621.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x271x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x271x2048): 69.291
Elapsed time for attention_prob_times_values (384x2048x2048x271): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x271): 57.016

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1651.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x272x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x272x2048): 142.098
Elapsed time for attention_prob_times_values (384x2048x2048x272): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x272): 134.333

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3659.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x273x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x273x2048): 71.071
Elapsed time for attention_prob_times_values (384x2048x2048x273): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x273): 57.246

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1686.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x274x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x274x2048): 103.372
Elapsed time for attention_prob_times_values (384x2048x2048x274): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x274): 95.985

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2656.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x275x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x275x2048): 71.769
Elapsed time for attention_prob_times_values (384x2048x2048x275): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x275): 58.234

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1721.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x276x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x276x2048): 105.264
Elapsed time for attention_prob_times_values (384x2048x2048x276): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x276): 98.520

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2735.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x277x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x277x2048): 72.188
Elapsed time for attention_prob_times_values (384x2048x2048x277): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x277): 51.878

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1628.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x278x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x278x2048): 105.636
Elapsed time for attention_prob_times_values (384x2048x2048x278): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x278): 98.232

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2754.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x279x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x279x2048): 72.297
Elapsed time for attention_prob_times_values (384x2048x2048x279): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x279): 57.845

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1745.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x280x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x280x2048): 128.077
Elapsed time for attention_prob_times_values (384x2048x2048x280): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x280): 135.748

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 3591.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x281x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x281x2048): 72.438
Elapsed time for attention_prob_times_values (384x2048x2048x281): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x281): 58.103

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1763.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x282x2048): 0.0327
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x282x2048): 27.822
Elapsed time for attention_prob_times_values (384x2048x2048x282): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x282): 98.499

Attention duration (in seconds): 0.0419
Attention throughput (in TFLOP/s): 1190.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0419
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x283x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x283x2048): 73.161
Elapsed time for attention_prob_times_values (384x2048x2048x283): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x283): 59.556

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1807.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x284x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x284x2048): 106.606
Elapsed time for attention_prob_times_values (384x2048x2048x284): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x284): 99.623

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2845.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x285x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x285x2048): 73.360
Elapsed time for attention_prob_times_values (384x2048x2048x285): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x285): 59.945

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1828.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x286x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x286x2048): 108.171
Elapsed time for attention_prob_times_values (384x2048x2048x286): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x286): 55.637

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2043.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x287x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x287x2048): 74.518
Elapsed time for attention_prob_times_values (384x2048x2048x287): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x287): 59.451

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1845.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x288x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x288x2048): 152.724
Elapsed time for attention_prob_times_values (384x2048x2048x288): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x288): 139.817

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 4087.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x289x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x289x2048): 72.065
Elapsed time for attention_prob_times_values (384x2048x2048x289): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x289): 59.479

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1830.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x290x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x290x2048): 103.594
Elapsed time for attention_prob_times_values (384x2048x2048x290): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x290): 100.754

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2879.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x291x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x291x2048): 71.317
Elapsed time for attention_prob_times_values (384x2048x2048x291): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x291): 60.794

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1856.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x292x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x292x2048): 104.336
Elapsed time for attention_prob_times_values (384x2048x2048x292): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x292): 100.210

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2900.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x293x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x293x2048): 69.581
Elapsed time for attention_prob_times_values (384x2048x2048x293): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x293): 60.355

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1840.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x294x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x294x2048): 102.391
Elapsed time for attention_prob_times_values (384x2048x2048x294): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x294): 102.696

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2928.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x295x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x295x2048): 71.612
Elapsed time for attention_prob_times_values (384x2048x2048x295): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x295): 55.545

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1792.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x296x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x296x2048): 131.453
Elapsed time for attention_prob_times_values (384x2048x2048x296): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x296): 141.384

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 3916.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x297x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x297x2048): 71.057
Elapsed time for attention_prob_times_values (384x2048x2048x297): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x297): 60.098

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 1878.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x298x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x298x2048): 101.039
Elapsed time for attention_prob_times_values (384x2048x2048x298): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x298): 104.477

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2972.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x299x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x299x2048): 70.638
Elapsed time for attention_prob_times_values (384x2048x2048x299): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x299): 61.797

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1913.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x300x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x300x2048): 106.209
Elapsed time for attention_prob_times_values (384x2048x2048x300): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x300): 103.740

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 3056.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x301x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x301x2048): 70.432
Elapsed time for attention_prob_times_values (384x2048x2048x301): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x301): 61.652

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1921.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x302x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x302x2048): 105.915
Elapsed time for attention_prob_times_values (384x2048x2048x302): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x302): 105.021

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 3091.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x303x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x303x2048): 70.322
Elapsed time for attention_prob_times_values (384x2048x2048x303): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x303): 47.462

Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 1666.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0344
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x304x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x304x2048): 149.961
Elapsed time for attention_prob_times_values (384x2048x2048x304): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x304): 144.353

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 4339.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x305x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x305x2048): 69.506
Elapsed time for attention_prob_times_values (384x2048x2048x305): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x305): 58.749

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1884.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x306x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x306x2048): 105.997
Elapsed time for attention_prob_times_values (384x2048x2048x306): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x306): 105.945

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 3146.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x307x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x307x2048): 70.126
Elapsed time for attention_prob_times_values (384x2048x2048x307): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x307): 62.423

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1967.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x308x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x308x2048): 106.827
Elapsed time for attention_prob_times_values (384x2048x2048x308): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x308): 105.695

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 3174.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x309x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x309x2048): 70.484
Elapsed time for attention_prob_times_values (384x2048x2048x309): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x309): 62.875

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1991.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x310x2048): 0.0228
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x310x2048): 43.880
Elapsed time for attention_prob_times_values (384x2048x2048x310): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x310): 106.709

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 1869.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x311x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x311x2048): 69.424
Elapsed time for attention_prob_times_values (384x2048x2048x311): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x311): 61.680

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1969.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x312x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x312x2048): 136.213
Elapsed time for attention_prob_times_values (384x2048x2048x312): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x312): 147.478

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4284.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x313x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x313x2048): 70.146
Elapsed time for attention_prob_times_values (384x2048x2048x313): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x313): 62.527

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2006.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x314x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x314x2048): 108.785
Elapsed time for attention_prob_times_values (384x2048x2048x314): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x314): 106.606

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3277.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x315x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x315x2048): 70.392
Elapsed time for attention_prob_times_values (384x2048x2048x315): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x315): 64.042

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2047.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x316x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x316x2048): 107.852
Elapsed time for attention_prob_times_values (384x2048x2048x316): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x316): 101.325

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3199.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x317x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x317x2048): 71.133
Elapsed time for attention_prob_times_values (384x2048x2048x317): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x317): 64.428

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2077.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x318x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x318x2048): 111.726
Elapsed time for attention_prob_times_values (384x2048x2048x318): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x318): 108.165

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 3386.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x319x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x319x2048): 69.856
Elapsed time for attention_prob_times_values (384x2048x2048x319): 0.0321
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x319): 32.048

Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 1357.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0468
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x320x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x320x2048): 156.104
Elapsed time for attention_prob_times_values (384x2048x2048x320): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x320): 159.408

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 4889.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x321x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x321x2048): 70.163
Elapsed time for attention_prob_times_values (384x2048x2048x321): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x321): 64.261

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2085.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x322x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x322x2048): 107.650
Elapsed time for attention_prob_times_values (384x2048x2048x322): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x322): 110.200

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3396.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x323x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x323x2048): 69.374
Elapsed time for attention_prob_times_values (384x2048x2048x323): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x323): 64.605

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2092.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x324x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x324x2048): 107.576
Elapsed time for attention_prob_times_values (384x2048x2048x324): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x324): 108.410

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3388.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x325x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x325x2048): 70.361
Elapsed time for attention_prob_times_values (384x2048x2048x325): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x325): 64.885

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2124.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x326x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x326x2048): 108.179
Elapsed time for attention_prob_times_values (384x2048x2048x326): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x326): 103.409

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3337.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x327x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x327x2048): 69.659
Elapsed time for attention_prob_times_values (384x2048x2048x327): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x327): 62.166

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 2079.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x328x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x328x2048): 136.263
Elapsed time for attention_prob_times_values (384x2048x2048x328): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x328): 152.785

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4573.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x329x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x329x2048): 71.586
Elapsed time for attention_prob_times_values (384x2048x2048x329): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x329): 64.818

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2166.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x330x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x330x2048): 109.158
Elapsed time for attention_prob_times_values (384x2048x2048x330): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x330): 111.317

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3520.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x331x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x331x2048): 72.172
Elapsed time for attention_prob_times_values (384x2048x2048x331): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x331): 66.425

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2215.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x332x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x332x2048): 111.682
Elapsed time for attention_prob_times_values (384x2048x2048x332): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x332): 110.531

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3569.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x333x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x333x2048): 72.523
Elapsed time for attention_prob_times_values (384x2048x2048x333): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x333): 66.482

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2235.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x334x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x334x2048): 110.166
Elapsed time for attention_prob_times_values (384x2048x2048x334): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x334): 111.798

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3585.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x335x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x335x2048): 73.439
Elapsed time for attention_prob_times_values (384x2048x2048x335): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x335): 65.968

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2252.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x336x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x336x2048): 146.044
Elapsed time for attention_prob_times_values (384x2048x2048x336): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x336): 157.451

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4924.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x337x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x337x2048): 75.157
Elapsed time for attention_prob_times_values (384x2048x2048x337): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x337): 67.595

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2319.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x338x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x338x2048): 111.752
Elapsed time for attention_prob_times_values (384x2048x2048x338): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x338): 112.093

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3658.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x339x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x339x2048): 74.392
Elapsed time for attention_prob_times_values (384x2048x2048x339): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x339): 67.646

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2322.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x340x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x340x2048): 113.964
Elapsed time for attention_prob_times_values (384x2048x2048x340): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x340): 114.381

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3753.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x341x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x341x2048): 75.102
Elapsed time for attention_prob_times_values (384x2048x2048x341): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x341): 68.029

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2353.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x342x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x342x2048): 115.031
Elapsed time for attention_prob_times_values (384x2048x2048x342): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x342): 112.486

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3760.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x1x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x1x2048): 0.924
Elapsed time for attention_prob_times_values (512x2048x2048x1): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x1): 1.407

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x2x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x2x2048): 1.212
Elapsed time for attention_prob_times_values (512x2048x2048x2): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x2): 2.815

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x3x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x3x2048): 2.720
Elapsed time for attention_prob_times_values (512x2048x2048x3): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x3): 3.087

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 3.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x4x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x4x2048): 2.384
Elapsed time for attention_prob_times_values (512x2048x2048x4): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x4): 5.321

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 4.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x5x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x5x2048): 4.650
Elapsed time for attention_prob_times_values (512x2048x2048x5): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x5): 5.099

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 7.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x6x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x6x2048): 3.353
Elapsed time for attention_prob_times_values (512x2048x2048x6): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x6): 8.260

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 8.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x7x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x7x2048): 6.338
Elapsed time for attention_prob_times_values (512x2048x2048x7): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x7): 6.952

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 12.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x8x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x8x2048): 9.400
Elapsed time for attention_prob_times_values (512x2048x2048x8): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x8): 10.607

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 19.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x9x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x9x2048): 6.199
Elapsed time for attention_prob_times_values (512x2048x2048x9): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x9): 8.865

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 15.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x10x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x10x2048): 5.960
Elapsed time for attention_prob_times_values (512x2048x2048x10): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x10): 13.298

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 18.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x11x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x11x2048): 7.659
Elapsed time for attention_prob_times_values (512x2048x2048x11): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x11): 10.703

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 21.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x12x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x12x2048): 7.179
Elapsed time for attention_prob_times_values (512x2048x2048x12): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x12): 16.063

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 24.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x13x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x13x2048): 8.848
Elapsed time for attention_prob_times_values (512x2048x2048x13): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x13): 10.956

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 25.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x14x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x14x2048): 8.524
Elapsed time for attention_prob_times_values (512x2048x2048x14): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x14): 18.165

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 31.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x15x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x15x2048): 10.175
Elapsed time for attention_prob_times_values (512x2048x2048x15): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x15): 15.134

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 34.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x16x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x16x2048): 18.463
Elapsed time for attention_prob_times_values (512x2048x2048x16): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x16): 21.757

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 59.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x17x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x17x2048): 11.501
Elapsed time for attention_prob_times_values (512x2048x2048x17): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x17): 15.939

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 41.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x18x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x18x2048): 10.891
Elapsed time for attention_prob_times_values (512x2048x2048x18): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x18): 22.301

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 47.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x19x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x19x2048): 5.609
Elapsed time for attention_prob_times_values (512x2048x2048x19): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x19): 17.403

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 28.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x20x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x20x2048): 11.911
Elapsed time for attention_prob_times_values (512x2048x2048x20): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x20): 24.237

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 55.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x21x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x21x2048): 14.198
Elapsed time for attention_prob_times_values (512x2048x2048x21): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x21): 18.246

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 57.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x22x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x22x2048): 13.355
Elapsed time for attention_prob_times_values (512x2048x2048x22): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x22): 27.965

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 67.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x23x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x23x2048): 15.663
Elapsed time for attention_prob_times_values (512x2048x2048x23): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x23): 22.000

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 70.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x24x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x24x2048): 29.510
Elapsed time for attention_prob_times_values (512x2048x2048x24): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x24): 30.261

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 119.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x25x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x25x2048): 17.051
Elapsed time for attention_prob_times_values (512x2048x2048x25): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x25): 22.988

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 80.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x26x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x26x2048): 15.769
Elapsed time for attention_prob_times_values (512x2048x2048x26): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x26): 28.440

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 86.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x27x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x27x2048): 17.704
Elapsed time for attention_prob_times_values (512x2048x2048x27): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x27): 23.290

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 88.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x28x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x28x2048): 16.185
Elapsed time for attention_prob_times_values (512x2048x2048x28): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x28): 30.596

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 95.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x29x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x29x2048): 19.200
Elapsed time for attention_prob_times_values (512x2048x2048x29): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x29): 25.522

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 101.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x30x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x30x2048): 17.556
Elapsed time for attention_prob_times_values (512x2048x2048x30): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x30): 33.953

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 109.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x31x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x31x2048): 20.389
Elapsed time for attention_prob_times_values (512x2048x2048x31): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x31): 27.668

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 114.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x32x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x32x2048): 39.085
Elapsed time for attention_prob_times_values (512x2048x2048x32): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x32): 42.047

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 202.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x33x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x33x2048): 18.847
Elapsed time for attention_prob_times_values (512x2048x2048x33): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x33): 24.063

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 108.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x34x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x34x2048): 14.870
Elapsed time for attention_prob_times_values (512x2048x2048x34): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x34): 38.259

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 112.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x35x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x35x2048): 19.701
Elapsed time for attention_prob_times_values (512x2048x2048x35): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x35): 24.885

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 118.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x36x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x36x2048): 15.014
Elapsed time for attention_prob_times_values (512x2048x2048x36): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x36): 40.341

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 120.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x37x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x37x2048): 20.224
Elapsed time for attention_prob_times_values (512x2048x2048x37): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x37): 26.299

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 128.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x38x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x38x2048): 16.244
Elapsed time for attention_prob_times_values (512x2048x2048x38): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x38): 42.463

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 135.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x39x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x39x2048): 21.341
Elapsed time for attention_prob_times_values (512x2048x2048x39): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x39): 26.440

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 138.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x40x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x40x2048): 39.115
Elapsed time for attention_prob_times_values (512x2048x2048x40): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x40): 51.825

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 267.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x41x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x41x2048): 21.828
Elapsed time for attention_prob_times_values (512x2048x2048x41): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x41): 28.376

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 151.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x42x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x42x2048): 17.714
Elapsed time for attention_prob_times_values (512x2048x2048x42): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x42): 46.446

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 160.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x43x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x43x2048): 22.246
Elapsed time for attention_prob_times_values (512x2048x2048x43): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x43): 30.421

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 163.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x44x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x44x2048): 17.710
Elapsed time for attention_prob_times_values (512x2048x2048x44): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x44): 46.667

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 166.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x45x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x45x2048): 23.687
Elapsed time for attention_prob_times_values (512x2048x2048x45): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x45): 31.384

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 178.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x46x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x46x2048): 18.389
Elapsed time for attention_prob_times_values (512x2048x2048x46): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x46): 50.079

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 181.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x47x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x47x2048): 24.335
Elapsed time for attention_prob_times_values (512x2048x2048x47): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x47): 31.975

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 190.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x48x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x48x2048): 43.577
Elapsed time for attention_prob_times_values (512x2048x2048x48): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x48): 61.772

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 357.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x49x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x49x2048): 25.081
Elapsed time for attention_prob_times_values (512x2048x2048x49): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x49): 32.757

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 202.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x50x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x50x2048): 19.568
Elapsed time for attention_prob_times_values (512x2048x2048x50): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x50): 54.865

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 209.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x51x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x51x2048): 26.060
Elapsed time for attention_prob_times_values (512x2048x2048x51): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x51): 35.801

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 222.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x52x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x52x2048): 19.779
Elapsed time for attention_prob_times_values (512x2048x2048x52): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x52): 56.680

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 219.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x53x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x53x2048): 27.403
Elapsed time for attention_prob_times_values (512x2048x2048x53): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x53): 36.903

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 239.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x54x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x54x2048): 21.023
Elapsed time for attention_prob_times_values (512x2048x2048x54): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x54): 58.707

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 239.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x55x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x55x2048): 27.217
Elapsed time for attention_prob_times_values (512x2048x2048x55): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x55): 36.882

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 246.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x56x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x56x2048): 53.668
Elapsed time for attention_prob_times_values (512x2048x2048x56): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x56): 70.769

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 488.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x57x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x57x2048): 15.686
Elapsed time for attention_prob_times_values (512x2048x2048x57): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x57): 37.959

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 180.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x58x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x58x2048): 22.218
Elapsed time for attention_prob_times_values (512x2048x2048x58): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x58): 62.893

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 270.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x59x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x59x2048): 29.542
Elapsed time for attention_prob_times_values (512x2048x2048x59): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x59): 40.612

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 286.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x60x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x60x2048): 21.108
Elapsed time for attention_prob_times_values (512x2048x2048x60): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x60): 64.587

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 270.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x61x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x61x2048): 29.362
Elapsed time for attention_prob_times_values (512x2048x2048x61): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x61): 41.738

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 297.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x62x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x62x2048): 22.785
Elapsed time for attention_prob_times_values (512x2048x2048x62): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x62): 66.788

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 297.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x63x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x63x2048): 29.849
Elapsed time for attention_prob_times_values (512x2048x2048x63): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x63): 41.355

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 307.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 57.194
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 80.912

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 603.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 26.642
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 40.579

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 293.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 19.780
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 68.905

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 284.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 27.585
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 41.929

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 311.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 19.069
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 69.412

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 284.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 28.140
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 43.367

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 328.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 20.104
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 73.666

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 307.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 27.650
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 44.511

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 336.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 66.785
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 85.057

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 748.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 28.350
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 45.004

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 352.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 51.256
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 76.210

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 628.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 29.677
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 45.961

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 374.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 52.256
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 78.738

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 659.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 29.632
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 48.254

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 390.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 53.072
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 80.023

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 686.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 30.030
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 48.396

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 403.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 74.702
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 93.831

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 914.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 30.329
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 50.280

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 420.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 56.316
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 83.788

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 757.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 31.042
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 50.047

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 435.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 57.191
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 86.652

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 792.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 31.545
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 52.792

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 459.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 58.743
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 87.542

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 826.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 31.560
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 53.682

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 472.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 74.816
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 101.613

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1034.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 32.090
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 54.712

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 490.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 59.811
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 91.723

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 886.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 32.598
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 54.622

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 505.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 60.947
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 90.465

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 910.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 33.180
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 56.316

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 527.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 63.603
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 92.594

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 961.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 34.795
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 57.674

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 558.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 90.213
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 110.461

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 1291.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 31.991
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 58.297

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 542.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 58.865
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 97.273

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 971.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 31.058
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 61.016

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 550.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 60.669
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 100.381

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1020.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 30.960
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 60.715

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 558.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 60.882
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 101.675

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1047.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 29.862
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 62.375

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 560.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 83.296
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 94.345

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1238.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 31.421
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 62.604

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 591.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 64.022
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 105.307

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1134.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 31.814
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 64.544

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 612.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 57.612
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 105.668

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1081.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 31.801
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 66.020

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 627.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 66.079
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 109.880

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1217.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 31.378
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 67.051

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 635.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 92.978
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 125.349

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1601.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 31.025
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 66.748

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 640.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 66.937
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 114.492

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1288.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 34.034
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 69.106

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 701.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 69.600
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 114.015

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1339.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 34.789
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 70.600

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 728.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 68.833
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 116.989

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1365.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 33.776
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 70.923

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 726.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 89.224
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 133.654

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1712.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 34.423
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 72.847

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 753.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 71.789
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 118.695

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1453.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 36.035
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 75.238

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 797.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 73.160
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 121.467

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1506.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 37.117
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 74.467

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 823.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 74.457
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 125.314

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1564.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 36.831
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 76.210

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 838.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 109.132
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 143.345

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 2106.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 33.847
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 55.379

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 719.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 71.238
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 93.578

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1395.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 34.470
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 58.693

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 754.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 72.552
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 94.869

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1438.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 34.280
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 59.405

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 766.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 73.287
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 91.896

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1447.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 33.229
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 57.589

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 753.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 97.340
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 116.090

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1906.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 33.388
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 57.783

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 767.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 74.744
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 93.262

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1514.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 35.168
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 59.901

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 814.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 74.973
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 93.609

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1540.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 35.003
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 58.968

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 818.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 76.139
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 91.378

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1557.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 34.924
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 58.836

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 827.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 105.523
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 134.960

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2250.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 56.211
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 55.884

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1071.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 75.215
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 93.495

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1604.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 57.565
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 59.606

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1134.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 77.793
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 96.859

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1682.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 58.061
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 60.295

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1160.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 80.484
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 99.403

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1756.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 58.302
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0226
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 28.676

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 764.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 102.889
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 134.133

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 2329.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 55.191
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 58.017

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1138.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 80.810
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 100.355

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1812.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 59.131
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 62.956

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1242.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 76.852
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 102.247

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1798.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 59.040
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 62.621

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1253.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 81.630
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 103.066

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1890.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 60.292
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 62.400

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1280.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 120.968
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 147.894

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2794.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 57.952
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 63.363

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1278.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 79.727
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 105.584

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1930.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 57.560
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 65.564

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1310.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 80.820
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 106.625

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1976.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 57.882
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 63.616

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1310.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 81.700
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 107.133

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 2016.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 58.813
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 63.813

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1338.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 105.026
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 145.844

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 2686.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 58.898
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 64.935

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1366.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 82.564
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 109.931

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2098.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 60.123
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 67.688

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1424.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 84.383
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 111.053

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 2157.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 59.667
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 68.897

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1446.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 84.997
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 111.576

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 2195.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 59.732
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 67.782

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1452.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 120.861
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 157.722

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3147.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 61.265
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 67.831

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1488.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 85.696
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 113.234

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 2268.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 61.453
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 70.704

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1537.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 87.272
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 115.243

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 2334.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 61.514
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 70.910

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1556.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 87.746
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 116.317

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 2375.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 61.677
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 68.876

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1553.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 112.720
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 162.770

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 3196.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 61.666
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 69.658

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1578.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 89.826
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 116.258

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 2457.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 63.082
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 73.376

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1653.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 91.173
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 116.763

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 2508.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 63.342
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 73.354

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1674.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 92.377
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 116.953

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 2554.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 63.735
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 73.459

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1697.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 130.112
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 168.523

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 3671.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 61.654
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 73.861

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1688.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 88.063
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 121.334

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2576.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 61.951
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 76.111

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1733.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 88.576
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 119.176

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 2591.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 62.425
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 76.188

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1758.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 89.158
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 117.640

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2612.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 62.329
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 73.967

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1750.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 116.598
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 170.745

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 3602.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 62.830
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 73.705

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1772.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 90.493
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 120.890

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2717.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 63.865
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 76.485

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1835.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 91.435
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 120.938

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2759.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 64.107
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 76.651

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1858.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 91.372
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 121.559

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2790.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 64.394
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 76.127

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1875.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 132.609
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 175.331

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 4077.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 64.883
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 75.351

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1891.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 92.237
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 122.175

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2864.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 64.352
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 77.866

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1929.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 94.079
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 125.104

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2953.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 65.304
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 78.274

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1966.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 95.166
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 124.357

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2992.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 65.382
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 77.544

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1977.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 122.698
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 185.636

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 4136.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 65.913
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 76.904

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1996.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 96.787
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 124.865

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 3080.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 67.036
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 54.183

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1700.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 95.789
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 125.005

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 3091.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 67.083
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 81.921

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2111.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 98.716
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 127.999

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 3204.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 68.938
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 81.300

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2154.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 140.944
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 189.857

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 4691.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 65.966
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 80.663

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2113.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 93.386
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 129.711

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 3176.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 66.362
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 82.836

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2164.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 95.085
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 131.808

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3259.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 67.198
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 84.324

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2215.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 96.026
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 131.550

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 3302.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 67.243
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 82.994

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2219.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 122.074
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 187.583

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4436.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 66.930
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 82.586

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2227.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 98.187
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 135.325

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3442.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 67.979
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 87.042

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2318.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 97.816
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 135.906

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 3469.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 68.915
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 87.633

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2362.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 98.935
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 132.972

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 3488.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.027
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 86.676

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2372.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 139.376
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 201.870

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 5111.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 69.183
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 87.331

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2403.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 100.886
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 141.070

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3676.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 70.508
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 90.155

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2482.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 102.219
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 140.056

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 3722.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 71.370
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 90.727

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2526.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 102.689
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 140.497

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 3767.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 70.641
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 90.436

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2528.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 125.282
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 196.610

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 4897.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 71.276
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 91.187

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2570.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 104.820
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 142.253

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 3892.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 72.718
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 94.234

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2657.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 105.970
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 145.786

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 3988.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 73.522
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 95.032

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2704.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 107.229
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 142.597

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 4008.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 71.634
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 93.982

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2672.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x256x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x256x2048): 149.715
Elapsed time for attention_prob_times_values (512x2048x2048x256): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x256): 212.559

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 5797.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
