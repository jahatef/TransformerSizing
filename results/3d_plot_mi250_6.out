num_attention_heads: 96, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x1x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x1x2048): 1.042
Elapsed time for attention_prob_times_values (384x2048x2048x1): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x1): 0.211

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 0.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x2x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x2x2048): 1.937
Elapsed time for attention_prob_times_values (384x2048x2048x2): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x2): 1.445

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x3x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x3x2048): 2.668
Elapsed time for attention_prob_times_values (384x2048x2048x3): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x3): 2.056

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 2.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x4x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x4x2048): 3.428
Elapsed time for attention_prob_times_values (384x2048x2048x4): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x4): 2.856

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 4.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x5x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x5x2048): 0.971
Elapsed time for attention_prob_times_values (384x2048x2048x5): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x5): 3.768

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x6x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x6x2048): 4.920
Elapsed time for attention_prob_times_values (384x2048x2048x6): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x6): 4.557

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 7.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x7x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x7x2048): 5.480
Elapsed time for attention_prob_times_values (384x2048x2048x7): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x7): 4.803

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 8.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x8x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x8x2048): 6.381
Elapsed time for attention_prob_times_values (384x2048x2048x8): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x8): 7.190

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 11.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x9x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x9x2048): 6.834
Elapsed time for attention_prob_times_values (384x2048x2048x9): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x9): 7.670

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 13.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x10x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x10x2048): 7.502
Elapsed time for attention_prob_times_values (384x2048x2048x10): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x10): 8.388

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 15.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x11x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x11x2048): 8.739
Elapsed time for attention_prob_times_values (384x2048x2048x11): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x11): 9.362

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 18.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x12x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x12x2048): 9.502
Elapsed time for attention_prob_times_values (384x2048x2048x12): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x12): 10.583

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 21.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x13x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x13x2048): 9.895
Elapsed time for attention_prob_times_values (384x2048x2048x13): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x13): 11.020

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 23.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x14x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x14x2048): 10.709
Elapsed time for attention_prob_times_values (384x2048x2048x14): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x14): 11.749

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 25.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x15x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x15x2048): 11.690
Elapsed time for attention_prob_times_values (384x2048x2048x15): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x15): 12.679

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 29.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x16x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x16x2048): 12.657
Elapsed time for attention_prob_times_values (384x2048x2048x16): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x16): 14.084

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 33.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x17x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x17x2048): 12.951
Elapsed time for attention_prob_times_values (384x2048x2048x17): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x17): 14.148

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 35.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x18x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x18x2048): 13.762
Elapsed time for attention_prob_times_values (384x2048x2048x18): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x18): 15.517

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 39.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x19x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x19x2048): 13.896
Elapsed time for attention_prob_times_values (384x2048x2048x19): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x19): 15.904

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 41.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x20x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x20x2048): 15.014
Elapsed time for attention_prob_times_values (384x2048x2048x20): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x20): 17.200

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 46.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x21x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x21x2048): 15.542
Elapsed time for attention_prob_times_values (384x2048x2048x21): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x21): 17.444

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 48.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x22x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x22x2048): 16.138
Elapsed time for attention_prob_times_values (384x2048x2048x22): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x22): 18.672

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 53.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x23x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x23x2048): 17.427
Elapsed time for attention_prob_times_values (384x2048x2048x23): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x23): 18.985

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 57.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x24x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x24x2048): 18.174
Elapsed time for attention_prob_times_values (384x2048x2048x24): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x24): 19.772

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 61.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x25x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x25x2048): 18.416
Elapsed time for attention_prob_times_values (384x2048x2048x25): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x25): 20.483

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 64.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x26x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x26x2048): 19.051
Elapsed time for attention_prob_times_values (384x2048x2048x26): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x26): 21.560

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 69.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x27x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x27x2048): 19.771
Elapsed time for attention_prob_times_values (384x2048x2048x27): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x27): 21.985

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 73.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x28x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x28x2048): 20.325
Elapsed time for attention_prob_times_values (384x2048x2048x28): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x28): 23.032

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 78.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x29x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x29x2048): 20.818
Elapsed time for attention_prob_times_values (384x2048x2048x29): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x29): 21.943

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 79.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x30x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x30x2048): 21.406
Elapsed time for attention_prob_times_values (384x2048x2048x30): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x30): 23.508

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 85.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x31x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x31x2048): 20.743
Elapsed time for attention_prob_times_values (384x2048x2048x31): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x31): 23.709

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 86.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x32x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x32x2048): 40.880
Elapsed time for attention_prob_times_values (384x2048x2048x32): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x32): 25.157

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 124.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x33x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x33x2048): 26.743
Elapsed time for attention_prob_times_values (384x2048x2048x33): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x33): 23.605

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 102.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x34x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x34x2048): 27.137
Elapsed time for attention_prob_times_values (384x2048x2048x34): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x34): 26.882

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 113.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x35x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x35x2048): 19.237
Elapsed time for attention_prob_times_values (384x2048x2048x35): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x35): 26.626

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 95.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x36x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x36x2048): 26.014
Elapsed time for attention_prob_times_values (384x2048x2048x36): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x36): 28.916

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 119.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x37x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x37x2048): 26.468
Elapsed time for attention_prob_times_values (384x2048x2048x37): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x37): 29.190

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 124.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x38x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x38x2048): 27.167
Elapsed time for attention_prob_times_values (384x2048x2048x38): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x38): 30.629

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 131.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x39x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x39x2048): 27.248
Elapsed time for attention_prob_times_values (384x2048x2048x39): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x39): 30.943

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 134.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x40x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x40x2048): 27.617
Elapsed time for attention_prob_times_values (384x2048x2048x40): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x40): 32.601

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 142.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x41x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x41x2048): 27.909
Elapsed time for attention_prob_times_values (384x2048x2048x41): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x41): 32.298

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 145.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x42x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x42x2048): 28.420
Elapsed time for attention_prob_times_values (384x2048x2048x42): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x42): 32.740

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 150.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x43x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x43x2048): 28.657
Elapsed time for attention_prob_times_values (384x2048x2048x43): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x43): 31.380

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 150.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x44x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x44x2048): 16.675
Elapsed time for attention_prob_times_values (384x2048x2048x44): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x44): 33.896

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 114.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x45x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x45x2048): 29.863
Elapsed time for attention_prob_times_values (384x2048x2048x45): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x45): 33.758

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 165.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x46x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x46x2048): 31.500
Elapsed time for attention_prob_times_values (384x2048x2048x46): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x46): 36.207

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 178.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x47x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x47x2048): 31.892
Elapsed time for attention_prob_times_values (384x2048x2048x47): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x47): 35.970

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 182.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x48x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x48x2048): 31.975
Elapsed time for attention_prob_times_values (384x2048x2048x48): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x48): 38.378

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 191.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x49x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x49x2048): 32.738
Elapsed time for attention_prob_times_values (384x2048x2048x49): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x49): 37.716

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 196.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x50x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x50x2048): 29.194
Elapsed time for attention_prob_times_values (384x2048x2048x50): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x50): 38.640

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 189.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x51x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x51x2048): 33.843
Elapsed time for attention_prob_times_values (384x2048x2048x51): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x51): 36.576

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 203.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x52x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x52x2048): 35.331
Elapsed time for attention_prob_times_values (384x2048x2048x52): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x52): 39.548

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 219.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x53x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x53x2048): 35.203
Elapsed time for attention_prob_times_values (384x2048x2048x53): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x53): 40.208

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 224.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x54x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x54x2048): 36.747
Elapsed time for attention_prob_times_values (384x2048x2048x54): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x54): 41.576

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 236.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x55x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x55x2048): 36.964
Elapsed time for attention_prob_times_values (384x2048x2048x55): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x55): 41.405

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 240.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x56x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x56x2048): 38.116
Elapsed time for attention_prob_times_values (384x2048x2048x56): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x56): 44.767

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 257.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x57x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x57x2048): 37.477
Elapsed time for attention_prob_times_values (384x2048x2048x57): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x57): 43.301

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 254.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x58x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x58x2048): 37.887
Elapsed time for attention_prob_times_values (384x2048x2048x58): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x58): 44.636

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 263.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x59x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x59x2048): 38.179
Elapsed time for attention_prob_times_values (384x2048x2048x59): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x59): 43.215

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 264.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x60x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x60x2048): 37.708
Elapsed time for attention_prob_times_values (384x2048x2048x60): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x60): 46.164

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 275.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x61x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x61x2048): 39.734
Elapsed time for attention_prob_times_values (384x2048x2048x61): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x61): 45.672

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 285.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x62x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x62x2048): 40.288
Elapsed time for attention_prob_times_values (384x2048x2048x62): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x62): 46.429

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 293.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x63x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x63x2048): 40.828
Elapsed time for attention_prob_times_values (384x2048x2048x63): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x63): 47.063

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 301.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x64x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x64x2048): 54.579
Elapsed time for attention_prob_times_values (384x2048x2048x64): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x64): 51.020

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 369.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x65x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x65x2048): 41.314
Elapsed time for attention_prob_times_values (384x2048x2048x65): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x65): 32.771

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 259.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x66x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x66x2048): 43.944
Elapsed time for attention_prob_times_values (384x2048x2048x66): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x66): 32.089

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 266.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x67x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x67x2048): 43.044
Elapsed time for attention_prob_times_values (384x2048x2048x67): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x67): 32.797

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 271.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x68x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x68x2048): 43.269
Elapsed time for attention_prob_times_values (384x2048x2048x68): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x68): 35.625

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 288.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x69x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x69x2048): 43.256
Elapsed time for attention_prob_times_values (384x2048x2048x69): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x69): 34.501

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 286.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x70x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x70x2048): 44.605
Elapsed time for attention_prob_times_values (384x2048x2048x70): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x70): 35.634

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 299.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x71x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x71x2048): 43.804
Elapsed time for attention_prob_times_values (384x2048x2048x71): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x71): 36.182

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 303.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x72x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x72x2048): 46.358
Elapsed time for attention_prob_times_values (384x2048x2048x72): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x72): 35.197

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 310.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x73x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x73x2048): 44.911
Elapsed time for attention_prob_times_values (384x2048x2048x73): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x73): 35.792

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 312.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x74x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x74x2048): 45.913
Elapsed time for attention_prob_times_values (384x2048x2048x74): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x74): 38.501

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 332.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x75x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x75x2048): 42.462
Elapsed time for attention_prob_times_values (384x2048x2048x75): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x75): 37.647

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 320.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x76x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x76x2048): 46.844
Elapsed time for attention_prob_times_values (384x2048x2048x76): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x76): 38.656

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 344.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x77x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x77x2048): 45.975
Elapsed time for attention_prob_times_values (384x2048x2048x77): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x77): 37.502

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 339.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x78x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x78x2048): 45.581
Elapsed time for attention_prob_times_values (384x2048x2048x78): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x78): 40.218

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 355.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x79x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x79x2048): 46.112
Elapsed time for attention_prob_times_values (384x2048x2048x79): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x79): 38.876

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 354.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x80x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x80x2048): 49.538
Elapsed time for attention_prob_times_values (384x2048x2048x80): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x80): 38.051

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 365.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x81x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x81x2048): 47.200
Elapsed time for attention_prob_times_values (384x2048x2048x81): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x81): 39.923

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 371.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x82x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x82x2048): 45.041
Elapsed time for attention_prob_times_values (384x2048x2048x82): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x82): 40.126

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 368.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x83x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x83x2048): 48.236
Elapsed time for attention_prob_times_values (384x2048x2048x83): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x83): 40.963

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 389.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x84x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x84x2048): 48.648
Elapsed time for attention_prob_times_values (384x2048x2048x84): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x84): 41.564

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 397.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x85x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x85x2048): 48.718
Elapsed time for attention_prob_times_values (384x2048x2048x85): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x85): 39.989

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 393.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x86x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x86x2048): 49.515
Elapsed time for attention_prob_times_values (384x2048x2048x86): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x86): 42.298

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 413.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x87x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x87x2048): 49.697
Elapsed time for attention_prob_times_values (384x2048x2048x87): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x87): 16.891

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 230.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x88x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x88x2048): 50.576
Elapsed time for attention_prob_times_values (384x2048x2048x88): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x88): 41.274

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 420.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x89x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x89x2048): 46.547
Elapsed time for attention_prob_times_values (384x2048x2048x89): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x89): 42.582

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 415.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x90x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x90x2048): 49.506
Elapsed time for attention_prob_times_values (384x2048x2048x90): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x90): 45.370

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 446.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x91x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x91x2048): 49.590
Elapsed time for attention_prob_times_values (384x2048x2048x91): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x91): 43.579

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 442.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x92x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x92x2048): 49.439
Elapsed time for attention_prob_times_values (384x2048x2048x92): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x92): 45.304

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 455.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x93x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x93x2048): 49.325
Elapsed time for attention_prob_times_values (384x2048x2048x93): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x93): 45.038

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 457.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x94x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x94x2048): 51.095
Elapsed time for attention_prob_times_values (384x2048x2048x94): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x94): 46.959

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 480.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x95x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x95x2048): 49.900
Elapsed time for attention_prob_times_values (384x2048x2048x95): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x95): 45.565

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 471.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x96x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x96x2048): 64.927
Elapsed time for attention_prob_times_values (384x2048x2048x96): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x96): 47.090

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 545.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x97x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x97x2048): 50.840
Elapsed time for attention_prob_times_values (384x2048x2048x97): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x97): 46.970

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 492.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x98x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x98x2048): 52.504
Elapsed time for attention_prob_times_values (384x2048x2048x98): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x98): 47.703

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 509.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x99x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x99x2048): 50.843
Elapsed time for attention_prob_times_values (384x2048x2048x99): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x99): 47.760

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 506.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x100x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x100x2048): 30.430
Elapsed time for attention_prob_times_values (384x2048x2048x100): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x100): 48.103

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 386.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x101x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x101x2048): 49.526
Elapsed time for attention_prob_times_values (384x2048x2048x101): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x101): 48.706

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 514.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x102x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x102x2048): 49.976
Elapsed time for attention_prob_times_values (384x2048x2048x102): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x102): 49.352

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 524.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x103x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x103x2048): 52.122
Elapsed time for attention_prob_times_values (384x2048x2048x103): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x103): 47.146

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 527.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x104x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x104x2048): 53.674
Elapsed time for attention_prob_times_values (384x2048x2048x104): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x104): 48.154

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 545.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x105x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x105x2048): 53.130
Elapsed time for attention_prob_times_values (384x2048x2048x105): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x105): 49.947

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 558.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x106x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x106x2048): 54.318
Elapsed time for attention_prob_times_values (384x2048x2048x106): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x106): 51.542

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 578.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x107x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x107x2048): 53.872
Elapsed time for attention_prob_times_values (384x2048x2048x107): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x107): 49.092

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 566.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x108x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x108x2048): 54.021
Elapsed time for attention_prob_times_values (384x2048x2048x108): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x108): 51.624

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 587.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x109x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x109x2048): 52.341
Elapsed time for attention_prob_times_values (384x2048x2048x109): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x109): 51.204

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 580.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x110x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x110x2048): 55.702
Elapsed time for attention_prob_times_values (384x2048x2048x110): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x110): 53.383

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 616.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x111x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x111x2048): 55.278
Elapsed time for attention_prob_times_values (384x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x111): 50.549

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 602.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x112x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x112x2048): 55.368
Elapsed time for attention_prob_times_values (384x2048x2048x112): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x112): 52.830

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 621.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x113x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x113x2048): 54.853
Elapsed time for attention_prob_times_values (384x2048x2048x113): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x113): 51.910

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 618.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x114x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x114x2048): 54.112
Elapsed time for attention_prob_times_values (384x2048x2048x114): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x114): 54.777

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 636.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x115x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x115x2048): 55.710
Elapsed time for attention_prob_times_values (384x2048x2048x115): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x115): 52.470

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 636.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x116x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x116x2048): 57.012
Elapsed time for attention_prob_times_values (384x2048x2048x116): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x116): 54.820

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 663.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x117x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x117x2048): 55.990
Elapsed time for attention_prob_times_values (384x2048x2048x117): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x117): 52.480

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 648.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x118x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x118x2048): 57.441
Elapsed time for attention_prob_times_values (384x2048x2048x118): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x118): 54.888

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 677.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x119x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x119x2048): 57.103
Elapsed time for attention_prob_times_values (384x2048x2048x119): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x119): 53.529

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 671.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x120x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x120x2048): 57.891
Elapsed time for attention_prob_times_values (384x2048x2048x120): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x120): 56.413

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 699.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x121x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x121x2048): 53.824
Elapsed time for attention_prob_times_values (384x2048x2048x121): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x121): 43.242

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 591.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x122x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x122x2048): 57.531
Elapsed time for attention_prob_times_values (384x2048x2048x122): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x122): 56.884

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 711.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x123x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x123x2048): 55.982
Elapsed time for attention_prob_times_values (384x2048x2048x123): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x123): 43.804

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 615.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x124x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x124x2048): 56.988
Elapsed time for attention_prob_times_values (384x2048x2048x124): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x124): 57.050

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 719.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x125x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x125x2048): 57.497
Elapsed time for attention_prob_times_values (384x2048x2048x125): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x125): 43.741

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 631.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x126x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x126x2048): 56.252
Elapsed time for attention_prob_times_values (384x2048x2048x126): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x126): 56.600

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 722.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x127x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x127x2048): 55.505
Elapsed time for attention_prob_times_values (384x2048x2048x127): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x127): 43.732

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 631.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x128x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x128x2048): 69.021
Elapsed time for attention_prob_times_values (384x2048x2048x128): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x128): 61.256

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 843.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x129x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x129x2048): 56.002
Elapsed time for attention_prob_times_values (384x2048x2048x129): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x129): 43.477

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 640.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x130x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x130x2048): 58.286
Elapsed time for attention_prob_times_values (384x2048x2048x130): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x130): 45.785

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 676.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x131x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x131x2048): 57.829
Elapsed time for attention_prob_times_values (384x2048x2048x131): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x131): 44.919

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 671.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x132x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x132x2048): 58.836
Elapsed time for attention_prob_times_values (384x2048x2048x132): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x132): 46.229

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 692.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x133x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x133x2048): 58.144
Elapsed time for attention_prob_times_values (384x2048x2048x133): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x133): 46.105

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 692.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x134x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x134x2048): 57.722
Elapsed time for attention_prob_times_values (384x2048x2048x134): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x134): 48.163

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 712.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x135x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x135x2048): 56.786
Elapsed time for attention_prob_times_values (384x2048x2048x135): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x135): 46.360

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 697.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x136x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x136x2048): 24.730
Elapsed time for attention_prob_times_values (384x2048x2048x136): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x136): 44.847

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 438.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x137x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x137x2048): 58.872
Elapsed time for attention_prob_times_values (384x2048x2048x137): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x137): 45.335

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 709.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x138x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x138x2048): 58.759
Elapsed time for attention_prob_times_values (384x2048x2048x138): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x138): 49.324

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 747.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x139x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x139x2048): 58.778
Elapsed time for attention_prob_times_values (384x2048x2048x139): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x139): 46.269

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 726.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x140x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x140x2048): 60.347
Elapsed time for attention_prob_times_values (384x2048x2048x140): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x140): 49.428

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 767.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x141x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x141x2048): 59.100
Elapsed time for attention_prob_times_values (384x2048x2048x141): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x141): 47.045

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 744.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x142x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x142x2048): 59.032
Elapsed time for attention_prob_times_values (384x2048x2048x142): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x142): 49.824

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 773.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x143x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x143x2048): 59.587
Elapsed time for attention_prob_times_values (384x2048x2048x143): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x143): 47.159

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 758.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x144x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x144x2048): 61.830
Elapsed time for attention_prob_times_values (384x2048x2048x144): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x144): 49.076

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 793.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x145x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x145x2048): 58.907
Elapsed time for attention_prob_times_values (384x2048x2048x145): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x145): 48.342

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 774.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x146x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x146x2048): 59.526
Elapsed time for attention_prob_times_values (384x2048x2048x146): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x146): 51.268

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 809.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x147x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x147x2048): 60.483
Elapsed time for attention_prob_times_values (384x2048x2048x147): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x147): 48.823

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 798.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x148x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x148x2048): 61.450
Elapsed time for attention_prob_times_values (384x2048x2048x148): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x148): 51.067

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 829.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x149x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x149x2048): 61.492
Elapsed time for attention_prob_times_values (384x2048x2048x149): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x149): 49.326

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 819.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x150x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x150x2048): 62.779
Elapsed time for attention_prob_times_values (384x2048x2048x150): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x150): 49.793

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 836.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x151x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x151x2048): 59.751
Elapsed time for attention_prob_times_values (384x2048x2048x151): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x151): 51.138

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 835.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x152x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x152x2048): 60.919
Elapsed time for attention_prob_times_values (384x2048x2048x152): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x152): 51.049

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 847.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x153x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x153x2048): 61.027
Elapsed time for attention_prob_times_values (384x2048x2048x153): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x153): 51.006

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 852.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x154x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x154x2048): 58.776
Elapsed time for attention_prob_times_values (384x2048x2048x154): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x154): 53.371

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 863.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x155x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x155x2048): 61.615
Elapsed time for attention_prob_times_values (384x2048x2048x155): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x155): 51.304

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 869.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x156x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x156x2048): 62.675
Elapsed time for attention_prob_times_values (384x2048x2048x156): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x156): 54.095

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 907.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x157x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x157x2048): 62.027
Elapsed time for attention_prob_times_values (384x2048x2048x157): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x157): 50.971

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 879.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x158x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x158x2048): 63.789
Elapsed time for attention_prob_times_values (384x2048x2048x158): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x158): 54.356

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 928.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x159x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x159x2048): 63.114
Elapsed time for attention_prob_times_values (384x2048x2048x159): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x159): 53.323

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 919.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x160x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x160x2048): 76.383
Elapsed time for attention_prob_times_values (384x2048x2048x160): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x160): 55.450

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1028.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x161x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x161x2048): 62.528
Elapsed time for attention_prob_times_values (384x2048x2048x161): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x161): 53.707

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 929.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x162x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x162x2048): 63.728
Elapsed time for attention_prob_times_values (384x2048x2048x162): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x162): 56.043

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 965.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x163x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x163x2048): 62.947
Elapsed time for attention_prob_times_values (384x2048x2048x163): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x163): 54.055

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 946.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x164x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x164x2048): 64.702
Elapsed time for attention_prob_times_values (384x2048x2048x164): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x164): 57.262

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 994.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x165x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x165x2048): 63.433
Elapsed time for attention_prob_times_values (384x2048x2048x165): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x165): 55.521

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 975.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x166x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x166x2048): 63.383
Elapsed time for attention_prob_times_values (384x2048x2048x166): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x166): 57.601

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 999.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x167x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x167x2048): 63.867
Elapsed time for attention_prob_times_values (384x2048x2048x167): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x167): 54.356

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 978.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x168x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x168x2048): 63.539
Elapsed time for attention_prob_times_values (384x2048x2048x168): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x168): 55.107

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 988.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x169x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x169x2048): 62.050
Elapsed time for attention_prob_times_values (384x2048x2048x169): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x169): 54.489

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 977.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x170x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x170x2048): 65.085
Elapsed time for attention_prob_times_values (384x2048x2048x170): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x170): 56.460

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1024.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x171x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x171x2048): 64.084
Elapsed time for attention_prob_times_values (384x2048x2048x171): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x171): 55.958

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1017.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x172x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x172x2048): 64.627
Elapsed time for attention_prob_times_values (384x2048x2048x172): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x172): 58.866

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1055.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x173x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x173x2048): 64.206
Elapsed time for attention_prob_times_values (384x2048x2048x173): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x173): 56.572

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1035.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x174x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x174x2048): 65.409
Elapsed time for attention_prob_times_values (384x2048x2048x174): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x174): 60.004

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1083.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x175x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x175x2048): 64.831
Elapsed time for attention_prob_times_values (384x2048x2048x175): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x175): 58.311

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1068.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x176x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x176x2048): 67.793
Elapsed time for attention_prob_times_values (384x2048x2048x176): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x176): 56.775

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1081.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x177x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x177x2048): 64.108
Elapsed time for attention_prob_times_values (384x2048x2048x177): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x177): 58.231

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1073.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x178x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x178x2048): 66.594
Elapsed time for attention_prob_times_values (384x2048x2048x178): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x178): 61.108

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1127.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x179x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x179x2048): 63.489
Elapsed time for attention_prob_times_values (384x2048x2048x179): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x179): 58.401

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1081.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x180x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x180x2048): 67.261
Elapsed time for attention_prob_times_values (384x2048x2048x180): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x180): 61.735

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1150.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x181x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x181x2048): 63.182
Elapsed time for attention_prob_times_values (384x2048x2048x181): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x181): 56.925

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1076.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x182x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x182x2048): 67.586
Elapsed time for attention_prob_times_values (384x2048x2048x182): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x182): 61.232

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1160.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x183x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x183x2048): 65.479
Elapsed time for attention_prob_times_values (384x2048x2048x183): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x183): 58.797

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1124.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x184x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x184x2048): 67.055
Elapsed time for attention_prob_times_values (384x2048x2048x184): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x184): 59.961

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1155.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x185x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x185x2048): 66.391
Elapsed time for attention_prob_times_values (384x2048x2048x185): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x185): 60.274

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1159.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x186x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x186x2048): 67.629
Elapsed time for attention_prob_times_values (384x2048x2048x186): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x186): 63.098

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1203.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x187x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x187x2048): 63.105
Elapsed time for attention_prob_times_values (384x2048x2048x187): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x187): 60.332

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1143.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x188x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x188x2048): 66.154
Elapsed time for attention_prob_times_values (384x2048x2048x188): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x188): 61.678

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1188.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x189x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x189x2048): 66.274
Elapsed time for attention_prob_times_values (384x2048x2048x189): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x189): 61.964

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1198.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x190x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x190x2048): 67.340
Elapsed time for attention_prob_times_values (384x2048x2048x190): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x190): 60.724

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1201.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x191x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x191x2048): 65.932
Elapsed time for attention_prob_times_values (384x2048x2048x191): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x191): 61.232

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1200.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x192x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x192x2048): 75.300
Elapsed time for attention_prob_times_values (384x2048x2048x192): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x192): 64.792

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1323.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x193x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x193x2048): 66.501
Elapsed time for attention_prob_times_values (384x2048x2048x193): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x193): 51.648

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1110.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x194x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x194x2048): 64.241
Elapsed time for attention_prob_times_values (384x2048x2048x194): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x194): 53.592

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1121.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x195x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x195x2048): 66.291
Elapsed time for attention_prob_times_values (384x2048x2048x195): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x195): 51.820

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1121.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x196x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x196x2048): 68.229
Elapsed time for attention_prob_times_values (384x2048x2048x196): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x196): 53.253

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1158.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x197x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x197x2048): 67.272
Elapsed time for attention_prob_times_values (384x2048x2048x197): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x197): 53.187

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1156.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x198x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x198x2048): 65.208
Elapsed time for attention_prob_times_values (384x2048x2048x198): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x198): 53.698

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1152.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x199x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x199x2048): 67.763
Elapsed time for attention_prob_times_values (384x2048x2048x199): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x199): 52.174

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1158.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x200x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x200x2048): 68.729
Elapsed time for attention_prob_times_values (384x2048x2048x200): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x200): 50.243

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1146.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x201x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x201x2048): 64.731
Elapsed time for attention_prob_times_values (384x2048x2048x201): 0.0258
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x201): 25.119

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 718.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x202x2048): 67.927
Elapsed time for attention_prob_times_values (384x2048x2048x202): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x202): 54.550

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1206.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x203x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x203x2048): 67.373
Elapsed time for attention_prob_times_values (384x2048x2048x203): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x203): 50.526

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1156.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x204x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x204x2048): 69.428
Elapsed time for attention_prob_times_values (384x2048x2048x204): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x204): 55.148

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1237.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x205x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x205x2048): 68.305
Elapsed time for attention_prob_times_values (384x2048x2048x205): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x205): 51.164

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1182.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x206x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x206x2048): 67.579
Elapsed time for attention_prob_times_values (384x2048x2048x206): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x206): 54.053

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1220.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x207x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x207x2048): 68.001
Elapsed time for attention_prob_times_values (384x2048x2048x207): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x207): 52.588

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1210.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x208x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x208x2048): 70.348
Elapsed time for attention_prob_times_values (384x2048x2048x208): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x208): 52.996

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1239.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x209x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x209x2048): 68.375
Elapsed time for attention_prob_times_values (384x2048x2048x209): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x209): 52.920

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1228.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x210x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x210x2048): 67.178
Elapsed time for attention_prob_times_values (384x2048x2048x210): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x210): 54.876

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1249.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x211x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x211x2048): 67.180
Elapsed time for attention_prob_times_values (384x2048x2048x211): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x211): 53.556

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1238.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x212x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x212x2048): 70.341
Elapsed time for attention_prob_times_values (384x2048x2048x212): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x212): 54.225

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1278.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x213x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x213x2048): 67.849
Elapsed time for attention_prob_times_values (384x2048x2048x213): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x213): 53.370

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1252.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x214x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x214x2048): 69.533
Elapsed time for attention_prob_times_values (384x2048x2048x214): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x214): 55.246

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1296.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x215x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x215x2048): 35.743
Elapsed time for attention_prob_times_values (384x2048x2048x215): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x215): 54.390

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 912.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x216x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x216x2048): 71.130
Elapsed time for attention_prob_times_values (384x2048x2048x216): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x216): 54.179

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1307.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x217x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x217x2048): 69.235
Elapsed time for attention_prob_times_values (384x2048x2048x217): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x217): 54.787

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1305.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x218x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x218x2048): 69.586
Elapsed time for attention_prob_times_values (384x2048x2048x218): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x218): 56.982

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1343.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x219x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x219x2048): 69.831
Elapsed time for attention_prob_times_values (384x2048x2048x219): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x219): 55.709

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1334.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x220x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x220x2048): 69.804
Elapsed time for attention_prob_times_values (384x2048x2048x220): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x220): 57.777

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1367.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x221x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x221x2048): 69.354
Elapsed time for attention_prob_times_values (384x2048x2048x221): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x221): 54.845

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1330.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x222x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x222x2048): 70.938
Elapsed time for attention_prob_times_values (384x2048x2048x222): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x222): 58.808

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1402.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x223x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x223x2048): 70.398
Elapsed time for attention_prob_times_values (384x2048x2048x223): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x223): 55.841

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1364.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x224x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x224x2048): 79.912
Elapsed time for attention_prob_times_values (384x2048x2048x224): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x224): 59.605

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1502.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x225x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x225x2048): 68.288
Elapsed time for attention_prob_times_values (384x2048x2048x225): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x225): 57.123

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1374.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x226x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x226x2048): 70.686
Elapsed time for attention_prob_times_values (384x2048x2048x226): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x226): 59.660

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1435.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x227x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x227x2048): 66.957
Elapsed time for attention_prob_times_values (384x2048x2048x227): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x227): 57.587

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1379.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x228x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x228x2048): 70.768
Elapsed time for attention_prob_times_values (384x2048x2048x228): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x228): 60.208

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1455.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x229x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x229x2048): 67.724
Elapsed time for attention_prob_times_values (384x2048x2048x229): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x229): 55.985

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1377.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x230x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x230x2048): 71.157
Elapsed time for attention_prob_times_values (384x2048x2048x230): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x230): 58.672

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1451.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x231x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x231x2048): 68.629
Elapsed time for attention_prob_times_values (384x2048x2048x231): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x231): 57.627

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1419.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x232x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x232x2048): 72.496
Elapsed time for attention_prob_times_values (384x2048x2048x232): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x232): 57.773

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1462.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x233x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x233x2048): 66.968
Elapsed time for attention_prob_times_values (384x2048x2048x233): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x233): 57.078

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1407.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x234x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x234x2048): 68.626
Elapsed time for attention_prob_times_values (384x2048x2048x234): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x234): 60.308

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1472.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x235x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x235x2048): 67.195
Elapsed time for attention_prob_times_values (384x2048x2048x235): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x235): 57.073

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1421.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x236x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x236x2048): 70.319
Elapsed time for attention_prob_times_values (384x2048x2048x236): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x236): 59.840

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1495.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x237x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x237x2048): 70.128
Elapsed time for attention_prob_times_values (384x2048x2048x237): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x237): 58.994

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1487.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x238x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x238x2048): 71.742
Elapsed time for attention_prob_times_values (384x2048x2048x238): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x238): 61.943

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1549.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x239x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x239x2048): 70.519
Elapsed time for attention_prob_times_values (384x2048x2048x239): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x239): 57.751

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1486.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x240x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x240x2048): 55.933
Elapsed time for attention_prob_times_values (384x2048x2048x240): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x240): 61.076

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1372.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x241x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x241x2048): 53.249
Elapsed time for attention_prob_times_values (384x2048x2048x241): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x241): 59.145

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1322.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x242x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x242x2048): 54.408
Elapsed time for attention_prob_times_values (384x2048x2048x242): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x242): 62.123

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1374.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x243x2048): 52.806
Elapsed time for attention_prob_times_values (384x2048x2048x243): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x243): 59.719

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1332.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x244x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x244x2048): 54.913
Elapsed time for attention_prob_times_values (384x2048x2048x244): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x244): 63.271

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1403.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x245x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x245x2048): 53.422
Elapsed time for attention_prob_times_values (384x2048x2048x245): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x245): 60.872

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1363.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x246x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x246x2048): 52.777
Elapsed time for attention_prob_times_values (384x2048x2048x246): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x246): 60.655

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1358.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x247x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x247x2048): 54.595
Elapsed time for attention_prob_times_values (384x2048x2048x247): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x247): 61.074

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1392.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x248x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x248x2048): 55.876
Elapsed time for attention_prob_times_values (384x2048x2048x248): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x248): 61.177

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1416.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x249x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x249x2048): 51.417
Elapsed time for attention_prob_times_values (384x2048x2048x249): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x249): 57.189

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1318.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x250x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x250x2048): 54.404
Elapsed time for attention_prob_times_values (384x2048x2048x250): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x250): 62.724

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1423.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x251x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x251x2048): 53.504
Elapsed time for attention_prob_times_values (384x2048x2048x251): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x251): 63.735

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1427.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x252x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x252x2048): 53.618
Elapsed time for attention_prob_times_values (384x2048x2048x252): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x252): 64.660

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1443.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x253x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x253x2048): 54.065
Elapsed time for attention_prob_times_values (384x2048x2048x253): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x253): 61.817

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1425.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x254x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x254x2048): 54.811
Elapsed time for attention_prob_times_values (384x2048x2048x254): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x254): 64.428

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1469.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x255x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x255x2048): 53.346
Elapsed time for attention_prob_times_values (384x2048x2048x255): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x255): 65.117

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1460.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x256x2048): 0.0249
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x256x2048): 33.126
Elapsed time for attention_prob_times_values (384x2048x2048x256): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x256): 67.539

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 1111.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x257x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x257x2048): 54.561
Elapsed time for attention_prob_times_values (384x2048x2048x257): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x257): 53.368

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1354.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x258x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x258x2048): 55.703
Elapsed time for attention_prob_times_values (384x2048x2048x258): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x258): 56.246

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1409.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x259x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x259x2048): 56.481
Elapsed time for attention_prob_times_values (384x2048x2048x259): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x259): 52.623

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1377.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x260x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x260x2048): 58.078
Elapsed time for attention_prob_times_values (384x2048x2048x260): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x260): 57.218

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 1462.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x261x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x261x2048): 56.858
Elapsed time for attention_prob_times_values (384x2048x2048x261): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x261): 54.996

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1423.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x262x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x262x2048): 56.551
Elapsed time for attention_prob_times_values (384x2048x2048x262): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x262): 57.570

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1458.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x263x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x263x2048): 54.734
Elapsed time for attention_prob_times_values (384x2048x2048x263): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x263): 55.399

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1412.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x264x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x264x2048): 58.869
Elapsed time for attention_prob_times_values (384x2048x2048x264): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x264): 54.955

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1463.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x265x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x265x2048): 56.343
Elapsed time for attention_prob_times_values (384x2048x2048x265): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x265): 56.994

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1464.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x266x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x266x2048): 56.626
Elapsed time for attention_prob_times_values (384x2048x2048x266): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x266): 59.935

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 1510.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x267x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x267x2048): 55.380
Elapsed time for attention_prob_times_values (384x2048x2048x267): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x267): 57.399

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1467.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x268x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x268x2048): 57.614
Elapsed time for attention_prob_times_values (384x2048x2048x268): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x268): 60.594

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1543.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x269x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x269x2048): 57.363
Elapsed time for attention_prob_times_values (384x2048x2048x269): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x269): 58.204

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1514.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x270x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x270x2048): 57.600
Elapsed time for attention_prob_times_values (384x2048x2048x270): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x270): 60.144

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1548.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x271x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x271x2048): 55.125
Elapsed time for attention_prob_times_values (384x2048x2048x271): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x271): 58.087

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1493.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x272x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x272x2048): 59.057
Elapsed time for attention_prob_times_values (384x2048x2048x272): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x272): 75.968

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1761.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x273x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x273x2048): 54.737
Elapsed time for attention_prob_times_values (384x2048x2048x273): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x273): 58.538

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1504.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x274x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x274x2048): 56.557
Elapsed time for attention_prob_times_values (384x2048x2048x274): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x274): 60.642

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1561.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x275x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x275x2048): 54.813
Elapsed time for attention_prob_times_values (384x2048x2048x275): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x275): 58.437

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1514.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x276x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x276x2048): 57.746
Elapsed time for attention_prob_times_values (384x2048x2048x276): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x276): 61.571

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 1601.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x277x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x277x2048): 57.300
Elapsed time for attention_prob_times_values (384x2048x2048x277): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x277): 59.416

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1573.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x278x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x278x2048): 57.336
Elapsed time for attention_prob_times_values (384x2048x2048x278): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x278): 60.575

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1594.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x279x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x279x2048): 56.829
Elapsed time for attention_prob_times_values (384x2048x2048x279): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x279): 59.612

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1580.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x280x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x280x2048): 59.328
Elapsed time for attention_prob_times_values (384x2048x2048x280): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x280): 77.911

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1835.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x281x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x281x2048): 57.142
Elapsed time for attention_prob_times_values (384x2048x2048x281): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x281): 60.204

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1603.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x282x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x282x2048): 57.543
Elapsed time for attention_prob_times_values (384x2048x2048x282): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x282): 61.886

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1636.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x283x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x283x2048): 57.133
Elapsed time for attention_prob_times_values (384x2048x2048x283): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x283): 58.509

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1591.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x284x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x284x2048): 58.283
Elapsed time for attention_prob_times_values (384x2048x2048x284): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x284): 63.543

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1679.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x285x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x285x2048): 57.238
Elapsed time for attention_prob_times_values (384x2048x2048x285): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x285): 61.595

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1644.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x286x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x286x2048): 56.083
Elapsed time for attention_prob_times_values (384x2048x2048x286): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x286): 63.622

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1658.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x287x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x287x2048): 57.997
Elapsed time for attention_prob_times_values (384x2048x2048x287): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x287): 59.773

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1642.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x288x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x288x2048): 76.983
Elapsed time for attention_prob_times_values (384x2048x2048x288): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x288): 81.770

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2220.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x289x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x289x2048): 60.096
Elapsed time for attention_prob_times_values (384x2048x2048x289): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x289): 61.730

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1710.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x290x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x290x2048): 61.578
Elapsed time for attention_prob_times_values (384x2048x2048x290): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x290): 64.133

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1771.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x291x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x291x2048): 60.518
Elapsed time for attention_prob_times_values (384x2048x2048x291): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x291): 62.214

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1735.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x292x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x292x2048): 61.836
Elapsed time for attention_prob_times_values (384x2048x2048x292): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x292): 63.037

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1771.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x293x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x293x2048): 58.321
Elapsed time for attention_prob_times_values (384x2048x2048x293): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x293): 63.284

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1728.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x294x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x294x2048): 61.452
Elapsed time for attention_prob_times_values (384x2048x2048x294): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x294): 63.289

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1781.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x295x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x295x2048): 60.351
Elapsed time for attention_prob_times_values (384x2048x2048x295): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x295): 64.072

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1781.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x296x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x296x2048): 61.643
Elapsed time for attention_prob_times_values (384x2048x2048x296): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x296): 58.344

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1723.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x297x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x297x2048): 60.031
Elapsed time for attention_prob_times_values (384x2048x2048x297): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x297): 63.567

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1781.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x298x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x298x2048): 60.275
Elapsed time for attention_prob_times_values (384x2048x2048x298): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x298): 65.736

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1819.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x299x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x299x2048): 59.660
Elapsed time for attention_prob_times_values (384x2048x2048x299): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x299): 63.435

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1785.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x300x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x300x2048): 61.681
Elapsed time for attention_prob_times_values (384x2048x2048x300): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x300): 67.534

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1877.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x301x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x301x2048): 60.631
Elapsed time for attention_prob_times_values (384x2048x2048x301): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x301): 64.553

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1827.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x302x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x302x2048): 59.723
Elapsed time for attention_prob_times_values (384x2048x2048x302): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x302): 68.442

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1869.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x303x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x303x2048): 60.494
Elapsed time for attention_prob_times_values (384x2048x2048x303): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x303): 65.738

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1852.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x304x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x304x2048): 59.686
Elapsed time for attention_prob_times_values (384x2048x2048x304): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x304): 80.119

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2018.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x305x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x305x2048): 60.352
Elapsed time for attention_prob_times_values (384x2048x2048x305): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x305): 65.542

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1859.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x306x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x306x2048): 59.675
Elapsed time for attention_prob_times_values (384x2048x2048x306): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x306): 68.484

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1893.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x307x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x307x2048): 58.174
Elapsed time for attention_prob_times_values (384x2048x2048x307): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x307): 67.019

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1854.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x308x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x308x2048): 62.294
Elapsed time for attention_prob_times_values (384x2048x2048x308): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x308): 68.640

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1951.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x309x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x309x2048): 59.545
Elapsed time for attention_prob_times_values (384x2048x2048x309): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x309): 65.589

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1870.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x310x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x310x2048): 61.679
Elapsed time for attention_prob_times_values (384x2048x2048x310): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x310): 68.603

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1952.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x311x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x311x2048): 60.549
Elapsed time for attention_prob_times_values (384x2048x2048x311): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x311): 66.291

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1908.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x312x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x312x2048): 62.789
Elapsed time for attention_prob_times_values (384x2048x2048x312): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x312): 85.919

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2194.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x313x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x313x2048): 60.394
Elapsed time for attention_prob_times_values (384x2048x2048x313): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x313): 65.753

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 1910.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x314x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x314x2048): 61.070
Elapsed time for attention_prob_times_values (384x2048x2048x314): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x314): 67.615

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1953.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x315x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x315x2048): 61.044
Elapsed time for attention_prob_times_values (384x2048x2048x315): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x315): 67.286

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1954.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x316x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x316x2048): 60.353
Elapsed time for attention_prob_times_values (384x2048x2048x316): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x316): 69.568

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1979.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x317x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x317x2048): 59.296
Elapsed time for attention_prob_times_values (384x2048x2048x317): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x317): 68.413

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 1951.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x318x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x318x2048): 61.664
Elapsed time for attention_prob_times_values (384x2048x2048x318): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x318): 68.260

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1996.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x319x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x319x2048): 60.692
Elapsed time for attention_prob_times_values (384x2048x2048x319): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x319): 69.166

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1998.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x320x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x320x2048): 79.968
Elapsed time for attention_prob_times_values (384x2048x2048x320): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x320): 88.832

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2609.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x321x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x321x2048): 64.161
Elapsed time for attention_prob_times_values (384x2048x2048x321): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x321): 59.084

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1912.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x322x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x322x2048): 65.225
Elapsed time for attention_prob_times_values (384x2048x2048x322): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x322): 62.551

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 1991.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x323x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x323x2048): 61.793
Elapsed time for attention_prob_times_values (384x2048x2048x323): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x323): 60.508

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1912.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x324x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x324x2048): 66.015
Elapsed time for attention_prob_times_values (384x2048x2048x324): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x324): 63.528

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 2031.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x325x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x325x2048): 64.112
Elapsed time for attention_prob_times_values (384x2048x2048x325): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x325): 60.738

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1963.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x326x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x326x2048): 63.643
Elapsed time for attention_prob_times_values (384x2048x2048x326): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x326): 63.669

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 2009.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x327x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x327x2048): 63.519
Elapsed time for attention_prob_times_values (384x2048x2048x327): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x327): 61.120

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1972.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x328x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x328x2048): 65.526
Elapsed time for attention_prob_times_values (384x2048x2048x328): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x328): 76.749

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2244.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x329x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x329x2048): 61.100
Elapsed time for attention_prob_times_values (384x2048x2048x329): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x329): 60.376

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 1934.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x330x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x330x2048): 59.954
Elapsed time for attention_prob_times_values (384x2048x2048x330): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x330): 63.422

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 1968.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x331x2048): 0.0179
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x331x2048): 59.682
Elapsed time for attention_prob_times_values (384x2048x2048x331): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x331): 59.545

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1909.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x332x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x332x2048): 64.698
Elapsed time for attention_prob_times_values (384x2048x2048x332): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x332): 64.948

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 2082.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x333x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x333x2048): 61.713
Elapsed time for attention_prob_times_values (384x2048x2048x333): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x333): 59.632

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1954.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x334x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x334x2048): 63.945
Elapsed time for attention_prob_times_values (384x2048x2048x334): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x334): 63.361

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 2056.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x335x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x335x2048): 61.907
Elapsed time for attention_prob_times_values (384x2048x2048x335): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x335): 60.608

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1984.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x336x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x336x2048): 64.526
Elapsed time for attention_prob_times_values (384x2048x2048x336): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x336): 78.806

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2306.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x337x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x337x2048): 61.155
Elapsed time for attention_prob_times_values (384x2048x2048x337): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x337): 60.022

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1974.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x338x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x338x2048): 61.337
Elapsed time for attention_prob_times_values (384x2048x2048x338): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x338): 64.066

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 2048.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x339x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x339x2048): 62.801
Elapsed time for attention_prob_times_values (384x2048x2048x339): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x339): 62.493

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 2053.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x340x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x340x2048): 64.131
Elapsed time for attention_prob_times_values (384x2048x2048x340): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x340): 65.307

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 2127.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x341x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x341x2048): 63.220
Elapsed time for attention_prob_times_values (384x2048x2048x341): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x341): 62.557

Attention duration (in seconds): 0.0349
Attention throughput (in TFLOP/s): 2073.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x1x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x1x2048): 0.986
Elapsed time for attention_prob_times_values (512x2048x2048x1): 0.0196
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x1): 0.220

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x2x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x2x2048): 1.981
Elapsed time for attention_prob_times_values (512x2048x2048x2): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x2): 1.419

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x3x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x3x2048): 2.689
Elapsed time for attention_prob_times_values (512x2048x2048x3): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x3): 2.072

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x4x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x4x2048): 3.506
Elapsed time for attention_prob_times_values (512x2048x2048x4): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x4): 2.916

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 4.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x5x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x5x2048): 4.274
Elapsed time for attention_prob_times_values (512x2048x2048x5): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x5): 3.642

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 6.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x6x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x6x2048): 5.116
Elapsed time for attention_prob_times_values (512x2048x2048x6): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x6): 4.378

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 8.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x7x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x7x2048): 5.826
Elapsed time for attention_prob_times_values (512x2048x2048x7): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x7): 5.143

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 10.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x8x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x8x2048): 6.337
Elapsed time for attention_prob_times_values (512x2048x2048x8): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x8): 7.226

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 13.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x9x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x9x2048): 6.889
Elapsed time for attention_prob_times_values (512x2048x2048x9): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x9): 7.237

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 15.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x10x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x10x2048): 6.041
Elapsed time for attention_prob_times_values (512x2048x2048x10): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x10): 8.940

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 16.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x11x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x11x2048): 8.740
Elapsed time for attention_prob_times_values (512x2048x2048x11): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x11): 9.415

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 21.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x12x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x12x2048): 9.517
Elapsed time for attention_prob_times_values (512x2048x2048x12): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x12): 10.619

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 25.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x13x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x13x2048): 10.349
Elapsed time for attention_prob_times_values (512x2048x2048x13): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x13): 10.980

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 27.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x14x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x14x2048): 11.107
Elapsed time for attention_prob_times_values (512x2048x2048x14): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x14): 11.582

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 31.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x15x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x15x2048): 11.922
Elapsed time for attention_prob_times_values (512x2048x2048x15): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x15): 12.780

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 35.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x16x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x16x2048): 12.742
Elapsed time for attention_prob_times_values (512x2048x2048x16): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x16): 14.187

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 40.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x17x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x17x2048): 13.031
Elapsed time for attention_prob_times_values (512x2048x2048x17): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x17): 14.423

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 42.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x18x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x18x2048): 13.839
Elapsed time for attention_prob_times_values (512x2048x2048x18): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x18): 15.650

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 47.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x19x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x19x2048): 14.482
Elapsed time for attention_prob_times_values (512x2048x2048x19): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x19): 16.053

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 51.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x20x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x20x2048): 15.223
Elapsed time for attention_prob_times_values (512x2048x2048x20): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x20): 17.297

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 56.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x21x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x21x2048): 16.037
Elapsed time for attention_prob_times_values (512x2048x2048x21): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x21): 17.573

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 60.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x22x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x22x2048): 16.782
Elapsed time for attention_prob_times_values (512x2048x2048x22): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x22): 18.848

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 66.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x23x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x23x2048): 17.525
Elapsed time for attention_prob_times_values (512x2048x2048x23): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x23): 19.030

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 70.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x24x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x24x2048): 18.275
Elapsed time for attention_prob_times_values (512x2048x2048x24): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x24): 20.677

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 77.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x25x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x25x2048): 17.919
Elapsed time for attention_prob_times_values (512x2048x2048x25): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x25): 19.913

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 77.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x26x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x26x2048): 19.229
Elapsed time for attention_prob_times_values (512x2048x2048x26): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x26): 21.555

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 86.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x27x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x27x2048): 19.879
Elapsed time for attention_prob_times_values (512x2048x2048x27): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x27): 21.576

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 90.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x28x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x28x2048): 20.506
Elapsed time for attention_prob_times_values (512x2048x2048x28): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x28): 22.882

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 97.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x29x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x29x2048): 21.133
Elapsed time for attention_prob_times_values (512x2048x2048x29): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x29): 23.571

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 103.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x30x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x30x2048): 20.904
Elapsed time for attention_prob_times_values (512x2048x2048x30): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x30): 24.890

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 107.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x31x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x31x2048): 22.669
Elapsed time for attention_prob_times_values (512x2048x2048x31): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x31): 25.056

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 116.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x32x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x32x2048): 40.959
Elapsed time for attention_prob_times_values (512x2048x2048x32): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x32): 26.103

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 159.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x33x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x33x2048): 27.042
Elapsed time for attention_prob_times_values (512x2048x2048x33): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x33): 26.645

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 137.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x34x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x34x2048): 27.373
Elapsed time for attention_prob_times_values (512x2048x2048x34): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x34): 27.629

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 144.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x35x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x35x2048): 24.991
Elapsed time for attention_prob_times_values (512x2048x2048x35): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x35): 28.064

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 142.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x36x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x36x2048): 26.336
Elapsed time for attention_prob_times_values (512x2048x2048x36): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x36): 29.381

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 152.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x37x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x37x2048): 25.454
Elapsed time for attention_prob_times_values (512x2048x2048x37): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x37): 27.780

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 149.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x38x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x38x2048): 26.340
Elapsed time for attention_prob_times_values (512x2048x2048x38): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x38): 30.804

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 163.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x39x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x39x2048): 27.695
Elapsed time for attention_prob_times_values (512x2048x2048x39): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x39): 31.077

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 172.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x40x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x40x2048): 27.786
Elapsed time for attention_prob_times_values (512x2048x2048x40): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x40): 32.941

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 180.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x41x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x41x2048): 26.851
Elapsed time for attention_prob_times_values (512x2048x2048x41): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x41): 31.662

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 177.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x42x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x42x2048): 28.990
Elapsed time for attention_prob_times_values (512x2048x2048x42): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x42): 33.131

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 193.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x43x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x43x2048): 29.358
Elapsed time for attention_prob_times_values (512x2048x2048x43): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x43): 33.594

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 199.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x44x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x44x2048): 30.142
Elapsed time for attention_prob_times_values (512x2048x2048x44): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x44): 34.912

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 210.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x45x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x45x2048): 30.602
Elapsed time for attention_prob_times_values (512x2048x2048x45): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x45): 32.746

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 209.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x46x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x46x2048): 31.703
Elapsed time for attention_prob_times_values (512x2048x2048x46): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x46): 36.409

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 228.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x47x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x47x2048): 30.684
Elapsed time for attention_prob_times_values (512x2048x2048x47): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x47): 36.326

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 228.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x48x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x48x2048): 33.301
Elapsed time for attention_prob_times_values (512x2048x2048x48): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x48): 37.352

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 246.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x49x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x49x2048): 33.503
Elapsed time for attention_prob_times_values (512x2048x2048x49): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x49): 38.012

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 253.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x50x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x50x2048): 14.977
Elapsed time for attention_prob_times_values (512x2048x2048x50): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x50): 38.881

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 156.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x51x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x51x2048): 34.834
Elapsed time for attention_prob_times_values (512x2048x2048x51): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x51): 38.872

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 270.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x52x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x52x2048): 35.627
Elapsed time for attention_prob_times_values (512x2048x2048x52): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x52): 40.721

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 285.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x53x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x53x2048): 35.270
Elapsed time for attention_prob_times_values (512x2048x2048x53): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x53): 38.091

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 279.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x54x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x54x2048): 36.194
Elapsed time for attention_prob_times_values (512x2048x2048x54): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x54): 42.174

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 301.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x55x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x55x2048): 36.339
Elapsed time for attention_prob_times_values (512x2048x2048x55): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x55): 41.283

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 304.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x56x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x56x2048): 38.340
Elapsed time for attention_prob_times_values (512x2048x2048x56): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x56): 43.884

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 327.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x57x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x57x2048): 37.880
Elapsed time for attention_prob_times_values (512x2048x2048x57): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x57): 40.904

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 319.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x58x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x58x2048): 38.208
Elapsed time for attention_prob_times_values (512x2048x2048x58): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x58): 44.781

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 340.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x59x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x59x2048): 37.821
Elapsed time for attention_prob_times_values (512x2048x2048x59): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x59): 43.635

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 339.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x60x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x60x2048): 39.742
Elapsed time for attention_prob_times_values (512x2048x2048x60): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x60): 46.453

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 364.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x61x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x61x2048): 39.800
Elapsed time for attention_prob_times_values (512x2048x2048x61): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x61): 44.326

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 361.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x62x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x62x2048): 39.458
Elapsed time for attention_prob_times_values (512x2048x2048x62): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x62): 45.804

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 370.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x63x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x63x2048): 41.217
Elapsed time for attention_prob_times_values (512x2048x2048x63): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x63): 45.176

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 382.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 55.214
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 51.186

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 478.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 13.788
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 33.455

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 178.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 44.192
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 34.952

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 361.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 42.458
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 34.087

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 354.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 44.965
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 34.629

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 371.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 43.483
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 35.516

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 376.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 45.641
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 36.873

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 397.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 44.104
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 36.468

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 394.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 46.178
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 35.501

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 401.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 43.552
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 35.942

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 398.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 45.250
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 37.315

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 419.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 46.183
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 38.000

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 432.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 45.664
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 38.734

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 440.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 45.397
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 37.128

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 434.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 48.096
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 39.617

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 467.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 46.023
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 39.752

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 463.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 49.915
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 38.500

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 478.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 47.314
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 38.529

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 472.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 47.821
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 42.247

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 504.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 48.438
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 40.697

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 503.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 47.668
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 42.006

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 513.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 48.150
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 40.696

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 512.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 49.198
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 43.198

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 540.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 49.520
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 41.622

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 537.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 49.695
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 41.007

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 539.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 47.781
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 43.988

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 555.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 47.583
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 45.454

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 569.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 48.809
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 44.180

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 573.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0247
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 15.989
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 45.750

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 296.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0333
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 49.280
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 42.921

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 579.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 49.755
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 47.453

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 619.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 50.361
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 46.292

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 621.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 64.213
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 47.799

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 712.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 52.160
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 47.328

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 651.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 52.220
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 49.060

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 670.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 52.635
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 47.003

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 664.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 53.890
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 49.972

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 700.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 52.389
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 47.332

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 677.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 54.305
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 49.220

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 710.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 53.176
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 47.902

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 699.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 55.419
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 49.075

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 728.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 52.125
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 49.490

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 717.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 51.882
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 52.380

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 742.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 54.286
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 50.924

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 755.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 54.764
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 53.210

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 782.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 53.528
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 51.149

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 765.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 54.747
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 53.799

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 800.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 55.743
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 52.309

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 802.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 55.820
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 52.821

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 814.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 55.385
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 50.901

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 802.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 54.483
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 54.348

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 829.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 55.384
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 53.436

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 836.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 56.218
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 55.013

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 861.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 55.737
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 54.428

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 860.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 56.127
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 56.719

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 888.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 57.508
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 53.335

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 878.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 59.225
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 56.898

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 928.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 56.017
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 43.425

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 788.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 57.073
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 53.982

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 901.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 56.905
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 44.448

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 817.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 59.157
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 58.331

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 969.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 56.471
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 44.007

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 822.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 58.342
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 59.639

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 987.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 56.310
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 43.926

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 832.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 67.843
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 62.799

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1108.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 56.520
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 43.094

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 837.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 57.833
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 47.206

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 896.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 58.430
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 44.863

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 881.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 57.771
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 46.957

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 906.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 57.332
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 46.321

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 903.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 60.150
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 48.513

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 953.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 57.932
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 46.768

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 925.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 61.362
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 44.869

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 933.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 59.373
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 47.155

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 952.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 60.448
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 49.632

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 994.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 59.747
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 47.773

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 975.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 61.268
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 50.680

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1026.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 59.051
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 47.991

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 986.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 61.675
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 50.353

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1039.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 58.300
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 48.347

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 997.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 62.280
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 48.754

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1039.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 60.825
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 49.433

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1043.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 61.787
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 52.177

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1089.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 60.557
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 49.366

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1053.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 61.984
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 50.941

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1090.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 60.110
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 50.883

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1081.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 63.207
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 53.306

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1142.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 62.739
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 50.641

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1113.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 64.600
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 50.395

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1132.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 62.327
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 51.411

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1133.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 62.460
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 54.582

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1179.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 62.615
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 51.762

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1154.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 63.422
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 54.486

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1201.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 61.296
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 52.769

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1169.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 64.768
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 54.731

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1231.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 64.130
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 53.371

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1216.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 77.871
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 55.751

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1364.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 60.920
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 54.769

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1218.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 64.387
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 55.540

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1267.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 63.592
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 52.811

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1233.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 65.442
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 57.025

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1310.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 64.069
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 55.936

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1291.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 65.285
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 58.005

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1336.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 63.557
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 56.634

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1310.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 64.806
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 56.326

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1325.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 62.899
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 56.810

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1320.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 64.692
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 59.047

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1373.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 64.751
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 57.228

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1359.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 66.367
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 59.168

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1407.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 65.308
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 56.801

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1374.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 65.248
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 59.895

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1420.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 62.857
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 58.682

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1388.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 68.868
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 56.164

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1423.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 65.543
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 54.820

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1380.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 66.879
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 59.309

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1461.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 65.168
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 57.522

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1428.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 66.187
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 62.274

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1508.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 66.205
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 60.239

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1490.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 67.900
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 61.840

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1537.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 63.481
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 60.872

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1483.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 67.419
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 58.772

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1507.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 66.669
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 60.944

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1536.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 67.973
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 63.714

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1595.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 67.121
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 62.082

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1572.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 67.099
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 64.056

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1605.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 65.787
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 60.952

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1558.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 67.409
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 64.756

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1634.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 65.815
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 58.807

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1545.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 78.551
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 66.461

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1800.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 65.407
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 52.614

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1465.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 66.968
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 55.203

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1528.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 67.244
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 52.429

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1495.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 69.153
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 53.426

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1537.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 67.552
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 54.045

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 1538.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 65.109
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 54.466

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 1527.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0287
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 68.400
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 54.168

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1564.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 68.835
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 50.089

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1507.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 67.957
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 52.226

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1542.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 67.973
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 54.926

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1594.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 65.572
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 51.347

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1519.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 69.952
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 55.247

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1636.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 66.997
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 52.800

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 1572.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 70.087
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 55.646

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1659.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 67.205
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 52.889

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1590.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 69.767
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 53.308

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1631.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 68.769
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 53.888

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1639.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 67.653
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 56.553

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 1678.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 66.707
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 54.170

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1636.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 67.360
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 55.382

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1671.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 69.542
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 54.540

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1688.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 69.727
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 57.400

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1747.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 67.500
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 55.161

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1692.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 70.622
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 54.136

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1716.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 69.715
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 53.355

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1700.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 70.975
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 57.346

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1792.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 70.081
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 55.313

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1754.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 71.624
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 58.862

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1841.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 70.585
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 56.496

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1796.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 71.732
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 58.760

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1857.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 70.427
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 56.860

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1816.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 80.142
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 57.313

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1938.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 69.524
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 56.811

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1821.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 70.615
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 59.998

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1897.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 69.839
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 58.103

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1863.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 71.470
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 57.971

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1888.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 68.018
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 58.452

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1862.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 70.251
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 59.367

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1914.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 70.197
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 57.218

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1883.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 69.808
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 57.405

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1890.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 70.072
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 58.833

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1926.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 69.772
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 59.351

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1940.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 70.273
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 59.259

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1953.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 69.440
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 59.303

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1951.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 70.254
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 59.720

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1977.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 70.329
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 60.397

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1998.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.729
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 58.642

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 1966.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 54.389
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 58.799

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1751.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 54.080
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 60.580

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 1778.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0189
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 54.891
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 63.039

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1833.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0192
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 54.428
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 59.643

Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 1785.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 54.816
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 63.811

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 1857.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0355
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0198
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 53.133
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 59.373

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 1773.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 55.342
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 64.070

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1885.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 54.723
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 61.081

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1840.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0193
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 55.155
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 62.284

Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 1872.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0364
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 52.996
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 62.919

Attention duration (in seconds): 0.0372
Attention throughput (in TFLOP/s): 1848.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0372
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0203
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 52.920
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 64.919

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1880.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0198
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 54.511
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 63.232

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1895.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0197
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 54.817
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 64.320

Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 1923.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0200
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 54.448
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 63.700

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 1915.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 54.712
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 65.823

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1956.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0211
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 52.012
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 66.099

Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 1913.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x1x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x1x2048): 0.686
Elapsed time for attention_prob_times_values (1024x2048x2048x1): 0.0393
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x1): 0.219

Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 0.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x2x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x2x2048): 2.006
Elapsed time for attention_prob_times_values (1024x2048x2048x2): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x2): 1.468

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x3x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x3x2048): 2.697
Elapsed time for attention_prob_times_values (1024x2048x2048x3): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x3): 2.082

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 4.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x4x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x4x2048): 3.521
Elapsed time for attention_prob_times_values (1024x2048x2048x4): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x4): 3.070

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 6.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x5x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x5x2048): 4.285
Elapsed time for attention_prob_times_values (1024x2048x2048x5): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x5): 3.627

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 8.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x6x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x6x2048): 5.135
Elapsed time for attention_prob_times_values (1024x2048x2048x6): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x6): 4.575

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 12.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x7x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x7x2048): 5.948
Elapsed time for attention_prob_times_values (1024x2048x2048x7): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x7): 5.283

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 15.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x8x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x8x2048): 6.522
Elapsed time for attention_prob_times_values (1024x2048x2048x8): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x8): 6.995

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 20.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x9x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x9x2048): 7.064
Elapsed time for attention_prob_times_values (1024x2048x2048x9): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x9): 7.629

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 23.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x10x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x10x2048): 8.035
Elapsed time for attention_prob_times_values (1024x2048x2048x10): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x10): 8.404

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 28.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x11x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x11x2048): 8.829
Elapsed time for attention_prob_times_values (1024x2048x2048x11): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x11): 9.433

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 34.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x12x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x12x2048): 9.313
Elapsed time for attention_prob_times_values (1024x2048x2048x12): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x12): 10.516

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 39.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x13x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x13x2048): 10.199
Elapsed time for attention_prob_times_values (1024x2048x2048x13): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x13): 11.133

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 45.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x14x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x14x2048): 10.770
Elapsed time for attention_prob_times_values (1024x2048x2048x14): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x14): 12.454

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 51.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x15x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x15x2048): 11.947
Elapsed time for attention_prob_times_values (1024x2048x2048x15): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x15): 12.838

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 58.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x16x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x16x2048): 12.810
Elapsed time for attention_prob_times_values (1024x2048x2048x16): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x16): 13.883

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 66.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x17x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x17x2048): 13.122
Elapsed time for attention_prob_times_values (1024x2048x2048x17): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x17): 14.132

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 71.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x18x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x18x2048): 13.872
Elapsed time for attention_prob_times_values (1024x2048x2048x18): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x18): 15.299

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 80.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x19x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x19x2048): 14.569
Elapsed time for attention_prob_times_values (1024x2048x2048x19): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x19): 16.129

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 88.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x20x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x20x2048): 15.306
Elapsed time for attention_prob_times_values (1024x2048x2048x20): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x20): 17.395

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 97.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x21x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x21x2048): 15.482
Elapsed time for attention_prob_times_values (1024x2048x2048x21): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x21): 17.138

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 101.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x22x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x22x2048): 16.228
Elapsed time for attention_prob_times_values (1024x2048x2048x22): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x22): 18.904

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 113.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x23x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x23x2048): 17.452
Elapsed time for attention_prob_times_values (1024x2048x2048x23): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x23): 18.631

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 121.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x24x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x24x2048): 17.692
Elapsed time for attention_prob_times_values (1024x2048x2048x24): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x24): 20.003

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 131.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x25x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x25x2048): 18.155
Elapsed time for attention_prob_times_values (1024x2048x2048x25): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x25): 20.384

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 139.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x26x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x26x2048): 18.836
Elapsed time for attention_prob_times_values (1024x2048x2048x26): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x26): 21.339

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 150.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x27x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x27x2048): 19.127
Elapsed time for attention_prob_times_values (1024x2048x2048x27): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x27): 21.569

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 157.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x28x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x28x2048): 19.850
Elapsed time for attention_prob_times_values (1024x2048x2048x28): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x28): 22.831

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 169.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x29x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x29x2048): 20.561
Elapsed time for attention_prob_times_values (1024x2048x2048x29): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x29): 23.045

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 179.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x30x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x30x2048): 21.416
Elapsed time for attention_prob_times_values (1024x2048x2048x30): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x30): 14.132

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 144.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x31x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x31x2048): 22.652
Elapsed time for attention_prob_times_values (1024x2048x2048x31): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x31): 24.508

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 206.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x32x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x32x2048): 41.292
Elapsed time for attention_prob_times_values (1024x2048x2048x32): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x32): 25.973

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 286.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x33x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x33x2048): 27.205
Elapsed time for attention_prob_times_values (1024x2048x2048x33): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x33): 26.772

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 249.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x34x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x34x2048): 27.499
Elapsed time for attention_prob_times_values (1024x2048x2048x34): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x34): 27.716

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 262.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x35x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x35x2048): 26.280
Elapsed time for attention_prob_times_values (1024x2048x2048x35): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x35): 27.422

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 261.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x36x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x36x2048): 25.975
Elapsed time for attention_prob_times_values (1024x2048x2048x36): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x36): 28.369

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 271.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x37x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x37x2048): 26.355
Elapsed time for attention_prob_times_values (1024x2048x2048x37): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x37): 29.074

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 283.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x38x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x38x2048): 26.605
Elapsed time for attention_prob_times_values (1024x2048x2048x38): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x38): 30.725

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 299.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x39x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x39x2048): 27.774
Elapsed time for attention_prob_times_values (1024x2048x2048x39): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x39): 30.975

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 314.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x40x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x40x2048): 28.597
Elapsed time for attention_prob_times_values (1024x2048x2048x40): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x40): 33.077

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 337.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x41x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x41x2048): 27.277
Elapsed time for attention_prob_times_values (1024x2048x2048x41): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x41): 32.617

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 334.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x42x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x42x2048): 28.179
Elapsed time for attention_prob_times_values (1024x2048x2048x42): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x42): 33.529

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 352.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x43x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x43x2048): 29.135
Elapsed time for attention_prob_times_values (1024x2048x2048x43): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x43): 31.805

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 357.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x44x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x44x2048): 29.267
Elapsed time for attention_prob_times_values (1024x2048x2048x44): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x44): 35.144

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 383.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x45x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x45x2048): 30.752
Elapsed time for attention_prob_times_values (1024x2048x2048x45): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x45): 35.060

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 401.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x46x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x46x2048): 31.812
Elapsed time for attention_prob_times_values (1024x2048x2048x46): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x46): 35.540

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 419.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x47x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x47x2048): 32.213
Elapsed time for attention_prob_times_values (1024x2048x2048x47): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x47): 35.553

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 430.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x48x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x48x2048): 32.067
Elapsed time for attention_prob_times_values (1024x2048x2048x48): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x48): 39.080

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 457.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x49x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x49x2048): 33.354
Elapsed time for attention_prob_times_values (1024x2048x2048x49): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x49): 38.053

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 471.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x50x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x50x2048): 34.366
Elapsed time for attention_prob_times_values (1024x2048x2048x50): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x50): 39.073

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 493.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x51x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x51x2048): 34.098
Elapsed time for attention_prob_times_values (1024x2048x2048x51): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x51): 39.111

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 500.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x52x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x52x2048): 35.661
Elapsed time for attention_prob_times_values (1024x2048x2048x52): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x52): 40.923

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 533.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x53x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x53x2048): 35.141
Elapsed time for attention_prob_times_values (1024x2048x2048x53): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x53): 40.622

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 536.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x54x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x54x2048): 35.951
Elapsed time for attention_prob_times_values (1024x2048x2048x54): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x54): 41.288

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 557.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x55x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x55x2048): 35.468
Elapsed time for attention_prob_times_values (1024x2048x2048x55): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x55): 41.905

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 566.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x56x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x56x2048): 38.525
Elapsed time for attention_prob_times_values (1024x2048x2048x56): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x56): 43.888

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 615.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x57x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x57x2048): 37.880
Elapsed time for attention_prob_times_values (1024x2048x2048x57): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x57): 43.523

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 617.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x58x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x58x2048): 37.050
Elapsed time for attention_prob_times_values (1024x2048x2048x58): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x58): 45.067

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 630.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x59x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x59x2048): 39.004
Elapsed time for attention_prob_times_values (1024x2048x2048x59): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x59): 44.657

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 655.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x60x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x60x2048): 38.746
Elapsed time for attention_prob_times_values (1024x2048x2048x60): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x60): 45.764

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 671.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x61x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x61x2048): 39.025
Elapsed time for attention_prob_times_values (1024x2048x2048x61): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x61): 46.125

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 687.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x62x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x62x2048): 39.955
Elapsed time for attention_prob_times_values (1024x2048x2048x62): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x62): 48.093

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 720.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x63x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x63x2048): 41.359
Elapsed time for attention_prob_times_values (1024x2048x2048x63): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x63): 47.397

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 739.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x64x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x64x2048): 57.567
Elapsed time for attention_prob_times_values (1024x2048x2048x64): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x64): 51.359

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 922.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x65x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x65x2048): 43.290
Elapsed time for attention_prob_times_values (1024x2048x2048x65): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x65): 33.665

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 653.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x66x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x66x2048): 44.477
Elapsed time for attention_prob_times_values (1024x2048x2048x66): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x66): 34.272

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 677.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x67x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x67x2048): 43.390
Elapsed time for attention_prob_times_values (1024x2048x2048x67): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x67): 34.121

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 678.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x68x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x68x2048): 45.071
Elapsed time for attention_prob_times_values (1024x2048x2048x68): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x68): 35.775

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 717.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x69x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x69x2048): 44.424
Elapsed time for attention_prob_times_values (1024x2048x2048x69): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x69): 35.547

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 720.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x70x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x70x2048): 45.790
Elapsed time for attention_prob_times_values (1024x2048x2048x70): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x70): 36.992

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 757.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x71x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x71x2048): 45.452
Elapsed time for attention_prob_times_values (1024x2048x2048x71): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x71): 36.551

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 759.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x72x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x72x2048): 46.747
Elapsed time for attention_prob_times_values (1024x2048x2048x72): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x72): 35.963

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 772.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x73x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x73x2048): 43.377
Elapsed time for attention_prob_times_values (1024x2048x2048x73): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x73): 36.286

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 760.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x74x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x74x2048): 44.753
Elapsed time for attention_prob_times_values (1024x2048x2048x74): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x74): 36.199

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 780.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x75x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x75x2048): 44.599
Elapsed time for attention_prob_times_values (1024x2048x2048x75): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x75): 38.001

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 810.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x76x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x76x2048): 47.194
Elapsed time for attention_prob_times_values (1024x2048x2048x76): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x76): 39.846

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 864.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x77x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x77x2048): 45.425
Elapsed time for attention_prob_times_values (1024x2048x2048x77): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x77): 38.919

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 848.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x78x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x78x2048): 48.156
Elapsed time for attention_prob_times_values (1024x2048x2048x78): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x78): 40.576

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 902.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x79x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x79x2048): 48.125
Elapsed time for attention_prob_times_values (1024x2048x2048x79): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x79): 39.725

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 903.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x80x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x80x2048): 49.991
Elapsed time for attention_prob_times_values (1024x2048x2048x80): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x80): 39.392

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 925.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x81x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x81x2048): 46.293
Elapsed time for attention_prob_times_values (1024x2048x2048x81): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x81): 40.667

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 920.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x82x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x82x2048): 47.456
Elapsed time for attention_prob_times_values (1024x2048x2048x82): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x82): 42.355

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 962.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x83x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x83x2048): 48.737
Elapsed time for attention_prob_times_values (1024x2048x2048x83): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x83): 41.364

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 973.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x84x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x84x2048): 48.972
Elapsed time for attention_prob_times_values (1024x2048x2048x84): 0.0237
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x84): 30.478

Attention duration (in seconds): 0.0384
Attention throughput (in TFLOP/s): 826.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0384
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x85x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x85x2048): 49.130
Elapsed time for attention_prob_times_values (1024x2048x2048x85): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x85): 41.399

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 999.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x86x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x86x2048): 49.043
Elapsed time for attention_prob_times_values (1024x2048x2048x86): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x86): 41.484

Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 1011.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x87x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x87x2048): 48.745
Elapsed time for attention_prob_times_values (1024x2048x2048x87): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x87): 42.391

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 1031.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x88x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x88x2048): 50.130
Elapsed time for attention_prob_times_values (1024x2048x2048x88): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x88): 43.085

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 1065.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x89x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x89x2048): 47.246
Elapsed time for attention_prob_times_values (1024x2048x2048x89): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x89): 43.184

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1049.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x90x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x90x2048): 49.727
Elapsed time for attention_prob_times_values (1024x2048x2048x90): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x90): 44.766

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 1107.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0328
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x91x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x91x2048): 49.798
Elapsed time for attention_prob_times_values (1024x2048x2048x91): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x91): 43.632

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1104.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x92x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x92x2048): 48.812
Elapsed time for attention_prob_times_values (1024x2048x2048x92): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x92): 46.091

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 1137.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0333
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x93x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x93x2048): 50.589
Elapsed time for attention_prob_times_values (1024x2048x2048x93): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x93): 45.128

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 1156.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x94x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x94x2048): 50.569
Elapsed time for attention_prob_times_values (1024x2048x2048x94): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x94): 47.374

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 1198.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x95x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x95x2048): 50.224
Elapsed time for attention_prob_times_values (1024x2048x2048x95): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x95): 45.922

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1187.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x96x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x96x2048): 63.907
Elapsed time for attention_prob_times_values (1024x2048x2048x96): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x96): 47.661

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1365.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x97x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x97x2048): 52.114
Elapsed time for attention_prob_times_values (1024x2048x2048x97): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x97): 46.983

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 1247.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x98x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x98x2048): 51.182
Elapsed time for attention_prob_times_values (1024x2048x2048x98): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x98): 48.788

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 1273.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x99x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x99x2048): 52.615
Elapsed time for attention_prob_times_values (1024x2048x2048x99): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x99): 47.385

Attention duration (in seconds): 0.0341
Attention throughput (in TFLOP/s): 1283.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0341
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x100x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x100x2048): 53.924
Elapsed time for attention_prob_times_values (1024x2048x2048x100): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x100): 49.299

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 1339.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x101x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x101x2048): 53.165
Elapsed time for attention_prob_times_values (1024x2048x2048x101): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x101): 47.455

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1316.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x102x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x102x2048): 51.078
Elapsed time for attention_prob_times_values (1024x2048x2048x102): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x102): 49.805

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 1336.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x103x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x103x2048): 52.942
Elapsed time for attention_prob_times_values (1024x2048x2048x103): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x103): 49.470

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1368.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x104x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x104x2048): 55.328
Elapsed time for attention_prob_times_values (1024x2048x2048x104): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x104): 50.161

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1420.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x105x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x105x2048): 51.633
Elapsed time for attention_prob_times_values (1024x2048x2048x105): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x105): 49.649

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1379.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x106x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x106x2048): 53.218
Elapsed time for attention_prob_times_values (1024x2048x2048x106): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x106): 52.152

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1448.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x107x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x107x2048): 52.437
Elapsed time for attention_prob_times_values (1024x2048x2048x107): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x107): 49.825

Attention duration (in seconds): 0.0360
Attention throughput (in TFLOP/s): 1417.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x108x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x108x2048): 55.555
Elapsed time for attention_prob_times_values (1024x2048x2048x108): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x108): 53.034

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 1519.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x109x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x109x2048): 53.669
Elapsed time for attention_prob_times_values (1024x2048x2048x109): 0.0191
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x109): 48.987

Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 1447.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x110x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x110x2048): 55.554
Elapsed time for attention_prob_times_values (1024x2048x2048x110): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x110): 52.045

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1531.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x111x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x111x2048): 54.731
Elapsed time for attention_prob_times_values (1024x2048x2048x111): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x111): 51.868

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1531.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x112x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x112x2048): 57.781
Elapsed time for attention_prob_times_values (1024x2048x2048x112): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x112): 53.817

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 1616.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x113x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x113x2048): 55.396
Elapsed time for attention_prob_times_values (1024x2048x2048x113): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x113): 52.617

Attention duration (in seconds): 0.0360
Attention throughput (in TFLOP/s): 1578.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x114x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x114x2048): 56.094
Elapsed time for attention_prob_times_values (1024x2048x2048x114): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x114): 53.651

Attention duration (in seconds): 0.0357
Attention throughput (in TFLOP/s): 1617.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0357
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x115x2048): 0.0181
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x115x2048): 54.703
Elapsed time for attention_prob_times_values (1024x2048x2048x115): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x115): 53.533

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1609.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x116x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x116x2048): 57.012
Elapsed time for attention_prob_times_values (1024x2048x2048x116): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x116): 54.154

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 1666.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x117x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x117x2048): 55.685
Elapsed time for attention_prob_times_values (1024x2048x2048x117): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x117): 51.071

Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 1611.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0377
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x118x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x118x2048): 56.968
Elapsed time for attention_prob_times_values (1024x2048x2048x118): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x118): 56.026

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 1723.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x119x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x119x2048): 57.476
Elapsed time for attention_prob_times_values (1024x2048x2048x119): 0.0194
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x119): 52.801

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 1692.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x120x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x120x2048): 58.598
Elapsed time for attention_prob_times_values (1024x2048x2048x120): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x120): 56.758

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1787.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x121x2048): 0.0183
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x121x2048): 56.766
Elapsed time for attention_prob_times_values (1024x2048x2048x121): 0.0245
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x121): 42.503

Attention duration (in seconds): 0.0428
Attention throughput (in TFLOP/s): 1519.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0428
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x122x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x122x2048): 58.105
Elapsed time for attention_prob_times_values (1024x2048x2048x122): 0.0189
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x122): 55.434

Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 1787.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x123x2048): 0.0187
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x123x2048): 56.436
Elapsed time for attention_prob_times_values (1024x2048x2048x123): 0.0238
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x123): 44.456

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 1579.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x124x2048): 0.0185
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x124x2048): 57.567
Elapsed time for attention_prob_times_values (1024x2048x2048x124): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x124): 57.519

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 1841.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x125x2048): 0.0188
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x125x2048): 57.078
Elapsed time for attention_prob_times_values (1024x2048x2048x125): 0.0246
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x125): 43.648

Attention duration (in seconds): 0.0434
Attention throughput (in TFLOP/s): 1595.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0434
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x126x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x126x2048): 56.715
Elapsed time for attention_prob_times_values (1024x2048x2048x126): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x126): 59.453

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 1886.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0373
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x127x2048): 0.0189
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x127x2048): 57.584
Elapsed time for attention_prob_times_values (1024x2048x2048x127): 0.0247
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x127): 44.092

Attention duration (in seconds): 0.0437
Attention throughput (in TFLOP/s): 1635.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x1x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x1x2048): 1.059
Elapsed time for attention_prob_times_values (2048x2048x2048x1): 0.0788
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x1): 0.218

Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 0.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x2x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x2x2048): 1.997
Elapsed time for attention_prob_times_values (2048x2048x2048x2): 0.0381
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x2): 0.903

Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 2.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x3x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x3x2048): 2.713
Elapsed time for attention_prob_times_values (2048x2048x2048x3): 0.0252
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x3): 2.042

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 5.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x4x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x4x2048): 3.450
Elapsed time for attention_prob_times_values (2048x2048x2048x4): 0.0223
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x4): 3.082

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 9.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x5x2048): 0.0204
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x5x2048): 4.211
Elapsed time for attention_prob_times_values (2048x2048x2048x5): 0.0236
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x5): 3.647

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 13.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x6x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x6x2048): 5.125
Elapsed time for attention_prob_times_values (2048x2048x2048x6): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x6): 4.511

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 19.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x7x2048): 0.0197
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x7x2048): 6.094
Elapsed time for attention_prob_times_values (2048x2048x2048x7): 0.0232
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x7): 5.193

Attention duration (in seconds): 0.0429
Attention throughput (in TFLOP/s): 25.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x8x2048): 0.0210
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x8x2048): 6.541
Elapsed time for attention_prob_times_values (2048x2048x2048x8): 0.0189
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x8): 7.276

Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 34.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x9x2048): 0.0300
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x9x2048): 5.147
Elapsed time for attention_prob_times_values (2048x2048x2048x9): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x9): 7.758

Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 34.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x10x2048): 0.0220
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x10x2048): 7.801
Elapsed time for attention_prob_times_values (2048x2048x2048x10): 0.0193
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x10): 8.923

Attention duration (in seconds): 0.0413
Attention throughput (in TFLOP/s): 49.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x11x2048): 0.0214
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x11x2048): 8.812
Elapsed time for attention_prob_times_values (2048x2048x2048x11): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x11): 9.341

Attention duration (in seconds): 0.0417
Attention throughput (in TFLOP/s): 58.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x12x2048): 0.0215
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x12x2048): 9.582
Elapsed time for attention_prob_times_values (2048x2048x2048x12): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x12): 10.467

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 70.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0412
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x13x2048): 0.0216
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x13x2048): 10.329
Elapsed time for attention_prob_times_values (2048x2048x2048x13): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x13): 10.816

Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 79.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0423
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x14x2048): 0.0219
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x14x2048): 10.980
Elapsed time for attention_prob_times_values (2048x2048x2048x14): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x14): 12.094

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 92.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x15x2048): 0.0222
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x15x2048): 11.593
Elapsed time for attention_prob_times_values (2048x2048x2048x15): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x15): 12.740

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 103.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x16x2048): 0.0221
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x16x2048): 12.449
Elapsed time for attention_prob_times_values (2048x2048x2048x16): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x16): 13.961

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 118.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x17x2048): 0.0223
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x17x2048): 13.111
Elapsed time for attention_prob_times_values (2048x2048x2048x17): 0.0203
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x17): 14.412

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 130.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x18x2048): 0.0227
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x18x2048): 13.594
Elapsed time for attention_prob_times_values (2048x2048x2048x18): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x18): 15.684

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 145.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x19x2048): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x19x2048): 13.935
Elapsed time for attention_prob_times_values (2048x2048x2048x19): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x19): 16.145

Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 157.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0436
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x20x2048): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x20x2048): 14.713
Elapsed time for attention_prob_times_values (2048x2048x2048x20): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x20): 17.279

Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 174.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0432
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x21x2048): 0.0226
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x21x2048): 15.970
Elapsed time for attention_prob_times_values (2048x2048x2048x21): 0.0204
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x21): 17.681

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 192.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x22x2048): 0.0231
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x22x2048): 16.357
Elapsed time for attention_prob_times_values (2048x2048x2048x22): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x22): 18.977

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 210.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x23x2048): 0.0226
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x23x2048): 17.492
Elapsed time for attention_prob_times_values (2048x2048x2048x23): 0.0215
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x23): 18.376

Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 224.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x24x2048): 0.0231
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x24x2048): 17.849
Elapsed time for attention_prob_times_values (2048x2048x2048x24): 0.0200
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x24): 20.648

Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 248.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x25x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x25x2048): 17.078
Elapsed time for attention_prob_times_values (2048x2048x2048x25): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x25): 20.875

Attention duration (in seconds): 0.0457
Attention throughput (in TFLOP/s): 253.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x26x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x26x2048): 18.836
Elapsed time for attention_prob_times_values (2048x2048x2048x26): 0.0205
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x26): 21.839

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 283.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x27x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x27x2048): 19.608
Elapsed time for attention_prob_times_values (2048x2048x2048x27): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x27): 22.111

Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 301.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x28x2048): 0.0235
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x28x2048): 20.489
Elapsed time for attention_prob_times_values (2048x2048x2048x28): 0.0204
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x28): 23.523

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 328.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x29x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x29x2048): 21.011
Elapsed time for attention_prob_times_values (2048x2048x2048x29): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x29): 23.720

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 345.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x30x2048): 0.0241
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x30x2048): 21.396
Elapsed time for attention_prob_times_values (2048x2048x2048x30): 0.0208
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x30): 24.827

Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 367.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x31x2048): 0.0236
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x31x2048): 22.606
Elapsed time for attention_prob_times_values (2048x2048x2048x31): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x31): 25.032

Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 391.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x32x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x32x2048): 41.244
Elapsed time for attention_prob_times_values (2048x2048x2048x32): 0.0205
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x32): 26.882

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 553.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x33x2048): 0.0209
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x33x2048): 27.077
Elapsed time for attention_prob_times_values (2048x2048x2048x33): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x33): 26.914

Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 472.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x34x2048): 0.0220
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x34x2048): 26.551
Elapsed time for attention_prob_times_values (2048x2048x2048x34): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x34): 26.612

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 478.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x35x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x35x2048): 25.369
Elapsed time for attention_prob_times_values (2048x2048x2048x35): 0.0215
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x35): 28.016

Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 492.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x36x2048): 0.0230
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x36x2048): 26.870
Elapsed time for attention_prob_times_values (2048x2048x2048x36): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x36): 29.449

Attention duration (in seconds): 0.0440
Attention throughput (in TFLOP/s): 533.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0440
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x37x2048): 0.0238
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x37x2048): 26.705
Elapsed time for attention_prob_times_values (2048x2048x2048x37): 0.0217
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x37): 29.304

Attention duration (in seconds): 0.0455
Attention throughput (in TFLOP/s): 544.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0455
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x38x2048): 0.0240
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x38x2048): 27.201
Elapsed time for attention_prob_times_values (2048x2048x2048x38): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x38): 30.945

Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 579.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0451
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x39x2048): 0.0244
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x39x2048): 27.438
Elapsed time for attention_prob_times_values (2048x2048x2048x39): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x39): 30.337

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 590.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x40x2048): 0.0239
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x40x2048): 28.772
Elapsed time for attention_prob_times_values (2048x2048x2048x40): 0.0208
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x40): 33.073

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 646.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x41x2048): 0.0260
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x41x2048): 27.118
Elapsed time for attention_prob_times_values (2048x2048x2048x41): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x41): 32.149

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 632.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0479
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x42x2048): 0.0248
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x42x2048): 29.114
Elapsed time for attention_prob_times_values (2048x2048x2048x42): 0.0216
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x42): 33.474

Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 685.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0463
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x43x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x43x2048): 29.320
Elapsed time for attention_prob_times_values (2048x2048x2048x43): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x43): 33.416

Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 702.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0473
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x44x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x44x2048): 30.009
Elapsed time for attention_prob_times_values (2048x2048x2048x44): 0.0217
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x44): 34.849

Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 741.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x45x2048): 0.0255
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x45x2048): 30.338
Elapsed time for attention_prob_times_values (2048x2048x2048x45): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x45): 33.977

Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 753.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x46x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x46x2048): 31.495
Elapsed time for attention_prob_times_values (2048x2048x2048x46): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x46): 35.707

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 803.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x47x2048): 0.0257
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x47x2048): 31.443
Elapsed time for attention_prob_times_values (2048x2048x2048x47): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x47): 36.481

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 827.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0478
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x48x2048): 0.0250
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x48x2048): 32.979
Elapsed time for attention_prob_times_values (2048x2048x2048x48): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x48): 39.060

Attention duration (in seconds): 0.0461
Attention throughput (in TFLOP/s): 894.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x49x2048): 0.0262
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x49x2048): 32.077
Elapsed time for attention_prob_times_values (2048x2048x2048x49): 0.0222
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x49): 37.906

Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 886.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x50x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x50x2048): 34.286
Elapsed time for attention_prob_times_values (2048x2048x2048x50): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x50): 39.255

Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 951.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x51x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x51x2048): 34.800
Elapsed time for attention_prob_times_values (2048x2048x2048x51): 0.0225
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x51): 38.951

Attention duration (in seconds): 0.0477
Attention throughput (in TFLOP/s): 974.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0477
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x52x2048): 0.0253
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x52x2048): 35.267
Elapsed time for attention_prob_times_values (2048x2048x2048x52): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x52): 40.457

Attention duration (in seconds): 0.0474
Attention throughput (in TFLOP/s): 1017.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x53x2048): 0.0253
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x53x2048): 35.991
Elapsed time for attention_prob_times_values (2048x2048x2048x53): 0.0223
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x53): 40.842

Attention duration (in seconds): 0.0476
Attention throughput (in TFLOP/s): 1052.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0476
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x54x2048): 0.0254
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x54x2048): 36.511
Elapsed time for attention_prob_times_values (2048x2048x2048x54): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x54): 41.334

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 1085.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0479
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x55x2048): 0.0256
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x55x2048): 36.891
Elapsed time for attention_prob_times_values (2048x2048x2048x55): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x55): 41.451

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 1112.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x56x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x56x2048): 38.200
Elapsed time for attention_prob_times_values (2048x2048x2048x56): 0.0214
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x56): 45.048

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 1198.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x57x2048): 0.0260
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x57x2048): 37.667
Elapsed time for attention_prob_times_values (2048x2048x2048x57): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x57): 43.642

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 1192.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x58x2048): 0.0259
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x58x2048): 38.428
Elapsed time for attention_prob_times_values (2048x2048x2048x58): 0.0229
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x58): 43.558

Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 1224.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x59x2048): 0.0266
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x59x2048): 38.095
Elapsed time for attention_prob_times_values (2048x2048x2048x59): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x59): 44.567

Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 1252.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0494
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x60x2048): 0.0261
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x60x2048): 39.535
Elapsed time for attention_prob_times_values (2048x2048x2048x60): 0.0220
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x60): 46.847

Attention duration (in seconds): 0.0481
Attention throughput (in TFLOP/s): 1329.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0481
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x61x2048): 0.0262
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x61x2048): 40.058
Elapsed time for attention_prob_times_values (2048x2048x2048x61): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x61): 45.421

Attention duration (in seconds): 0.0492
Attention throughput (in TFLOP/s): 1341.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x62x2048): 0.0265
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x62x2048): 40.264
Elapsed time for attention_prob_times_values (2048x2048x2048x62): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x62): 46.964

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 1387.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x63x2048): 0.0265
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x63x2048): 40.769
Elapsed time for attention_prob_times_values (2048x2048x2048x63): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x63): 47.643

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 1428.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
