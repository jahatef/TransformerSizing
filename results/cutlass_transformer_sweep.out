cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
[2023-09-04 19:05:53,045] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-09-04 19:05:53,521] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.158.60, master_port=6000
[2023-09-04 19:05:53,521] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-04 19:05:53,525] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 251.487
b: 256, m: 2048, n: 128, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 94.239
b: 256, m: 2048, n: 2048, k: 128,
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 77.434
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 213.751
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 254.396
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 253.466

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 200.100
MLP duration (in seconds): 0.0346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 244.929
b: 256, m: 2048, n: 130, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 83.561
b: 256, m: 2048, n: 2048, k: 130,
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 52.295
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 209.871
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 250.098
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 252.116

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 182.125
MLP duration (in seconds): 0.0361
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 249.086
b: 256, m: 2048, n: 132, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 84.822
b: 256, m: 2048, n: 2048, k: 132,
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 66.900
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 213.812
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 253.112
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 253.419

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 193.290
MLP duration (in seconds): 0.0369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 251.559
b: 256, m: 2048, n: 134, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 85.558
b: 256, m: 2048, n: 2048, k: 134,
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 53.281
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 217.243
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 256.400
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0190
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 254.266

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 188.120
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0664
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 245.015
b: 256, m: 2048, n: 136, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 86.651
b: 256, m: 2048, n: 2048, k: 136,
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 73.581
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 213.376
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 249.245
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0195
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 255.116

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 195.702
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0677
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 248.389
b: 256, m: 2048, n: 138, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 87.770
b: 256, m: 2048, n: 2048, k: 138,
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 54.291
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 216.679
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 252.626
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 255.949

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 189.094
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 251.898
b: 256, m: 2048, n: 140, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 89.713
b: 256, m: 2048, n: 2048, k: 140,
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 69.742
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 220.421
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 258.035
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 256.899

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 200.127
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 246.339
b: 256, m: 2048, n: 142, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 90.339
b: 256, m: 2048, n: 2048, k: 142,
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 55.465
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 216.356
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 246.123
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 255.614

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 190.545
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0748
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 249.979
b: 256, m: 2048, n: 144, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 91.728
b: 256, m: 2048, n: 2048, k: 144,
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 77.620
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 219.671
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 253.053
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 256.512

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 203.511
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0741
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 252.094
b: 256, m: 2048, n: 146, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 92.407
b: 256, m: 2048, n: 2048, k: 146,
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 56.793
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 222.979
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 257.983
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 257.554

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 196.247
MLP duration (in seconds): 0.0444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 256.857
b: 256, m: 2048, n: 148, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 94.510
b: 256, m: 2048, n: 2048, k: 148,
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 73.520
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 226.637
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 260.673
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 258.281

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 207.894
MLP duration (in seconds): 0.0453
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 250.833
b: 256, m: 2048, n: 150, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 94.839
b: 256, m: 2048, n: 2048, k: 150,
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 57.956
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 222.428
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 254.286
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0234
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 258.279

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 197.739
MLP duration (in seconds): 0.0471
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0809
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 252.636
b: 256, m: 2048, n: 152, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 96.429
b: 256, m: 2048, n: 2048, k: 152,
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 79.714
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 225.492
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 257.557
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 257.069

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 209.194
MLP duration (in seconds): 0.0482
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 256.791
b: 256, m: 2048, n: 154, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 97.245
b: 256, m: 2048, n: 2048, k: 154,
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 59.561
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 228.526
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 259.973
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 257.072

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 203.633
MLP duration (in seconds): 0.0493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 247.467
b: 256, m: 2048, n: 156, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 99.302
b: 256, m: 2048, n: 2048, k: 156,
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 76.724
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 224.619
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 253.332
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 257.503

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 206.917
MLP duration (in seconds): 0.0512
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 253.058
b: 256, m: 2048, n: 158, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 99.792
b: 256, m: 2048, n: 2048, k: 158,
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 60.896
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 227.830
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 255.690
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 257.990

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 203.942
MLP duration (in seconds): 0.0522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0884
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 255.886
b: 256, m: 2048, n: 160, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 101.490
b: 256, m: 2048, n: 2048, k: 160,
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 86.266
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 230.725
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 259.321
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 259.000

Attention duration (in seconds): 0.0350
Attention throughput (in TFLOP/s): 216.246
MLP duration (in seconds): 0.0530
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0880
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 258.435
b: 256, m: 2048, n: 162, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 102.190
b: 256, m: 2048, n: 2048, k: 162,
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 62.307
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 233.530
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 261.268
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 259.542

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 209.352
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 253.566
b: 256, m: 2048, n: 164, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 104.431
b: 256, m: 2048, n: 2048, k: 164,
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 80.527
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 229.668
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 255.346
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 258.053

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 214.400
MLP duration (in seconds): 0.0563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0932
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 256.390
b: 256, m: 2048, n: 166, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 104.618
b: 256, m: 2048, n: 2048, k: 166,
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 64.004
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 232.846
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 258.794
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 258.635

Attention duration (in seconds): 0.0386
Attention throughput (in TFLOP/s): 210.337
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0957
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 258.852
b: 256, m: 2048, n: 168, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 106.012
b: 256, m: 2048, n: 2048, k: 168,
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 87.814
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 236.622
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 259.376
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 259.532

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 221.466
MLP duration (in seconds): 0.0584
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0959
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 253.087
b: 256, m: 2048, n: 170, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 106.854
b: 256, m: 2048, n: 2048, k: 170,
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 65.636
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 231.749
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 254.597
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 257.674

Attention duration (in seconds): 0.0403
Attention throughput (in TFLOP/s): 210.426
MLP duration (in seconds): 0.0606
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 254.378
b: 256, m: 2048, n: 172, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 109.163
b: 256, m: 2048, n: 2048, k: 172,
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 84.354
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 234.535
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 256.682
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 258.164

Attention duration (in seconds): 0.0396
Attention throughput (in TFLOP/s): 218.965
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 257.486
b: 256, m: 2048, n: 174, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 108.905
b: 256, m: 2048, n: 2048, k: 174,
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 67.201
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 237.658
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 259.063
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0316
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 257.101

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 215.331
MLP duration (in seconds): 0.0630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 252.168
b: 256, m: 2048, n: 176, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 111.216
b: 256, m: 2048, n: 2048, k: 176,
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 92.033
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 233.190
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 254.055
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 257.449

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 220.435
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 254.212
b: 256, m: 2048, n: 178, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 111.696
b: 256, m: 2048, n: 2048, k: 178,
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 68.684
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 235.858
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 256.648
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 258.165

Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 215.095
MLP duration (in seconds): 0.0661
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 257.294
b: 256, m: 2048, n: 180, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 114.539
b: 256, m: 2048, n: 2048, k: 180,
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 87.557
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 238.982
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 259.317
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 258.679

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 224.204
MLP duration (in seconds): 0.0672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 259.142
b: 256, m: 2048, n: 182, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 113.849
b: 256, m: 2048, n: 2048, k: 182,
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 70.053
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 241.713
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 261.059
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 259.421

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 220.135
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 254.503
b: 256, m: 2048, n: 184, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 115.356
b: 256, m: 2048, n: 2048, k: 184,
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 94.867
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 237.386
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 255.834
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 257.935

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 224.798
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 256.853
b: 256, m: 2048, n: 186, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 116.053
b: 256, m: 2048, n: 2048, k: 186,
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 71.461
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 240.057
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 257.639
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0359
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 258.497

Attention duration (in seconds): 0.0458
Attention throughput (in TFLOP/s): 220.116
MLP duration (in seconds): 0.0720
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 258.809
b: 256, m: 2048, n: 188, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 119.452
b: 256, m: 2048, n: 2048, k: 188,
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 91.143
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 242.971
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 259.558
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 259.142

Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 228.447
MLP duration (in seconds): 0.0732
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.141
b: 256, m: 2048, n: 190, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 118.213
b: 256, m: 2048, n: 2048, k: 190,
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 72.861
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 238.667
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 256.240
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 259.717

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 219.802
MLP duration (in seconds): 0.0751
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 257.115
b: 256, m: 2048, n: 192, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 121.063
b: 256, m: 2048, n: 2048, k: 192,
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 103.537
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 241.264
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 258.433
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 260.384

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 230.506
MLP duration (in seconds): 0.0763
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 259.299
b: 256, m: 2048, n: 194, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 120.444
b: 256, m: 2048, n: 2048, k: 194,
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 68.714
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 244.081
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 259.472
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 259.509

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 222.770
MLP duration (in seconds): 0.0779
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 253.867
b: 256, m: 2048, n: 196, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 123.941
b: 256, m: 2048, n: 2048, k: 196,
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 86.009
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 239.123
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 254.650
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 257.638

Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 225.168
MLP duration (in seconds): 0.0805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 255.289
b: 256, m: 2048, n: 198, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 120.396
b: 256, m: 2048, n: 2048, k: 198,
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 69.141
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 241.634
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 257.115
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 259.643

Attention duration (in seconds): 0.0515
Attention throughput (in TFLOP/s): 220.914
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 257.924
b: 256, m: 2048, n: 200, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 125.187
b: 256, m: 2048, n: 2048, k: 200,
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 97.081
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 244.119
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 258.927
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 259.671

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 231.587
MLP duration (in seconds): 0.0828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 258.884
b: 256, m: 2048, n: 202, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 120.886
b: 256, m: 2048, n: 2048, k: 202,
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 70.399
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 246.752
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 260.277
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 260.696

Attention duration (in seconds): 0.0526
Attention throughput (in TFLOP/s): 224.816
MLP duration (in seconds): 0.0841
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 254.489
b: 256, m: 2048, n: 204, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 128.145
b: 256, m: 2048, n: 2048, k: 204,
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 88.638
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 242.624
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.266
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 260.216

Attention duration (in seconds): 0.0528
Attention throughput (in TFLOP/s): 228.194
MLP duration (in seconds): 0.0865
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 256.417
b: 256, m: 2048, n: 206, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 123.321
b: 256, m: 2048, n: 2048, k: 206,
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 71.604
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 245.220
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 258.160
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 259.586

Attention duration (in seconds): 0.0547
Attention throughput (in TFLOP/s): 224.436
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 259.069
b: 256, m: 2048, n: 208, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 130.421
b: 256, m: 2048, n: 2048, k: 208,
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 100.558
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 247.600
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0447
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 260.018
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 260.605

Attention duration (in seconds): 0.0532
Attention throughput (in TFLOP/s): 235.031
MLP duration (in seconds): 0.0892
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 254.388
b: 256, m: 2048, n: 210, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 125.715
b: 256, m: 2048, n: 2048, k: 210,
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 72.975
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 243.795
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 256.166
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0455
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 260.093

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 224.264
MLP duration (in seconds): 0.0917
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 256.509
b: 256, m: 2048, n: 212, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 131.628
b: 256, m: 2048, n: 2048, k: 212,
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 91.278
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 245.786
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 257.709
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 261.111

Attention duration (in seconds): 0.0560
Attention throughput (in TFLOP/s): 231.727
MLP duration (in seconds): 0.0930
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1490
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 258.348
b: 256, m: 2048, n: 214, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 127.694
b: 256, m: 2048, n: 2048, k: 214,
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 73.946
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 248.941
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 260.489
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 260.660

Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 228.400
MLP duration (in seconds): 0.0944
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1522
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 259.667
b: 256, m: 2048, n: 216, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 134.315
b: 256, m: 2048, n: 2048, k: 216,
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 102.140
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 251.037
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 261.763
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 261.125

Attention duration (in seconds): 0.0566
Attention throughput (in TFLOP/s): 237.494
MLP duration (in seconds): 0.0958
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 255.782
b: 256, m: 2048, n: 218, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 129.900
b: 256, m: 2048, n: 2048, k: 218,
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 75.155
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 246.841
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 257.581
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 260.819

Attention duration (in seconds): 0.0602
Attention throughput (in TFLOP/s): 227.618
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 257.578
b: 256, m: 2048, n: 220, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 136.548
b: 256, m: 2048, n: 2048, k: 220,
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 94.867
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 248.738
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0500
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 260.095
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0500
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 259.978

Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 234.916
MLP duration (in seconds): 0.0999
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1593
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 259.491
b: 256, m: 2048, n: 222, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 131.926
b: 256, m: 2048, n: 2048, k: 222,
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 76.988
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 249.747
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 261.053
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 261.449

Attention duration (in seconds): 0.0613
Attention throughput (in TFLOP/s): 231.423
MLP duration (in seconds): 0.1013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 256.005
b: 256, m: 2048, n: 224, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 139.299
b: 256, m: 2048, n: 2048, k: 224,
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 108.160
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 247.894
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 257.482
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 261.518

Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 236.795
MLP duration (in seconds): 0.1038
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1648
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 257.695
b: 256, m: 2048, n: 226, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 134.117
b: 256, m: 2048, n: 2048, k: 226,
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 77.862
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 249.919
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 259.059
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 261.115

Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 231.308
MLP duration (in seconds): 0.1054
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1689
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 259.264
b: 256, m: 2048, n: 228, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 140.728
b: 256, m: 2048, n: 2048, k: 228,
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 99.114
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 252.005
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0534
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 261.434
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 260.969

Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 238.443
MLP duration (in seconds): 0.1068
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 255.333
b: 256, m: 2048, n: 230, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 136.266
b: 256, m: 2048, n: 2048, k: 230,
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 79.734
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 248.093
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 256.863
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 260.778

Attention duration (in seconds): 0.0658
Attention throughput (in TFLOP/s): 230.683
MLP duration (in seconds): 0.1097
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1756
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 257.119
b: 256, m: 2048, n: 232, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 143.167
b: 256, m: 2048, n: 2048, k: 232,
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 108.746
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 250.262
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 258.411
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 260.786

Attention duration (in seconds): 0.0646
Attention throughput (in TFLOP/s): 238.937
MLP duration (in seconds): 0.1113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1760
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 259.155
b: 256, m: 2048, n: 234, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 138.300
b: 256, m: 2048, n: 2048, k: 234,
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 81.365
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 250.076
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 260.885
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 260.900

Attention duration (in seconds): 0.0670
Attention throughput (in TFLOP/s): 234.236
MLP duration (in seconds): 0.1127
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 259.708
b: 256, m: 2048, n: 236, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 144.527
b: 256, m: 2048, n: 2048, k: 236,
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 102.266
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 252.063
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 261.682
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 261.494

Attention duration (in seconds): 0.0665
Attention throughput (in TFLOP/s): 240.187
MLP duration (in seconds): 0.1143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 257.149
b: 256, m: 2048, n: 238, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 140.719
b: 256, m: 2048, n: 2048, k: 238,
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 82.826
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 250.053
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0587
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 258.871
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0579
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 262.508

Attention duration (in seconds): 0.0694
Attention throughput (in TFLOP/s): 233.986
MLP duration (in seconds): 0.1167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 259.418
b: 256, m: 2048, n: 240, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 148.440
b: 256, m: 2048, n: 2048, k: 240,
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 112.707
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 251.723
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0593
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 260.553
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 262.516

Attention duration (in seconds): 0.0681
Attention throughput (in TFLOP/s): 242.174
MLP duration (in seconds): 0.1182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1863
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 260.639
b: 256, m: 2048, n: 242, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 142.532
b: 256, m: 2048, n: 2048, k: 242,
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 84.703
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 252.951
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 261.324
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 260.793

Attention duration (in seconds): 0.0706
Attention throughput (in TFLOP/s): 237.543
MLP duration (in seconds): 0.1204
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1910
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 255.370
b: 256, m: 2048, n: 244, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 148.869
b: 256, m: 2048, n: 2048, k: 244,
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 105.509
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0161
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 248.729
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 257.170
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 261.173

Attention duration (in seconds): 0.0715
Attention throughput (in TFLOP/s): 238.223
MLP duration (in seconds): 0.1233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 257.297
b: 256, m: 2048, n: 246, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 144.992
b: 256, m: 2048, n: 2048, k: 246,
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 86.267
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 250.751
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0627
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 258.926
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 261.703

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 235.984
MLP duration (in seconds): 0.1248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 259.194
b: 256, m: 2048, n: 248, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 152.446
b: 256, m: 2048, n: 2048, k: 248,
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 114.730
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 253.008
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 257.892
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 261.457

Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 243.348
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1994
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 255.064
b: 256, m: 2048, n: 250, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 147.282
b: 256, m: 2048, n: 2048, k: 250,
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 88.230
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 249.012
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 256.647
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 260.777

Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 235.171
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 256.506
b: 256, m: 2048, n: 252, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 153.969
b: 256, m: 2048, n: 2048, k: 252,
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 109.346
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 251.093
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 258.070
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 260.834

Attention duration (in seconds): 0.0753
Attention throughput (in TFLOP/s): 240.823
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 257.864
b: 256, m: 2048, n: 254, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 149.698
b: 256, m: 2048, n: 2048, k: 254,
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 90.143
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 253.142
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 259.866
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0662
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 261.510

Attention duration (in seconds): 0.0772
Attention throughput (in TFLOP/s): 238.558
MLP duration (in seconds): 0.1329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 263.258
b: 256, m: 2048, n: 256, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 158.652
b: 256, m: 2048, n: 2048, k: 256,
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 122.612
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 256.006
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 265.244
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 263.977

Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 248.404
MLP duration (in seconds): 0.1330
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 259.642
b: 256, m: 2048, n: 258, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 126.023
b: 256, m: 2048, n: 2048, k: 258,
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 85.167
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 251.495
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 260.820
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 264.934

Attention duration (in seconds): 0.0803
Attention throughput (in TFLOP/s): 236.383
MLP duration (in seconds): 0.1360
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 261.170
b: 256, m: 2048, n: 260, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 130.750
b: 256, m: 2048, n: 2048, k: 260,
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 103.536
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 254.211
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.1035
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 175.333
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 263.531

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 241.938
MLP duration (in seconds): 0.1724
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 258.245
b: 256, m: 2048, n: 262, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 127.617
b: 256, m: 2048, n: 2048, k: 262,
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 85.432
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 255.874
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 167.310
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 264.499

Attention duration (in seconds): 0.0825
Attention throughput (in TFLOP/s): 236.955
MLP duration (in seconds): 0.1798
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2623
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 259.360
b: 256, m: 2048, n: 264, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 130.539
b: 256, m: 2048, n: 2048, k: 264,
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 114.969
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 252.909
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0717
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 260.911
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 264.364

Attention duration (in seconds): 0.0819
Attention throughput (in TFLOP/s): 242.372
MLP duration (in seconds): 0.1425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 259.927
b: 256, m: 2048, n: 266, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 129.104
b: 256, m: 2048, n: 2048, k: 266,
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 85.682
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 255.017
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 261.574
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 264.454

Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 238.251
MLP duration (in seconds): 0.1444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 261.076
b: 256, m: 2048, n: 268, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 134.219
b: 256, m: 2048, n: 2048, k: 268,
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 105.104
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 256.066
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 262.615
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 264.587

Attention duration (in seconds): 0.0840
Attention throughput (in TFLOP/s): 243.304
MLP duration (in seconds): 0.1463
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0558
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 262.824
b: 256, m: 2048, n: 270, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 130.623
b: 256, m: 2048, n: 2048, k: 270,
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 86.102
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 257.660
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0989
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 197.961
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 264.908

Attention duration (in seconds): 0.0860
Attention throughput (in TFLOP/s): 241.023
MLP duration (in seconds): 0.1727
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2587
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 260.308
b: 256, m: 2048, n: 272, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 134.969
b: 256, m: 2048, n: 2048, k: 272,
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 118.780
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 253.264
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.1094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 181.455
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 263.484

Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 244.315
MLP duration (in seconds): 0.1848
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2709
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 260.942
b: 256, m: 2048, n: 274, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 132.445
b: 256, m: 2048, n: 2048, k: 274,
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 86.684
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 256.146
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 262.594
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 264.858

Attention duration (in seconds): 0.0888
Attention throughput (in TFLOP/s): 240.136
MLP duration (in seconds): 0.1528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 262.110
b: 256, m: 2048, n: 276, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 137.835
b: 256, m: 2048, n: 2048, k: 276,
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 107.210
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 257.557
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0777
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 263.021
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 264.859

Attention duration (in seconds): 0.0882
Attention throughput (in TFLOP/s): 245.315
MLP duration (in seconds): 0.1549
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 258.969
b: 256, m: 2048, n: 278, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 134.176
b: 256, m: 2048, n: 2048, k: 278,
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 87.302
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 254.814
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 260.434
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 263.028

Attention duration (in seconds): 0.0917
Attention throughput (in TFLOP/s): 239.195
MLP duration (in seconds): 0.1585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 258.896
b: 256, m: 2048, n: 280, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 138.572
b: 256, m: 2048, n: 2048, k: 280,
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 120.067
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 255.353
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 260.513
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0801
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 262.819

Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 244.704
MLP duration (in seconds): 0.1609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0608
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 263.130
b: 256, m: 2048, n: 282, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 135.889
b: 256, m: 2048, n: 2048, k: 282,
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 88.592
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 259.477
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 264.197
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0806
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 264.906

Attention duration (in seconds): 0.0927
Attention throughput (in TFLOP/s): 243.333
MLP duration (in seconds): 0.1614
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 260.057
b: 256, m: 2048, n: 284, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 142.676
b: 256, m: 2048, n: 2048, k: 284,
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 110.213
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 255.336
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.1321
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 163.946
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 265.921

Attention duration (in seconds): 0.0934
Attention throughput (in TFLOP/s): 244.743
MLP duration (in seconds): 0.2135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 260.281
b: 256, m: 2048, n: 286, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 137.236
b: 256, m: 2048, n: 2048, k: 286,
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 88.943
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 255.999
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 262.476
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0828
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 265.252

Attention duration (in seconds): 0.0961
Attention throughput (in TFLOP/s): 241.282
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 263.027
b: 256, m: 2048, n: 288, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 142.110
b: 256, m: 2048, n: 2048, k: 288,
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 124.368
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 258.108
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 167.143
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 266.645

Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 249.021
MLP duration (in seconds): 0.2167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 195.337
b: 256, m: 2048, n: 290, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 138.565
b: 256, m: 2048, n: 2048, k: 290,
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 89.679
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 258.361
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.1316
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 171.522
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 265.214

Attention duration (in seconds): 0.1200
Attention throughput (in TFLOP/s): 198.570
MLP duration (in seconds): 0.2167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0658
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 260.915
b: 256, m: 2048, n: 292, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 145.277
b: 256, m: 2048, n: 2048, k: 292,
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 112.503
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 256.879
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0874
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 261.907
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 264.263

Attention duration (in seconds): 0.0980
Attention throughput (in TFLOP/s): 246.457
MLP duration (in seconds): 0.1740
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2720
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 261.802
b: 256, m: 2048, n: 294, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 141.565
b: 256, m: 2048, n: 2048, k: 294,
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 91.453
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 258.221
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 263.437
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 263.439

Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 243.928
MLP duration (in seconds): 0.1762
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0672
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 262.506
b: 256, m: 2048, n: 296, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 146.282
b: 256, m: 2048, n: 2048, k: 296,
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 125.820
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 257.397
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0897
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 262.239
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 263.125

Attention duration (in seconds): 0.0994
Attention throughput (in TFLOP/s): 249.309
MLP duration (in seconds): 0.1791
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2785
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0691
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 258.741
b: 256, m: 2048, n: 298, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 142.956
b: 256, m: 2048, n: 2048, k: 298,
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 92.312
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 253.647
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 258.937
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 262.599

Attention duration (in seconds): 0.1040
Attention throughput (in TFLOP/s): 241.513
MLP duration (in seconds): 0.1828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 259.216
b: 256, m: 2048, n: 300, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 149.315
b: 256, m: 2048, n: 2048, k: 300,
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 114.426
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 255.203
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.1407
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 171.757
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 262.760

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 245.842
MLP duration (in seconds): 0.2326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3361
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0706
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 259.995
b: 256, m: 2048, n: 302, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 144.875
b: 256, m: 2048, n: 2048, k: 302,
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 93.502
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 256.716
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0936
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 261.497
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 262.713

Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 243.482
MLP duration (in seconds): 0.1868
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2927
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 254.736
b: 256, m: 2048, n: 304, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 150.500
b: 256, m: 2048, n: 2048, k: 304,
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 129.951
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 253.746
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0993
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 249.761
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0946
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 262.152

Attention duration (in seconds): 0.1068
Attention throughput (in TFLOP/s): 244.410
MLP duration (in seconds): 0.1940
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 258.461
b: 256, m: 2048, n: 306, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 146.218
b: 256, m: 2048, n: 2048, k: 306,
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 94.832
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 255.120
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0966
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 260.294
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 262.939

Attention duration (in seconds): 0.1090
Attention throughput (in TFLOP/s): 242.674
MLP duration (in seconds): 0.1922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0731
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 261.255
b: 256, m: 2048, n: 308, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 152.430
b: 256, m: 2048, n: 2048, k: 308,
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 115.994
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 256.487
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 260.121
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 262.509

Attention duration (in seconds): 0.1080
Attention throughput (in TFLOP/s): 248.113
MLP duration (in seconds): 0.1949
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 261.042
b: 256, m: 2048, n: 310, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 147.410
b: 256, m: 2048, n: 2048, k: 310,
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 94.141
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 257.870
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 261.380
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 263.166

Attention duration (in seconds): 0.1107
Attention throughput (in TFLOP/s): 245.030
MLP duration (in seconds): 0.1967
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 258.150
b: 256, m: 2048, n: 312, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 154.155
b: 256, m: 2048, n: 2048, k: 312,
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 131.093
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 255.125
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1584
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 164.969
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.0996
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 262.415

Attention duration (in seconds): 0.1110
Attention throughput (in TFLOP/s): 247.528
MLP duration (in seconds): 0.2580
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3690
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0939
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 211.345
b: 256, m: 2048, n: 314, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 149.635
b: 256, m: 2048, n: 2048, k: 314,
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 96.501
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 256.552
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 172.229
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 261.828

Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 211.996
MLP duration (in seconds): 0.2548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0772
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 260.468
b: 256, m: 2048, n: 316, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 156.320
b: 256, m: 2048, n: 2048, k: 316,
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 119.773
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 257.907
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 258.734
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 264.671

Attention duration (in seconds): 0.1132
Attention throughput (in TFLOP/s): 248.841
MLP duration (in seconds): 0.2049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0782
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 260.448
b: 256, m: 2048, n: 318, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 151.134
b: 256, m: 2048, n: 2048, k: 318,
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 97.787
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 257.669
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 215.549
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 266.514

Attention duration (in seconds): 0.1160
Attention throughput (in TFLOP/s): 245.768
MLP duration (in seconds): 0.2278
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3438
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0785
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 262.480
b: 256, m: 2048, n: 320, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 158.142
b: 256, m: 2048, n: 2048, k: 320,
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 137.843
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 259.226
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1573
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 174.733
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 266.299

Attention duration (in seconds): 0.1144
Attention throughput (in TFLOP/s): 252.330
MLP duration (in seconds): 0.2605
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3749
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0792
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 263.705
b: 256, m: 2048, n: 322, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 152.930
b: 256, m: 2048, n: 2048, k: 322,
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 92.211
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 261.121
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 265.220
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 265.929

Attention duration (in seconds): 0.1178
Attention throughput (in TFLOP/s): 247.955
MLP duration (in seconds): 0.2096
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0803
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 263.332
b: 256, m: 2048, n: 324, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 159.725
b: 256, m: 2048, n: 2048, k: 324,
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 113.430
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 259.417
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1156
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 243.837
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1068
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 263.934

Attention duration (in seconds): 0.1179
Attention throughput (in TFLOP/s): 250.804
MLP duration (in seconds): 0.2223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3402
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0826
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 259.028
b: 256, m: 2048, n: 326, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 154.423
b: 256, m: 2048, n: 2048, k: 326,
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 93.052
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 254.900
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 211.514
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 262.789

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 244.037
MLP duration (in seconds): 0.2434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3661
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0832
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 260.341
b: 256, m: 2048, n: 328, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 160.939
b: 256, m: 2048, n: 2048, k: 328,
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 129.902
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 257.429
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1556
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 185.562
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 262.468

Attention duration (in seconds): 0.1210
Attention throughput (in TFLOP/s): 250.229
MLP duration (in seconds): 0.2657
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3867
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 259.408
b: 256, m: 2048, n: 330, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 155.386
b: 256, m: 2048, n: 2048, k: 330,
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 92.904
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 258.842
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 164.022
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 263.949

Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 245.318
MLP duration (in seconds): 0.2890
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.1173
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 189.253
b: 256, m: 2048, n: 332, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 163.106
b: 256, m: 2048, n: 2048, k: 332,
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 115.223
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 254.870
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 187.896
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1124
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 263.122

Attention duration (in seconds): 0.1568
Attention throughput (in TFLOP/s): 197.746
MLP duration (in seconds): 0.2699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.1345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 166.975
b: 256, m: 2048, n: 334, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 157.938
b: 256, m: 2048, n: 2048, k: 334,
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 94.908
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 260.328
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1780
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 168.260
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1125
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 266.109

Attention duration (in seconds): 0.1754
Attention throughput (in TFLOP/s): 178.944
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4659
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0861
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 263.844
b: 256, m: 2048, n: 336, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 165.713
b: 256, m: 2048, n: 2048, k: 336,
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 133.605
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 261.521
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 171.916
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1138
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 266.344

Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 254.251
MLP duration (in seconds): 0.2901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.1271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 180.920
b: 256, m: 2048, n: 338, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 159.528
b: 256, m: 2048, n: 2048, k: 338,
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 96.054
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 258.029
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 161.932
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 265.584

Attention duration (in seconds): 0.1689
Attention throughput (in TFLOP/s): 190.109
MLP duration (in seconds): 0.3049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4738
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.1161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 200.530
b: 256, m: 2048, n: 340, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 168.229
b: 256, m: 2048, n: 2048, k: 340,
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 118.658
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 260.396
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 164.855
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1170
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 265.110

Attention duration (in seconds): 0.1563
Attention throughput (in TFLOP/s): 207.819
MLP duration (in seconds): 0.3053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.1327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 177.400
b: 256, m: 2048, n: 342, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 160.826
b: 256, m: 2048, n: 2048, k: 342,
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 96.846
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 261.266
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 164.281
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.1178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 266.442

Attention duration (in seconds): 0.1749
Attention throughput (in TFLOP/s): 187.878
MLP duration (in seconds): 0.3090
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.1222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 194.980
b: 256, m: 2048, n: 344, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 168.560
b: 256, m: 2048, n: 2048, k: 344,
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 133.509
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0306
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 259.581
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1451
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 218.954
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.1200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 264.716

Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 204.325
MLP duration (in seconds): 0.2651
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.1154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 208.899
b: 256, m: 2048, n: 346, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 163.253
b: 256, m: 2048, n: 2048, k: 346,
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 97.463
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 259.636
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 169.179
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.1207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 266.182

Attention duration (in seconds): 0.1585
Attention throughput (in TFLOP/s): 212.134
MLP duration (in seconds): 0.3107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4692
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.1281
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 190.295
b: 256, m: 2048, n: 348, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 172.358
b: 256, m: 2048, n: 2048, k: 348,
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 121.272
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 260.423
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 162.965
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.1225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 265.386

Attention duration (in seconds): 0.1698
Attention throughput (in TFLOP/s): 200.219
MLP duration (in seconds): 0.3220
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4918
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.1440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 171.269
b: 256, m: 2048, n: 350, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 164.519
b: 256, m: 2048, n: 2048, k: 350,
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 97.889
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 260.200
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.2067
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 159.069
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 266.634

Attention duration (in seconds): 0.1878
Attention throughput (in TFLOP/s): 183.063
MLP duration (in seconds): 0.3301
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 262.882
b: 256, m: 2048, n: 352, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 173.338
b: 256, m: 2048, n: 2048, k: 352,
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 139.690
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0320
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 259.567
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1977
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 168.222
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 265.695

Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 254.372
MLP duration (in seconds): 0.3229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.1037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 243.296
b: 256, m: 2048, n: 354, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 166.521
b: 256, m: 2048, n: 2048, k: 354,
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 99.533
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 258.619
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 165.197
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 262.596

Attention duration (in seconds): 0.1484
Attention throughput (in TFLOP/s): 236.893
MLP duration (in seconds): 0.3317
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1119
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 227.973
b: 256, m: 2048, n: 356, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 175.079
b: 256, m: 2048, n: 2048, k: 356,
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 123.610
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 260.015
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 208.224
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 263.254

Attention duration (in seconds): 0.1552
Attention throughput (in TFLOP/s): 229.079
MLP duration (in seconds): 0.2926
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4478
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 175.930
b: 256, m: 2048, n: 358, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 167.103
b: 256, m: 2048, n: 2048, k: 358,
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 100.844
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 258.156
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.2205
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 156.053
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 262.901

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 186.993
MLP duration (in seconds): 0.3513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5435
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 259.113
b: 256, m: 2048, n: 360, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 175.342
b: 256, m: 2048, n: 2048, k: 360,
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 139.024
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 258.064
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.2209
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 157.495
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1328
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 261.963

Attention duration (in seconds): 0.1444
Attention throughput (in TFLOP/s): 251.684
MLP duration (in seconds): 0.3537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 163.791
b: 256, m: 2048, n: 362, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 168.255
b: 256, m: 2048, n: 2048, k: 362,
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 100.913
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 258.247
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.2231
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 157.697
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 263.577

Attention duration (in seconds): 0.2075
Attention throughput (in TFLOP/s): 177.061
MLP duration (in seconds): 0.3565
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 171.295
b: 256, m: 2048, n: 364, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 178.351
b: 256, m: 2048, n: 2048, k: 364,
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 125.425
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 264.255
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.2211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 160.865
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 265.327

Attention duration (in seconds): 0.2000
Attention throughput (in TFLOP/s): 185.661
MLP duration (in seconds): 0.3551
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 192.090
b: 256, m: 2048, n: 366, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 169.825
b: 256, m: 2048, n: 2048, k: 366,
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 101.379
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 260.833
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.2324
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 154.710
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1361
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 264.269

Attention duration (in seconds): 0.1872
Attention throughput (in TFLOP/s): 200.437
MLP duration (in seconds): 0.3685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 223.611
b: 256, m: 2048, n: 368, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 180.605
b: 256, m: 2048, n: 2048, k: 368,
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 141.906
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 260.120
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1992
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 182.511
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 262.400

Attention duration (in seconds): 0.1668
Attention throughput (in TFLOP/s): 227.402
MLP duration (in seconds): 0.3377
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1639
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 168.211
b: 256, m: 2048, n: 370, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 171.929
b: 256, m: 2048, n: 2048, k: 370,
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 104.640
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 261.320
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.2116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 173.683
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1397
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 262.980

Attention duration (in seconds): 0.2112
Attention throughput (in TFLOP/s): 181.504
MLP duration (in seconds): 0.3513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1620
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 172.019
b: 256, m: 2048, n: 372, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 182.390
b: 256, m: 2048, n: 2048, k: 372,
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 127.548
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 259.092
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.2347
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 158.283
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1411
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 263.282

Attention duration (in seconds): 0.2084
Attention throughput (in TFLOP/s): 185.873
MLP duration (in seconds): 0.3758
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5842
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 262.293
b: 256, m: 2048, n: 374, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 174.273
b: 256, m: 2048, n: 2048, k: 374,
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 105.506
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 259.208
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.2441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 153.789
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 263.309

Attention duration (in seconds): 0.1558
Attention throughput (in TFLOP/s): 251.311
MLP duration (in seconds): 0.3867
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1080
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 263.460
b: 256, m: 2048, n: 376, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 183.502
b: 256, m: 2048, n: 2048, k: 376,
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 144.085
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 261.073
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.2444
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 155.262
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 264.000

Attention duration (in seconds): 0.1544
Attention throughput (in TFLOP/s): 256.285
MLP duration (in seconds): 0.3882
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5426
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 164.461
b: 256, m: 2048, n: 378, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 175.646
b: 256, m: 2048, n: 2048, k: 378,
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 105.547
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 261.911
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.2427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 158.018
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 264.307

Attention duration (in seconds): 0.2238
Attention throughput (in TFLOP/s): 178.606
MLP duration (in seconds): 0.3878
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 201.839
b: 256, m: 2048, n: 380, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 183.998
b: 256, m: 2048, n: 2048, k: 380,
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 130.236
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 258.897
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 159.373
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 261.954

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 210.206
MLP duration (in seconds): 0.3912
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5834
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 163.880
b: 256, m: 2048, n: 382, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 176.033
b: 256, m: 2048, n: 2048, k: 382,
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 108.203
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 259.029
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.2547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 153.818
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 261.919

Attention duration (in seconds): 0.2293
Attention throughput (in TFLOP/s): 177.973
MLP duration (in seconds): 0.4042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1859
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 159.667
b: 256, m: 2048, n: 384, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 188.777
b: 256, m: 2048, n: 2048, k: 384,
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 150.748
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 260.826
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.2569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 154.104
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 263.198

Attention duration (in seconds): 0.2337
Attention throughput (in TFLOP/s): 176.424
MLP duration (in seconds): 0.4072
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1858
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 161.489
b: 256, m: 2048, n: 386, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 150.191
b: 256, m: 2048, n: 2048, k: 386,
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 103.562
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 258.151
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.2615
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 152.937
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1534
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 260.762

Attention duration (in seconds): 0.2380
Attention throughput (in TFLOP/s): 175.009
MLP duration (in seconds): 0.4149
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6529
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1888
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 160.573
b: 256, m: 2048, n: 388, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 155.691
b: 256, m: 2048, n: 2048, k: 388,
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 125.455
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 258.942
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.2636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 153.334
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 260.561

Attention duration (in seconds): 0.2398
Attention throughput (in TFLOP/s): 175.499
MLP duration (in seconds): 0.4186
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6584
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 162.658
b: 256, m: 2048, n: 390, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 151.767
b: 256, m: 2048, n: 2048, k: 390,
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 103.118
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 262.297
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.2676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 152.561
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1552
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 263.053

Attention duration (in seconds): 0.2408
Attention throughput (in TFLOP/s): 176.502
MLP duration (in seconds): 0.4228
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1907
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 162.268
b: 256, m: 2048, n: 392, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 154.956
b: 256, m: 2048, n: 2048, k: 392,
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 140.553
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 257.431
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.2656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 155.277
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 264.970

Attention duration (in seconds): 0.2421
Attention throughput (in TFLOP/s): 177.310
MLP duration (in seconds): 0.4213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 162.346
b: 256, m: 2048, n: 394, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 152.226
b: 256, m: 2048, n: 2048, k: 394,
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 103.056
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 259.500
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.2736
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 152.329
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 171.354

Attention duration (in seconds): 0.2464
Attention throughput (in TFLOP/s): 175.970
MLP duration (in seconds): 0.5167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1922
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 164.245
b: 256, m: 2048, n: 396, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 158.958
b: 256, m: 2048, n: 2048, k: 396,
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 125.615
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 261.869
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.2810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 149.826
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 262.756

Attention duration (in seconds): 0.2445
Attention throughput (in TFLOP/s): 179.104
MLP duration (in seconds): 0.4412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6857
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 160.576
b: 256, m: 2048, n: 398, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 154.155
b: 256, m: 2048, n: 2048, k: 398,
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 104.720
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 260.810
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.2843
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 149.581
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1629
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 261.018

Attention duration (in seconds): 0.2531
Attention throughput (in TFLOP/s): 174.777
MLP duration (in seconds): 0.4472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7002
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 258.312
b: 256, m: 2048, n: 400, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 158.857
b: 256, m: 2048, n: 2048, k: 400,
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 143.221
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0414
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 259.048
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.2836
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 151.469
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.2369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 181.279

Attention duration (in seconds): 0.1776
Attention throughput (in TFLOP/s): 251.568
MLP duration (in seconds): 0.5205
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6980
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 158.853
b: 256, m: 2048, n: 402, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 155.463
b: 256, m: 2048, n: 2048, k: 402,
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 105.366
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 260.244
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.2874
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 150.916
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.2647
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 163.887

Attention duration (in seconds): 0.2602
Attention throughput (in TFLOP/s): 173.333
MLP duration (in seconds): 0.5521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.2105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 156.072
b: 256, m: 2048, n: 404, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 161.909
b: 256, m: 2048, n: 2048, k: 404,
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 129.218
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 260.216
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.2844
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 154.071
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.2678
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 163.604

Attention duration (in seconds): 0.2647
Attention throughput (in TFLOP/s): 172.069
MLP duration (in seconds): 0.5522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.2127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 156.056
b: 256, m: 2048, n: 406, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 156.896
b: 256, m: 2048, n: 2048, k: 406,
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 106.192
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 260.331
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.2882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 153.536
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1688
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 262.082

Attention duration (in seconds): 0.2689
Attention throughput (in TFLOP/s): 171.028
MLP duration (in seconds): 0.4570
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1281
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 261.537
b: 256, m: 2048, n: 408, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 161.543
b: 256, m: 2048, n: 2048, k: 408,
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 145.629
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 262.007
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.2539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 175.970
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 262.060

Attention duration (in seconds): 0.1822
Attention throughput (in TFLOP/s): 254.843
MLP duration (in seconds): 0.4244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1293
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 261.694
b: 256, m: 2048, n: 410, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 158.475
b: 256, m: 2048, n: 2048, k: 410,
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 107.157
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 263.096
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.2938
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 153.595
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 264.611

Attention duration (in seconds): 0.1860
Attention throughput (in TFLOP/s): 252.106
MLP duration (in seconds): 0.4643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.2131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 160.334
b: 256, m: 2048, n: 412, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 164.990
b: 256, m: 2048, n: 2048, k: 412,
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 131.693
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 261.397
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.2604
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 175.001
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.2660
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 171.291

Attention duration (in seconds): 0.2688
Attention throughput (in TFLOP/s): 176.095
MLP duration (in seconds): 0.5264
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 177.267
b: 256, m: 2048, n: 414, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 159.283
b: 256, m: 2048, n: 2048, k: 414,
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 107.355
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 258.235
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.2970
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 154.904
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1746
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 263.474

Attention duration (in seconds): 0.2531
Attention throughput (in TFLOP/s): 188.834
MLP duration (in seconds): 0.4716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.2073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 168.089
b: 256, m: 2048, n: 416, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 165.438
b: 256, m: 2048, n: 2048, k: 416,
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 151.149
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 262.627
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.3040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 152.802
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1855
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 250.482

Attention duration (in seconds): 0.2628
Attention throughput (in TFLOP/s): 183.561
MLP duration (in seconds): 0.4895
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7523
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.2249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 156.390
b: 256, m: 2048, n: 418, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 160.822
b: 256, m: 2048, n: 2048, k: 418,
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 108.652
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 263.300
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.2855
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 164.258
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.2708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 173.173

Attention duration (in seconds): 0.2833
Attention throughput (in TFLOP/s): 171.890
MLP duration (in seconds): 0.5564
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8397
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.2107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 168.573
b: 256, m: 2048, n: 420, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 168.539
b: 256, m: 2048, n: 2048, k: 420,
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 133.737
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 260.143
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.3133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 151.132
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.2914
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 162.488

Attention duration (in seconds): 0.2683
Attention throughput (in TFLOP/s): 183.229
MLP duration (in seconds): 0.6047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8730
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.2279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 157.342
b: 256, m: 2048, n: 422, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 162.674
b: 256, m: 2048, n: 2048, k: 422,
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 109.345
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 261.015
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.3229
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 148.042
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1829
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 261.392

Attention duration (in seconds): 0.2875
Attention throughput (in TFLOP/s): 172.572
MLP duration (in seconds): 0.5058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7933
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.2316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 156.252
b: 256, m: 2048, n: 424, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 167.981
b: 256, m: 2048, n: 2048, k: 424,
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 150.540
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 261.442
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.3194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 151.079
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 256.826

Attention duration (in seconds): 0.2893
Attention throughput (in TFLOP/s): 173.134
MLP duration (in seconds): 0.5073
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7966
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.2353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 155.305
b: 256, m: 2048, n: 426, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 162.995
b: 256, m: 2048, n: 2048, k: 426,
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 109.558
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 259.092
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.3252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 149.796
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.3042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 160.135

Attention duration (in seconds): 0.2962
Attention throughput (in TFLOP/s): 170.630
MLP duration (in seconds): 0.6294
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.2367
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 155.825
b: 256, m: 2048, n: 428, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 170.806
b: 256, m: 2048, n: 2048, k: 428,
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 135.190
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 260.514
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.3305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 148.770
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2960
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 166.132

Attention duration (in seconds): 0.2960
Attention throughput (in TFLOP/s): 172.311
MLP duration (in seconds): 0.6265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.2406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 154.738
b: 256, m: 2048, n: 430, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 165.666
b: 256, m: 2048, n: 2048, k: 430,
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 111.437
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 263.439
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.3289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 150.895
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.3106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 159.779

Attention duration (in seconds): 0.3015
Attention throughput (in TFLOP/s): 170.730
MLP duration (in seconds): 0.6396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9411
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.2366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 158.790
b: 256, m: 2048, n: 432, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 171.505
b: 256, m: 2048, n: 2048, k: 432,
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 153.272
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 263.101
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.3323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 150.749
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.3147
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 159.200

Attention duration (in seconds): 0.2957
Attention throughput (in TFLOP/s): 175.703
MLP duration (in seconds): 0.6470
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.2416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 156.932
b: 256, m: 2048, n: 434, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 166.807
b: 256, m: 2048, n: 2048, k: 434,
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 112.558
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 261.865
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.3366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 150.212
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.3126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 161.745

Attention duration (in seconds): 0.3038
Attention throughput (in TFLOP/s): 172.578
MLP duration (in seconds): 0.6492
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9530
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.2511
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 152.444
b: 256, m: 2048, n: 436, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 174.160
b: 256, m: 2048, n: 2048, k: 436,
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 136.501
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 263.056
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.3339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 152.826
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.3211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 158.906

Attention duration (in seconds): 0.3118
Attention throughput (in TFLOP/s): 169.672
MLP duration (in seconds): 0.6550
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.2474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 156.102
b: 256, m: 2048, n: 438, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 168.323
b: 256, m: 2048, n: 2048, k: 438,
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 113.081
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0490
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 262.609
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.3438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 149.780
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.3233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 159.268

Attention duration (in seconds): 0.3104
Attention throughput (in TFLOP/s): 171.993
MLP duration (in seconds): 0.6672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9775
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.2546
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 153.090
b: 256, m: 2048, n: 440, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 174.113
b: 256, m: 2048, n: 2048, k: 440,
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 154.957
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 262.614
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.3502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 148.415
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.1970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 263.749

Attention duration (in seconds): 0.3156
Attention throughput (in TFLOP/s): 170.657
MLP duration (in seconds): 0.5472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8628
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.2492
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 157.834
b: 256, m: 2048, n: 442, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 169.621
b: 256, m: 2048, n: 2048, k: 442,
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 114.593
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 260.118
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.3527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 148.699
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.3261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 160.833

Attention duration (in seconds): 0.3135
Attention throughput (in TFLOP/s): 173.347
MLP duration (in seconds): 0.6787
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9922
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.2571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 154.369
b: 256, m: 2048, n: 444, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 177.230
b: 256, m: 2048, n: 2048, k: 444,
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 142.366
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 260.571
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.3588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 147.489
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.3373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 156.898

Attention duration (in seconds): 0.3200
Attention throughput (in TFLOP/s): 171.355
MLP duration (in seconds): 0.6961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.2139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 187.216
b: 256, m: 2048, n: 446, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 171.383
b: 256, m: 2048, n: 2048, k: 446,
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 116.172
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0510
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 261.519
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.3752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 142.329
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.3383
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 157.820

Attention duration (in seconds): 0.2788
Attention throughput (in TFLOP/s): 198.402
MLP duration (in seconds): 0.7135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9923
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.2584
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 156.401
b: 256, m: 2048, n: 448, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 178.345
b: 256, m: 2048, n: 2048, k: 448,
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 161.609
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 262.689
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.3586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 150.258
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 263.436

Attention duration (in seconds): 0.3210
Attention throughput (in TFLOP/s): 173.845
MLP duration (in seconds): 0.5631
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.2660
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 153.249
b: 256, m: 2048, n: 450, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 173.393
b: 256, m: 2048, n: 2048, k: 450,
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 111.555
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 261.896
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.3643
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 149.215
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.3418
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 159.057

Attention duration (in seconds): 0.3322
Attention throughput (in TFLOP/s): 169.472
MLP duration (in seconds): 0.7060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0382
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.2515
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 163.533
b: 256, m: 2048, n: 452, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 179.842
b: 256, m: 2048, n: 2048, k: 452,
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 134.556
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 261.608
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.3745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 146.435
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.3464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 158.300

Attention duration (in seconds): 0.3165
Attention throughput (in TFLOP/s): 179.389
MLP duration (in seconds): 0.7210
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1797
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 230.984
b: 256, m: 2048, n: 454, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 174.576
b: 256, m: 2048, n: 2048, k: 454,
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 111.680
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0527
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 262.452
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.3763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 147.027
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.3406
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 162.461

Attention duration (in seconds): 0.2467
Attention throughput (in TFLOP/s): 232.208
MLP duration (in seconds): 0.7169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.2659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 157.423
b: 256, m: 2048, n: 456, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 179.326
b: 256, m: 2048, n: 2048, k: 456,
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 151.303
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 262.286
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.3688
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 151.341
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.3425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 162.985

Attention duration (in seconds): 0.3311
Attention throughput (in TFLOP/s): 174.517
MLP duration (in seconds): 0.7113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.2752
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 153.479
b: 256, m: 2048, n: 458, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 175.321
b: 256, m: 2048, n: 2048, k: 458,
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 111.979
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 262.695
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.3979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 141.528
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 263.500

Attention duration (in seconds): 0.3431
Attention throughput (in TFLOP/s): 169.830
MLP duration (in seconds): 0.6116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9547
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.2766
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 153.993
b: 256, m: 2048, n: 460, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 183.122
b: 256, m: 2048, n: 2048, k: 460,
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 136.770
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0547
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 259.564
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.3827
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 148.425
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2186
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 259.853

Attention duration (in seconds): 0.3440
Attention throughput (in TFLOP/s): 170.879
MLP duration (in seconds): 0.6013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.2644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 162.550
b: 256, m: 2048, n: 462, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 176.694
b: 256, m: 2048, n: 2048, k: 462,
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 112.951
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 262.702
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.3859
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 148.466
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 262.027

Attention duration (in seconds): 0.3333
Attention throughput (in TFLOP/s): 177.866
MLP duration (in seconds): 0.6046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 155.375
b: 256, m: 2048, n: 464, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 183.224
b: 256, m: 2048, n: 2048, k: 464,
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 153.828
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 263.176
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.3862
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 149.630
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 261.298

Attention duration (in seconds): 0.3458
Attention throughput (in TFLOP/s): 172.899
MLP duration (in seconds): 0.6074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.2855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 153.117
b: 256, m: 2048, n: 466, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 178.156
b: 256, m: 2048, n: 2048, k: 466,
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 113.851
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 260.739
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.4101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 142.132
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.3745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 155.669

Attention duration (in seconds): 0.3558
Attention throughput (in TFLOP/s): 169.447
MLP duration (in seconds): 0.7846
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1404
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.2903
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 151.915
b: 256, m: 2048, n: 468, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 185.904
b: 256, m: 2048, n: 2048, k: 468,
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 139.452
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.980
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.3926
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 149.754
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.3695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 159.096

Attention duration (in seconds): 0.3594
Attention throughput (in TFLOP/s): 169.175
MLP duration (in seconds): 0.7622
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.2883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 154.251
b: 256, m: 2048, n: 470, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 178.609
b: 256, m: 2048, n: 2048, k: 470,
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 114.645
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 260.599
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.4022
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 147.419
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 261.270

Attention duration (in seconds): 0.3597
Attention throughput (in TFLOP/s): 170.485
MLP duration (in seconds): 0.6292
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9889
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.2916
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 153.840
b: 256, m: 2048, n: 472, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 185.691
b: 256, m: 2048, n: 2048, k: 472,
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 155.467
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 262.200
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.4143
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 144.344
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 261.700

Attention duration (in seconds): 0.3606
Attention throughput (in TFLOP/s): 171.489
MLP duration (in seconds): 0.6428
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.2996
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 151.002
b: 256, m: 2048, n: 474, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 179.583
b: 256, m: 2048, n: 2048, k: 474,
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 115.386
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 259.704
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.4021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 150.003
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 262.529

Attention duration (in seconds): 0.3721
Attention throughput (in TFLOP/s): 167.553
MLP duration (in seconds): 0.6318
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.2990
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 152.537
b: 256, m: 2048, n: 476, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 188.774
b: 256, m: 2048, n: 2048, k: 476,
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 139.888
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 260.722
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.4264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 142.628
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 264.065

Attention duration (in seconds): 0.3701
Attention throughput (in TFLOP/s): 169.866
MLP duration (in seconds): 0.6568
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.3052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 150.739
b: 256, m: 2048, n: 478, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 181.496
b: 256, m: 2048, n: 2048, k: 478,
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 116.615
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 258.937
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.4196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 146.161
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 261.946

Attention duration (in seconds): 0.3788
Attention throughput (in TFLOP/s): 167.318
MLP duration (in seconds): 0.6538
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.3076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 150.815
b: 256, m: 2048, n: 480, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 189.792
b: 256, m: 2048, n: 2048, k: 480,
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 160.735
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 258.707
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.4276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 144.635
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2347
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 263.502

Attention duration (in seconds): 0.3792
Attention throughput (in TFLOP/s): 168.547
MLP duration (in seconds): 0.6623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0415
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.2968
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 157.605
b: 256, m: 2048, n: 482, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 182.422
b: 256, m: 2048, n: 2048, k: 482,
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 117.751
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 260.651
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.4218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 147.845
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.3899
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 159.938

Attention duration (in seconds): 0.3711
Attention throughput (in TFLOP/s): 173.651
MLP duration (in seconds): 0.8117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1828
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.3096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 152.313
b: 256, m: 2048, n: 484, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 192.438
b: 256, m: 2048, n: 2048, k: 484,
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 142.327
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 264.001
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.4311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 145.882
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.3784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 166.196

Attention duration (in seconds): 0.3819
Attention throughput (in TFLOP/s): 170.105
MLP duration (in seconds): 0.8094
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

num_attention_heads: 16, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 251.922
b: 64, m: 2048, n: 512, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x512x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x512x2048): 125.975
b: 64, m: 2048, n: 2048, k: 512,
Elapsed time for attention_prob_times_values (64x2048x2048x512): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x512): 112.392
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 213.672
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0170
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 258.126
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 253.295

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 216.373
MLP duration (in seconds): 0.0344
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0573
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8224x24672, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8224x24672, b=2048): 241.854
b: 64, m: 2048, n: 514, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x514x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x514x2048): 112.175
b: 64, m: 2048, n: 2048, k: 514,
Elapsed time for attention_prob_times_values (64x2048x2048x514): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x514): 89.716
Elapsed time for attention_linear_projection (4x8224x8224, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8224x8224, b=2048): 205.882
Elapsed time for mlp_h_to_4h (4x8224x32896, b=2048): 0.0179
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8224x32896, b=2048): 247.918
Elapsed time for mlp_4h_to_h (4x32896x8224, b=2048): 0.0176
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32896x8224, b=2048): 251.647

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 202.096
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8256x24768, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8256x24768, b=2048): 243.795
b: 64, m: 2048, n: 516, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x516x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x516x2048): 114.431
b: 64, m: 2048, n: 2048, k: 516,
Elapsed time for attention_prob_times_values (64x2048x2048x516): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x516): 102.192
Elapsed time for attention_linear_projection (4x8256x8256, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8256x8256, b=2048): 207.671
Elapsed time for mlp_h_to_4h (4x8256x33024, b=2048): 0.0179
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8256x33024, b=2048): 249.200
Elapsed time for mlp_4h_to_h (4x33024x8256, b=2048): 0.0176
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33024x8256, b=2048): 253.413

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 207.043
MLP duration (in seconds): 0.0356
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0598
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8288x24864, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8288x24864, b=2048): 244.433
b: 64, m: 2048, n: 518, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x518x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x518x2048): 112.666
b: 64, m: 2048, n: 2048, k: 518,
Elapsed time for attention_prob_times_values (64x2048x2048x518): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x518): 89.815
Elapsed time for attention_linear_projection (4x8288x8288, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8288x8288, b=2048): 207.755
Elapsed time for mlp_h_to_4h (4x8288x33152, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8288x33152, b=2048): 250.088
Elapsed time for mlp_4h_to_h (4x33152x8288, b=2048): 0.0177
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33152x8288, b=2048): 253.645

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 203.993
MLP duration (in seconds): 0.0357
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0605
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 246.044
b: 64, m: 2048, n: 520, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x520x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x520x2048): 113.871
b: 64, m: 2048, n: 2048, k: 520,
Elapsed time for attention_prob_times_values (64x2048x2048x520): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x520): 109.352
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 209.973
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 251.221
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0179
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 252.907

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 210.233
MLP duration (in seconds): 0.0360
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8352x25056, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8352x25056, b=2048): 246.474
b: 64, m: 2048, n: 522, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x522x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x522x2048): 112.806
b: 64, m: 2048, n: 2048, k: 522,
Elapsed time for attention_prob_times_values (64x2048x2048x522): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x522): 89.667
Elapsed time for attention_linear_projection (4x8352x8352, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8352x8352, b=2048): 210.282
Elapsed time for mlp_h_to_4h (4x8352x33408, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8352x33408, b=2048): 252.221
Elapsed time for mlp_4h_to_h (4x33408x8352, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33408x8352, b=2048): 252.221

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 205.647
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0612
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8384x25152, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8384x25152, b=2048): 247.814
b: 64, m: 2048, n: 524, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x524x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x524x2048): 116.102
b: 64, m: 2048, n: 2048, k: 524,
Elapsed time for attention_prob_times_values (64x2048x2048x524): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x524): 102.657
Elapsed time for attention_linear_projection (4x8384x8384, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8384x8384, b=2048): 211.915
Elapsed time for mlp_h_to_4h (4x8384x33536, b=2048): 0.0182
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8384x33536, b=2048): 253.265
Elapsed time for mlp_4h_to_h (4x33536x8384, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33536x8384, b=2048): 254.271

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 210.649
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8416x25248, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8416x25248, b=2048): 248.346
b: 64, m: 2048, n: 526, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x526x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x526x2048): 113.496
b: 64, m: 2048, n: 2048, k: 526,
Elapsed time for attention_prob_times_values (64x2048x2048x526): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x526): 89.874
Elapsed time for attention_linear_projection (4x8416x8416, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8416x8416, b=2048): 212.010
Elapsed time for mlp_h_to_4h (4x8416x33664, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8416x33664, b=2048): 254.109
Elapsed time for mlp_4h_to_h (4x33664x8416, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33664x8416, b=2048): 253.550

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 207.253
MLP duration (in seconds): 0.0366
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0617
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 250.021
b: 64, m: 2048, n: 528, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x528x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x528x2048): 116.097
b: 64, m: 2048, n: 2048, k: 528,
Elapsed time for attention_prob_times_values (64x2048x2048x528): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x528): 111.210
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 213.961
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 255.196
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 253.030

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 214.169
MLP duration (in seconds): 0.0368
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8480x25440, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8480x25440, b=2048): 250.409
b: 64, m: 2048, n: 530, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x530x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x530x2048): 114.700
b: 64, m: 2048, n: 2048, k: 530,
Elapsed time for attention_prob_times_values (64x2048x2048x530): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x530): 89.923
Elapsed time for attention_linear_projection (4x8480x8480, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8480x8480, b=2048): 214.156
Elapsed time for mlp_h_to_4h (4x8480x33920, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8480x33920, b=2048): 254.947
Elapsed time for mlp_4h_to_h (4x33920x8480, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33920x8480, b=2048): 252.096

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 209.084
MLP duration (in seconds): 0.0372
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0624
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8512x25536, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8512x25536, b=2048): 251.923
b: 64, m: 2048, n: 532, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x532x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x532x2048): 117.631
b: 64, m: 2048, n: 2048, k: 532,
Elapsed time for attention_prob_times_values (64x2048x2048x532): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x532): 103.219
Elapsed time for attention_linear_projection (4x8512x8512, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8512x8512, b=2048): 215.747
Elapsed time for mlp_h_to_4h (4x8512x34048, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8512x34048, b=2048): 256.020
Elapsed time for mlp_4h_to_h (4x34048x8512, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34048x8512, b=2048): 253.498

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 214.209
MLP duration (in seconds): 0.0373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0621
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8544x25632, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8544x25632, b=2048): 249.943
b: 64, m: 2048, n: 534, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x534x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x534x2048): 115.024
b: 64, m: 2048, n: 2048, k: 534,
Elapsed time for attention_prob_times_values (64x2048x2048x534): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x534): 89.883
Elapsed time for attention_linear_projection (4x8544x8544, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8544x8544, b=2048): 215.838
Elapsed time for mlp_h_to_4h (4x8544x34176, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8544x34176, b=2048): 255.622
Elapsed time for mlp_4h_to_h (4x34176x8544, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34176x8544, b=2048): 250.942

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 209.449
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0634
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 253.914
b: 64, m: 2048, n: 536, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x536x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x536x2048): 117.500
b: 64, m: 2048, n: 2048, k: 536,
Elapsed time for attention_prob_times_values (64x2048x2048x536): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x536): 111.891
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 217.692
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 257.265
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 252.497

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 217.621
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8608x25824, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8608x25824, b=2048): 243.405
b: 64, m: 2048, n: 538, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x538x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x538x2048): 115.974
b: 64, m: 2048, n: 2048, k: 538,
Elapsed time for attention_prob_times_values (64x2048x2048x538): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x538): 90.334
Elapsed time for attention_linear_projection (4x8608x8608, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8608x8608, b=2048): 209.701
Elapsed time for mlp_h_to_4h (4x8608x34432, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8608x34432, b=2048): 246.993
Elapsed time for mlp_4h_to_h (4x34432x8608, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34432x8608, b=2048): 252.461

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 205.511
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8640x25920, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8640x25920, b=2048): 244.902
b: 64, m: 2048, n: 540, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x540x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x540x2048): 119.330
b: 64, m: 2048, n: 2048, k: 540,
Elapsed time for attention_prob_times_values (64x2048x2048x540): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x540): 104.000
Elapsed time for attention_linear_projection (4x8640x8640, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8640x8640, b=2048): 211.350
Elapsed time for mlp_h_to_4h (4x8640x34560, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8640x34560, b=2048): 248.171
Elapsed time for mlp_4h_to_h (4x34560x8640, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34560x8640, b=2048): 254.033

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 210.576
MLP duration (in seconds): 0.0390
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0650
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8672x26016, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8672x26016, b=2048): 244.872
b: 64, m: 2048, n: 542, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x542x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x542x2048): 116.569
b: 64, m: 2048, n: 2048, k: 542,
Elapsed time for attention_prob_times_values (64x2048x2048x542): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x542): 91.128
Elapsed time for attention_linear_projection (4x8672x8672, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8672x8672, b=2048): 211.741
Elapsed time for mlp_h_to_4h (4x8672x34688, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8672x34688, b=2048): 250.037
Elapsed time for mlp_4h_to_h (4x34688x8672, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34688x8672, b=2048): 251.690

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 207.133
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0659
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 247.061
b: 64, m: 2048, n: 544, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x544x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x544x2048): 119.510
b: 64, m: 2048, n: 2048, k: 544,
Elapsed time for attention_prob_times_values (64x2048x2048x544): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x544): 115.184
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 213.437
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 249.987
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 253.181

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 214.524
MLP duration (in seconds): 0.0395
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8736x26208, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8736x26208, b=2048): 246.596
b: 64, m: 2048, n: 546, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x546x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x546x2048): 117.609
b: 64, m: 2048, n: 2048, k: 546,
Elapsed time for attention_prob_times_values (64x2048x2048x546): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x546): 91.739
Elapsed time for attention_linear_projection (4x8736x8736, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8736x8736, b=2048): 213.574
Elapsed time for mlp_h_to_4h (4x8736x34944, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8736x34944, b=2048): 249.780
Elapsed time for mlp_4h_to_h (4x34944x8736, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34944x8736, b=2048): 253.432

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 208.858
MLP duration (in seconds): 0.0398
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8768x26304, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8768x26304, b=2048): 248.616
b: 64, m: 2048, n: 548, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x548x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x548x2048): 121.396
b: 64, m: 2048, n: 2048, k: 548,
Elapsed time for attention_prob_times_values (64x2048x2048x548): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x548): 105.181
Elapsed time for attention_linear_projection (4x8768x8768, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8768x8768, b=2048): 215.352
Elapsed time for mlp_h_to_4h (4x8768x35072, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8768x35072, b=2048): 251.494
Elapsed time for mlp_4h_to_h (4x35072x8768, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35072x8768, b=2048): 252.283

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 214.199
MLP duration (in seconds): 0.0400
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0663
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8800x26400, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8800x26400, b=2048): 247.526
b: 64, m: 2048, n: 550, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x550x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x550x2048): 118.652
b: 64, m: 2048, n: 2048, k: 550,
Elapsed time for attention_prob_times_values (64x2048x2048x550): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x550): 91.699
Elapsed time for attention_linear_projection (4x8800x8800, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8800x8800, b=2048): 214.955
Elapsed time for mlp_h_to_4h (4x8800x35200, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8800x35200, b=2048): 251.912
Elapsed time for mlp_4h_to_h (4x35200x8800, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35200x8800, b=2048): 252.707

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 209.927
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0672
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 250.432
b: 64, m: 2048, n: 552, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x552x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x552x2048): 121.161
b: 64, m: 2048, n: 2048, k: 552,
Elapsed time for attention_prob_times_values (64x2048x2048x552): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x552): 115.017
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 217.012
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 254.325
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 253.906

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 217.561
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8864x26592, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8864x26592, b=2048): 248.028
b: 64, m: 2048, n: 554, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x554x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x554x2048): 119.229
b: 64, m: 2048, n: 2048, k: 554,
Elapsed time for attention_prob_times_values (64x2048x2048x554): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x554): 92.641
Elapsed time for attention_linear_projection (4x8864x8864, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8864x8864, b=2048): 217.583
Elapsed time for mlp_h_to_4h (4x8864x35456, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8864x35456, b=2048): 253.385
Elapsed time for mlp_4h_to_h (4x35456x8864, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35456x8864, b=2048): 251.500

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 211.241
MLP duration (in seconds): 0.0408
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8896x26688, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8896x26688, b=2048): 249.612
b: 64, m: 2048, n: 556, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x556x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x556x2048): 123.023
b: 64, m: 2048, n: 2048, k: 556,
Elapsed time for attention_prob_times_values (64x2048x2048x556): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x556): 106.399
Elapsed time for attention_linear_projection (4x8896x8896, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8896x8896, b=2048): 218.848
Elapsed time for mlp_h_to_4h (4x8896x35584, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8896x35584, b=2048): 254.650
Elapsed time for mlp_4h_to_h (4x35584x8896, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35584x8896, b=2048): 253.057

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 216.284
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8928x26784, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8928x26784, b=2048): 250.068
b: 64, m: 2048, n: 558, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x558x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x558x2048): 120.056
b: 64, m: 2048, n: 2048, k: 558,
Elapsed time for attention_prob_times_values (64x2048x2048x558): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x558): 93.219
Elapsed time for attention_linear_projection (4x8928x8928, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x8928x8928, b=2048): 219.261
Elapsed time for mlp_h_to_4h (4x8928x35712, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8928x35712, b=2048): 255.540
Elapsed time for mlp_4h_to_h (4x35712x8928, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35712x8928, b=2048): 253.140

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 213.043
MLP duration (in seconds): 0.0411
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0684
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 251.684
b: 64, m: 2048, n: 560, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x560x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x560x2048): 122.965
b: 64, m: 2048, n: 2048, k: 560,
Elapsed time for attention_prob_times_values (64x2048x2048x560): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x560): 117.642
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 220.641
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 258.201
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 255.274

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 220.065
MLP duration (in seconds): 0.0410
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8992x26976, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8992x26976, b=2048): 242.934
b: 64, m: 2048, n: 562, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x562x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x562x2048): 120.836
b: 64, m: 2048, n: 2048, k: 562,
Elapsed time for attention_prob_times_values (64x2048x2048x562): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x562): 93.506
Elapsed time for attention_linear_projection (4x8992x8992, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x8992x8992, b=2048): 212.807
Elapsed time for mlp_h_to_4h (4x8992x35968, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8992x35968, b=2048): 248.081
Elapsed time for mlp_4h_to_h (4x35968x8992, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35968x8992, b=2048): 252.661

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 208.506
MLP duration (in seconds): 0.0423
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0706
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9024x27072, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9024x27072, b=2048): 246.216
b: 64, m: 2048, n: 564, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x564x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x564x2048): 124.952
b: 64, m: 2048, n: 2048, k: 564,
Elapsed time for attention_prob_times_values (64x2048x2048x564): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x564): 108.790
Elapsed time for attention_linear_projection (4x9024x9024, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9024x9024, b=2048): 215.223
Elapsed time for mlp_h_to_4h (4x9024x36096, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9024x36096, b=2048): 248.814
Elapsed time for mlp_4h_to_h (4x36096x9024, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36096x9024, b=2048): 254.468

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 214.820
MLP duration (in seconds): 0.0424
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0701
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9056x27168, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9056x27168, b=2048): 244.859
b: 64, m: 2048, n: 566, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x566x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x566x2048): 121.614
b: 64, m: 2048, n: 2048, k: 566,
Elapsed time for attention_prob_times_values (64x2048x2048x566): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x566): 94.172
Elapsed time for attention_linear_projection (4x9056x9056, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9056x9056, b=2048): 214.835
Elapsed time for mlp_h_to_4h (4x9056x36224, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9056x36224, b=2048): 249.026
Elapsed time for mlp_4h_to_h (4x36224x9056, b=2048): 0.0211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36224x9056, b=2048): 254.247

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 210.335
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0712
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 247.428
b: 64, m: 2048, n: 568, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x568x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x568x2048): 124.346
b: 64, m: 2048, n: 2048, k: 568,
Elapsed time for attention_prob_times_values (64x2048x2048x568): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x568): 117.191
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 216.537
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 250.476
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 253.760

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 217.339
MLP duration (in seconds): 0.0429
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0707
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9120x27360, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9120x27360, b=2048): 245.738
b: 64, m: 2048, n: 570, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x570x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x570x2048): 123.049
b: 64, m: 2048, n: 2048, k: 570,
Elapsed time for attention_prob_times_values (64x2048x2048x570): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x570): 94.669
Elapsed time for attention_linear_projection (4x9120x9120, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9120x9120, b=2048): 216.479
Elapsed time for mlp_h_to_4h (4x9120x36480, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9120x36480, b=2048): 250.346
Elapsed time for mlp_4h_to_h (4x36480x9120, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36480x9120, b=2048): 252.809

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 211.616
MLP duration (in seconds): 0.0433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0720
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9152x27456, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9152x27456, b=2048): 248.070
b: 64, m: 2048, n: 572, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x572x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x572x2048): 126.389
b: 64, m: 2048, n: 2048, k: 572,
Elapsed time for attention_prob_times_values (64x2048x2048x572): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x572): 110.220
Elapsed time for attention_linear_projection (4x9152x9152, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9152x9152, b=2048): 218.191
Elapsed time for mlp_h_to_4h (4x9152x36608, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9152x36608, b=2048): 251.445
Elapsed time for mlp_4h_to_h (4x36608x9152, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36608x9152, b=2048): 254.682

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 217.194
MLP duration (in seconds): 0.0434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0715
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9184x27552, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9184x27552, b=2048): 247.599
b: 64, m: 2048, n: 574, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x574x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x574x2048): 123.510
b: 64, m: 2048, n: 2048, k: 574,
Elapsed time for attention_prob_times_values (64x2048x2048x574): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x574): 95.355
Elapsed time for attention_linear_projection (4x9184x9184, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9184x9184, b=2048): 218.337
Elapsed time for mlp_h_to_4h (4x9184x36736, b=2048): 0.0219
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9184x36736, b=2048): 252.028
Elapsed time for mlp_4h_to_h (4x36736x9184, b=2048): 0.0219
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36736x9184, b=2048): 252.121

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 213.334
MLP duration (in seconds): 0.0439
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0727
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 249.793
b: 64, m: 2048, n: 576, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x576x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x576x2048): 125.828
b: 64, m: 2048, n: 2048, k: 576,
Elapsed time for attention_prob_times_values (64x2048x2048x576): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x576): 120.958
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 219.663
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 253.515
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0219
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 253.970

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 220.397
MLP duration (in seconds): 0.0439
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0719
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9248x27744, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9248x27744, b=2048): 249.237
b: 64, m: 2048, n: 578, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x578x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x578x2048): 124.252
b: 64, m: 2048, n: 2048, k: 578,
Elapsed time for attention_prob_times_values (64x2048x2048x578): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x578): 92.888
Elapsed time for attention_linear_projection (4x9248x9248, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9248x9248, b=2048): 219.719
Elapsed time for mlp_h_to_4h (4x9248x36992, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9248x36992, b=2048): 253.197
Elapsed time for mlp_4h_to_h (4x36992x9248, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36992x9248, b=2048): 253.055

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 214.071
MLP duration (in seconds): 0.0443
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0734
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9280x27840, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9280x27840, b=2048): 250.930
b: 64, m: 2048, n: 580, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x580x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x580x2048): 128.044
b: 64, m: 2048, n: 2048, k: 580,
Elapsed time for attention_prob_times_values (64x2048x2048x580): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x580): 106.642
Elapsed time for attention_linear_projection (4x9280x9280, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9280x9280, b=2048): 221.748
Elapsed time for mlp_h_to_4h (4x9280x37120, b=2048): 0.0223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9280x37120, b=2048): 253.182
Elapsed time for mlp_4h_to_h (4x37120x9280, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37120x9280, b=2048): 256.075

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 219.239
MLP duration (in seconds): 0.0443
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9312x27936, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9312x27936, b=2048): 251.193
b: 64, m: 2048, n: 582, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x582x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x582x2048): 125.136
b: 64, m: 2048, n: 2048, k: 582,
Elapsed time for attention_prob_times_values (64x2048x2048x582): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x582): 92.789
Elapsed time for attention_linear_projection (4x9312x9312, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9312x9312, b=2048): 221.611
Elapsed time for mlp_h_to_4h (4x9312x37248, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9312x37248, b=2048): 255.922
Elapsed time for mlp_4h_to_h (4x37248x9312, b=2048): 0.0225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37248x9312, b=2048): 252.172

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 215.703
MLP duration (in seconds): 0.0447
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0740
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 252.862
b: 64, m: 2048, n: 584, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x584x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x584x2048): 127.144
b: 64, m: 2048, n: 2048, k: 584,
Elapsed time for attention_prob_times_values (64x2048x2048x584): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x584): 117.185
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 223.211
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 256.884
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0224
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 255.150

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 222.603
MLP duration (in seconds): 0.0447
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0732
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9376x28128, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9376x28128, b=2048): 252.913
b: 64, m: 2048, n: 586, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x586x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x586x2048): 125.744
b: 64, m: 2048, n: 2048, k: 586,
Elapsed time for attention_prob_times_values (64x2048x2048x586): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x586): 93.261
Elapsed time for attention_linear_projection (4x9376x9376, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9376x9376, b=2048): 223.570
Elapsed time for mlp_h_to_4h (4x9376x37504, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9376x37504, b=2048): 255.157
Elapsed time for mlp_4h_to_h (4x37504x9376, b=2048): 0.0225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37504x9376, b=2048): 255.575

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 217.345
MLP duration (in seconds): 0.0451
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0745
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9408x28224, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9408x28224, b=2048): 254.657
b: 64, m: 2048, n: 588, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x588x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x588x2048): 129.950
b: 64, m: 2048, n: 2048, k: 588,
Elapsed time for attention_prob_times_values (64x2048x2048x588): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x588): 108.113
Elapsed time for attention_linear_projection (4x9408x9408, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9408x9408, b=2048): 225.140
Elapsed time for mlp_h_to_4h (4x9408x37632, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9408x37632, b=2048): 256.730
Elapsed time for mlp_4h_to_h (4x37632x9408, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37632x9408, b=2048): 254.632

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 222.761
MLP duration (in seconds): 0.0454
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0742
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9440x28320, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9440x28320, b=2048): 254.591
b: 64, m: 2048, n: 590, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x590x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x590x2048): 126.121
b: 64, m: 2048, n: 2048, k: 590,
Elapsed time for attention_prob_times_values (64x2048x2048x590): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x590): 93.423
Elapsed time for attention_linear_projection (4x9440x9440, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9440x9440, b=2048): 225.125
Elapsed time for mlp_h_to_4h (4x9440x37760, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9440x37760, b=2048): 258.423
Elapsed time for mlp_4h_to_h (4x37760x9440, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37760x9440, b=2048): 252.345

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 218.763
MLP duration (in seconds): 0.0457
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 257.007
b: 64, m: 2048, n: 592, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x592x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x592x2048): 129.865
b: 64, m: 2048, n: 2048, k: 592,
Elapsed time for attention_prob_times_values (64x2048x2048x592): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x592): 119.600
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 226.937
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 258.319
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 256.156

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 226.699
MLP duration (in seconds): 0.0457
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0745
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9504x28512, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9504x28512, b=2048): 246.580
b: 64, m: 2048, n: 594, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x594x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x594x2048): 127.692
b: 64, m: 2048, n: 2048, k: 594,
Elapsed time for attention_prob_times_values (64x2048x2048x594): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x594): 94.109
Elapsed time for attention_linear_projection (4x9504x9504, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9504x9504, b=2048): 218.947
Elapsed time for mlp_h_to_4h (4x9504x38016, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9504x38016, b=2048): 250.701
Elapsed time for mlp_4h_to_h (4x38016x9504, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38016x9504, b=2048): 254.316

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 213.942
MLP duration (in seconds): 0.0469
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0775
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9536x28608, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9536x28608, b=2048): 248.240
b: 64, m: 2048, n: 596, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x596x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x596x2048): 131.899
b: 64, m: 2048, n: 2048, k: 596,
Elapsed time for attention_prob_times_values (64x2048x2048x596): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x596): 109.227
Elapsed time for attention_linear_projection (4x9536x9536, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9536x9536, b=2048): 220.595
Elapsed time for mlp_h_to_4h (4x9536x38144, b=2048): 0.0237
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9536x38144, b=2048): 251.518
Elapsed time for mlp_4h_to_h (4x38144x9536, b=2048): 0.0233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38144x9536, b=2048): 255.352

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 219.145
MLP duration (in seconds): 0.0470
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0771
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9568x28704, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9568x28704, b=2048): 248.294
b: 64, m: 2048, n: 598, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x598x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x598x2048): 128.136
b: 64, m: 2048, n: 2048, k: 598,
Elapsed time for attention_prob_times_values (64x2048x2048x598): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x598): 94.457
Elapsed time for attention_linear_projection (4x9568x9568, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9568x9568, b=2048): 220.444
Elapsed time for mlp_h_to_4h (4x9568x38272, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9568x38272, b=2048): 252.457
Elapsed time for mlp_4h_to_h (4x38272x9568, b=2048): 0.0237
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38272x9568, b=2048): 252.926

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 215.424
MLP duration (in seconds): 0.0475
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0783
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 250.366
b: 64, m: 2048, n: 600, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x600x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x600x2048): 131.224
b: 64, m: 2048, n: 2048, k: 600,
Elapsed time for attention_prob_times_values (64x2048x2048x600): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x600): 119.862
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 222.389
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0240
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 251.337
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 256.721

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 222.618
MLP duration (in seconds): 0.0476
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9632x28896, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9632x28896, b=2048): 249.702
b: 64, m: 2048, n: 602, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x602x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x602x2048): 128.809
b: 64, m: 2048, n: 2048, k: 602,
Elapsed time for attention_prob_times_values (64x2048x2048x602): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x602): 94.783
Elapsed time for attention_linear_projection (4x9632x9632, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9632x9632, b=2048): 222.382
Elapsed time for mlp_h_to_4h (4x9632x38528, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9632x38528, b=2048): 251.278
Elapsed time for mlp_4h_to_h (4x38528x9632, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38528x9632, b=2048): 252.327

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 216.870
MLP duration (in seconds): 0.0483
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0793
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9664x28992, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9664x28992, b=2048): 251.403
b: 64, m: 2048, n: 604, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x604x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x604x2048): 133.630
b: 64, m: 2048, n: 2048, k: 604,
Elapsed time for attention_prob_times_values (64x2048x2048x604): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x604): 110.630
Elapsed time for attention_linear_projection (4x9664x9664, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9664x9664, b=2048): 224.003
Elapsed time for mlp_h_to_4h (4x9664x38656, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9664x38656, b=2048): 252.848
Elapsed time for mlp_4h_to_h (4x38656x9664, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38656x9664, b=2048): 255.943

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 222.318
MLP duration (in seconds): 0.0481
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0786
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9696x29088, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9696x29088, b=2048): 251.795
b: 64, m: 2048, n: 606, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x606x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x606x2048): 129.542
b: 64, m: 2048, n: 2048, k: 606,
Elapsed time for attention_prob_times_values (64x2048x2048x606): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x606): 95.353
Elapsed time for attention_linear_projection (4x9696x9696, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9696x9696, b=2048): 224.206
Elapsed time for mlp_h_to_4h (4x9696x38784, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9696x38784, b=2048): 253.778
Elapsed time for mlp_4h_to_h (4x38784x9696, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38784x9696, b=2048): 254.292

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 218.713
MLP duration (in seconds): 0.0485
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 253.470
b: 64, m: 2048, n: 608, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x608x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x608x2048): 133.245
b: 64, m: 2048, n: 2048, k: 608,
Elapsed time for attention_prob_times_values (64x2048x2048x608): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x608): 122.536
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 226.175
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 255.130
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 255.777

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 226.081
MLP duration (in seconds): 0.0486
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0789
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9760x29280, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9760x29280, b=2048): 253.383
b: 64, m: 2048, n: 610, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x610x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x610x2048): 130.632
b: 64, m: 2048, n: 2048, k: 610,
Elapsed time for attention_prob_times_values (64x2048x2048x610): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x610): 96.063
Elapsed time for attention_linear_projection (4x9760x9760, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9760x9760, b=2048): 225.999
Elapsed time for mlp_h_to_4h (4x9760x39040, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9760x39040, b=2048): 254.829
Elapsed time for mlp_4h_to_h (4x39040x9760, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39040x9760, b=2048): 253.620

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 220.374
MLP duration (in seconds): 0.0491
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0804
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9792x29376, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9792x29376, b=2048): 254.872
b: 64, m: 2048, n: 612, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x612x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x612x2048): 134.949
b: 64, m: 2048, n: 2048, k: 612,
Elapsed time for attention_prob_times_values (64x2048x2048x612): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x612): 111.137
Elapsed time for attention_linear_projection (4x9792x9792, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9792x9792, b=2048): 227.397
Elapsed time for mlp_h_to_4h (4x9792x39168, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9792x39168, b=2048): 256.102
Elapsed time for mlp_4h_to_h (4x39168x9792, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39168x9792, b=2048): 255.449

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 225.424
MLP duration (in seconds): 0.0491
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0799
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9824x29472, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9824x29472, b=2048): 254.474
b: 64, m: 2048, n: 614, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x614x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x614x2048): 131.326
b: 64, m: 2048, n: 2048, k: 614,
Elapsed time for attention_prob_times_values (64x2048x2048x614): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x614): 96.335
Elapsed time for attention_linear_projection (4x9824x9824, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9824x9824, b=2048): 227.582
Elapsed time for mlp_h_to_4h (4x9824x39296, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9824x39296, b=2048): 256.358
Elapsed time for mlp_4h_to_h (4x39296x9824, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39296x9824, b=2048): 254.807

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 221.573
MLP duration (in seconds): 0.0495
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 256.308
b: 64, m: 2048, n: 616, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x616x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x616x2048): 134.371
b: 64, m: 2048, n: 2048, k: 616,
Elapsed time for attention_prob_times_values (64x2048x2048x616): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x616): 122.083
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 228.988
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 257.849
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 254.991

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 228.548
MLP duration (in seconds): 0.0497
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0804
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9888x29664, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9888x29664, b=2048): 246.833
b: 64, m: 2048, n: 618, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x618x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x618x2048): 131.669
b: 64, m: 2048, n: 2048, k: 618,
Elapsed time for attention_prob_times_values (64x2048x2048x618): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x618): 97.132
Elapsed time for attention_linear_projection (4x9888x9888, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x9888x9888, b=2048): 221.343
Elapsed time for mlp_h_to_4h (4x9888x39552, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9888x39552, b=2048): 248.030
Elapsed time for mlp_4h_to_h (4x39552x9888, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39552x9888, b=2048): 253.983

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 216.626
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0837
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9920x29760, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9920x29760, b=2048): 249.819
b: 64, m: 2048, n: 620, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x620x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x620x2048): 135.994
b: 64, m: 2048, n: 2048, k: 620,
Elapsed time for attention_prob_times_values (64x2048x2048x620): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x620): 110.891
Elapsed time for attention_linear_projection (4x9920x9920, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x9920x9920, b=2048): 222.808
Elapsed time for mlp_h_to_4h (4x9920x39680, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9920x39680, b=2048): 251.700
Elapsed time for mlp_4h_to_h (4x39680x9920, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39680x9920, b=2048): 255.751

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 222.014
MLP duration (in seconds): 0.0508
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9952x29856, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9952x29856, b=2048): 248.759
b: 64, m: 2048, n: 622, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x622x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x622x2048): 132.196
b: 64, m: 2048, n: 2048, k: 622,
Elapsed time for attention_prob_times_values (64x2048x2048x622): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x622): 96.775
Elapsed time for attention_linear_projection (4x9952x9952, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x9952x9952, b=2048): 223.101
Elapsed time for mlp_h_to_4h (4x9952x39808, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9952x39808, b=2048): 250.120
Elapsed time for mlp_4h_to_h (4x39808x9952, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39808x9952, b=2048): 253.155

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 218.122
MLP duration (in seconds): 0.0516
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0844
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 250.416
b: 64, m: 2048, n: 624, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x624x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x624x2048): 136.512
b: 64, m: 2048, n: 2048, k: 624,
Elapsed time for attention_prob_times_values (64x2048x2048x624): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x624): 125.022
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 224.692
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 252.392
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 256.676

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 225.313
MLP duration (in seconds): 0.0513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0833
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10016x30048, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10016x30048, b=2048): 250.128
b: 64, m: 2048, n: 626, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x626x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x626x2048): 133.285
b: 64, m: 2048, n: 2048, k: 626,
Elapsed time for attention_prob_times_values (64x2048x2048x626): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x626): 98.493
Elapsed time for attention_linear_projection (4x10016x10016, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x10016x10016, b=2048): 224.756
Elapsed time for mlp_h_to_4h (4x10016x40064, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10016x40064, b=2048): 250.938
Elapsed time for mlp_4h_to_h (4x40064x10016, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40064x10016, b=2048): 254.761

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 219.862
MLP duration (in seconds): 0.0520
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0850
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10048x30144, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10048x30144, b=2048): 251.617
b: 64, m: 2048, n: 628, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x628x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x628x2048): 138.274
b: 64, m: 2048, n: 2048, k: 628,
Elapsed time for attention_prob_times_values (64x2048x2048x628): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x628): 113.366
Elapsed time for attention_linear_projection (4x10048x10048, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x10048x10048, b=2048): 226.099
Elapsed time for mlp_h_to_4h (4x10048x40192, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10048x40192, b=2048): 252.197
Elapsed time for mlp_4h_to_h (4x40192x10048, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40192x10048, b=2048): 256.100

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 224.677
MLP duration (in seconds): 0.0521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0845
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10080x30240, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10080x30240, b=2048): 249.884
b: 64, m: 2048, n: 630, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x630x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x630x2048): 134.201
b: 64, m: 2048, n: 2048, k: 630,
Elapsed time for attention_prob_times_values (64x2048x2048x630): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x630): 98.455
Elapsed time for attention_linear_projection (4x10080x10080, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10080x10080, b=2048): 226.303
Elapsed time for mlp_h_to_4h (4x10080x40320, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10080x40320, b=2048): 252.514
Elapsed time for mlp_4h_to_h (4x40320x10080, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40320x10080, b=2048): 253.970

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 220.295
MLP duration (in seconds): 0.0526
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0859
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 253.292
b: 64, m: 2048, n: 632, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x632x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x632x2048): 138.115
b: 64, m: 2048, n: 2048, k: 632,
Elapsed time for attention_prob_times_values (64x2048x2048x632): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x632): 124.444
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 227.697
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 255.006
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 256.305

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 227.891
MLP duration (in seconds): 0.0524
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0848
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10144x30432, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10144x30432, b=2048): 251.596
b: 64, m: 2048, n: 634, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x634x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x634x2048): 134.798
b: 64, m: 2048, n: 2048, k: 634,
Elapsed time for attention_prob_times_values (64x2048x2048x634): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x634): 99.273
Elapsed time for attention_linear_projection (4x10144x10144, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10144x10144, b=2048): 228.062
Elapsed time for mlp_h_to_4h (4x10144x40576, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10144x40576, b=2048): 254.228
Elapsed time for mlp_4h_to_h (4x40576x10144, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40576x10144, b=2048): 255.622

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 221.964
MLP duration (in seconds): 0.0529
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0864
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10176x30528, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10176x30528, b=2048): 254.675
b: 64, m: 2048, n: 636, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x636x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x636x2048): 139.913
b: 64, m: 2048, n: 2048, k: 636,
Elapsed time for attention_prob_times_values (64x2048x2048x636): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x636): 114.562
Elapsed time for attention_linear_projection (4x10176x10176, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10176x10176, b=2048): 229.473
Elapsed time for mlp_h_to_4h (4x10176x40704, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10176x40704, b=2048): 256.092
Elapsed time for mlp_4h_to_h (4x40704x10176, b=2048): 0.0266
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40704x10176, b=2048): 255.021

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 227.724
MLP duration (in seconds): 0.0531
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0859
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10208x30624, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10208x30624, b=2048): 254.772
b: 64, m: 2048, n: 638, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x638x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x638x2048): 134.833
b: 64, m: 2048, n: 2048, k: 638,
Elapsed time for attention_prob_times_values (64x2048x2048x638): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x638): 100.080
Elapsed time for attention_linear_projection (4x10208x10208, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10208x10208, b=2048): 229.594
Elapsed time for mlp_h_to_4h (4x10208x40832, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10208x40832, b=2048): 255.624
Elapsed time for mlp_4h_to_h (4x40832x10208, b=2048): 0.0268
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40832x10208, b=2048): 254.706

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 224.286
MLP duration (in seconds): 0.0535
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0870
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 256.497
b: 64, m: 2048, n: 640, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x640x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x640x2048): 140.094
b: 64, m: 2048, n: 2048, k: 640,
Elapsed time for attention_prob_times_values (64x2048x2048x640): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x640): 127.886
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 231.087
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0266
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 258.000
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 257.231

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 231.397
MLP duration (in seconds): 0.0534
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10272x30816, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10272x30816, b=2048): 256.580
b: 64, m: 2048, n: 642, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x642x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x642x2048): 123.804
b: 64, m: 2048, n: 2048, k: 642,
Elapsed time for attention_prob_times_values (64x2048x2048x642): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x642): 97.043
Elapsed time for attention_linear_projection (4x10272x10272, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10272x10272, b=2048): 231.037
Elapsed time for mlp_h_to_4h (4x10272x41088, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10272x41088, b=2048): 255.936
Elapsed time for mlp_4h_to_h (4x41088x10272, b=2048): 0.0272
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41088x10272, b=2048): 254.075

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 223.451
MLP duration (in seconds): 0.0542
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0883
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10304x30912, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10304x30912, b=2048): 258.682
b: 64, m: 2048, n: 644, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x644x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x644x2048): 127.632
b: 64, m: 2048, n: 2048, k: 644,
Elapsed time for attention_prob_times_values (64x2048x2048x644): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x644): 111.448
Elapsed time for attention_linear_projection (4x10304x10304, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10304x10304, b=2048): 232.620
Elapsed time for mlp_h_to_4h (4x10304x41216, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10304x41216, b=2048): 258.707
Elapsed time for mlp_4h_to_h (4x41216x10304, b=2048): 0.0272
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41216x10304, b=2048): 255.896

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 228.599
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0875
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10336x31008, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10336x31008, b=2048): 257.612
b: 64, m: 2048, n: 646, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x646x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x646x2048): 124.671
b: 64, m: 2048, n: 2048, k: 646,
Elapsed time for attention_prob_times_values (64x2048x2048x646): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x646): 97.289
Elapsed time for attention_linear_projection (4x10336x10336, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10336x10336, b=2048): 232.871
Elapsed time for mlp_h_to_4h (4x10336x41344, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10336x41344, b=2048): 256.649
Elapsed time for mlp_4h_to_h (4x41344x10336, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41344x10336, b=2048): 255.557

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 224.694
MLP duration (in seconds): 0.0547
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0889
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 259.756
b: 64, m: 2048, n: 648, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x648x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x648x2048): 127.039
b: 64, m: 2048, n: 2048, k: 648,
Elapsed time for attention_prob_times_values (64x2048x2048x648): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x648): 123.868
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 234.278
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 260.496
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0272
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 259.433

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 231.718
MLP duration (in seconds): 0.0542
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10400x31200, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10400x31200, b=2048): 250.653
b: 64, m: 2048, n: 650, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x650x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x650x2048): 125.336
b: 64, m: 2048, n: 2048, k: 650,
Elapsed time for attention_prob_times_values (64x2048x2048x650): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x650): 97.163
Elapsed time for attention_linear_projection (4x10400x10400, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_linear_projection (4x10400x10400, b=2048): 226.282
Elapsed time for mlp_h_to_4h (4x10400x41600, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10400x41600, b=2048): 251.615
Elapsed time for mlp_4h_to_h (4x41600x10400, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41600x10400, b=2048): 257.278

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 219.848
MLP duration (in seconds): 0.0557
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10432x31296, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10432x31296, b=2048): 254.238
b: 64, m: 2048, n: 652, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x652x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x652x2048): 129.503
b: 64, m: 2048, n: 2048, k: 652,
Elapsed time for attention_prob_times_values (64x2048x2048x652): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x652): 112.910
Elapsed time for attention_linear_projection (4x10432x10432, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_linear_projection (4x10432x10432, b=2048): 227.698
Elapsed time for mlp_h_to_4h (4x10432x41728, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10432x41728, b=2048): 255.074
Elapsed time for mlp_4h_to_h (4x41728x10432, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41728x10432, b=2048): 258.347

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 225.884
MLP duration (in seconds): 0.0556
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0902
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10464x31392, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10464x31392, b=2048): 253.987
b: 64, m: 2048, n: 654, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x654x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x654x2048): 125.848
b: 64, m: 2048, n: 2048, k: 654,
Elapsed time for attention_prob_times_values (64x2048x2048x654): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x654): 98.480
Elapsed time for attention_linear_projection (4x10464x10464, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10464x10464, b=2048): 227.855
Elapsed time for mlp_h_to_4h (4x10464x41856, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10464x41856, b=2048): 254.127
Elapsed time for mlp_4h_to_h (4x41856x10464, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41856x10464, b=2048): 258.893

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 222.431
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0914
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 255.843
b: 64, m: 2048, n: 656, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x656x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x656x2048): 128.719
b: 64, m: 2048, n: 2048, k: 656,
Elapsed time for attention_prob_times_values (64x2048x2048x656): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x656): 126.320
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 229.258
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 256.664
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 257.846

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 229.275
MLP duration (in seconds): 0.0561
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0907
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10528x31584, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10528x31584, b=2048): 255.303
b: 64, m: 2048, n: 658, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x658x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x658x2048): 127.205
b: 64, m: 2048, n: 2048, k: 658,
Elapsed time for attention_prob_times_values (64x2048x2048x658): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x658): 98.536
Elapsed time for attention_linear_projection (4x10528x10528, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10528x10528, b=2048): 228.994
Elapsed time for mlp_h_to_4h (4x10528x42112, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10528x42112, b=2048): 255.599
Elapsed time for mlp_4h_to_h (4x42112x10528, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42112x10528, b=2048): 258.130

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 223.690
MLP duration (in seconds): 0.0566
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0922
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10560x31680, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10560x31680, b=2048): 256.757
b: 64, m: 2048, n: 660, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x660x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x660x2048): 130.965
b: 64, m: 2048, n: 2048, k: 660,
Elapsed time for attention_prob_times_values (64x2048x2048x660): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x660): 114.085
Elapsed time for attention_linear_projection (4x10560x10560, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10560x10560, b=2048): 230.811
Elapsed time for mlp_h_to_4h (4x10560x42240, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10560x42240, b=2048): 258.062
Elapsed time for mlp_4h_to_h (4x42240x10560, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42240x10560, b=2048): 258.882

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 228.564
MLP duration (in seconds): 0.0565
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0916
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10592x31776, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10592x31776, b=2048): 255.476
b: 64, m: 2048, n: 662, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x662x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x662x2048): 127.442
b: 64, m: 2048, n: 2048, k: 662,
Elapsed time for attention_prob_times_values (64x2048x2048x662): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x662): 99.585
Elapsed time for attention_linear_projection (4x10592x10592, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10592x10592, b=2048): 230.594
Elapsed time for mlp_h_to_4h (4x10592x42368, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10592x42368, b=2048): 256.846
Elapsed time for mlp_4h_to_h (4x42368x10592, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42368x10592, b=2048): 257.489

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 224.520
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0931
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 257.768
b: 64, m: 2048, n: 664, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x664x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x664x2048): 130.085
b: 64, m: 2048, n: 2048, k: 664,
Elapsed time for attention_prob_times_values (64x2048x2048x664): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x664): 125.869
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 232.364
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 259.461
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 258.368

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 231.361
MLP duration (in seconds): 0.0571
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0922
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10656x31968, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10656x31968, b=2048): 256.563
b: 64, m: 2048, n: 666, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x666x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x666x2048): 128.245
b: 64, m: 2048, n: 2048, k: 666,
Elapsed time for attention_prob_times_values (64x2048x2048x666): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x666): 99.101
Elapsed time for attention_linear_projection (4x10656x10656, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10656x10656, b=2048): 233.026
Elapsed time for mlp_h_to_4h (4x10656x42624, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10656x42624, b=2048): 256.450
Elapsed time for mlp_4h_to_h (4x42624x10656, b=2048): 0.0288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42624x10656, b=2048): 258.675

Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 225.739
MLP duration (in seconds): 0.0578
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10688x32064, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10688x32064, b=2048): 258.265
b: 64, m: 2048, n: 668, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x668x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x668x2048): 132.494
b: 64, m: 2048, n: 2048, k: 668,
Elapsed time for attention_prob_times_values (64x2048x2048x668): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x668): 115.859
Elapsed time for attention_linear_projection (4x10688x10688, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10688x10688, b=2048): 234.603
Elapsed time for mlp_h_to_4h (4x10688x42752, b=2048): 0.0288
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10688x42752, b=2048): 259.605
Elapsed time for mlp_4h_to_h (4x42752x10688, b=2048): 0.0288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42752x10688, b=2048): 259.912

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 230.957
MLP duration (in seconds): 0.0576
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0932
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10720x32160, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10720x32160, b=2048): 257.538
b: 64, m: 2048, n: 670, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x670x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x670x2048): 128.576
b: 64, m: 2048, n: 2048, k: 670,
Elapsed time for attention_prob_times_values (64x2048x2048x670): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x670): 100.313
Elapsed time for attention_linear_projection (4x10720x10720, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10720x10720, b=2048): 234.733
Elapsed time for mlp_h_to_4h (4x10720x42880, b=2048): 0.0291
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10720x42880, b=2048): 258.501
Elapsed time for mlp_4h_to_h (4x42880x10720, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42880x10720, b=2048): 256.784

Attention duration (in seconds): 0.0363
Attention throughput (in TFLOP/s): 227.060
MLP duration (in seconds): 0.0585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 259.724
b: 64, m: 2048, n: 672, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x672x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x672x2048): 131.882
b: 64, m: 2048, n: 2048, k: 672,
Elapsed time for attention_prob_times_values (64x2048x2048x672): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x672): 128.839
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 236.488
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0291
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 260.059
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 258.991

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 234.251
MLP duration (in seconds): 0.0584
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0938
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10784x32352, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10784x32352, b=2048): 250.505
b: 64, m: 2048, n: 674, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x674x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x674x2048): 129.863
b: 64, m: 2048, n: 2048, k: 674,
Elapsed time for attention_prob_times_values (64x2048x2048x674): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x674): 100.992
Elapsed time for attention_linear_projection (4x10784x10784, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x10784x10784, b=2048): 228.276
Elapsed time for mlp_h_to_4h (4x10784x43136, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10784x43136, b=2048): 251.114
Elapsed time for mlp_4h_to_h (4x43136x10784, b=2048): 0.0296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43136x10784, b=2048): 257.127

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 222.334
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0975
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10816x32448, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10816x32448, b=2048): 252.023
b: 64, m: 2048, n: 676, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x676x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x676x2048): 134.081
b: 64, m: 2048, n: 2048, k: 676,
Elapsed time for attention_prob_times_values (64x2048x2048x676): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x676): 117.202
Elapsed time for attention_linear_projection (4x10816x10816, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x10816x10816, b=2048): 229.961
Elapsed time for mlp_h_to_4h (4x10816x43264, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10816x43264, b=2048): 252.869
Elapsed time for mlp_4h_to_h (4x43264x10816, b=2048): 0.0299
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43264x10816, b=2048): 256.776

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 227.111
MLP duration (in seconds): 0.0602
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0971
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10848x32544, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10848x32544, b=2048): 250.147
b: 64, m: 2048, n: 678, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x678x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x678x2048): 130.444
b: 64, m: 2048, n: 2048, k: 678,
Elapsed time for attention_prob_times_values (64x2048x2048x678): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x678): 101.565
Elapsed time for attention_linear_projection (4x10848x10848, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10848x10848, b=2048): 229.740
Elapsed time for mlp_h_to_4h (4x10848x43392, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10848x43392, b=2048): 252.639
Elapsed time for mlp_4h_to_h (4x43392x10848, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43392x10848, b=2048): 256.053

Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 222.757
MLP duration (in seconds): 0.0606
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0985
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 251.446
b: 64, m: 2048, n: 680, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x680x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x680x2048): 133.127
b: 64, m: 2048, n: 2048, k: 680,
Elapsed time for attention_prob_times_values (64x2048x2048x680): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x680): 128.577
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 231.373
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 254.293
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 256.965

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 228.764
MLP duration (in seconds): 0.0607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0978
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10912x32736, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10912x32736, b=2048): 251.371
b: 64, m: 2048, n: 682, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x682x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x682x2048): 131.024
b: 64, m: 2048, n: 2048, k: 682,
Elapsed time for attention_prob_times_values (64x2048x2048x682): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x682): 102.348
Elapsed time for attention_linear_projection (4x10912x10912, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10912x10912, b=2048): 231.361
Elapsed time for mlp_h_to_4h (4x10912x43648, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10912x43648, b=2048): 253.414
Elapsed time for mlp_4h_to_h (4x43648x10912, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43648x10912, b=2048): 255.661

Attention duration (in seconds): 0.0381
Attention throughput (in TFLOP/s): 224.113
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0994
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10944x32832, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10944x32832, b=2048): 252.517
b: 64, m: 2048, n: 684, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x684x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x684x2048): 135.227
b: 64, m: 2048, n: 2048, k: 684,
Elapsed time for attention_prob_times_values (64x2048x2048x684): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x684): 119.093
Elapsed time for attention_linear_projection (4x10944x10944, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10944x10944, b=2048): 232.891
Elapsed time for mlp_h_to_4h (4x10944x43776, b=2048): 0.0307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10944x43776, b=2048): 255.341
Elapsed time for mlp_4h_to_h (4x43776x10944, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43776x10944, b=2048): 256.633

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 228.667
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0989
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10976x32928, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10976x32928, b=2048): 252.534
b: 64, m: 2048, n: 686, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x686x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x686x2048): 131.792
b: 64, m: 2048, n: 2048, k: 686,
Elapsed time for attention_prob_times_values (64x2048x2048x686): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x686): 103.037
Elapsed time for attention_linear_projection (4x10976x10976, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x10976x10976, b=2048): 233.140
Elapsed time for mlp_h_to_4h (4x10976x43904, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10976x43904, b=2048): 253.142
Elapsed time for mlp_4h_to_h (4x43904x10976, b=2048): 0.0307
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43904x10976, b=2048): 256.817

Attention duration (in seconds): 0.0383
Attention throughput (in TFLOP/s): 225.473
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1002
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 254.096
b: 64, m: 2048, n: 688, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x688x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x688x2048): 135.092
b: 64, m: 2048, n: 2048, k: 688,
Elapsed time for attention_prob_times_values (64x2048x2048x688): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x688): 131.570
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 234.416
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 256.999
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 257.928

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 231.773
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0991
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11040x33120, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11040x33120, b=2048): 253.918
b: 64, m: 2048, n: 690, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x690x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x690x2048): 132.696
b: 64, m: 2048, n: 2048, k: 690,
Elapsed time for attention_prob_times_values (64x2048x2048x690): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x690): 103.342
Elapsed time for attention_linear_projection (4x11040x11040, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11040x11040, b=2048): 234.868
Elapsed time for mlp_h_to_4h (4x11040x44160, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11040x44160, b=2048): 254.482
Elapsed time for mlp_4h_to_h (4x44160x11040, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44160x11040, b=2048): 256.128

Attention duration (in seconds): 0.0385
Attention throughput (in TFLOP/s): 226.881
MLP duration (in seconds): 0.0626
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11072x33216, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11072x33216, b=2048): 255.376
b: 64, m: 2048, n: 692, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x692x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x692x2048): 136.604
b: 64, m: 2048, n: 2048, k: 692,
Elapsed time for attention_prob_times_values (64x2048x2048x692): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x692): 120.087
Elapsed time for attention_linear_projection (4x11072x11072, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11072x11072, b=2048): 235.908
Elapsed time for mlp_h_to_4h (4x11072x44288, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11072x44288, b=2048): 258.439
Elapsed time for mlp_4h_to_h (4x44288x11072, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44288x11072, b=2048): 257.214

Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 231.450
MLP duration (in seconds): 0.0623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1002
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11104x33312, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11104x33312, b=2048): 255.839
b: 64, m: 2048, n: 694, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x694x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x694x2048): 133.318
b: 64, m: 2048, n: 2048, k: 694,
Elapsed time for attention_prob_times_values (64x2048x2048x694): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x694): 103.384
Elapsed time for attention_linear_projection (4x11104x11104, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11104x11104, b=2048): 236.123
Elapsed time for mlp_h_to_4h (4x11104x44416, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11104x44416, b=2048): 257.735
Elapsed time for mlp_4h_to_h (4x44416x11104, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44416x11104, b=2048): 257.609

Attention duration (in seconds): 0.0386
Attention throughput (in TFLOP/s): 228.394
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 259.662
b: 64, m: 2048, n: 696, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x696x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x696x2048): 136.354
b: 64, m: 2048, n: 2048, k: 696,
Elapsed time for attention_prob_times_values (64x2048x2048x696): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x696): 131.338
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 237.804
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 260.033
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 258.666

Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 236.001
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1003
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11168x33504, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11168x33504, b=2048): 250.660
b: 64, m: 2048, n: 698, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x698x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x698x2048): 134.292
b: 64, m: 2048, n: 2048, k: 698,
Elapsed time for attention_prob_times_values (64x2048x2048x698): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x698): 104.262
Elapsed time for attention_linear_projection (4x11168x11168, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11168x11168, b=2048): 230.285
Elapsed time for mlp_h_to_4h (4x11168x44672, b=2048): 0.0326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11168x44672, b=2048): 250.979
Elapsed time for mlp_4h_to_h (4x44672x11168, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44672x11168, b=2048): 256.799

Attention duration (in seconds): 0.0397
Attention throughput (in TFLOP/s): 224.683
MLP duration (in seconds): 0.0644
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11200x33600, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11200x33600, b=2048): 252.091
b: 64, m: 2048, n: 700, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x700x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x700x2048): 138.536
b: 64, m: 2048, n: 2048, k: 700,
Elapsed time for attention_prob_times_values (64x2048x2048x700): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x700): 120.096
Elapsed time for attention_linear_projection (4x11200x11200, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11200x11200, b=2048): 231.949
Elapsed time for mlp_h_to_4h (4x11200x44800, b=2048): 0.0325
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11200x44800, b=2048): 252.821
Elapsed time for mlp_4h_to_h (4x44800x11200, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44800x11200, b=2048): 258.573

Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 229.120
MLP duration (in seconds): 0.0643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11232x33696, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11232x33696, b=2048): 251.309
b: 64, m: 2048, n: 702, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x702x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x702x2048): 134.912
b: 64, m: 2048, n: 2048, k: 702,
Elapsed time for attention_prob_times_values (64x2048x2048x702): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x702): 104.895
Elapsed time for attention_linear_projection (4x11232x11232, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11232x11232, b=2048): 231.662
Elapsed time for mlp_h_to_4h (4x11232x44928, b=2048): 0.0328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11232x44928, b=2048): 251.938
Elapsed time for mlp_4h_to_h (4x44928x11232, b=2048): 0.0320
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44928x11232, b=2048): 258.221

Attention duration (in seconds): 0.0400
Attention throughput (in TFLOP/s): 225.636
MLP duration (in seconds): 0.0648
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 252.197
b: 64, m: 2048, n: 704, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x704x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x704x2048): 138.524
b: 64, m: 2048, n: 2048, k: 704,
Elapsed time for attention_prob_times_values (64x2048x2048x704): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x704): 134.905
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 233.283
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 254.379
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 257.077

Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 231.586
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11296x33888, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11296x33888, b=2048): 251.381
b: 64, m: 2048, n: 706, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x706x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x706x2048): 136.414
b: 64, m: 2048, n: 2048, k: 706,
Elapsed time for attention_prob_times_values (64x2048x2048x706): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x706): 102.911
Elapsed time for attention_linear_projection (4x11296x11296, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11296x11296, b=2048): 233.399
Elapsed time for mlp_h_to_4h (4x11296x45184, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11296x45184, b=2048): 253.111
Elapsed time for mlp_4h_to_h (4x45184x11296, b=2048): 0.0325
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45184x11296, b=2048): 257.536

Attention duration (in seconds): 0.0404
Attention throughput (in TFLOP/s): 225.932
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11328x33984, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11328x33984, b=2048): 253.177
b: 64, m: 2048, n: 708, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x708x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x708x2048): 140.279
b: 64, m: 2048, n: 2048, k: 708,
Elapsed time for attention_prob_times_values (64x2048x2048x708): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x708): 118.657
Elapsed time for attention_linear_projection (4x11328x11328, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11328x11328, b=2048): 234.555
Elapsed time for mlp_h_to_4h (4x11328x45312, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11328x45312, b=2048): 255.079
Elapsed time for mlp_4h_to_h (4x45312x11328, b=2048): 0.0325
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45312x11328, b=2048): 258.583

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 230.464
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11360x34080, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11360x34080, b=2048): 252.639
b: 64, m: 2048, n: 710, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x710x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x710x2048): 136.917
b: 64, m: 2048, n: 2048, k: 710,
Elapsed time for attention_prob_times_values (64x2048x2048x710): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x710): 102.565
Elapsed time for attention_linear_projection (4x11360x11360, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11360x11360, b=2048): 234.640
Elapsed time for mlp_h_to_4h (4x11360x45440, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11360x45440, b=2048): 253.146
Elapsed time for mlp_4h_to_h (4x45440x11360, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45440x11360, b=2048): 256.801

Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 226.983
MLP duration (in seconds): 0.0663
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 254.055
b: 64, m: 2048, n: 712, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x712x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x712x2048): 139.367
b: 64, m: 2048, n: 2048, k: 712,
Elapsed time for attention_prob_times_values (64x2048x2048x712): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x712): 130.976
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 236.164
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 256.417
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 257.755

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 233.064
MLP duration (in seconds): 0.0662
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11424x34272, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11424x34272, b=2048): 254.164
b: 64, m: 2048, n: 714, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x714x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x714x2048): 137.465
b: 64, m: 2048, n: 2048, k: 714,
Elapsed time for attention_prob_times_values (64x2048x2048x714): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x714): 102.459
Elapsed time for attention_linear_projection (4x11424x11424, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11424x11424, b=2048): 236.416
Elapsed time for mlp_h_to_4h (4x11424x45696, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11424x45696, b=2048): 253.846
Elapsed time for mlp_4h_to_h (4x45696x11424, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45696x11424, b=2048): 258.151

Attention duration (in seconds): 0.0408
Attention throughput (in TFLOP/s): 228.351
MLP duration (in seconds): 0.0668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11456x34368, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11456x34368, b=2048): 255.522
b: 64, m: 2048, n: 716, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x716x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x716x2048): 142.114
b: 64, m: 2048, n: 2048, k: 716,
Elapsed time for attention_prob_times_values (64x2048x2048x716): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x716): 119.624
Elapsed time for attention_linear_projection (4x11456x11456, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11456x11456, b=2048): 237.517
Elapsed time for mlp_h_to_4h (4x11456x45824, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11456x45824, b=2048): 257.933
Elapsed time for mlp_4h_to_h (4x45824x11456, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45824x11456, b=2048): 257.364

Attention duration (in seconds): 0.0402
Attention throughput (in TFLOP/s): 232.983
MLP duration (in seconds): 0.0668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11488x34464, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11488x34464, b=2048): 255.679
b: 64, m: 2048, n: 718, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x718x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x718x2048): 137.999
b: 64, m: 2048, n: 2048, k: 718,
Elapsed time for attention_prob_times_values (64x2048x2048x718): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x718): 103.171
Elapsed time for attention_linear_projection (4x11488x11488, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11488x11488, b=2048): 237.693
Elapsed time for mlp_h_to_4h (4x11488x45952, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11488x45952, b=2048): 255.980
Elapsed time for mlp_4h_to_h (4x45952x11488, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45952x11488, b=2048): 257.291

Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 229.772
MLP duration (in seconds): 0.0674
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 256.864
b: 64, m: 2048, n: 720, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x720x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x720x2048): 141.363
b: 64, m: 2048, n: 2048, k: 720,
Elapsed time for attention_prob_times_values (64x2048x2048x720): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x720): 133.451
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 239.264
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 259.446
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 258.532

Attention duration (in seconds): 0.0401
Attention throughput (in TFLOP/s): 236.092
MLP duration (in seconds): 0.0672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11552x34656, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11552x34656, b=2048): 256.961
b: 64, m: 2048, n: 722, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x722x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x722x2048): 138.685
b: 64, m: 2048, n: 2048, k: 722,
Elapsed time for attention_prob_times_values (64x2048x2048x722): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x722): 102.996
Elapsed time for attention_linear_projection (4x11552x11552, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11552x11552, b=2048): 239.258
Elapsed time for mlp_h_to_4h (4x11552x46208, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11552x46208, b=2048): 256.610
Elapsed time for mlp_4h_to_h (4x46208x11552, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46208x11552, b=2048): 258.625

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 230.961
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11584x34752, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11584x34752, b=2048): 258.201
b: 64, m: 2048, n: 724, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x724x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x724x2048): 142.871
b: 64, m: 2048, n: 2048, k: 724,
Elapsed time for attention_prob_times_values (64x2048x2048x724): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x724): 118.992
Elapsed time for attention_linear_projection (4x11584x11584, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11584x11584, b=2048): 240.479
Elapsed time for mlp_h_to_4h (4x11584x46336, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11584x46336, b=2048): 260.383
Elapsed time for mlp_4h_to_h (4x46336x11584, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46336x11584, b=2048): 257.779

Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 235.324
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11616x34848, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11616x34848, b=2048): 258.489
b: 64, m: 2048, n: 726, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x726x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x726x2048): 139.596
b: 64, m: 2048, n: 2048, k: 726,
Elapsed time for attention_prob_times_values (64x2048x2048x726): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x726): 103.403
Elapsed time for attention_linear_projection (4x11616x11616, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11616x11616, b=2048): 240.861
Elapsed time for mlp_h_to_4h (4x11616x46464, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11616x46464, b=2048): 258.135
Elapsed time for mlp_4h_to_h (4x46464x11616, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46464x11616, b=2048): 258.123

Attention duration (in seconds): 0.0414
Attention throughput (in TFLOP/s): 232.440
MLP duration (in seconds): 0.0685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 259.553
b: 64, m: 2048, n: 728, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x728x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x728x2048): 142.922
b: 64, m: 2048, n: 2048, k: 728,
Elapsed time for attention_prob_times_values (64x2048x2048x728): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x728): 132.695
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 242.064
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 262.283
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 261.779

Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 238.516
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11680x35040, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11680x35040, b=2048): 253.773
b: 64, m: 2048, n: 730, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x730x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x730x2048): 139.661
b: 64, m: 2048, n: 2048, k: 730,
Elapsed time for attention_prob_times_values (64x2048x2048x730): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x730): 103.881
Elapsed time for attention_linear_projection (4x11680x11680, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11680x11680, b=2048): 234.307
Elapsed time for mlp_h_to_4h (4x11680x46720, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11680x46720, b=2048): 254.167
Elapsed time for mlp_4h_to_h (4x46720x11680, b=2048): 0.0344
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46720x11680, b=2048): 259.940

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 228.588
MLP duration (in seconds): 0.0696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11712x35136, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11712x35136, b=2048): 254.781
b: 64, m: 2048, n: 732, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x732x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x732x2048): 143.807
b: 64, m: 2048, n: 2048, k: 732,
Elapsed time for attention_prob_times_values (64x2048x2048x732): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x732): 118.858
Elapsed time for attention_linear_projection (4x11712x11712, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11712x11712, b=2048): 235.499
Elapsed time for mlp_h_to_4h (4x11712x46848, b=2048): 0.0349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11712x46848, b=2048): 257.462
Elapsed time for mlp_4h_to_h (4x46848x11712, b=2048): 0.0345
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46848x11712, b=2048): 260.781

Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 232.503
MLP duration (in seconds): 0.0694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11744x35232, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11744x35232, b=2048): 255.038
b: 64, m: 2048, n: 734, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x734x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x734x2048): 139.432
b: 64, m: 2048, n: 2048, k: 734,
Elapsed time for attention_prob_times_values (64x2048x2048x734): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x734): 104.220
Elapsed time for attention_linear_projection (4x11744x11744, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11744x11744, b=2048): 235.452
Elapsed time for mlp_h_to_4h (4x11744x46976, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11744x46976, b=2048): 255.878
Elapsed time for mlp_4h_to_h (4x46976x11744, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46976x11744, b=2048): 261.193

Attention duration (in seconds): 0.0428
Attention throughput (in TFLOP/s): 229.680
MLP duration (in seconds): 0.0699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 256.044
b: 64, m: 2048, n: 736, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x736x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x736x2048): 144.078
b: 64, m: 2048, n: 2048, k: 736,
Elapsed time for attention_prob_times_values (64x2048x2048x736): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x736): 135.181
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 236.990
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 258.623
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0349
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 260.207

Attention duration (in seconds): 0.0419
Attention throughput (in TFLOP/s): 235.911
MLP duration (in seconds): 0.0701
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11808x35424, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11808x35424, b=2048): 256.290
b: 64, m: 2048, n: 738, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x738x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x738x2048): 141.276
b: 64, m: 2048, n: 2048, k: 738,
Elapsed time for attention_prob_times_values (64x2048x2048x738): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x738): 105.252
Elapsed time for attention_linear_projection (4x11808x11808, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11808x11808, b=2048): 236.931
Elapsed time for mlp_h_to_4h (4x11808x47232, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11808x47232, b=2048): 257.553
Elapsed time for mlp_4h_to_h (4x47232x11808, b=2048): 0.0351
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47232x11808, b=2048): 260.471

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 231.196
MLP duration (in seconds): 0.0706
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11840x35520, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11840x35520, b=2048): 257.630
b: 64, m: 2048, n: 740, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x740x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x740x2048): 145.379
b: 64, m: 2048, n: 2048, k: 740,
Elapsed time for attention_prob_times_values (64x2048x2048x740): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x740): 122.183
Elapsed time for attention_linear_projection (4x11840x11840, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11840x11840, b=2048): 238.175
Elapsed time for mlp_h_to_4h (4x11840x47360, b=2048): 0.0354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11840x47360, b=2048): 259.887
Elapsed time for mlp_4h_to_h (4x47360x11840, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47360x11840, b=2048): 261.346

Attention duration (in seconds): 0.0424
Attention throughput (in TFLOP/s): 235.569
MLP duration (in seconds): 0.0705
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11872x35616, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11872x35616, b=2048): 257.628
b: 64, m: 2048, n: 742, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x742x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x742x2048): 142.551
b: 64, m: 2048, n: 2048, k: 742,
Elapsed time for attention_prob_times_values (64x2048x2048x742): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x742): 106.098
Elapsed time for attention_linear_projection (4x11872x11872, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11872x11872, b=2048): 238.374
Elapsed time for mlp_h_to_4h (4x11872x47488, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11872x47488, b=2048): 258.363
Elapsed time for mlp_4h_to_h (4x47488x11872, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47488x11872, b=2048): 262.029

Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 232.654
MLP duration (in seconds): 0.0710
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 258.813
b: 64, m: 2048, n: 744, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x744x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x744x2048): 145.479
b: 64, m: 2048, n: 2048, k: 744,
Elapsed time for attention_prob_times_values (64x2048x2048x744): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x744): 134.836
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 239.885
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 261.480
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0356
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 260.609

Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 238.442
MLP duration (in seconds): 0.0712
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11936x35808, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11936x35808, b=2048): 258.951
b: 64, m: 2048, n: 746, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x746x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x746x2048): 142.625
b: 64, m: 2048, n: 2048, k: 746,
Elapsed time for attention_prob_times_values (64x2048x2048x746): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x746): 107.099
Elapsed time for attention_linear_projection (4x11936x11936, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11936x11936, b=2048): 239.707
Elapsed time for mlp_h_to_4h (4x11936x47744, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11936x47744, b=2048): 259.118
Elapsed time for mlp_4h_to_h (4x47744x11936, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47744x11936, b=2048): 260.983

Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 233.980
MLP duration (in seconds): 0.0718
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11968x35904, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11968x35904, b=2048): 260.339
b: 64, m: 2048, n: 748, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x748x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x748x2048): 147.581
b: 64, m: 2048, n: 2048, k: 748,
Elapsed time for attention_prob_times_values (64x2048x2048x748): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x748): 123.278
Elapsed time for attention_linear_projection (4x11968x11968, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11968x11968, b=2048): 241.271
Elapsed time for mlp_h_to_4h (4x11968x47872, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11968x47872, b=2048): 262.752
Elapsed time for mlp_4h_to_h (4x47872x11968, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47872x11968, b=2048): 262.016

Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 238.378
MLP duration (in seconds): 0.0716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12000x36000, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12000x36000, b=2048): 260.454
b: 64, m: 2048, n: 750, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x750x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x750x2048): 143.512
b: 64, m: 2048, n: 2048, k: 750,
Elapsed time for attention_prob_times_values (64x2048x2048x750): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x750): 107.522
Elapsed time for attention_linear_projection (4x12000x12000, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12000x12000, b=2048): 241.291
Elapsed time for mlp_h_to_4h (4x12000x48000, b=2048): 0.0361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12000x48000, b=2048): 261.282
Elapsed time for mlp_4h_to_h (4x48000x12000, b=2048): 0.0362
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48000x12000, b=2048): 260.574

Attention duration (in seconds): 0.0435
Attention throughput (in TFLOP/s): 235.440
MLP duration (in seconds): 0.0723
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 261.838
b: 64, m: 2048, n: 752, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x752x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x752x2048): 147.133
b: 64, m: 2048, n: 2048, k: 752,
Elapsed time for attention_prob_times_values (64x2048x2048x752): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x752): 137.425
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 242.734
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0359
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 264.200
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0363
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 261.176

Attention duration (in seconds): 0.0426
Attention throughput (in TFLOP/s): 241.502
MLP duration (in seconds): 0.0722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12064x36192, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12064x36192, b=2048): 253.344
b: 64, m: 2048, n: 754, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x754x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x754x2048): 144.216
b: 64, m: 2048, n: 2048, k: 754,
Elapsed time for attention_prob_times_values (64x2048x2048x754): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x754): 108.760
Elapsed time for attention_linear_projection (4x12064x12064, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x12064x12064, b=2048): 235.300
Elapsed time for mlp_h_to_4h (4x12064x48256, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12064x48256, b=2048): 255.012
Elapsed time for mlp_4h_to_h (4x48256x12064, b=2048): 0.0364
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48256x12064, b=2048): 261.725

Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 230.464
MLP duration (in seconds): 0.0738
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12096x36288, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12096x36288, b=2048): 254.617
b: 64, m: 2048, n: 756, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x756x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x756x2048): 149.081
b: 64, m: 2048, n: 2048, k: 756,
Elapsed time for attention_prob_times_values (64x2048x2048x756): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x756): 123.530
Elapsed time for attention_linear_projection (4x12096x12096, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x12096x12096, b=2048): 237.069
Elapsed time for mlp_h_to_4h (4x12096x48384, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12096x48384, b=2048): 257.196
Elapsed time for mlp_4h_to_h (4x48384x12096, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48384x12096, b=2048): 262.710

Attention duration (in seconds): 0.0444
Attention throughput (in TFLOP/s): 234.433
MLP duration (in seconds): 0.0738
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12128x36384, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12128x36384, b=2048): 254.289
b: 64, m: 2048, n: 758, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x758x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x758x2048): 144.895
b: 64, m: 2048, n: 2048, k: 758,
Elapsed time for attention_prob_times_values (64x2048x2048x758): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x758): 108.801
Elapsed time for attention_linear_projection (4x12128x12128, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12128x12128, b=2048): 236.645
Elapsed time for mlp_h_to_4h (4x12128x48512, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12128x48512, b=2048): 256.217
Elapsed time for mlp_4h_to_h (4x48512x12128, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48512x12128, b=2048): 260.503

Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 231.459
MLP duration (in seconds): 0.0746
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 255.927
b: 64, m: 2048, n: 760, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x760x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x760x2048): 148.312
b: 64, m: 2048, n: 2048, k: 760,
Elapsed time for attention_prob_times_values (64x2048x2048x760): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x760): 137.404
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 238.409
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 258.521
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 261.862

Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 237.273
MLP duration (in seconds): 0.0745
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12192x36576, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12192x36576, b=2048): 255.638
b: 64, m: 2048, n: 762, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x762x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x762x2048): 145.647
b: 64, m: 2048, n: 2048, k: 762,
Elapsed time for attention_prob_times_values (64x2048x2048x762): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x762): 109.802
Elapsed time for attention_linear_projection (4x12192x12192, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12192x12192, b=2048): 238.024
Elapsed time for mlp_h_to_4h (4x12192x48768, b=2048): 0.0381
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12192x48768, b=2048): 255.678
Elapsed time for mlp_4h_to_h (4x48768x12192, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48768x12192, b=2048): 261.951

Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 232.869
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12224x36672, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12224x36672, b=2048): 257.332
b: 64, m: 2048, n: 764, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x764x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x764x2048): 150.738
b: 64, m: 2048, n: 2048, k: 764,
Elapsed time for attention_prob_times_values (64x2048x2048x764): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x764): 125.887
Elapsed time for attention_linear_projection (4x12224x12224, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12224x12224, b=2048): 239.621
Elapsed time for mlp_h_to_4h (4x12224x48896, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12224x48896, b=2048): 259.304
Elapsed time for mlp_4h_to_h (4x48896x12224, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48896x12224, b=2048): 261.099

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 237.231
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12256x36768, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12256x36768, b=2048): 255.632
b: 64, m: 2048, n: 766, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x766x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x766x2048): 145.954
b: 64, m: 2048, n: 2048, k: 766,
Elapsed time for attention_prob_times_values (64x2048x2048x766): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x766): 110.647
Elapsed time for attention_linear_projection (4x12256x12256, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12256x12256, b=2048): 239.580
Elapsed time for mlp_h_to_4h (4x12256x49024, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12256x49024, b=2048): 256.707
Elapsed time for mlp_4h_to_h (4x49024x12256, b=2048): 0.0377
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49024x12256, b=2048): 261.396

Attention duration (in seconds): 0.0457
Attention throughput (in TFLOP/s): 233.465
MLP duration (in seconds): 0.0760
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 258.436
b: 64, m: 2048, n: 768, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x768x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x768x2048): 150.801
b: 64, m: 2048, n: 2048, k: 768,
Elapsed time for attention_prob_times_values (64x2048x2048x768): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x768): 140.326
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 241.012
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 261.236
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0377
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 262.230

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 240.069
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12320x36960, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12320x36960, b=2048): 256.940
b: 64, m: 2048, n: 770, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x770x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x770x2048): 134.838
b: 64, m: 2048, n: 2048, k: 770,
Elapsed time for attention_prob_times_values (64x2048x2048x770): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x770): 107.315
Elapsed time for attention_linear_projection (4x12320x12320, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12320x12320, b=2048): 240.681
Elapsed time for mlp_h_to_4h (4x12320x49280, b=2048): 0.0386
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12320x49280, b=2048): 257.870
Elapsed time for mlp_4h_to_h (4x49280x12320, b=2048): 0.0381
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49280x12320, b=2048): 260.897

Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 232.771
MLP duration (in seconds): 0.0767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12352x37056, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12352x37056, b=2048): 259.661
b: 64, m: 2048, n: 772, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x772x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x772x2048): 138.882
b: 64, m: 2048, n: 2048, k: 772,
Elapsed time for attention_prob_times_values (64x2048x2048x772): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x772): 123.238
Elapsed time for attention_linear_projection (4x12352x12352, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12352x12352, b=2048): 242.353
Elapsed time for mlp_h_to_4h (4x12352x49408, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12352x49408, b=2048): 259.724
Elapsed time for mlp_4h_to_h (4x49408x12352, b=2048): 0.0382
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49408x12352, b=2048): 261.427

Attention duration (in seconds): 0.0455
Attention throughput (in TFLOP/s): 237.752
MLP duration (in seconds): 0.0767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12384x37152, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12384x37152, b=2048): 256.823
b: 64, m: 2048, n: 774, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x774x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x774x2048): 135.118
b: 64, m: 2048, n: 2048, k: 774,
Elapsed time for attention_prob_times_values (64x2048x2048x774): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x774): 107.169
Elapsed time for attention_linear_projection (4x12384x12384, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12384x12384, b=2048): 242.071
Elapsed time for mlp_h_to_4h (4x12384x49536, b=2048): 0.0387
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12384x49536, b=2048): 259.377
Elapsed time for mlp_4h_to_h (4x49536x12384, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49536x12384, b=2048): 261.793

Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 233.096
MLP duration (in seconds): 0.0771
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 261.430
b: 64, m: 2048, n: 776, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x776x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x776x2048): 137.525
b: 64, m: 2048, n: 2048, k: 776,
Elapsed time for attention_prob_times_values (64x2048x2048x776): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x776): 136.154
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 243.705
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0387
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 261.148
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 262.927

Attention duration (in seconds): 0.0454
Attention throughput (in TFLOP/s): 240.690
MLP duration (in seconds): 0.0771
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12448x37344, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12448x37344, b=2048): 252.516
b: 64, m: 2048, n: 778, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x778x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x778x2048): 135.827
b: 64, m: 2048, n: 2048, k: 778,
Elapsed time for attention_prob_times_values (64x2048x2048x778): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x778): 108.155
Elapsed time for attention_linear_projection (4x12448x12448, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x12448x12448, b=2048): 236.812
Elapsed time for mlp_h_to_4h (4x12448x49792, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12448x49792, b=2048): 253.266
Elapsed time for mlp_4h_to_h (4x49792x12448, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49792x12448, b=2048): 261.299

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 229.833
MLP duration (in seconds): 0.0790
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12480x37440, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12480x37440, b=2048): 253.796
b: 64, m: 2048, n: 780, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x780x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x780x2048): 140.053
b: 64, m: 2048, n: 2048, k: 780,
Elapsed time for attention_prob_times_values (64x2048x2048x780): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x780): 124.022
Elapsed time for attention_linear_projection (4x12480x12480, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x12480x12480, b=2048): 238.371
Elapsed time for mlp_h_to_4h (4x12480x49920, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12480x49920, b=2048): 254.757
Elapsed time for mlp_4h_to_h (4x49920x12480, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49920x12480, b=2048): 261.748

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 233.824
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12512x37536, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12512x37536, b=2048): 252.260
b: 64, m: 2048, n: 782, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x782x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x782x2048): 136.125
b: 64, m: 2048, n: 2048, k: 782,
Elapsed time for attention_prob_times_values (64x2048x2048x782): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x782): 108.297
Elapsed time for attention_linear_projection (4x12512x12512, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12512x12512, b=2048): 238.300
Elapsed time for mlp_h_to_4h (4x12512x50048, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12512x50048, b=2048): 253.472
Elapsed time for mlp_4h_to_h (4x50048x12512, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50048x12512, b=2048): 259.942

Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 230.145
MLP duration (in seconds): 0.0799
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 256.157
b: 64, m: 2048, n: 784, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x784x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x784x2048): 139.195
b: 64, m: 2048, n: 2048, k: 784,
Elapsed time for attention_prob_times_values (64x2048x2048x784): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x784): 138.312
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 239.664
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0403
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 255.710
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 259.236

Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 237.233
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12576x37728, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12576x37728, b=2048): 252.317
b: 64, m: 2048, n: 786, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x786x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x786x2048): 137.012
b: 64, m: 2048, n: 2048, k: 786,
Elapsed time for attention_prob_times_values (64x2048x2048x786): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x786): 107.744
Elapsed time for attention_linear_projection (4x12576x12576, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12576x12576, b=2048): 239.113
Elapsed time for mlp_h_to_4h (4x12576x50304, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12576x50304, b=2048): 254.679
Elapsed time for mlp_4h_to_h (4x50304x12576, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50304x12576, b=2048): 259.096

Attention duration (in seconds): 0.0486
Attention throughput (in TFLOP/s): 230.434
MLP duration (in seconds): 0.0807
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12608x37824, b=2048): 0.0306
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12608x37824, b=2048): 254.974
b: 64, m: 2048, n: 788, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x788x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x788x2048): 141.456
b: 64, m: 2048, n: 2048, k: 788,
Elapsed time for attention_prob_times_values (64x2048x2048x788): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x788): 123.679
Elapsed time for attention_linear_projection (4x12608x12608, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12608x12608, b=2048): 240.596
Elapsed time for mlp_h_to_4h (4x12608x50432, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12608x50432, b=2048): 257.373
Elapsed time for mlp_4h_to_h (4x50432x12608, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50432x12608, b=2048): 260.011

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 235.253
MLP duration (in seconds): 0.0805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12640x37920, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12640x37920, b=2048): 253.173
b: 64, m: 2048, n: 790, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x790x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x790x2048): 137.666
b: 64, m: 2048, n: 2048, k: 790,
Elapsed time for attention_prob_times_values (64x2048x2048x790): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x790): 108.636
Elapsed time for attention_linear_projection (4x12640x12640, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12640x12640, b=2048): 240.637
Elapsed time for mlp_h_to_4h (4x12640x50560, b=2048): 0.0410
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12640x50560, b=2048): 255.216
Elapsed time for mlp_4h_to_h (4x50560x12640, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50560x12640, b=2048): 259.335

Attention duration (in seconds): 0.0489
Attention throughput (in TFLOP/s): 231.559
MLP duration (in seconds): 0.0814
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 256.283
b: 64, m: 2048, n: 792, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x792x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x792x2048): 140.217
b: 64, m: 2048, n: 2048, k: 792,
Elapsed time for attention_prob_times_values (64x2048x2048x792): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x792): 138.154
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 241.978
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 259.730
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 260.823

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 238.052
MLP duration (in seconds): 0.0809
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12704x38112, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12704x38112, b=2048): 252.925
b: 64, m: 2048, n: 794, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x794x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x794x2048): 137.766
b: 64, m: 2048, n: 2048, k: 794,
Elapsed time for attention_prob_times_values (64x2048x2048x794): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x794): 108.544
Elapsed time for attention_linear_projection (4x12704x12704, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12704x12704, b=2048): 241.808
Elapsed time for mlp_h_to_4h (4x12704x50816, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12704x50816, b=2048): 256.451
Elapsed time for mlp_4h_to_h (4x50816x12704, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50816x12704, b=2048): 259.851

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 231.739
MLP duration (in seconds): 0.0819
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12736x38208, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12736x38208, b=2048): 257.346
b: 64, m: 2048, n: 796, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x796x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x796x2048): 142.835
b: 64, m: 2048, n: 2048, k: 796,
Elapsed time for attention_prob_times_values (64x2048x2048x796): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x796): 126.201
Elapsed time for attention_linear_projection (4x12736x12736, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12736x12736, b=2048): 243.505
Elapsed time for mlp_h_to_4h (4x12736x50944, b=2048): 0.0411
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12736x50944, b=2048): 258.786
Elapsed time for mlp_4h_to_h (4x50944x12736, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50944x12736, b=2048): 260.801

Attention duration (in seconds): 0.0483
Attention throughput (in TFLOP/s): 237.920
MLP duration (in seconds): 0.0818
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12768x38304, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12768x38304, b=2048): 254.079
b: 64, m: 2048, n: 798, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x798x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x798x2048): 138.749
b: 64, m: 2048, n: 2048, k: 798,
Elapsed time for attention_prob_times_values (64x2048x2048x798): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x798): 108.925
Elapsed time for attention_linear_projection (4x12768x12768, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12768x12768, b=2048): 243.242
Elapsed time for mlp_h_to_4h (4x12768x51072, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12768x51072, b=2048): 257.139
Elapsed time for mlp_4h_to_h (4x51072x12768, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51072x12768, b=2048): 259.260

Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 232.964
MLP duration (in seconds): 0.0828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 258.540
b: 64, m: 2048, n: 800, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x800x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x800x2048): 142.069
b: 64, m: 2048, n: 2048, k: 800,
Elapsed time for attention_prob_times_values (64x2048x2048x800): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x800): 141.478
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 245.017
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 260.642
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 260.583

Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 240.775
MLP duration (in seconds): 0.0824
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12832x38496, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12832x38496, b=2048): 255.071
b: 64, m: 2048, n: 802, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x802x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x802x2048): 139.531
b: 64, m: 2048, n: 2048, k: 802,
Elapsed time for attention_prob_times_values (64x2048x2048x802): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x802): 109.471
Elapsed time for attention_linear_projection (4x12832x12832, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12832x12832, b=2048): 244.694
Elapsed time for mlp_h_to_4h (4x12832x51328, b=2048): 0.0418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12832x51328, b=2048): 258.264
Elapsed time for mlp_4h_to_h (4x51328x12832, b=2048): 0.0414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51328x12832, b=2048): 260.567

Attention duration (in seconds): 0.0498
Attention throughput (in TFLOP/s): 234.104
MLP duration (in seconds): 0.0832
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12864x38592, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12864x38592, b=2048): 259.604
b: 64, m: 2048, n: 804, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x804x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x804x2048): 144.443
b: 64, m: 2048, n: 2048, k: 804,
Elapsed time for attention_prob_times_values (64x2048x2048x804): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x804): 127.147
Elapsed time for attention_linear_projection (4x12864x12864, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12864x12864, b=2048): 246.017
Elapsed time for mlp_h_to_4h (4x12864x51456, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12864x51456, b=2048): 261.535
Elapsed time for mlp_4h_to_h (4x51456x12864, b=2048): 0.0417
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51456x12864, b=2048): 259.928

Attention duration (in seconds): 0.0487
Attention throughput (in TFLOP/s): 240.243
MLP duration (in seconds): 0.0832
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12896x38688, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12896x38688, b=2048): 256.300
b: 64, m: 2048, n: 806, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x806x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x806x2048): 140.183
b: 64, m: 2048, n: 2048, k: 806,
Elapsed time for attention_prob_times_values (64x2048x2048x806): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x806): 109.784
Elapsed time for attention_linear_projection (4x12896x12896, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x12896x12896, b=2048): 246.140
Elapsed time for mlp_h_to_4h (4x12896x51584, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12896x51584, b=2048): 259.268
Elapsed time for mlp_4h_to_h (4x51584x12896, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51584x12896, b=2048): 259.599

Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 235.329
MLP duration (in seconds): 0.0840
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 260.907
b: 64, m: 2048, n: 808, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x808x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x808x2048): 142.938
b: 64, m: 2048, n: 2048, k: 808,
Elapsed time for attention_prob_times_values (64x2048x2048x808): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x808): 140.542
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 247.326
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0419
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 261.629
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 260.815

Attention duration (in seconds): 0.0487
Attention throughput (in TFLOP/s): 242.832
MLP duration (in seconds): 0.0839
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12960x38880, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12960x38880, b=2048): 250.559
b: 64, m: 2048, n: 810, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x810x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x810x2048): 140.933
b: 64, m: 2048, n: 2048, k: 810,
Elapsed time for attention_prob_times_values (64x2048x2048x810): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x810): 111.088
Elapsed time for attention_linear_projection (4x12960x12960, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x12960x12960, b=2048): 240.378
Elapsed time for mlp_h_to_4h (4x12960x51840, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12960x51840, b=2048): 253.380
Elapsed time for mlp_4h_to_h (4x51840x12960, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51840x12960, b=2048): 261.057

Attention duration (in seconds): 0.0514
Attention throughput (in TFLOP/s): 231.087
MLP duration (in seconds): 0.0856
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12992x38976, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12992x38976, b=2048): 254.823
b: 64, m: 2048, n: 812, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x812x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x812x2048): 146.183
b: 64, m: 2048, n: 2048, k: 812,
Elapsed time for attention_prob_times_values (64x2048x2048x812): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x812): 129.101
Elapsed time for attention_linear_projection (4x12992x12992, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x12992x12992, b=2048): 242.091
Elapsed time for mlp_h_to_4h (4x12992x51968, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12992x51968, b=2048): 256.765
Elapsed time for mlp_4h_to_h (4x51968x12992, b=2048): 0.0426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51968x12992, b=2048): 259.882

Attention duration (in seconds): 0.0503
Attention throughput (in TFLOP/s): 237.065
MLP duration (in seconds): 0.0856
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13024x39072, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13024x39072, b=2048): 251.456
b: 64, m: 2048, n: 814, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x814x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x814x2048): 142.123
b: 64, m: 2048, n: 2048, k: 814,
Elapsed time for attention_prob_times_values (64x2048x2048x814): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x814): 111.923
Elapsed time for attention_linear_projection (4x13024x13024, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13024x13024, b=2048): 241.781
Elapsed time for mlp_h_to_4h (4x13024x52096, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13024x52096, b=2048): 254.465
Elapsed time for mlp_4h_to_h (4x52096x13024, b=2048): 0.0427
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52096x13024, b=2048): 260.309

Attention duration (in seconds): 0.0516
Attention throughput (in TFLOP/s): 232.239
MLP duration (in seconds): 0.0864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1380
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 255.942
b: 64, m: 2048, n: 816, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x816x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x816x2048): 145.750
b: 64, m: 2048, n: 2048, k: 816,
Elapsed time for attention_prob_times_values (64x2048x2048x816): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x816): 143.946
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 243.339
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.939
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 261.991

Attention duration (in seconds): 0.0503
Attention throughput (in TFLOP/s): 239.693
MLP duration (in seconds): 0.0859
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13088x39264, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13088x39264, b=2048): 254.782
b: 64, m: 2048, n: 818, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x818x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x818x2048): 142.899
b: 64, m: 2048, n: 2048, k: 818,
Elapsed time for attention_prob_times_values (64x2048x2048x818): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x818): 112.199
Elapsed time for attention_linear_projection (4x13088x13088, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13088x13088, b=2048): 242.929
Elapsed time for mlp_h_to_4h (4x13088x52352, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13088x52352, b=2048): 258.217
Elapsed time for mlp_4h_to_h (4x52352x13088, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52352x13088, b=2048): 262.186

Attention duration (in seconds): 0.0516
Attention throughput (in TFLOP/s): 234.644
MLP duration (in seconds): 0.0863
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13120x39360, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13120x39360, b=2048): 259.339
b: 64, m: 2048, n: 820, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x820x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x820x2048): 147.825
b: 64, m: 2048, n: 2048, k: 820,
Elapsed time for attention_prob_times_values (64x2048x2048x820): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x820): 130.641
Elapsed time for attention_linear_projection (4x13120x13120, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13120x13120, b=2048): 244.209
Elapsed time for mlp_h_to_4h (4x13120x52480, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13120x52480, b=2048): 260.075
Elapsed time for mlp_4h_to_h (4x52480x13120, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52480x13120, b=2048): 263.055

Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 240.723
MLP duration (in seconds): 0.0863
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13152x39456, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13152x39456, b=2048): 255.576
b: 64, m: 2048, n: 822, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x822x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x822x2048): 143.576
b: 64, m: 2048, n: 2048, k: 822,
Elapsed time for attention_prob_times_values (64x2048x2048x822): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x822): 112.330
Elapsed time for attention_linear_projection (4x13152x13152, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13152x13152, b=2048): 244.167
Elapsed time for mlp_h_to_4h (4x13152x52608, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13152x52608, b=2048): 258.809
Elapsed time for mlp_4h_to_h (4x52608x13152, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52608x13152, b=2048): 261.497

Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 235.539
MLP duration (in seconds): 0.0872
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1390
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 260.042
b: 64, m: 2048, n: 824, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x824x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x824x2048): 146.737
b: 64, m: 2048, n: 2048, k: 824,
Elapsed time for attention_prob_times_values (64x2048x2048x824): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x824): 143.115
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 245.427
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 261.101
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 262.896

Attention duration (in seconds): 0.0506
Attention throughput (in TFLOP/s): 242.784
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13216x39648, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13216x39648, b=2048): 256.565
b: 64, m: 2048, n: 826, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x826x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x826x2048): 144.185
b: 64, m: 2048, n: 2048, k: 826,
Elapsed time for attention_prob_times_values (64x2048x2048x826): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x826): 112.973
Elapsed time for attention_linear_projection (4x13216x13216, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13216x13216, b=2048): 245.094
Elapsed time for mlp_h_to_4h (4x13216x52864, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13216x52864, b=2048): 259.975
Elapsed time for mlp_4h_to_h (4x52864x13216, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52864x13216, b=2048): 260.944

Attention duration (in seconds): 0.0521
Attention throughput (in TFLOP/s): 236.556
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1400
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13248x39744, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13248x39744, b=2048): 260.764
b: 64, m: 2048, n: 828, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x828x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x828x2048): 149.040
b: 64, m: 2048, n: 2048, k: 828,
Elapsed time for attention_prob_times_values (64x2048x2048x828): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x828): 130.750
Elapsed time for attention_linear_projection (4x13248x13248, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13248x13248, b=2048): 246.705
Elapsed time for mlp_h_to_4h (4x13248x52992, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13248x52992, b=2048): 261.791
Elapsed time for mlp_4h_to_h (4x52992x13248, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52992x13248, b=2048): 263.350

Attention duration (in seconds): 0.0511
Attention throughput (in TFLOP/s): 242.393
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13280x39840, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13280x39840, b=2048): 258.192
b: 64, m: 2048, n: 830, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x830x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x830x2048): 144.805
b: 64, m: 2048, n: 2048, k: 830,
Elapsed time for attention_prob_times_values (64x2048x2048x830): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x830): 113.156
Elapsed time for attention_linear_projection (4x13280x13280, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13280x13280, b=2048): 246.707
Elapsed time for mlp_h_to_4h (4x13280x53120, b=2048): 0.0443
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13280x53120, b=2048): 260.810
Elapsed time for mlp_4h_to_h (4x53120x13280, b=2048): 0.0442
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53120x13280, b=2048): 261.468

Attention duration (in seconds): 0.0523
Attention throughput (in TFLOP/s): 238.028
MLP duration (in seconds): 0.0885
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 260.978
b: 64, m: 2048, n: 832, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x832x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x832x2048): 148.032
b: 64, m: 2048, n: 2048, k: 832,
Elapsed time for attention_prob_times_values (64x2048x2048x832): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x832): 146.116
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 247.898
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 262.468
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 260.394

Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 244.459
MLP duration (in seconds): 0.0888
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1400
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13344x40032, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13344x40032, b=2048): 250.480
b: 64, m: 2048, n: 834, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x834x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x834x2048): 144.673
b: 64, m: 2048, n: 2048, k: 834,
Elapsed time for attention_prob_times_values (64x2048x2048x834): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x834): 111.739
Elapsed time for attention_linear_projection (4x13344x13344, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13344x13344, b=2048): 241.629
Elapsed time for mlp_h_to_4h (4x13344x53376, b=2048): 0.0459
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13344x53376, b=2048): 254.393
Elapsed time for mlp_4h_to_h (4x53376x13344, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53376x13344, b=2048): 260.475

Attention duration (in seconds): 0.0541
Attention throughput (in TFLOP/s): 232.181
MLP duration (in seconds): 0.0907
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13376x40128, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13376x40128, b=2048): 254.907
b: 64, m: 2048, n: 836, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x836x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x836x2048): 149.809
b: 64, m: 2048, n: 2048, k: 836,
Elapsed time for attention_prob_times_values (64x2048x2048x836): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x836): 127.861
Elapsed time for attention_linear_projection (4x13376x13376, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13376x13376, b=2048): 242.694
Elapsed time for mlp_h_to_4h (4x13376x53504, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13376x53504, b=2048): 256.439
Elapsed time for mlp_4h_to_h (4x53504x13376, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53504x13376, b=2048): 261.677

Attention duration (in seconds): 0.0531
Attention throughput (in TFLOP/s): 237.796
MLP duration (in seconds): 0.0905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1436
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13408x40224, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13408x40224, b=2048): 251.654
b: 64, m: 2048, n: 838, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x838x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x838x2048): 146.065
b: 64, m: 2048, n: 2048, k: 838,
Elapsed time for attention_prob_times_values (64x2048x2048x838): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x838): 111.210
Elapsed time for attention_linear_projection (4x13408x13408, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13408x13408, b=2048): 242.668
Elapsed time for mlp_h_to_4h (4x13408x53632, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13408x53632, b=2048): 254.827
Elapsed time for mlp_4h_to_h (4x53632x13408, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53632x13408, b=2048): 259.699

Attention duration (in seconds): 0.0544
Attention throughput (in TFLOP/s): 233.218
MLP duration (in seconds): 0.0916
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0347
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 255.765
b: 64, m: 2048, n: 840, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x840x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x840x2048): 148.435
b: 64, m: 2048, n: 2048, k: 840,
Elapsed time for attention_prob_times_values (64x2048x2048x840): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x840): 141.591
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 244.173
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 257.179
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 260.722

Attention duration (in seconds): 0.0531
Attention throughput (in TFLOP/s): 240.117
MLP duration (in seconds): 0.0914
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1445
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13472x40416, b=2048): 0.0354
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13472x40416, b=2048): 252.144
b: 64, m: 2048, n: 842, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x842x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x842x2048): 146.196
b: 64, m: 2048, n: 2048, k: 842,
Elapsed time for attention_prob_times_values (64x2048x2048x842): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x842): 111.491
Elapsed time for attention_linear_projection (4x13472x13472, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13472x13472, b=2048): 243.846
Elapsed time for mlp_h_to_4h (4x13472x53888, b=2048): 0.0467
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13472x53888, b=2048): 254.824
Elapsed time for mlp_4h_to_h (4x53888x13472, b=2048): 0.0459
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53888x13472, b=2048): 258.910

Attention duration (in seconds): 0.0547
Attention throughput (in TFLOP/s): 233.887
MLP duration (in seconds): 0.0926
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1473
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13504x40512, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13504x40512, b=2048): 255.965
b: 64, m: 2048, n: 844, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x844x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x844x2048): 151.039
b: 64, m: 2048, n: 2048, k: 844,
Elapsed time for attention_prob_times_values (64x2048x2048x844): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x844): 128.718
Elapsed time for attention_linear_projection (4x13504x13504, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13504x13504, b=2048): 245.221
Elapsed time for mlp_h_to_4h (4x13504x54016, b=2048): 0.0463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13504x54016, b=2048): 258.130
Elapsed time for mlp_4h_to_h (4x54016x13504, b=2048): 0.0459
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54016x13504, b=2048): 260.435

Attention duration (in seconds): 0.0537
Attention throughput (in TFLOP/s): 239.331
MLP duration (in seconds): 0.0922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1459
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13536x40608, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13536x40608, b=2048): 253.276
b: 64, m: 2048, n: 846, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x846x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x846x2048): 146.529
b: 64, m: 2048, n: 2048, k: 846,
Elapsed time for attention_prob_times_values (64x2048x2048x846): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x846): 111.902
Elapsed time for attention_linear_projection (4x13536x13536, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13536x13536, b=2048): 244.671
Elapsed time for mlp_h_to_4h (4x13536x54144, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13536x54144, b=2048): 256.187
Elapsed time for mlp_4h_to_h (4x54144x13536, b=2048): 0.0464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54144x13536, b=2048): 258.792

Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 234.902
MLP duration (in seconds): 0.0933
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1483
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 258.093
b: 64, m: 2048, n: 848, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x848x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x848x2048): 150.689
b: 64, m: 2048, n: 2048, k: 848,
Elapsed time for attention_prob_times_values (64x2048x2048x848): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x848): 143.974
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 246.365
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 259.252
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0461
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 261.579

Attention duration (in seconds): 0.0535
Attention throughput (in TFLOP/s): 242.595
MLP duration (in seconds): 0.0927
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13600x40800, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13600x40800, b=2048): 254.300
b: 64, m: 2048, n: 850, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x850x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x850x2048): 147.426
b: 64, m: 2048, n: 2048, k: 850,
Elapsed time for attention_prob_times_values (64x2048x2048x850): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x850): 111.736
Elapsed time for attention_linear_projection (4x13600x13600, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13600x13600, b=2048): 245.948
Elapsed time for mlp_h_to_4h (4x13600x54400, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13600x54400, b=2048): 257.596
Elapsed time for mlp_4h_to_h (4x54400x13600, b=2048): 0.0463
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54400x13600, b=2048): 261.875

Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 235.912
MLP duration (in seconds): 0.0933
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1486
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13632x40896, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13632x40896, b=2048): 260.003
b: 64, m: 2048, n: 852, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x852x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x852x2048): 151.999
b: 64, m: 2048, n: 2048, k: 852,
Elapsed time for attention_prob_times_values (64x2048x2048x852): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x852): 128.116
Elapsed time for attention_linear_projection (4x13632x13632, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13632x13632, b=2048): 247.461
Elapsed time for mlp_h_to_4h (4x13632x54528, b=2048): 0.0466
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13632x54528, b=2048): 261.448
Elapsed time for mlp_4h_to_h (4x54528x13632, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54528x13632, b=2048): 263.396

Attention duration (in seconds): 0.0540
Attention throughput (in TFLOP/s): 242.411
MLP duration (in seconds): 0.0928
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1468
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13664x40992, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13664x40992, b=2048): 257.763
b: 64, m: 2048, n: 854, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x854x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x854x2048): 147.721
b: 64, m: 2048, n: 2048, k: 854,
Elapsed time for attention_prob_times_values (64x2048x2048x854): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x854): 111.030
Elapsed time for attention_linear_projection (4x13664x13664, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13664x13664, b=2048): 247.230
Elapsed time for mlp_h_to_4h (4x13664x54656, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13664x54656, b=2048): 260.937
Elapsed time for mlp_4h_to_h (4x54656x13664, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54656x13664, b=2048): 263.294

Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 238.241
MLP duration (in seconds): 0.0934
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1486
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 262.922
b: 64, m: 2048, n: 856, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x856x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x856x2048): 151.251
b: 64, m: 2048, n: 2048, k: 856,
Elapsed time for attention_prob_times_values (64x2048x2048x856): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x856): 143.954
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 248.562
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 262.692
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 264.496

Attention duration (in seconds): 0.0537
Attention throughput (in TFLOP/s): 246.213
MLP duration (in seconds): 0.0933
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13728x41184, b=2048): 0.0360
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13728x41184, b=2048): 257.656
b: 64, m: 2048, n: 858, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x858x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x858x2048): 148.573
b: 64, m: 2048, n: 2048, k: 858,
Elapsed time for attention_prob_times_values (64x2048x2048x858): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x858): 112.985
Elapsed time for attention_linear_projection (4x13728x13728, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13728x13728, b=2048): 248.503
Elapsed time for mlp_h_to_4h (4x13728x54912, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13728x54912, b=2048): 261.218
Elapsed time for mlp_4h_to_h (4x54912x13728, b=2048): 0.0470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54912x13728, b=2048): 262.616

Attention duration (in seconds): 0.0556
Attention throughput (in TFLOP/s): 238.904
MLP duration (in seconds): 0.0943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1499
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13760x41280, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13760x41280, b=2048): 261.049
b: 64, m: 2048, n: 860, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x860x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x860x2048): 154.073
b: 64, m: 2048, n: 2048, k: 860,
Elapsed time for attention_prob_times_values (64x2048x2048x860): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x860): 130.355
Elapsed time for attention_linear_projection (4x13760x13760, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13760x13760, b=2048): 249.682
Elapsed time for mlp_h_to_4h (4x13760x55040, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13760x55040, b=2048): 262.534
Elapsed time for mlp_4h_to_h (4x55040x13760, b=2048): 0.0470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55040x13760, b=2048): 263.862

Attention duration (in seconds): 0.0546
Attention throughput (in TFLOP/s): 244.117
MLP duration (in seconds): 0.0943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1489
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13792x41376, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13792x41376, b=2048): 258.419
b: 64, m: 2048, n: 862, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x862x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x862x2048): 149.094
b: 64, m: 2048, n: 2048, k: 862,
Elapsed time for attention_prob_times_values (64x2048x2048x862): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x862): 113.492
Elapsed time for attention_linear_projection (4x13792x13792, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x13792x13792, b=2048): 249.447
Elapsed time for mlp_h_to_4h (4x13792x55168, b=2048): 0.0475
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13792x55168, b=2048): 262.466
Elapsed time for mlp_4h_to_h (4x55168x13792, b=2048): 0.0477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55168x13792, b=2048): 261.544

Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 239.756
MLP duration (in seconds): 0.0952
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1510
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 262.028
b: 64, m: 2048, n: 864, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x864x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x864x2048): 153.217
b: 64, m: 2048, n: 2048, k: 864,
Elapsed time for attention_prob_times_values (64x2048x2048x864): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x864): 146.569
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 250.644
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0475
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 263.476
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 262.766

Attention duration (in seconds): 0.0545
Attention throughput (in TFLOP/s): 246.679
MLP duration (in seconds): 0.0952
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1497
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13856x41568, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13856x41568, b=2048): 251.995
b: 64, m: 2048, n: 866, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x866x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x866x2048): 150.039
b: 64, m: 2048, n: 2048, k: 866,
Elapsed time for attention_prob_times_values (64x2048x2048x866): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x866): 113.296
Elapsed time for attention_linear_projection (4x13856x13856, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13856x13856, b=2048): 243.987
Elapsed time for mlp_h_to_4h (4x13856x55424, b=2048): 0.0492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13856x55424, b=2048): 255.515
Elapsed time for mlp_4h_to_h (4x55424x13856, b=2048): 0.0483
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55424x13856, b=2048): 260.478

Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 234.819
MLP duration (in seconds): 0.0975
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13888x41664, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13888x41664, b=2048): 256.227
b: 64, m: 2048, n: 868, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x868x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x868x2048): 154.829
b: 64, m: 2048, n: 2048, k: 868,
Elapsed time for attention_prob_times_values (64x2048x2048x868): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x868): 131.549
Elapsed time for attention_linear_projection (4x13888x13888, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13888x13888, b=2048): 245.438
Elapsed time for mlp_h_to_4h (4x13888x55552, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13888x55552, b=2048): 257.642
Elapsed time for mlp_4h_to_h (4x55552x13888, b=2048): 0.0483
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55552x13888, b=2048): 261.595

Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 240.530
MLP duration (in seconds): 0.0974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1538
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13920x41760, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13920x41760, b=2048): 252.926
b: 64, m: 2048, n: 870, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x870x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x870x2048): 150.189
b: 64, m: 2048, n: 2048, k: 870,
Elapsed time for attention_prob_times_values (64x2048x2048x870): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x870): 113.734
Elapsed time for attention_linear_projection (4x13920x13920, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13920x13920, b=2048): 245.515
Elapsed time for mlp_h_to_4h (4x13920x55680, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13920x55680, b=2048): 255.798
Elapsed time for mlp_4h_to_h (4x55680x13920, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55680x13920, b=2048): 261.582

Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 235.851
MLP duration (in seconds): 0.0982
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1560
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 256.621
b: 64, m: 2048, n: 872, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x872x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x872x2048): 153.764
b: 64, m: 2048, n: 2048, k: 872,
Elapsed time for attention_prob_times_values (64x2048x2048x872): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x872): 145.471
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 246.614
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 259.262
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 260.970

Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 242.452
MLP duration (in seconds): 0.0981
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1546
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13984x41952, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13984x41952, b=2048): 253.916
b: 64, m: 2048, n: 874, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x874x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x874x2048): 151.030
b: 64, m: 2048, n: 2048, k: 874,
Elapsed time for attention_prob_times_values (64x2048x2048x874): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x874): 115.240
Elapsed time for attention_linear_projection (4x13984x13984, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x13984x13984, b=2048): 246.655
Elapsed time for mlp_h_to_4h (4x13984x55936, b=2048): 0.0499
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13984x55936, b=2048): 257.071
Elapsed time for mlp_4h_to_h (4x55936x13984, b=2048): 0.0490
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55936x13984, b=2048): 261.386

Attention duration (in seconds): 0.0580
Attention throughput (in TFLOP/s): 237.050
MLP duration (in seconds): 0.0989
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1569
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14016x42048, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14016x42048, b=2048): 259.100
b: 64, m: 2048, n: 876, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x876x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x876x2048): 156.678
b: 64, m: 2048, n: 2048, k: 876,
Elapsed time for attention_prob_times_values (64x2048x2048x876): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x876): 132.139
Elapsed time for attention_linear_projection (4x14016x14016, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x14016x14016, b=2048): 248.254
Elapsed time for mlp_h_to_4h (4x14016x56064, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14016x56064, b=2048): 260.065
Elapsed time for mlp_4h_to_h (4x56064x14016, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56064x14016, b=2048): 262.359

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 243.255
MLP duration (in seconds): 0.0986
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1554
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14048x42144, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14048x42144, b=2048): 255.284
b: 64, m: 2048, n: 878, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x878x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x878x2048): 151.745
b: 64, m: 2048, n: 2048, k: 878,
Elapsed time for attention_prob_times_values (64x2048x2048x878): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x878): 115.100
Elapsed time for attention_linear_projection (4x14048x14048, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x14048x14048, b=2048): 247.858
Elapsed time for mlp_h_to_4h (4x14048x56192, b=2048): 0.0501
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14048x56192, b=2048): 257.982
Elapsed time for mlp_4h_to_h (4x56192x14048, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56192x14048, b=2048): 260.567

Attention duration (in seconds): 0.0582
Attention throughput (in TFLOP/s): 238.242
MLP duration (in seconds): 0.0998
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1580
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0375
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 260.058
b: 64, m: 2048, n: 880, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x880x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x880x2048): 156.252
b: 64, m: 2048, n: 2048, k: 880,
Elapsed time for attention_prob_times_values (64x2048x2048x880): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x880): 148.545
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 249.152
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 261.732
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0497
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 261.585

Attention duration (in seconds): 0.0567
Attention throughput (in TFLOP/s): 245.762
MLP duration (in seconds): 0.0993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1560
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14112x42336, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14112x42336, b=2048): 255.162
b: 64, m: 2048, n: 882, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x882x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x882x2048): 152.670
b: 64, m: 2048, n: 2048, k: 882,
Elapsed time for attention_prob_times_values (64x2048x2048x882): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x882): 116.939
Elapsed time for attention_linear_projection (4x14112x14112, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14112x14112, b=2048): 247.852
Elapsed time for mlp_h_to_4h (4x14112x56448, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14112x56448, b=2048): 258.818
Elapsed time for mlp_4h_to_h (4x56448x14112, b=2048): 0.0503
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56448x14112, b=2048): 259.570

Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 238.566
MLP duration (in seconds): 0.1007
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14144x42432, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14144x42432, b=2048): 259.843
b: 64, m: 2048, n: 884, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x884x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x884x2048): 158.109
b: 64, m: 2048, n: 2048, k: 884,
Elapsed time for attention_prob_times_values (64x2048x2048x884): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x884): 134.372
Elapsed time for attention_linear_projection (4x14144x14144, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14144x14144, b=2048): 250.096
Elapsed time for mlp_h_to_4h (4x14144x56576, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14144x56576, b=2048): 260.266
Elapsed time for mlp_4h_to_h (4x56576x14144, b=2048): 0.0501
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56576x14144, b=2048): 261.890

Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 244.599
MLP duration (in seconds): 0.1004
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1579
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14176x42528, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14176x42528, b=2048): 254.530
b: 64, m: 2048, n: 886, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x886x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x886x2048): 153.162
b: 64, m: 2048, n: 2048, k: 886,
Elapsed time for attention_prob_times_values (64x2048x2048x886): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x886): 117.851
Elapsed time for attention_linear_projection (4x14176x14176, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x14176x14176, b=2048): 248.293
Elapsed time for mlp_h_to_4h (4x14176x56704, b=2048): 0.0510
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14176x56704, b=2048): 258.424
Elapsed time for mlp_4h_to_h (4x56704x14176, b=2048): 0.0509
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56704x14176, b=2048): 258.855

Attention duration (in seconds): 0.0592
Attention throughput (in TFLOP/s): 238.499
MLP duration (in seconds): 0.1018
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1611
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 259.734
b: 64, m: 2048, n: 888, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x888x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x888x2048): 156.930
b: 64, m: 2048, n: 2048, k: 888,
Elapsed time for attention_prob_times_values (64x2048x2048x888): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x888): 147.724
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 249.833
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 261.491
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 262.092

Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 245.786
MLP duration (in seconds): 0.1011
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1588
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14240x42720, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14240x42720, b=2048): 249.373
b: 64, m: 2048, n: 890, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x890x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x890x2048): 153.748
b: 64, m: 2048, n: 2048, k: 890,
Elapsed time for attention_prob_times_values (64x2048x2048x890): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x890): 118.285
Elapsed time for attention_linear_projection (4x14240x14240, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14240x14240, b=2048): 243.270
Elapsed time for mlp_h_to_4h (4x14240x56960, b=2048): 0.0523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14240x56960, b=2048): 254.321
Elapsed time for mlp_4h_to_h (4x56960x14240, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56960x14240, b=2048): 259.933

Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 234.398
MLP duration (in seconds): 0.1034
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14272x42816, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14272x42816, b=2048): 254.335
b: 64, m: 2048, n: 892, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x892x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x892x2048): 159.337
b: 64, m: 2048, n: 2048, k: 892,
Elapsed time for attention_prob_times_values (64x2048x2048x892): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x892): 136.223
Elapsed time for attention_linear_projection (4x14272x14272, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_linear_projection (4x14272x14272, b=2048): 246.452
Elapsed time for mlp_h_to_4h (4x14272x57088, b=2048): 0.0521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14272x57088, b=2048): 256.380
Elapsed time for mlp_4h_to_h (4x57088x14272, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57088x14272, b=2048): 261.414

Attention duration (in seconds): 0.0594
Attention throughput (in TFLOP/s): 240.747
MLP duration (in seconds): 0.1031
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14304x42912, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14304x42912, b=2048): 250.682
b: 64, m: 2048, n: 894, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x894x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x894x2048): 154.664
b: 64, m: 2048, n: 2048, k: 894,
Elapsed time for attention_prob_times_values (64x2048x2048x894): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x894): 119.935
Elapsed time for attention_linear_projection (4x14304x14304, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14304x14304, b=2048): 244.800
Elapsed time for mlp_h_to_4h (4x14304x57216, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14304x57216, b=2048): 255.195
Elapsed time for mlp_4h_to_h (4x57216x14304, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57216x14304, b=2048): 259.456

Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 235.879
MLP duration (in seconds): 0.1042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1651
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 256.485
b: 64, m: 2048, n: 896, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x896x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x896x2048): 159.432
b: 64, m: 2048, n: 2048, k: 896,
Elapsed time for attention_prob_times_values (64x2048x2048x896): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x896): 151.598
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 247.751
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 258.819
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 262.620

Attention duration (in seconds): 0.0592
Attention throughput (in TFLOP/s): 243.905
MLP duration (in seconds): 0.1033
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14368x43104, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14368x43104, b=2048): 254.118
b: 64, m: 2048, n: 898, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x898x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x898x2048): 144.996
b: 64, m: 2048, n: 2048, k: 898,
Elapsed time for attention_prob_times_values (64x2048x2048x898): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x898): 116.670
Elapsed time for attention_linear_projection (4x14368x14368, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14368x14368, b=2048): 247.417
Elapsed time for mlp_h_to_4h (4x14368x57472, b=2048): 0.0524
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14368x57472, b=2048): 258.329
Elapsed time for mlp_4h_to_h (4x57472x14368, b=2048): 0.0516
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57472x14368, b=2048): 262.275

Attention duration (in seconds): 0.0611
Attention throughput (in TFLOP/s): 237.373
MLP duration (in seconds): 0.1040
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1650
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14400x43200, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14400x43200, b=2048): 258.668
b: 64, m: 2048, n: 900, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x900x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x900x2048): 150.064
b: 64, m: 2048, n: 2048, k: 900,
Elapsed time for attention_prob_times_values (64x2048x2048x900): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x900): 133.823
Elapsed time for attention_linear_projection (4x14400x14400, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14400x14400, b=2048): 248.625
Elapsed time for mlp_h_to_4h (4x14400x57600, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14400x57600, b=2048): 261.119
Elapsed time for mlp_4h_to_h (4x57600x14400, b=2048): 0.0514
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57600x14400, b=2048): 264.472

Attention duration (in seconds): 0.0599
Attention throughput (in TFLOP/s): 243.013
MLP duration (in seconds): 0.1034
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1633
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14432x43296, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14432x43296, b=2048): 256.995
b: 64, m: 2048, n: 902, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x902x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x902x2048): 145.862
b: 64, m: 2048, n: 2048, k: 902,
Elapsed time for attention_prob_times_values (64x2048x2048x902): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x902): 116.470
Elapsed time for attention_linear_projection (4x14432x14432, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14432x14432, b=2048): 248.456
Elapsed time for mlp_h_to_4h (4x14432x57728, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14432x57728, b=2048): 259.848
Elapsed time for mlp_4h_to_h (4x57728x14432, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57728x14432, b=2048): 264.873

Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 239.459
MLP duration (in seconds): 0.1041
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1651
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 261.950
b: 64, m: 2048, n: 904, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x904x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x904x2048): 148.510
b: 64, m: 2048, n: 2048, k: 904,
Elapsed time for attention_prob_times_values (64x2048x2048x904): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x904): 146.796
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 249.637
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 261.932
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0516
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 265.608

Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 246.495
MLP duration (in seconds): 0.1040
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14496x43488, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14496x43488, b=2048): 255.802
b: 64, m: 2048, n: 906, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x906x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x906x2048): 146.152
b: 64, m: 2048, n: 2048, k: 906,
Elapsed time for attention_prob_times_values (64x2048x2048x906): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x906): 115.778
Elapsed time for attention_linear_projection (4x14496x14496, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14496x14496, b=2048): 249.491
Elapsed time for mlp_h_to_4h (4x14496x57984, b=2048): 0.0529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14496x57984, b=2048): 260.513
Elapsed time for mlp_4h_to_h (4x57984x14496, b=2048): 0.0527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57984x14496, b=2048): 261.548

Attention duration (in seconds): 0.0617
Attention throughput (in TFLOP/s): 238.944
MLP duration (in seconds): 0.1055
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1672
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14528x43584, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14528x43584, b=2048): 261.118
b: 64, m: 2048, n: 908, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x908x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x908x2048): 151.197
b: 64, m: 2048, n: 2048, k: 908,
Elapsed time for attention_prob_times_values (64x2048x2048x908): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x908): 133.767
Elapsed time for attention_linear_projection (4x14528x14528, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14528x14528, b=2048): 250.759
Elapsed time for mlp_h_to_4h (4x14528x58112, b=2048): 0.0527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14528x58112, b=2048): 262.347
Elapsed time for mlp_4h_to_h (4x58112x14528, b=2048): 0.0527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58112x14528, b=2048): 262.651

Attention duration (in seconds): 0.0604
Attention throughput (in TFLOP/s): 245.198
MLP duration (in seconds): 0.1054
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14560x43680, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14560x43680, b=2048): 257.466
b: 64, m: 2048, n: 910, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x910x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x910x2048): 146.398
b: 64, m: 2048, n: 2048, k: 910,
Elapsed time for attention_prob_times_values (64x2048x2048x910): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x910): 116.191
Elapsed time for attention_linear_projection (4x14560x14560, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14560x14560, b=2048): 250.759
Elapsed time for mlp_h_to_4h (4x14560x58240, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14560x58240, b=2048): 259.718
Elapsed time for mlp_4h_to_h (4x58240x14560, b=2048): 0.0530
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58240x14560, b=2048): 262.084

Attention duration (in seconds): 0.0619
Attention throughput (in TFLOP/s): 240.371
MLP duration (in seconds): 0.1065
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1684
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 261.320
b: 64, m: 2048, n: 912, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x912x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x912x2048): 150.274
b: 64, m: 2048, n: 2048, k: 912,
Elapsed time for attention_prob_times_values (64x2048x2048x912): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x912): 149.323
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 251.901
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 261.876
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 261.667

Attention duration (in seconds): 0.0604
Attention throughput (in TFLOP/s): 247.099
MLP duration (in seconds): 0.1066
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14624x43872, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14624x43872, b=2048): 252.099
b: 64, m: 2048, n: 914, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x914x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x914x2048): 147.273
b: 64, m: 2048, n: 2048, k: 914,
Elapsed time for attention_prob_times_values (64x2048x2048x914): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x914): 115.386
Elapsed time for attention_linear_projection (4x14624x14624, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14624x14624, b=2048): 243.492
Elapsed time for mlp_h_to_4h (4x14624x58496, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14624x58496, b=2048): 254.165
Elapsed time for mlp_4h_to_h (4x58496x14624, b=2048): 0.0538
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58496x14624, b=2048): 260.536

Attention duration (in seconds): 0.0637
Attention throughput (in TFLOP/s): 235.537
MLP duration (in seconds): 0.1089
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1726
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14656x43968, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14656x43968, b=2048): 256.737
b: 64, m: 2048, n: 916, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x916x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x916x2048): 152.450
b: 64, m: 2048, n: 2048, k: 916,
Elapsed time for attention_prob_times_values (64x2048x2048x916): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x916): 133.626
Elapsed time for attention_linear_projection (4x14656x14656, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x14656x14656, b=2048): 247.093
Elapsed time for mlp_h_to_4h (4x14656x58624, b=2048): 0.0548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14656x58624, b=2048): 256.717
Elapsed time for mlp_4h_to_h (4x58624x14656, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58624x14656, b=2048): 262.778

Attention duration (in seconds): 0.0623
Attention throughput (in TFLOP/s): 241.853
MLP duration (in seconds): 0.1084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1707
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14688x44064, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14688x44064, b=2048): 252.967
b: 64, m: 2048, n: 918, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x918x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x918x2048): 147.475
b: 64, m: 2048, n: 2048, k: 918,
Elapsed time for attention_prob_times_values (64x2048x2048x918): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x918): 116.263
Elapsed time for attention_linear_projection (4x14688x14688, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14688x14688, b=2048): 245.778
Elapsed time for mlp_h_to_4h (4x14688x58752, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14688x58752, b=2048): 254.894
Elapsed time for mlp_4h_to_h (4x58752x14688, b=2048): 0.0542
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58752x14688, b=2048): 260.755

Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 236.758
MLP duration (in seconds): 0.1097
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1736
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0415
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 256.927
b: 64, m: 2048, n: 920, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x920x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x920x2048): 150.995
b: 64, m: 2048, n: 2048, k: 920,
Elapsed time for attention_prob_times_values (64x2048x2048x920): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x920): 148.166
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 248.312
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 257.829
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0542
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 261.966

Attention duration (in seconds): 0.0624
Attention throughput (in TFLOP/s): 243.580
MLP duration (in seconds): 0.1093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1716
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14752x44256, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14752x44256, b=2048): 252.634
b: 64, m: 2048, n: 922, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x922x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x922x2048): 148.382
b: 64, m: 2048, n: 2048, k: 922,
Elapsed time for attention_prob_times_values (64x2048x2048x922): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x922): 115.048
Elapsed time for attention_linear_projection (4x14752x14752, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_linear_projection (4x14752x14752, b=2048): 245.956
Elapsed time for mlp_h_to_4h (4x14752x59008, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14752x59008, b=2048): 255.232
Elapsed time for mlp_4h_to_h (4x59008x14752, b=2048): 0.0549
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59008x14752, b=2048): 259.824

Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 236.557
MLP duration (in seconds): 0.1108
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1752
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14784x44352, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14784x44352, b=2048): 257.104
b: 64, m: 2048, n: 924, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x924x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x924x2048): 153.441
b: 64, m: 2048, n: 2048, k: 924,
Elapsed time for attention_prob_times_values (64x2048x2048x924): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x924): 133.470
Elapsed time for attention_linear_projection (4x14784x14784, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14784x14784, b=2048): 248.960
Elapsed time for mlp_h_to_4h (4x14784x59136, b=2048): 0.0556
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14784x59136, b=2048): 257.840
Elapsed time for mlp_4h_to_h (4x59136x14784, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59136x14784, b=2048): 262.922

Attention duration (in seconds): 0.0631
Attention throughput (in TFLOP/s): 242.659
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1732
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14816x44448, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14816x44448, b=2048): 253.716
b: 64, m: 2048, n: 926, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x926x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x926x2048): 148.379
b: 64, m: 2048, n: 2048, k: 926,
Elapsed time for attention_prob_times_values (64x2048x2048x926): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x926): 116.451
Elapsed time for attention_linear_projection (4x14816x14816, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x14816x14816, b=2048): 246.703
Elapsed time for mlp_h_to_4h (4x14816x59264, b=2048): 0.0560
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14816x59264, b=2048): 256.925
Elapsed time for mlp_4h_to_h (4x59264x14816, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59264x14816, b=2048): 259.977

Attention duration (in seconds): 0.0647
Attention throughput (in TFLOP/s): 237.630
MLP duration (in seconds): 0.1113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 258.336
b: 64, m: 2048, n: 928, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x928x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x928x2048): 152.821
b: 64, m: 2048, n: 2048, k: 928,
Elapsed time for attention_prob_times_values (64x2048x2048x928): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x928): 151.250
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 250.279
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0558
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 259.036
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0552
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 261.862

Attention duration (in seconds): 0.0629
Attention throughput (in TFLOP/s): 245.417
MLP duration (in seconds): 0.1110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1739
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14880x44640, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14880x44640, b=2048): 254.318
b: 64, m: 2048, n: 930, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x930x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x930x2048): 149.595
b: 64, m: 2048, n: 2048, k: 930,
Elapsed time for attention_prob_times_values (64x2048x2048x930): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x930): 116.311
Elapsed time for attention_linear_projection (4x14880x14880, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x14880x14880, b=2048): 247.833
Elapsed time for mlp_h_to_4h (4x14880x59520, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14880x59520, b=2048): 257.532
Elapsed time for mlp_4h_to_h (4x59520x14880, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59520x14880, b=2048): 260.531

Attention duration (in seconds): 0.0651
Attention throughput (in TFLOP/s): 238.381
MLP duration (in seconds): 0.1120
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1771
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14912x44736, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14912x44736, b=2048): 259.275
b: 64, m: 2048, n: 932, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x932x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x932x2048): 154.133
b: 64, m: 2048, n: 2048, k: 932,
Elapsed time for attention_prob_times_values (64x2048x2048x932): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x932): 135.111
Elapsed time for attention_linear_projection (4x14912x14912, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_linear_projection (4x14912x14912, b=2048): 251.023
Elapsed time for mlp_h_to_4h (4x14912x59648, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14912x59648, b=2048): 259.989
Elapsed time for mlp_4h_to_h (4x59648x14912, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59648x14912, b=2048): 261.729

Attention duration (in seconds): 0.0636
Attention throughput (in TFLOP/s): 244.800
MLP duration (in seconds): 0.1117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1754
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14944x44832, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14944x44832, b=2048): 254.515
b: 64, m: 2048, n: 934, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x934x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x934x2048): 149.970
b: 64, m: 2048, n: 2048, k: 934,
Elapsed time for attention_prob_times_values (64x2048x2048x934): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x934): 117.065
Elapsed time for attention_linear_projection (4x14944x14944, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x14944x14944, b=2048): 248.641
Elapsed time for mlp_h_to_4h (4x14944x59776, b=2048): 0.0567
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14944x59776, b=2048): 258.349
Elapsed time for mlp_4h_to_h (4x59776x14944, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59776x14944, b=2048): 260.390

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 238.863
MLP duration (in seconds): 0.1129
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 260.648
b: 64, m: 2048, n: 936, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x936x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x936x2048): 153.052
b: 64, m: 2048, n: 2048, k: 936,
Elapsed time for attention_prob_times_values (64x2048x2048x936): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x936): 150.334
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 252.539
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0560
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 262.459
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 264.716

Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 247.413
MLP duration (in seconds): 0.1115
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1750
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15008x45024, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15008x45024, b=2048): 257.228
b: 64, m: 2048, n: 938, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x938x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x938x2048): 150.763
b: 64, m: 2048, n: 2048, k: 938,
Elapsed time for attention_prob_times_values (64x2048x2048x938): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x938): 117.540
Elapsed time for attention_linear_projection (4x15008x15008, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x15008x15008, b=2048): 252.198
Elapsed time for mlp_h_to_4h (4x15008x60032, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15008x60032, b=2048): 261.595
Elapsed time for mlp_4h_to_h (4x60032x15008, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60032x15008, b=2048): 265.146

Attention duration (in seconds): 0.0653
Attention throughput (in TFLOP/s): 241.489
MLP duration (in seconds): 0.1121
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1774
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15040x45120, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15040x45120, b=2048): 262.621
b: 64, m: 2048, n: 940, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x940x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x940x2048): 155.593
b: 64, m: 2048, n: 2048, k: 940,
Elapsed time for attention_prob_times_values (64x2048x2048x940): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x940): 135.183
Elapsed time for attention_linear_projection (4x15040x15040, b=2048): 0.0146
Throughput (in TFLOP/s) for attention_linear_projection (4x15040x15040, b=2048): 253.514
Elapsed time for mlp_h_to_4h (4x15040x60160, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15040x60160, b=2048): 263.673
Elapsed time for mlp_4h_to_h (4x60160x15040, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60160x15040, b=2048): 265.031

Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 247.667
MLP duration (in seconds): 0.1122
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15072x45216, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15072x45216, b=2048): 259.050
b: 64, m: 2048, n: 942, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x942x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x942x2048): 151.319
b: 64, m: 2048, n: 2048, k: 942,
Elapsed time for attention_prob_times_values (64x2048x2048x942): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x942): 117.251
Elapsed time for attention_linear_projection (4x15072x15072, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15072x15072, b=2048): 253.297
Elapsed time for mlp_h_to_4h (4x15072x60288, b=2048): 0.0567
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15072x60288, b=2048): 262.527
Elapsed time for mlp_4h_to_h (4x60288x15072, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60288x15072, b=2048): 264.150

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 242.913
MLP duration (in seconds): 0.1131
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 263.335
b: 64, m: 2048, n: 944, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x944x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x944x2048): 155.421
b: 64, m: 2048, n: 2048, k: 944,
Elapsed time for attention_prob_times_values (64x2048x2048x944): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x944): 153.380
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 254.621
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0566
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 263.953
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 264.906

Attention duration (in seconds): 0.0638
Attention throughput (in TFLOP/s): 250.125
MLP duration (in seconds): 0.1131
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1769
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15136x45408, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15136x45408, b=2048): 254.240
b: 64, m: 2048, n: 946, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x946x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x946x2048): 151.778
b: 64, m: 2048, n: 2048, k: 946,
Elapsed time for attention_prob_times_values (64x2048x2048x946): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x946): 117.880
Elapsed time for attention_linear_projection (4x15136x15136, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15136x15136, b=2048): 247.754
Elapsed time for mlp_h_to_4h (4x15136x60544, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15136x60544, b=2048): 257.722
Elapsed time for mlp_4h_to_h (4x60544x15136, b=2048): 0.0568
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60544x15136, b=2048): 264.358

Attention duration (in seconds): 0.0671
Attention throughput (in TFLOP/s): 238.909
MLP duration (in seconds): 0.1151
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1821
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15168x45504, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15168x45504, b=2048): 258.828
b: 64, m: 2048, n: 948, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x948x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x948x2048): 157.206
b: 64, m: 2048, n: 2048, k: 948,
Elapsed time for attention_prob_times_values (64x2048x2048x948): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x948): 136.211
Elapsed time for attention_linear_projection (4x15168x15168, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x15168x15168, b=2048): 249.191
Elapsed time for mlp_h_to_4h (4x15168x60672, b=2048): 0.0578
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15168x60672, b=2048): 260.899
Elapsed time for mlp_4h_to_h (4x60672x15168, b=2048): 0.0568
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60672x15168, b=2048): 265.380

Attention duration (in seconds): 0.0658
Attention throughput (in TFLOP/s): 244.648
MLP duration (in seconds): 0.1146
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1804
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15200x45600, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15200x45600, b=2048): 255.923
b: 64, m: 2048, n: 950, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x950x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x950x2048): 151.868
b: 64, m: 2048, n: 2048, k: 950,
Elapsed time for attention_prob_times_values (64x2048x2048x950): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x950): 117.159
Elapsed time for attention_linear_projection (4x15200x15200, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15200x15200, b=2048): 249.113
Elapsed time for mlp_h_to_4h (4x15200x60800, b=2048): 0.0585
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15200x60800, b=2048): 258.688
Elapsed time for mlp_4h_to_h (4x60800x15200, b=2048): 0.0576
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60800x15200, b=2048): 262.795

Attention duration (in seconds): 0.0673
Attention throughput (in TFLOP/s): 240.212
MLP duration (in seconds): 0.1161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1834
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0438
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 260.140
b: 64, m: 2048, n: 952, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x952x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x952x2048): 156.179
b: 64, m: 2048, n: 2048, k: 952,
Elapsed time for attention_prob_times_values (64x2048x2048x952): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x952): 152.328
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 250.336
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 261.035
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0573
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 265.466

Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 247.180
MLP duration (in seconds): 0.1155
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15264x45792, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15264x45792, b=2048): 256.930
b: 64, m: 2048, n: 954, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x954x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x954x2048): 152.172
b: 64, m: 2048, n: 2048, k: 954,
Elapsed time for attention_prob_times_values (64x2048x2048x954): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x954): 117.691
Elapsed time for attention_linear_projection (4x15264x15264, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x15264x15264, b=2048): 250.057
Elapsed time for mlp_h_to_4h (4x15264x61056, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15264x61056, b=2048): 259.647
Elapsed time for mlp_4h_to_h (4x61056x15264, b=2048): 0.0580
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61056x15264, b=2048): 263.201

Attention duration (in seconds): 0.0676
Attention throughput (in TFLOP/s): 241.188
MLP duration (in seconds): 0.1168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1844
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15296x45888, b=2048): 0.0441
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15296x45888, b=2048): 260.955
b: 64, m: 2048, n: 956, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x956x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x956x2048): 158.381
b: 64, m: 2048, n: 2048, k: 956,
Elapsed time for attention_prob_times_values (64x2048x2048x956): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x956): 137.247
Elapsed time for attention_linear_projection (4x15296x15296, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15296x15296, b=2048): 251.453
Elapsed time for mlp_h_to_4h (4x15296x61184, b=2048): 0.0586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15296x61184, b=2048): 261.650
Elapsed time for mlp_4h_to_h (4x61184x15296, b=2048): 0.0577
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61184x15296, b=2048): 265.940

Attention duration (in seconds): 0.0663
Attention throughput (in TFLOP/s): 246.778
MLP duration (in seconds): 0.1163
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1826
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15328x45984, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15328x45984, b=2048): 257.742
b: 64, m: 2048, n: 958, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x958x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x958x2048): 152.616
b: 64, m: 2048, n: 2048, k: 958,
Elapsed time for attention_prob_times_values (64x2048x2048x958): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x958): 116.897
Elapsed time for attention_linear_projection (4x15328x15328, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x15328x15328, b=2048): 251.084
Elapsed time for mlp_h_to_4h (4x15328x61312, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15328x61312, b=2048): 260.480
Elapsed time for mlp_4h_to_h (4x61312x15328, b=2048): 0.0580
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61312x15328, b=2048): 265.675

Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 241.896
MLP duration (in seconds): 0.1171
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1850
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 262.302
b: 64, m: 2048, n: 960, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x960x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x960x2048): 157.721
b: 64, m: 2048, n: 2048, k: 960,
Elapsed time for attention_prob_times_values (64x2048x2048x960): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x960): 156.025
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 252.570
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0590
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 262.194
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 263.886

Attention duration (in seconds): 0.0661
Attention throughput (in TFLOP/s): 249.565
MLP duration (in seconds): 0.1176
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1837
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15392x46176, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15392x46176, b=2048): 256.975
b: 64, m: 2048, n: 962, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x962x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x962x2048): 153.394
b: 64, m: 2048, n: 2048, k: 962,
Elapsed time for attention_prob_times_values (64x2048x2048x962): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x962): 115.385
Elapsed time for attention_linear_projection (4x15392x15392, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15392x15392, b=2048): 252.252
Elapsed time for mlp_h_to_4h (4x15392x61568, b=2048): 0.0599
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15392x61568, b=2048): 259.276
Elapsed time for mlp_4h_to_h (4x61568x15392, b=2048): 0.0592
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61568x15392, b=2048): 262.405

Attention duration (in seconds): 0.0685
Attention throughput (in TFLOP/s): 241.581
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15424x46272, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15424x46272, b=2048): 261.465
b: 64, m: 2048, n: 964, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x964x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x964x2048): 159.683
b: 64, m: 2048, n: 2048, k: 964,
Elapsed time for attention_prob_times_values (64x2048x2048x964): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x964): 135.578
Elapsed time for attention_linear_projection (4x15424x15424, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15424x15424, b=2048): 253.459
Elapsed time for mlp_h_to_4h (4x15424x61696, b=2048): 0.0596
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15424x61696, b=2048): 261.603
Elapsed time for mlp_4h_to_h (4x61696x15424, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61696x15424, b=2048): 263.778

Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 247.564
MLP duration (in seconds): 0.1187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1859
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15456x46368, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15456x46368, b=2048): 257.771
b: 64, m: 2048, n: 966, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x966x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x966x2048): 154.491
b: 64, m: 2048, n: 2048, k: 966,
Elapsed time for attention_prob_times_values (64x2048x2048x966): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x966): 115.834
Elapsed time for attention_linear_projection (4x15456x15456, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15456x15456, b=2048): 253.200
Elapsed time for mlp_h_to_4h (4x15456x61824, b=2048): 0.0600
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15456x61824, b=2048): 260.731
Elapsed time for mlp_4h_to_h (4x61824x15456, b=2048): 0.0596
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61824x15456, b=2048): 262.525

Attention duration (in seconds): 0.0688
Attention throughput (in TFLOP/s): 242.478
MLP duration (in seconds): 0.1197
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 263.173
b: 64, m: 2048, n: 968, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x968x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x968x2048): 158.228
b: 64, m: 2048, n: 2048, k: 968,
Elapsed time for attention_prob_times_values (64x2048x2048x968): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x968): 150.931
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 254.670
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 264.743
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 265.813

Attention duration (in seconds): 0.0670
Attention throughput (in TFLOP/s): 250.294
MLP duration (in seconds): 0.1185
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1855
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15520x46560, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15520x46560, b=2048): 254.824
b: 64, m: 2048, n: 970, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x970x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x970x2048): 155.087
b: 64, m: 2048, n: 2048, k: 970,
Elapsed time for attention_prob_times_values (64x2048x2048x970): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x970): 115.758
Elapsed time for attention_linear_projection (4x15520x15520, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15520x15520, b=2048): 248.805
Elapsed time for mlp_h_to_4h (4x15520x62080, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15520x62080, b=2048): 257.975
Elapsed time for mlp_4h_to_h (4x62080x15520, b=2048): 0.0595
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62080x15520, b=2048): 265.331

Attention duration (in seconds): 0.0702
Attention throughput (in TFLOP/s): 239.776
MLP duration (in seconds): 0.1207
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1909
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15552x46656, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15552x46656, b=2048): 260.046
b: 64, m: 2048, n: 972, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x972x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x972x2048): 160.878
b: 64, m: 2048, n: 2048, k: 972,
Elapsed time for attention_prob_times_values (64x2048x2048x972): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x972): 136.176
Elapsed time for attention_linear_projection (4x15552x15552, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15552x15552, b=2048): 249.843
Elapsed time for mlp_h_to_4h (4x15552x62208, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15552x62208, b=2048): 260.944
Elapsed time for mlp_4h_to_h (4x62208x15552, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62208x15552, b=2048): 266.902

Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 246.089
MLP duration (in seconds): 0.1201
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1888
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15584x46752, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15584x46752, b=2048): 255.931
b: 64, m: 2048, n: 974, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x974x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x974x2048): 156.126
b: 64, m: 2048, n: 2048, k: 974,
Elapsed time for attention_prob_times_values (64x2048x2048x974): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x974): 117.349
Elapsed time for attention_linear_projection (4x15584x15584, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15584x15584, b=2048): 249.198
Elapsed time for mlp_h_to_4h (4x15584x62336, b=2048): 0.0613
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15584x62336, b=2048): 259.493
Elapsed time for mlp_4h_to_h (4x62336x15584, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62336x15584, b=2048): 262.738

Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 240.887
MLP duration (in seconds): 0.1219
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1923
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 260.031
b: 64, m: 2048, n: 976, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x976x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x976x2048): 160.796
b: 64, m: 2048, n: 2048, k: 976,
Elapsed time for attention_prob_times_values (64x2048x2048x976): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x976): 153.786
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 251.435
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0609
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 262.239
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 263.487

Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 248.058
MLP duration (in seconds): 0.1216
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1902
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15648x46944, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15648x46944, b=2048): 254.872
b: 64, m: 2048, n: 978, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x978x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x978x2048): 156.622
b: 64, m: 2048, n: 2048, k: 978,
Elapsed time for attention_prob_times_values (64x2048x2048x978): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x978): 118.350
Elapsed time for attention_linear_projection (4x15648x15648, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15648x15648, b=2048): 250.957
Elapsed time for mlp_h_to_4h (4x15648x62592, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15648x62592, b=2048): 258.278
Elapsed time for mlp_4h_to_h (4x62592x15648, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62592x15648, b=2048): 263.111

Attention duration (in seconds): 0.0710
Attention throughput (in TFLOP/s): 240.820
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1941
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15680x47040, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15680x47040, b=2048): 258.687
b: 64, m: 2048, n: 980, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x980x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x980x2048): 163.186
b: 64, m: 2048, n: 2048, k: 980,
Elapsed time for attention_prob_times_values (64x2048x2048x980): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x980): 137.536
Elapsed time for attention_linear_projection (4x15680x15680, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15680x15680, b=2048): 252.523
Elapsed time for mlp_h_to_4h (4x15680x62720, b=2048): 0.0619
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15680x62720, b=2048): 260.272
Elapsed time for mlp_4h_to_h (4x62720x15680, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62720x15680, b=2048): 264.075

Attention duration (in seconds): 0.0697
Attention throughput (in TFLOP/s): 246.212
MLP duration (in seconds): 0.1229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1926
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15712x47136, b=2048): 0.0475
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15712x47136, b=2048): 255.275
b: 64, m: 2048, n: 982, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x982x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x982x2048): 157.835
b: 64, m: 2048, n: 2048, k: 982,
Elapsed time for attention_prob_times_values (64x2048x2048x982): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x982): 119.045
Elapsed time for attention_linear_projection (4x15712x15712, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15712x15712, b=2048): 252.280
Elapsed time for mlp_h_to_4h (4x15712x62848, b=2048): 0.0623
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15712x62848, b=2048): 259.666
Elapsed time for mlp_4h_to_h (4x62848x15712, b=2048): 0.0615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62848x15712, b=2048): 263.121

Attention duration (in seconds): 0.0713
Attention throughput (in TFLOP/s): 241.582
MLP duration (in seconds): 0.1238
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1951
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 261.557
b: 64, m: 2048, n: 984, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x984x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x984x2048): 161.818
b: 64, m: 2048, n: 2048, k: 984,
Elapsed time for attention_prob_times_values (64x2048x2048x984): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x984): 153.511
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 253.565
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0616
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 263.662
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0609
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 266.617

Attention duration (in seconds): 0.0693
Attention throughput (in TFLOP/s): 249.647
MLP duration (in seconds): 0.1225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1918
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15776x47328, b=2048): 0.0474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15776x47328, b=2048): 258.103
b: 64, m: 2048, n: 986, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x986x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x986x2048): 158.602
b: 64, m: 2048, n: 2048, k: 986,
Elapsed time for attention_prob_times_values (64x2048x2048x986): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x986): 120.203
Elapsed time for attention_linear_projection (4x15776x15776, b=2048): 0.0161
Throughput (in TFLOP/s) for attention_linear_projection (4x15776x15776, b=2048): 252.903
Elapsed time for mlp_h_to_4h (4x15776x63104, b=2048): 0.0624
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15776x63104, b=2048): 261.263
Elapsed time for mlp_4h_to_h (4x63104x15776, b=2048): 0.0617
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63104x15776, b=2048): 264.290

Attention duration (in seconds): 0.0713
Attention throughput (in TFLOP/s): 243.744
MLP duration (in seconds): 0.1241
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1954
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15808x47424, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15808x47424, b=2048): 261.910
b: 64, m: 2048, n: 988, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x988x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x988x2048): 163.864
b: 64, m: 2048, n: 2048, k: 988,
Elapsed time for attention_prob_times_values (64x2048x2048x988): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x988): 139.406
Elapsed time for attention_linear_projection (4x15808x15808, b=2048): 0.0161
Throughput (in TFLOP/s) for attention_linear_projection (4x15808x15808, b=2048): 254.167
Elapsed time for mlp_h_to_4h (4x15808x63232, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15808x63232, b=2048): 263.549
Elapsed time for mlp_4h_to_h (4x63232x15808, b=2048): 0.0616
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63232x15808, b=2048): 265.837

Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 248.944
MLP duration (in seconds): 0.1237
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1938
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15840x47520, b=2048): 0.0477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15840x47520, b=2048): 258.808
b: 64, m: 2048, n: 990, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x990x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x990x2048): 158.781
b: 64, m: 2048, n: 2048, k: 990,
Elapsed time for attention_prob_times_values (64x2048x2048x990): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x990): 120.756
Elapsed time for attention_linear_projection (4x15840x15840, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15840x15840, b=2048): 253.990
Elapsed time for mlp_h_to_4h (4x15840x63360, b=2048): 0.0627
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15840x63360, b=2048): 262.077
Elapsed time for mlp_4h_to_h (4x63360x15840, b=2048): 0.0623
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63360x15840, b=2048): 263.750

Attention duration (in seconds): 0.0716
Attention throughput (in TFLOP/s): 244.553
MLP duration (in seconds): 0.1251
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 263.094
b: 64, m: 2048, n: 992, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x992x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x992x2048): 163.444
b: 64, m: 2048, n: 2048, k: 992,
Elapsed time for attention_prob_times_values (64x2048x2048x992): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x992): 155.567
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 255.329
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 264.273
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 264.134

Attention duration (in seconds): 0.0699
Attention throughput (in TFLOP/s): 251.389
MLP duration (in seconds): 0.1250
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1949
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15904x47712, b=2048): 0.0490
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15904x47712, b=2048): 253.614
b: 64, m: 2048, n: 994, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x994x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x994x2048): 159.604
b: 64, m: 2048, n: 2048, k: 994,
Elapsed time for attention_prob_times_values (64x2048x2048x994): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x994): 121.074
Elapsed time for attention_linear_projection (4x15904x15904, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x15904x15904, b=2048): 249.354
Elapsed time for mlp_h_to_4h (4x15904x63616, b=2048): 0.0647
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15904x63616, b=2048): 256.312
Elapsed time for mlp_4h_to_h (4x63616x15904, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63616x15904, b=2048): 261.542

Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 240.407
MLP duration (in seconds): 0.1281
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15936x47808, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15936x47808, b=2048): 256.736
b: 64, m: 2048, n: 996, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x996x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x996x2048): 164.657
b: 64, m: 2048, n: 2048, k: 996,
Elapsed time for attention_prob_times_values (64x2048x2048x996): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x996): 140.245
Elapsed time for attention_linear_projection (4x15936x15936, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x15936x15936, b=2048): 247.683
Elapsed time for mlp_h_to_4h (4x15936x63744, b=2048): 0.0646
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15936x63744, b=2048): 257.757
Elapsed time for mlp_4h_to_h (4x63744x15936, b=2048): 0.0633
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63744x15936, b=2048): 262.833

Attention duration (in seconds): 0.0725
Attention throughput (in TFLOP/s): 244.384
MLP duration (in seconds): 0.1279
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2004
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15968x47904, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15968x47904, b=2048): 252.667
b: 64, m: 2048, n: 998, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x998x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x998x2048): 159.779
b: 64, m: 2048, n: 2048, k: 998,
Elapsed time for attention_prob_times_values (64x2048x2048x998): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x998): 122.089
Elapsed time for attention_linear_projection (4x15968x15968, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x15968x15968, b=2048): 247.992
Elapsed time for mlp_h_to_4h (4x15968x63872, b=2048): 0.0652
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15968x63872, b=2048): 256.124
Elapsed time for mlp_4h_to_h (4x63872x15968, b=2048): 0.0638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63872x15968, b=2048): 261.789

Attention duration (in seconds): 0.0742
Attention throughput (in TFLOP/s): 239.683
MLP duration (in seconds): 0.1291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 258.071
b: 64, m: 2048, n: 1000, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1000x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1000x2048): 163.684
b: 64, m: 2048, n: 2048, k: 1000,
Elapsed time for attention_prob_times_values (64x2048x2048x1000): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1000): 155.275
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 249.687
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 258.910
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 263.051

Attention duration (in seconds): 0.0723
Attention throughput (in TFLOP/s): 246.924
MLP duration (in seconds): 0.1286
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16032x48096, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16032x48096, b=2048): 252.688
b: 64, m: 2048, n: 1002, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1002x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1002x2048): 160.557
b: 64, m: 2048, n: 2048, k: 1002,
Elapsed time for attention_prob_times_values (64x2048x2048x1002): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1002): 122.946
Elapsed time for attention_linear_projection (4x16032x16032, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16032x16032, b=2048): 248.664
Elapsed time for mlp_h_to_4h (4x16032x64128, b=2048): 0.0658
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16032x64128, b=2048): 255.938
Elapsed time for mlp_4h_to_h (4x64128x16032, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64128x16032, b=2048): 261.804

Attention duration (in seconds): 0.0747
Attention throughput (in TFLOP/s): 240.036
MLP duration (in seconds): 0.1302
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16064x48192, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16064x48192, b=2048): 257.248
b: 64, m: 2048, n: 1004, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1004x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1004x2048): 166.175
b: 64, m: 2048, n: 2048, k: 1004,
Elapsed time for attention_prob_times_values (64x2048x2048x1004): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1004): 141.957
Elapsed time for attention_linear_projection (4x16064x16064, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16064x16064, b=2048): 249.904
Elapsed time for mlp_h_to_4h (4x16064x64256, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16064x64256, b=2048): 258.668
Elapsed time for mlp_4h_to_h (4x64256x16064, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64256x16064, b=2048): 262.834

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 245.545
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16096x48288, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16096x48288, b=2048): 252.903
b: 64, m: 2048, n: 1006, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1006x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1006x2048): 161.819
b: 64, m: 2048, n: 2048, k: 1006,
Elapsed time for attention_prob_times_values (64x2048x2048x1006): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1006): 124.045
Elapsed time for attention_linear_projection (4x16096x16096, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x16096x16096, b=2048): 249.596
Elapsed time for mlp_h_to_4h (4x16096x64384, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16096x64384, b=2048): 257.331
Elapsed time for mlp_4h_to_h (4x64384x16096, b=2048): 0.0652
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64384x16096, b=2048): 260.544

Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 240.627
MLP duration (in seconds): 0.1312
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 258.880
b: 64, m: 2048, n: 1008, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1008x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1008x2048): 165.983
b: 64, m: 2048, n: 2048, k: 1008,
Elapsed time for attention_prob_times_values (64x2048x2048x1008): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1008): 157.211
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 251.474
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 259.816
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 262.683

Attention duration (in seconds): 0.0730
Attention throughput (in TFLOP/s): 248.223
MLP duration (in seconds): 0.1305
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16160x48480, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16160x48480, b=2048): 253.633
b: 64, m: 2048, n: 1010, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1010x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1010x2048): 162.521
b: 64, m: 2048, n: 2048, k: 1010,
Elapsed time for attention_prob_times_values (64x2048x2048x1010): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1010): 125.431
Elapsed time for attention_linear_projection (4x16160x16160, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16160x16160, b=2048): 250.741
Elapsed time for mlp_h_to_4h (4x16160x64640, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16160x64640, b=2048): 257.159
Elapsed time for mlp_4h_to_h (4x64640x16160, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64640x16160, b=2048): 261.547

Attention duration (in seconds): 0.0753
Attention throughput (in TFLOP/s): 241.585
MLP duration (in seconds): 0.1320
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16192x48576, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16192x48576, b=2048): 259.205
b: 64, m: 2048, n: 1012, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1012x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1012x2048): 168.092
b: 64, m: 2048, n: 2048, k: 1012,
Elapsed time for attention_prob_times_values (64x2048x2048x1012): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1012): 144.375
Elapsed time for attention_linear_projection (4x16192x16192, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x16192x16192, b=2048): 252.101
Elapsed time for mlp_h_to_4h (4x16192x64768, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16192x64768, b=2048): 260.109
Elapsed time for mlp_4h_to_h (4x64768x16192, b=2048): 0.0652
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64768x16192, b=2048): 263.472

Attention duration (in seconds): 0.0738
Attention throughput (in TFLOP/s): 247.712
MLP duration (in seconds): 0.1313
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16224x48672, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16224x48672, b=2048): 255.557
b: 64, m: 2048, n: 1014, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1014x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1014x2048): 162.839
b: 64, m: 2048, n: 2048, k: 1014,
Elapsed time for attention_prob_times_values (64x2048x2048x1014): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1014): 126.654
Elapsed time for attention_linear_projection (4x16224x16224, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16224x16224, b=2048): 251.802
Elapsed time for mlp_h_to_4h (4x16224x64896, b=2048): 0.0668
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16224x64896, b=2048): 258.341
Elapsed time for mlp_4h_to_h (4x64896x16224, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64896x16224, b=2048): 261.081

Attention duration (in seconds): 0.0754
Attention throughput (in TFLOP/s): 243.244
MLP duration (in seconds): 0.1328
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 260.213
b: 64, m: 2048, n: 1016, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1016x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1016x2048): 166.206
b: 64, m: 2048, n: 2048, k: 1016,
Elapsed time for attention_prob_times_values (64x2048x2048x1016): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1016): 156.776
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 253.478
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 261.373
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0658
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 263.008

Attention duration (in seconds): 0.0738
Attention throughput (in TFLOP/s): 249.591
MLP duration (in seconds): 0.1321
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16288x48864, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16288x48864, b=2048): 254.843
b: 64, m: 2048, n: 1018, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1018x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1018x2048): 163.376
b: 64, m: 2048, n: 2048, k: 1018,
Elapsed time for attention_prob_times_values (64x2048x2048x1018): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1018): 127.992
Elapsed time for attention_linear_projection (4x16288x16288, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16288x16288, b=2048): 252.863
Elapsed time for mlp_h_to_4h (4x16288x65152, b=2048): 0.0672
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16288x65152, b=2048): 258.553
Elapsed time for mlp_4h_to_h (4x65152x16288, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65152x16288, b=2048): 261.739

Attention duration (in seconds): 0.0760
Attention throughput (in TFLOP/s): 243.238
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16320x48960, b=2048): 0.0502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16320x48960, b=2048): 260.842
b: 64, m: 2048, n: 1020, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1020x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1020x2048): 169.296
b: 64, m: 2048, n: 2048, k: 1020,
Elapsed time for attention_prob_times_values (64x2048x2048x1020): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1020): 145.554
Elapsed time for attention_linear_projection (4x16320x16320, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16320x16320, b=2048): 254.161
Elapsed time for mlp_h_to_4h (4x16320x65280, b=2048): 0.0667
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16320x65280, b=2048): 261.814
Elapsed time for mlp_4h_to_h (4x65280x16320, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65280x16320, b=2048): 262.209

Attention duration (in seconds): 0.0744
Attention throughput (in TFLOP/s): 249.483
MLP duration (in seconds): 0.1332
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16352x49056, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16352x49056, b=2048): 245.095
b: 64, m: 2048, n: 1022, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1022x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1022x2048): 164.370
b: 64, m: 2048, n: 2048, k: 1022,
Elapsed time for attention_prob_times_values (64x2048x2048x1022): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1022): 130.476
Elapsed time for attention_linear_projection (4x16352x16352, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x16352x16352, b=2048): 253.904
Elapsed time for mlp_h_to_4h (4x16352x65408, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16352x65408, b=2048): 260.177
Elapsed time for mlp_4h_to_h (4x65408x16352, b=2048): 0.0669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65408x16352, b=2048): 261.937

Attention duration (in seconds): 0.0784
Attention throughput (in TFLOP/s): 237.450
MLP duration (in seconds): 0.1343
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 262.921
b: 64, m: 2048, n: 1024, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1024x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1024x2048): 168.803
b: 64, m: 2048, n: 2048, k: 1024,
Elapsed time for attention_prob_times_values (64x2048x2048x1024): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1024): 160.384
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 255.163
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 264.094
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0670
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 262.528

Attention duration (in seconds): 0.0741
Attention throughput (in TFLOP/s): 252.237
MLP duration (in seconds): 0.1336
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16416x49248, b=2048): 0.0674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16416x49248, b=2048): 196.524
b: 64, m: 2048, n: 1026, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1026x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1026x2048): 152.730
b: 64, m: 2048, n: 2048, k: 1026,
Elapsed time for attention_prob_times_values (64x2048x2048x1026): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1026): 126.865
Elapsed time for attention_linear_projection (4x16416x16416, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x16416x16416, b=2048): 249.228
Elapsed time for mlp_h_to_4h (4x16416x65664, b=2048): 0.0692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16416x65664, b=2048): 255.124
Elapsed time for mlp_4h_to_h (4x65664x16416, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65664x16416, b=2048): 262.117

Attention duration (in seconds): 0.0931
Attention throughput (in TFLOP/s): 201.610
MLP duration (in seconds): 0.1366
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16448x49344, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16448x49344, b=2048): 257.515
b: 64, m: 2048, n: 1028, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1028x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1028x2048): 157.687
b: 64, m: 2048, n: 2048, k: 1028,
Elapsed time for attention_prob_times_values (64x2048x2048x1028): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1028): 143.201
Elapsed time for attention_linear_projection (4x16448x16448, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x16448x16448, b=2048): 249.988
Elapsed time for mlp_h_to_4h (4x16448x65792, b=2048): 0.0686
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16448x65792, b=2048): 258.468
Elapsed time for mlp_4h_to_h (4x65792x16448, b=2048): 0.0677
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65792x16448, b=2048): 262.030

Attention duration (in seconds): 0.0767
Attention throughput (in TFLOP/s): 245.479
MLP duration (in seconds): 0.1363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16480x49440, b=2048): 0.0530
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16480x49440, b=2048): 251.989
b: 64, m: 2048, n: 1030, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1030x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1030x2048): 153.691
b: 64, m: 2048, n: 2048, k: 1030,
Elapsed time for attention_prob_times_values (64x2048x2048x1030): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1030): 125.751
Elapsed time for attention_linear_projection (4x16480x16480, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16480x16480, b=2048): 249.213
Elapsed time for mlp_h_to_4h (4x16480x65920, b=2048): 0.0700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16480x65920, b=2048): 254.422
Elapsed time for mlp_4h_to_h (4x65920x16480, b=2048): 0.0680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65920x16480, b=2048): 261.627

Attention duration (in seconds): 0.0788
Attention throughput (in TFLOP/s): 239.831
MLP duration (in seconds): 0.1380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0520
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 257.892
b: 64, m: 2048, n: 1032, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1032x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1032x2048): 155.901
b: 64, m: 2048, n: 2048, k: 1032,
Elapsed time for attention_prob_times_values (64x2048x2048x1032): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1032): 155.609
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 250.821
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0689
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 259.472
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 262.814

Attention duration (in seconds): 0.0769
Attention throughput (in TFLOP/s): 246.803
MLP duration (in seconds): 0.1369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16544x49632, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16544x49632, b=2048): 253.578
b: 64, m: 2048, n: 1034, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1034x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1034x2048): 153.667
b: 64, m: 2048, n: 2048, k: 1034,
Elapsed time for attention_prob_times_values (64x2048x2048x1034): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1034): 125.066
Elapsed time for attention_linear_projection (4x16544x16544, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16544x16544, b=2048): 250.150
Elapsed time for mlp_h_to_4h (4x16544x66176, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16544x66176, b=2048): 257.263
Elapsed time for mlp_4h_to_h (4x66176x16544, b=2048): 0.0688
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66176x16544, b=2048): 260.861

Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 241.016
MLP duration (in seconds): 0.1385
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16576x49728, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16576x49728, b=2048): 258.851
b: 64, m: 2048, n: 1036, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1036x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1036x2048): 158.720
b: 64, m: 2048, n: 2048, k: 1036,
Elapsed time for attention_prob_times_values (64x2048x2048x1036): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1036): 142.849
Elapsed time for attention_linear_projection (4x16576x16576, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16576x16576, b=2048): 251.226
Elapsed time for mlp_h_to_4h (4x16576x66304, b=2048): 0.0693
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16576x66304, b=2048): 259.853
Elapsed time for mlp_4h_to_h (4x66304x16576, b=2048): 0.0687
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66304x16576, b=2048): 262.201

Attention duration (in seconds): 0.0775
Attention throughput (in TFLOP/s): 246.731
MLP duration (in seconds): 0.1380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16608x49824, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16608x49824, b=2048): 254.446
b: 64, m: 2048, n: 1038, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1038x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1038x2048): 154.312
b: 64, m: 2048, n: 2048, k: 1038,
Elapsed time for attention_prob_times_values (64x2048x2048x1038): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1038): 124.940
Elapsed time for attention_linear_projection (4x16608x16608, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16608x16608, b=2048): 250.745
Elapsed time for mlp_h_to_4h (4x16608x66432, b=2048): 0.0701
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16608x66432, b=2048): 258.025
Elapsed time for mlp_4h_to_h (4x66432x16608, b=2048): 0.0690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66432x16608, b=2048): 261.918

Attention duration (in seconds): 0.0794
Attention throughput (in TFLOP/s): 241.773
MLP duration (in seconds): 0.1391
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 259.510
b: 64, m: 2048, n: 1040, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1040x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1040x2048): 157.628
b: 64, m: 2048, n: 2048, k: 1040,
Elapsed time for attention_prob_times_values (64x2048x2048x1040): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1040): 158.106
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 252.645
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 260.934
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 262.443

Attention duration (in seconds): 0.0775
Attention throughput (in TFLOP/s): 248.638
MLP duration (in seconds): 0.1387
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16672x50016, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16672x50016, b=2048): 254.875
b: 64, m: 2048, n: 1042, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1042x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1042x2048): 154.397
b: 64, m: 2048, n: 2048, k: 1042,
Elapsed time for attention_prob_times_values (64x2048x2048x1042): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1042): 123.904
Elapsed time for attention_linear_projection (4x16672x16672, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16672x16672, b=2048): 251.544
Elapsed time for mlp_h_to_4h (4x16672x66688, b=2048): 0.0703
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16672x66688, b=2048): 259.171
Elapsed time for mlp_4h_to_h (4x66688x16672, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66688x16672, b=2048): 261.174

Attention duration (in seconds): 0.0798
Attention throughput (in TFLOP/s): 242.155
MLP duration (in seconds): 0.1400
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16704x50112, b=2048): 0.0527
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16704x50112, b=2048): 260.353
b: 64, m: 2048, n: 1044, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1044x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1044x2048): 159.631
b: 64, m: 2048, n: 2048, k: 1044,
Elapsed time for attention_prob_times_values (64x2048x2048x1044): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1044): 143.015
Elapsed time for attention_linear_projection (4x16704x16704, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16704x16704, b=2048): 252.790
Elapsed time for mlp_h_to_4h (4x16704x66816, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16704x66816, b=2048): 262.295
Elapsed time for mlp_4h_to_h (4x66816x16704, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66816x16704, b=2048): 264.740

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 248.199
MLP duration (in seconds): 0.1388
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16736x50208, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16736x50208, b=2048): 258.247
b: 64, m: 2048, n: 1046, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1046x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1046x2048): 154.908
b: 64, m: 2048, n: 2048, k: 1046,
Elapsed time for attention_prob_times_values (64x2048x2048x1046): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1046): 123.824
Elapsed time for attention_linear_projection (4x16736x16736, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16736x16736, b=2048): 256.731
Elapsed time for mlp_h_to_4h (4x16736x66944, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16736x66944, b=2048): 262.757
Elapsed time for mlp_4h_to_h (4x66944x16736, b=2048): 0.0689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66944x16736, b=2048): 266.291

Attention duration (in seconds): 0.0793
Attention throughput (in TFLOP/s): 245.501
MLP duration (in seconds): 0.1388
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0523
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 264.153
b: 64, m: 2048, n: 1048, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1048x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1048x2048): 158.053
b: 64, m: 2048, n: 2048, k: 1048,
Elapsed time for attention_prob_times_values (64x2048x2048x1048): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1048): 157.022
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 257.607
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0693
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 265.748
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 266.756

Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 252.793
MLP duration (in seconds): 0.1384
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16800x50400, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16800x50400, b=2048): 255.308
b: 64, m: 2048, n: 1050, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1050x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1050x2048): 155.439
b: 64, m: 2048, n: 2048, k: 1050,
Elapsed time for attention_prob_times_values (64x2048x2048x1050): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1050): 123.544
Elapsed time for attention_linear_projection (4x16800x16800, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x16800x16800, b=2048): 251.878
Elapsed time for mlp_h_to_4h (4x16800x67200, b=2048): 0.0716
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16800x67200, b=2048): 258.256
Elapsed time for mlp_4h_to_h (4x67200x16800, b=2048): 0.0696
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67200x16800, b=2048): 265.625

Attention duration (in seconds): 0.0809
Attention throughput (in TFLOP/s): 242.619
MLP duration (in seconds): 0.1413
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16832x50496, b=2048): 0.0537
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16832x50496, b=2048): 259.415
b: 64, m: 2048, n: 1052, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1052x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1052x2048): 160.647
b: 64, m: 2048, n: 2048, k: 1052,
Elapsed time for attention_prob_times_values (64x2048x2048x1052): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1052): 143.023
Elapsed time for attention_linear_projection (4x16832x16832, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_linear_projection (4x16832x16832, b=2048): 253.013
Elapsed time for mlp_h_to_4h (4x16832x67328, b=2048): 0.0709
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16832x67328, b=2048): 261.908
Elapsed time for mlp_4h_to_h (4x67328x16832, b=2048): 0.0694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67328x16832, b=2048): 267.665

Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 247.787
MLP duration (in seconds): 0.1403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16864x50592, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16864x50592, b=2048): 256.249
b: 64, m: 2048, n: 1054, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1054x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1054x2048): 155.970
b: 64, m: 2048, n: 2048, k: 1054,
Elapsed time for attention_prob_times_values (64x2048x2048x1054): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1054): 124.418
Elapsed time for attention_linear_projection (4x16864x16864, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x16864x16864, b=2048): 252.783
Elapsed time for mlp_h_to_4h (4x16864x67456, b=2048): 0.0720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16864x67456, b=2048): 258.879
Elapsed time for mlp_4h_to_h (4x67456x16864, b=2048): 0.0701
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67456x16864, b=2048): 265.791

Attention duration (in seconds): 0.0812
Attention throughput (in TFLOP/s): 243.592
MLP duration (in seconds): 0.1421
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 262.406
b: 64, m: 2048, n: 1056, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1056x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1056x2048): 159.999
b: 64, m: 2048, n: 2048, k: 1056,
Elapsed time for attention_prob_times_values (64x2048x2048x1056): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1056): 159.870
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 253.724
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0712
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 262.943
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0701
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 266.741

Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 251.184
MLP duration (in seconds): 0.1413
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16928x50784, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16928x50784, b=2048): 257.078
b: 64, m: 2048, n: 1058, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1058x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1058x2048): 156.213
b: 64, m: 2048, n: 2048, k: 1058,
Elapsed time for attention_prob_times_values (64x2048x2048x1058): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1058): 123.942
Elapsed time for attention_linear_projection (4x16928x16928, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16928x16928, b=2048): 253.255
Elapsed time for mlp_h_to_4h (4x16928x67712, b=2048): 0.0723
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16928x67712, b=2048): 259.720
Elapsed time for mlp_4h_to_h (4x67712x16928, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67712x16928, b=2048): 265.822

Attention duration (in seconds): 0.0815
Attention throughput (in TFLOP/s): 244.229
MLP duration (in seconds): 0.1430
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16960x50880, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16960x50880, b=2048): 262.009
b: 64, m: 2048, n: 1060, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1060x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1060x2048): 161.780
b: 64, m: 2048, n: 2048, k: 1060,
Elapsed time for attention_prob_times_values (64x2048x2048x1060): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1060): 143.366
Elapsed time for attention_linear_projection (4x16960x16960, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16960x16960, b=2048): 254.606
Elapsed time for mlp_h_to_4h (4x16960x67840, b=2048): 0.0719
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16960x67840, b=2048): 262.289
Elapsed time for mlp_4h_to_h (4x67840x16960, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67840x16960, b=2048): 266.921

Attention duration (in seconds): 0.0800
Attention throughput (in TFLOP/s): 249.996
MLP duration (in seconds): 0.1425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16992x50976, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16992x50976, b=2048): 256.971
b: 64, m: 2048, n: 1062, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1062x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1062x2048): 157.185
b: 64, m: 2048, n: 2048, k: 1062,
Elapsed time for attention_prob_times_values (64x2048x2048x1062): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1062): 124.062
Elapsed time for attention_linear_projection (4x16992x16992, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x16992x16992, b=2048): 254.297
Elapsed time for mlp_h_to_4h (4x16992x67968, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16992x67968, b=2048): 260.633
Elapsed time for mlp_4h_to_h (4x67968x16992, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67968x16992, b=2048): 263.688

Attention duration (in seconds): 0.0821
Attention throughput (in TFLOP/s): 244.509
MLP duration (in seconds): 0.1444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 261.892
b: 64, m: 2048, n: 1064, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1064x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1064x2048): 160.294
b: 64, m: 2048, n: 2048, k: 1064,
Elapsed time for attention_prob_times_values (64x2048x2048x1064): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1064): 158.901
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 255.603
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 263.548
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 267.225

Attention duration (in seconds): 0.0801
Attention throughput (in TFLOP/s): 251.295
MLP duration (in seconds): 0.1431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17056x51168, b=2048): 0.0555
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17056x51168, b=2048): 257.541
b: 64, m: 2048, n: 1066, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1066x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1066x2048): 157.818
b: 64, m: 2048, n: 2048, k: 1066,
Elapsed time for attention_prob_times_values (64x2048x2048x1066): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1066): 124.265
Elapsed time for attention_linear_projection (4x17056x17056, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x17056x17056, b=2048): 255.349
Elapsed time for mlp_h_to_4h (4x17056x68224, b=2048): 0.0730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17056x68224, b=2048): 261.191
Elapsed time for mlp_4h_to_h (4x68224x17056, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68224x17056, b=2048): 265.496

Attention duration (in seconds): 0.0824
Attention throughput (in TFLOP/s): 245.209
MLP duration (in seconds): 0.1448
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17088x51264, b=2048): 0.0547
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17088x51264, b=2048): 262.376
b: 64, m: 2048, n: 1068, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1068x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1068x2048): 162.990
b: 64, m: 2048, n: 2048, k: 1068,
Elapsed time for attention_prob_times_values (64x2048x2048x1068): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1068): 144.631
Elapsed time for attention_linear_projection (4x17088x17088, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17088x17088, b=2048): 256.580
Elapsed time for mlp_h_to_4h (4x17088x68352, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17088x68352, b=2048): 263.832
Elapsed time for mlp_4h_to_h (4x68352x17088, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68352x17088, b=2048): 266.524

Attention duration (in seconds): 0.0808
Attention throughput (in TFLOP/s): 250.938
MLP duration (in seconds): 0.1443
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17120x51360, b=2048): 0.0556
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17120x51360, b=2048): 259.250
b: 64, m: 2048, n: 1070, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1070x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1070x2048): 158.099
b: 64, m: 2048, n: 2048, k: 1070,
Elapsed time for attention_prob_times_values (64x2048x2048x1070): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1070): 124.184
Elapsed time for attention_linear_projection (4x17120x17120, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x17120x17120, b=2048): 256.208
Elapsed time for mlp_h_to_4h (4x17120x68480, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17120x68480, b=2048): 262.057
Elapsed time for mlp_4h_to_h (4x68480x17120, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68480x17120, b=2048): 265.951

Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 246.542
MLP duration (in seconds): 0.1455
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 263.178
b: 64, m: 2048, n: 1072, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1072x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1072x2048): 162.478
b: 64, m: 2048, n: 2048, k: 1072,
Elapsed time for attention_prob_times_values (64x2048x2048x1072): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1072): 161.813
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 257.747
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0728
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 264.818
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0720
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 267.839

Attention duration (in seconds): 0.0807
Attention throughput (in TFLOP/s): 253.038
MLP duration (in seconds): 0.1448
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17184x51552, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17184x51552, b=2048): 258.464
b: 64, m: 2048, n: 1074, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1074x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1074x2048): 158.815
b: 64, m: 2048, n: 2048, k: 1074,
Elapsed time for attention_prob_times_values (64x2048x2048x1074): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1074): 124.443
Elapsed time for attention_linear_projection (4x17184x17184, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17184x17184, b=2048): 257.136
Elapsed time for mlp_h_to_4h (4x17184x68736, b=2048): 0.0740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17184x68736, b=2048): 261.448
Elapsed time for mlp_4h_to_h (4x68736x17184, b=2048): 0.0731
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68736x17184, b=2048): 264.586

Attention duration (in seconds): 0.0832
Attention throughput (in TFLOP/s): 246.357
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17216x51648, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17216x51648, b=2048): 263.160
b: 64, m: 2048, n: 1076, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1076x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1076x2048): 163.967
b: 64, m: 2048, n: 2048, k: 1076,
Elapsed time for attention_prob_times_values (64x2048x2048x1076): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1076): 144.930
Elapsed time for attention_linear_projection (4x17216x17216, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17216x17216, b=2048): 258.488
Elapsed time for mlp_h_to_4h (4x17216x68864, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17216x68864, b=2048): 264.904
Elapsed time for mlp_4h_to_h (4x68864x17216, b=2048): 0.0727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68864x17216, b=2048): 267.151

Attention duration (in seconds): 0.0817
Attention throughput (in TFLOP/s): 252.034
MLP duration (in seconds): 0.1460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17248x51744, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17248x51744, b=2048): 258.972
b: 64, m: 2048, n: 1078, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1078x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1078x2048): 158.428
b: 64, m: 2048, n: 2048, k: 1078,
Elapsed time for attention_prob_times_values (64x2048x2048x1078): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1078): 123.906
Elapsed time for attention_linear_projection (4x17248x17248, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_linear_projection (4x17248x17248, b=2048): 257.697
Elapsed time for mlp_h_to_4h (4x17248x68992, b=2048): 0.0743
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17248x68992, b=2048): 262.569
Elapsed time for mlp_4h_to_h (4x68992x17248, b=2048): 0.0737
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68992x17248, b=2048): 264.638

Attention duration (in seconds): 0.0837
Attention throughput (in TFLOP/s): 246.759
MLP duration (in seconds): 0.1479
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 263.680
b: 64, m: 2048, n: 1080, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1080x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1080x2048): 162.563
b: 64, m: 2048, n: 2048, k: 1080,
Elapsed time for attention_prob_times_values (64x2048x2048x1080): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1080): 160.524
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 259.701
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 264.845
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0738
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 265.265

Attention duration (in seconds): 0.0817
Attention throughput (in TFLOP/s): 253.785
MLP duration (in seconds): 0.1477
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17312x51936, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17312x51936, b=2048): 254.746
b: 64, m: 2048, n: 1082, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1082x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1082x2048): 159.568
b: 64, m: 2048, n: 2048, k: 1082,
Elapsed time for attention_prob_times_values (64x2048x2048x1082): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1082): 124.550
Elapsed time for attention_linear_projection (4x17312x17312, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_linear_projection (4x17312x17312, b=2048): 253.525
Elapsed time for mlp_h_to_4h (4x17312x69248, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17312x69248, b=2048): 258.177
Elapsed time for mlp_4h_to_h (4x69248x17312, b=2048): 0.0746
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69248x17312, b=2048): 263.345

Attention duration (in seconds): 0.0855
Attention throughput (in TFLOP/s): 243.315
MLP duration (in seconds): 0.1507
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17344x52032, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17344x52032, b=2048): 259.515
b: 64, m: 2048, n: 1084, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1084x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1084x2048): 165.253
b: 64, m: 2048, n: 2048, k: 1084,
Elapsed time for attention_prob_times_values (64x2048x2048x1084): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1084): 145.946
Elapsed time for attention_linear_projection (4x17344x17344, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x17344x17344, b=2048): 254.821
Elapsed time for mlp_h_to_4h (4x17344x69376, b=2048): 0.0757
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17344x69376, b=2048): 260.476
Elapsed time for mlp_4h_to_h (4x69376x17344, b=2048): 0.0744
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69376x17344, b=2048): 264.963

Attention duration (in seconds): 0.0838
Attention throughput (in TFLOP/s): 249.069
MLP duration (in seconds): 0.1501
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17376x52128, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17376x52128, b=2048): 255.144
b: 64, m: 2048, n: 1086, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1086x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1086x2048): 159.949
b: 64, m: 2048, n: 2048, k: 1086,
Elapsed time for attention_prob_times_values (64x2048x2048x1086): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1086): 123.946
Elapsed time for attention_linear_projection (4x17376x17376, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17376x17376, b=2048): 252.147
Elapsed time for mlp_h_to_4h (4x17376x69504, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17376x69504, b=2048): 257.485
Elapsed time for mlp_4h_to_h (4x69504x17376, b=2048): 0.0753
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69504x17376, b=2048): 262.731

Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 243.267
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2383
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 259.355
b: 64, m: 2048, n: 1088, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1088x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1088x2048): 165.103
b: 64, m: 2048, n: 2048, k: 1088,
Elapsed time for attention_prob_times_values (64x2048x2048x1088): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1088): 164.747
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 253.903
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 260.587
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0755
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 263.078

Attention duration (in seconds): 0.0841
Attention throughput (in TFLOP/s): 250.130
MLP duration (in seconds): 0.1517
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17440x52320, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17440x52320, b=2048): 254.355
b: 64, m: 2048, n: 1090, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1090x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1090x2048): 160.938
b: 64, m: 2048, n: 2048, k: 1090,
Elapsed time for attention_prob_times_values (64x2048x2048x1090): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1090): 122.070
Elapsed time for attention_linear_projection (4x17440x17440, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17440x17440, b=2048): 253.143
Elapsed time for mlp_h_to_4h (4x17440x69760, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17440x69760, b=2048): 258.278
Elapsed time for mlp_4h_to_h (4x69760x17440, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69760x17440, b=2048): 261.905

Attention duration (in seconds): 0.0869
Attention throughput (in TFLOP/s): 242.873
MLP duration (in seconds): 0.1533
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2402
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17472x52416, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17472x52416, b=2048): 260.172
b: 64, m: 2048, n: 1092, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1092x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1092x2048): 167.061
b: 64, m: 2048, n: 2048, k: 1092,
Elapsed time for attention_prob_times_values (64x2048x2048x1092): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1092): 144.654
Elapsed time for attention_linear_projection (4x17472x17472, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17472x17472, b=2048): 254.900
Elapsed time for mlp_h_to_4h (4x17472x69888, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17472x69888, b=2048): 260.932
Elapsed time for mlp_4h_to_h (4x69888x17472, b=2048): 0.0756
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69888x17472, b=2048): 264.475

Attention duration (in seconds): 0.0849
Attention throughput (in TFLOP/s): 249.585
MLP duration (in seconds): 0.1523
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2372
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17504x52512, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17504x52512, b=2048): 255.549
b: 64, m: 2048, n: 1094, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1094x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1094x2048): 161.497
b: 64, m: 2048, n: 2048, k: 1094,
Elapsed time for attention_prob_times_values (64x2048x2048x1094): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1094): 122.293
Elapsed time for attention_linear_projection (4x17504x17504, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17504x17504, b=2048): 254.428
Elapsed time for mlp_h_to_4h (4x17504x70016, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17504x70016, b=2048): 258.998
Elapsed time for mlp_4h_to_h (4x70016x17504, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70016x17504, b=2048): 261.417

Attention duration (in seconds): 0.0871
Attention throughput (in TFLOP/s): 244.020
MLP duration (in seconds): 0.1543
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2414
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 261.257
b: 64, m: 2048, n: 1096, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1096x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1096x2048): 165.358
b: 64, m: 2048, n: 2048, k: 1096,
Elapsed time for attention_prob_times_values (64x2048x2048x1096): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1096): 159.863
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 258.025
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 264.657
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 266.103

Attention duration (in seconds): 0.0846
Attention throughput (in TFLOP/s): 252.068
MLP duration (in seconds): 0.1519
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17568x52704, b=2048): 0.0585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17568x52704, b=2048): 259.178
b: 64, m: 2048, n: 1098, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1098x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1098x2048): 161.420
b: 64, m: 2048, n: 2048, k: 1098,
Elapsed time for attention_prob_times_values (64x2048x2048x1098): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1098): 122.019
Elapsed time for attention_linear_projection (4x17568x17568, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17568x17568, b=2048): 257.615
Elapsed time for mlp_h_to_4h (4x17568x70272, b=2048): 0.0770
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17568x70272, b=2048): 262.763
Elapsed time for mlp_4h_to_h (4x70272x17568, b=2048): 0.0760
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70272x17568, b=2048): 266.273

Attention duration (in seconds): 0.0866
Attention throughput (in TFLOP/s): 247.056
MLP duration (in seconds): 0.1529
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2396
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17600x52800, b=2048): 0.0576
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17600x52800, b=2048): 264.288
b: 64, m: 2048, n: 1100, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1100x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1100x2048): 167.692
b: 64, m: 2048, n: 2048, k: 1100,
Elapsed time for attention_prob_times_values (64x2048x2048x1100): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1100): 142.445
Elapsed time for attention_linear_projection (4x17600x17600, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17600x17600, b=2048): 258.825
Elapsed time for mlp_h_to_4h (4x17600x70400, b=2048): 0.0765
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17600x70400, b=2048): 265.369
Elapsed time for mlp_4h_to_h (4x70400x17600, b=2048): 0.0757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70400x17600, b=2048): 268.210

Attention duration (in seconds): 0.0849
Attention throughput (in TFLOP/s): 253.068
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17632x52896, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17632x52896, b=2048): 259.218
b: 64, m: 2048, n: 1102, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1102x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1102x2048): 162.667
b: 64, m: 2048, n: 2048, k: 1102,
Elapsed time for attention_prob_times_values (64x2048x2048x1102): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1102): 123.267
Elapsed time for attention_linear_projection (4x17632x17632, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17632x17632, b=2048): 257.565
Elapsed time for mlp_h_to_4h (4x17632x70528, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17632x70528, b=2048): 263.778
Elapsed time for mlp_4h_to_h (4x70528x17632, b=2048): 0.0770
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70528x17632, b=2048): 264.590

Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 247.328
MLP duration (in seconds): 0.1542
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2414
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 265.109
b: 64, m: 2048, n: 1104, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1104x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1104x2048): 167.215
b: 64, m: 2048, n: 2048, k: 1104,
Elapsed time for attention_prob_times_values (64x2048x2048x1104): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1104): 162.027
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 259.710
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 266.175
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 266.149

Attention duration (in seconds): 0.0847
Attention throughput (in TFLOP/s): 255.309
MLP duration (in seconds): 0.1537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2384
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17696x53088, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17696x53088, b=2048): 254.287
b: 64, m: 2048, n: 1106, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1106x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1106x2048): 162.873
b: 64, m: 2048, n: 2048, k: 1106,
Elapsed time for attention_prob_times_values (64x2048x2048x1106): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1106): 123.770
Elapsed time for attention_linear_projection (4x17696x17696, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17696x17696, b=2048): 253.973
Elapsed time for mlp_h_to_4h (4x17696x70784, b=2048): 0.0798
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17696x70784, b=2048): 257.290
Elapsed time for mlp_4h_to_h (4x70784x17696, b=2048): 0.0779
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70784x17696, b=2048): 263.317

Attention duration (in seconds): 0.0892
Attention throughput (in TFLOP/s): 243.457
MLP duration (in seconds): 0.1577
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17728x53184, b=2048): 0.0599
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17728x53184, b=2048): 258.080
b: 64, m: 2048, n: 1108, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1108x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1108x2048): 169.118
b: 64, m: 2048, n: 2048, k: 1108,
Elapsed time for attention_prob_times_values (64x2048x2048x1108): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1108): 144.153
Elapsed time for attention_linear_projection (4x17728x17728, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17728x17728, b=2048): 254.601
Elapsed time for mlp_h_to_4h (4x17728x70912, b=2048): 0.0793
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17728x70912, b=2048): 259.631
Elapsed time for mlp_4h_to_h (4x70912x17728, b=2048): 0.0782
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70912x17728, b=2048): 263.434

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 248.352
MLP duration (in seconds): 0.1575
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17760x53280, b=2048): 0.0620
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17760x53280, b=2048): 250.238
b: 64, m: 2048, n: 1110, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1110x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1110x2048): 163.537
b: 64, m: 2048, n: 2048, k: 1110,
Elapsed time for attention_prob_times_values (64x2048x2048x1110): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1110): 124.372
Elapsed time for attention_linear_projection (4x17760x17760, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x17760x17760, b=2048): 253.128
Elapsed time for mlp_h_to_4h (4x17760x71040, b=2048): 0.0805
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17760x71040, b=2048): 256.761
Elapsed time for mlp_4h_to_h (4x71040x17760, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71040x17760, b=2048): 262.015

Attention duration (in seconds): 0.0908
Attention throughput (in TFLOP/s): 240.767
MLP duration (in seconds): 0.1594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2502
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 258.771
b: 64, m: 2048, n: 1112, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1112x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1112x2048): 167.391
b: 64, m: 2048, n: 2048, k: 1112,
Elapsed time for attention_prob_times_values (64x2048x2048x1112): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1112): 161.133
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 254.427
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0798
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 259.880
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 264.557

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 249.929
MLP duration (in seconds): 0.1582
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17824x53472, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17824x53472, b=2048): 255.038
b: 64, m: 2048, n: 1114, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1114x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1114x2048): 163.922
b: 64, m: 2048, n: 2048, k: 1114,
Elapsed time for attention_prob_times_values (64x2048x2048x1114): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1114): 125.044
Elapsed time for attention_linear_projection (4x17824x17824, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17824x17824, b=2048): 253.449
Elapsed time for mlp_h_to_4h (4x17824x71296, b=2048): 0.0806
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17824x71296, b=2048): 258.260
Elapsed time for mlp_4h_to_h (4x71296x17824, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71296x17824, b=2048): 261.397

Attention duration (in seconds): 0.0902
Attention throughput (in TFLOP/s): 244.097
MLP duration (in seconds): 0.1603
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2505
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17856x53568, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17856x53568, b=2048): 259.165
b: 64, m: 2048, n: 1116, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1116x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1116x2048): 170.373
b: 64, m: 2048, n: 2048, k: 1116,
Elapsed time for attention_prob_times_values (64x2048x2048x1116): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1116): 146.480
Elapsed time for attention_linear_projection (4x17856x17856, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17856x17856, b=2048): 254.952
Elapsed time for mlp_h_to_4h (4x17856x71424, b=2048): 0.0802
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17856x71424, b=2048): 260.541
Elapsed time for mlp_4h_to_h (4x71424x17856, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71424x17856, b=2048): 263.868

Attention duration (in seconds): 0.0886
Attention throughput (in TFLOP/s): 249.460
MLP duration (in seconds): 0.1594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2480
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17888x53664, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17888x53664, b=2048): 255.596
b: 64, m: 2048, n: 1118, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1118x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1118x2048): 164.888
b: 64, m: 2048, n: 2048, k: 1118,
Elapsed time for attention_prob_times_values (64x2048x2048x1118): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1118): 125.355
Elapsed time for attention_linear_projection (4x17888x17888, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x17888x17888, b=2048): 254.519
Elapsed time for mlp_h_to_4h (4x17888x71552, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17888x71552, b=2048): 259.168
Elapsed time for mlp_4h_to_h (4x71552x17888, b=2048): 0.0799
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71552x17888, b=2048): 262.292

Attention duration (in seconds): 0.0906
Attention throughput (in TFLOP/s): 244.818
MLP duration (in seconds): 0.1609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2514
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 260.522
b: 64, m: 2048, n: 1120, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1120x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1120x2048): 169.365
b: 64, m: 2048, n: 2048, k: 1120,
Elapsed time for attention_prob_times_values (64x2048x2048x1120): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1120): 164.055
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 256.353
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 261.944
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 264.600

Attention duration (in seconds): 0.0883
Attention throughput (in TFLOP/s): 251.886
MLP duration (in seconds): 0.1599
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17952x53856, b=2048): 0.0619
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17952x53856, b=2048): 255.809
b: 64, m: 2048, n: 1122, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1122x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1122x2048): 165.240
b: 64, m: 2048, n: 2048, k: 1122,
Elapsed time for attention_prob_times_values (64x2048x2048x1122): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1122): 125.454
Elapsed time for attention_linear_projection (4x17952x17952, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x17952x17952, b=2048): 255.282
Elapsed time for mlp_h_to_4h (4x17952x71808, b=2048): 0.0810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17952x71808, b=2048): 260.684
Elapsed time for mlp_4h_to_h (4x71808x17952, b=2048): 0.0807
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71808x17952, b=2048): 261.717

Attention duration (in seconds): 0.0911
Attention throughput (in TFLOP/s): 245.190
MLP duration (in seconds): 0.1617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2528
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17984x53952, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17984x53952, b=2048): 261.884
b: 64, m: 2048, n: 1124, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1124x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1124x2048): 171.723
b: 64, m: 2048, n: 2048, k: 1124,
Elapsed time for attention_prob_times_values (64x2048x2048x1124): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1124): 147.007
Elapsed time for attention_linear_projection (4x17984x17984, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x17984x17984, b=2048): 257.059
Elapsed time for mlp_h_to_4h (4x17984x71936, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17984x71936, b=2048): 262.330
Elapsed time for mlp_4h_to_h (4x71936x17984, b=2048): 0.0804
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71936x17984, b=2048): 263.592

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 251.901
MLP duration (in seconds): 0.1612
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2501
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18016x54048, b=2048): 0.0630
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18016x54048, b=2048): 253.136
b: 64, m: 2048, n: 1126, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1126x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1126x2048): 166.657
b: 64, m: 2048, n: 2048, k: 1126,
Elapsed time for attention_prob_times_values (64x2048x2048x1126): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1126): 126.403
Elapsed time for attention_linear_projection (4x18016x18016, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x18016x18016, b=2048): 256.403
Elapsed time for mlp_h_to_4h (4x18016x72064, b=2048): 0.0818
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18016x72064, b=2048): 260.158
Elapsed time for mlp_4h_to_h (4x72064x18016, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72064x18016, b=2048): 262.875

Attention duration (in seconds): 0.0922
Attention throughput (in TFLOP/s): 243.892
MLP duration (in seconds): 0.1627
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2549
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0609
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 262.699
b: 64, m: 2048, n: 1128, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1128x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1128x2048): 170.232
b: 64, m: 2048, n: 2048, k: 1128,
Elapsed time for attention_prob_times_values (64x2048x2048x1128): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1128): 163.283
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 257.880
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 263.719
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 263.819

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 253.730
MLP duration (in seconds): 0.1619
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18080x54240, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18080x54240, b=2048): 252.917
b: 64, m: 2048, n: 1130, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1130x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1130x2048): 166.603
b: 64, m: 2048, n: 2048, k: 1130,
Elapsed time for attention_prob_times_values (64x2048x2048x1130): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1130): 127.808
Elapsed time for attention_linear_projection (4x18080x18080, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18080x18080, b=2048): 252.555
Elapsed time for mlp_h_to_4h (4x18080x72320, b=2048): 0.0836
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18080x72320, b=2048): 256.147
Elapsed time for mlp_4h_to_h (4x72320x18080, b=2048): 0.0817
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72320x18080, b=2048): 262.154

Attention duration (in seconds): 0.0931
Attention throughput (in TFLOP/s): 243.082
MLP duration (in seconds): 0.1654
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18112x54336, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18112x54336, b=2048): 258.210
b: 64, m: 2048, n: 1132, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1132x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1132x2048): 173.027
b: 64, m: 2048, n: 2048, k: 1132,
Elapsed time for attention_prob_times_values (64x2048x2048x1132): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1132): 148.641
Elapsed time for attention_linear_projection (4x18112x18112, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18112x18112, b=2048): 253.800
Elapsed time for mlp_h_to_4h (4x18112x72448, b=2048): 0.0826
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18112x72448, b=2048): 260.215
Elapsed time for mlp_4h_to_h (4x72448x18112, b=2048): 0.0805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72448x18112, b=2048): 266.913

Attention duration (in seconds): 0.0912
Attention throughput (in TFLOP/s): 248.996
MLP duration (in seconds): 0.1632
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18144x54432, b=2048): 0.0631
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18144x54432, b=2048): 256.550
b: 64, m: 2048, n: 1134, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1134x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1134x2048): 167.248
b: 64, m: 2048, n: 2048, k: 1134,
Elapsed time for attention_prob_times_values (64x2048x2048x1134): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1134): 127.149
Elapsed time for attention_linear_projection (4x18144x18144, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18144x18144, b=2048): 255.604
Elapsed time for mlp_h_to_4h (4x18144x72576, b=2048): 0.0829
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18144x72576, b=2048): 260.255
Elapsed time for mlp_4h_to_h (4x72576x18144, b=2048): 0.0807
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72576x18144, b=2048): 267.261

Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 246.133
MLP duration (in seconds): 0.1636
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2562
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0618
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 262.780
b: 64, m: 2048, n: 1136, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1136x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1136x2048): 172.410
b: 64, m: 2048, n: 2048, k: 1136,
Elapsed time for attention_prob_times_values (64x2048x2048x1136): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1136): 165.999
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 256.637
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0821
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 263.573
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 267.883

Attention duration (in seconds): 0.0901
Attention throughput (in TFLOP/s): 253.847
MLP duration (in seconds): 0.1630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2531
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18208x54624, b=2048): 0.0631
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18208x54624, b=2048): 258.055
b: 64, m: 2048, n: 1138, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1138x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1138x2048): 167.640
b: 64, m: 2048, n: 2048, k: 1138,
Elapsed time for attention_prob_times_values (64x2048x2048x1138): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1138): 127.604
Elapsed time for attention_linear_projection (4x18208x18208, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18208x18208, b=2048): 255.775
Elapsed time for mlp_h_to_4h (4x18208x72832, b=2048): 0.0834
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18208x72832, b=2048): 260.647
Elapsed time for mlp_4h_to_h (4x72832x18208, b=2048): 0.0818
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72832x18208, b=2048): 265.636

Attention duration (in seconds): 0.0928
Attention throughput (in TFLOP/s): 247.254
MLP duration (in seconds): 0.1652
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2580
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18240x54720, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18240x54720, b=2048): 261.888
b: 64, m: 2048, n: 1140, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1140x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1140x2048): 173.802
b: 64, m: 2048, n: 2048, k: 1140,
Elapsed time for attention_prob_times_values (64x2048x2048x1140): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1140): 148.901
Elapsed time for attention_linear_projection (4x18240x18240, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18240x18240, b=2048): 257.056
Elapsed time for mlp_h_to_4h (4x18240x72960, b=2048): 0.0830
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18240x72960, b=2048): 262.763
Elapsed time for mlp_4h_to_h (4x72960x18240, b=2048): 0.0816
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72960x18240, b=2048): 267.165

Attention duration (in seconds): 0.0913
Attention throughput (in TFLOP/s): 252.279
MLP duration (in seconds): 0.1646
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2559
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18272x54816, b=2048): 0.0638
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18272x54816, b=2048): 257.270
b: 64, m: 2048, n: 1142, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1142x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1142x2048): 168.870
b: 64, m: 2048, n: 2048, k: 1142,
Elapsed time for attention_prob_times_values (64x2048x2048x1142): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1142): 129.543
Elapsed time for attention_linear_projection (4x18272x18272, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18272x18272, b=2048): 255.076
Elapsed time for mlp_h_to_4h (4x18272x73088, b=2048): 0.0841
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18272x73088, b=2048): 260.091
Elapsed time for mlp_4h_to_h (4x73088x18272, b=2048): 0.0832
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73088x18272, b=2048): 263.105

Attention duration (in seconds): 0.0936
Attention throughput (in TFLOP/s): 246.879
MLP duration (in seconds): 0.1673
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2609
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0630
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 261.377
b: 64, m: 2048, n: 1144, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1144x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1144x2048): 172.635
b: 64, m: 2048, n: 2048, k: 1144,
Elapsed time for attention_prob_times_values (64x2048x2048x1144): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1144): 164.783
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 255.737
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 262.246
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0831
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 264.206

Attention duration (in seconds): 0.0918
Attention throughput (in TFLOP/s): 252.693
MLP duration (in seconds): 0.1668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18336x55008, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18336x55008, b=2048): 255.672
b: 64, m: 2048, n: 1146, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1146x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1146x2048): 168.929
b: 64, m: 2048, n: 2048, k: 1146,
Elapsed time for attention_prob_times_values (64x2048x2048x1146): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1146): 129.925
Elapsed time for attention_linear_projection (4x18336x18336, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x18336x18336, b=2048): 254.984
Elapsed time for mlp_h_to_4h (4x18336x73344, b=2048): 0.0849
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18336x73344, b=2048): 259.455
Elapsed time for mlp_4h_to_h (4x73344x18336, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73344x18336, b=2048): 264.025

Attention duration (in seconds): 0.0946
Attention throughput (in TFLOP/s): 245.882
MLP duration (in seconds): 0.1684
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2630
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18368x55104, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18368x55104, b=2048): 263.533
b: 64, m: 2048, n: 1148, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1148x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1148x2048): 175.330
b: 64, m: 2048, n: 2048, k: 1148,
Elapsed time for attention_prob_times_values (64x2048x2048x1148): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1148): 149.159
Elapsed time for attention_linear_projection (4x18368x18368, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x18368x18368, b=2048): 259.147
Elapsed time for mlp_h_to_4h (4x18368x73472, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18368x73472, b=2048): 264.835
Elapsed time for mlp_4h_to_h (4x73472x18368, b=2048): 0.0827
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73472x18368, b=2048): 267.281

Attention duration (in seconds): 0.0919
Attention throughput (in TFLOP/s): 253.999
MLP duration (in seconds): 0.1662
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18400x55200, b=2048): 0.0640
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18400x55200, b=2048): 260.073
b: 64, m: 2048, n: 1150, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1150x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1150x2048): 169.697
b: 64, m: 2048, n: 2048, k: 1150,
Elapsed time for attention_prob_times_values (64x2048x2048x1150): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1150): 131.084
Elapsed time for attention_linear_projection (4x18400x18400, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18400x18400, b=2048): 258.395
Elapsed time for mlp_h_to_4h (4x18400x73600, b=2048): 0.0845
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18400x73600, b=2048): 262.697
Elapsed time for mlp_4h_to_h (4x73600x18400, b=2048): 0.0833
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73600x18400, b=2048): 266.425

Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 249.707
MLP duration (in seconds): 0.1677
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2615
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 265.292
b: 64, m: 2048, n: 1152, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1152x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1152x2048): 175.027
b: 64, m: 2048, n: 2048, k: 1152,
Elapsed time for attention_prob_times_values (64x2048x2048x1152): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1152): 168.043
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 259.826
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0836
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 266.403
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0830
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 268.348

Attention duration (in seconds): 0.0916
Attention throughput (in TFLOP/s): 256.622
MLP duration (in seconds): 0.1665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18464x55392, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18464x55392, b=2048): 259.303
b: 64, m: 2048, n: 1154, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1154x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1154x2048): 158.324
b: 64, m: 2048, n: 2048, k: 1154,
Elapsed time for attention_prob_times_values (64x2048x2048x1154): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1154): 127.531
Elapsed time for attention_linear_projection (4x18464x18464, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x18464x18464, b=2048): 258.988
Elapsed time for mlp_h_to_4h (4x18464x73856, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18464x73856, b=2048): 262.652
Elapsed time for mlp_4h_to_h (4x73856x18464, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73856x18464, b=2048): 265.302

Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 248.329
MLP duration (in seconds): 0.1693
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18496x55488, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18496x55488, b=2048): 264.651
b: 64, m: 2048, n: 1156, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1156x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1156x2048): 164.398
b: 64, m: 2048, n: 2048, k: 1156,
Elapsed time for attention_prob_times_values (64x2048x2048x1156): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1156): 148.315
Elapsed time for attention_linear_projection (4x18496x18496, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18496x18496, b=2048): 260.286
Elapsed time for mlp_h_to_4h (4x18496x73984, b=2048): 0.0848
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18496x73984, b=2048): 264.473
Elapsed time for mlp_4h_to_h (4x73984x18496, b=2048): 0.0844
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73984x18496, b=2048): 265.645

Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 254.340
MLP duration (in seconds): 0.1692
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2622
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18528x55584, b=2048): 0.0656
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18528x55584, b=2048): 257.231
b: 64, m: 2048, n: 1158, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1158x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1158x2048): 159.213
b: 64, m: 2048, n: 2048, k: 1158,
Elapsed time for attention_prob_times_values (64x2048x2048x1158): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1158): 128.080
Elapsed time for attention_linear_projection (4x18528x18528, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x18528x18528, b=2048): 257.927
Elapsed time for mlp_h_to_4h (4x18528x74112, b=2048): 0.0862
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18528x74112, b=2048): 260.914
Elapsed time for mlp_4h_to_h (4x74112x18528, b=2048): 0.0854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74112x18528, b=2048): 263.323

Attention duration (in seconds): 0.0962
Attention throughput (in TFLOP/s): 246.889
MLP duration (in seconds): 0.1717
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2678
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0643
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 263.300
b: 64, m: 2048, n: 1160, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1160x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1160x2048): 162.000
b: 64, m: 2048, n: 2048, k: 1160,
Elapsed time for attention_prob_times_values (64x2048x2048x1160): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1160): 162.960
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 258.847
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0854
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 264.313
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0853
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 264.616

Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 254.022
MLP duration (in seconds): 0.1707
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2645
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18592x55776, b=2048): 0.0668
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18592x55776, b=2048): 254.303
b: 64, m: 2048, n: 1162, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1162x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1162x2048): 158.947
b: 64, m: 2048, n: 2048, k: 1162,
Elapsed time for attention_prob_times_values (64x2048x2048x1162): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1162): 127.907
Elapsed time for attention_linear_projection (4x18592x18592, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x18592x18592, b=2048): 253.171
Elapsed time for mlp_h_to_4h (4x18592x74368, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18592x74368, b=2048): 257.050
Elapsed time for mlp_4h_to_h (4x74368x18592, b=2048): 0.0864
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74368x18592, b=2048): 262.275

Attention duration (in seconds): 0.0980
Attention throughput (in TFLOP/s): 243.933
MLP duration (in seconds): 0.1745
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2725
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18624x55872, b=2048): 0.0658
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18624x55872, b=2048): 259.100
b: 64, m: 2048, n: 1164, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1164x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1164x2048): 164.538
b: 64, m: 2048, n: 2048, k: 1164,
Elapsed time for attention_prob_times_values (64x2048x2048x1164): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1164): 148.043
Elapsed time for attention_linear_projection (4x18624x18624, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18624x18624, b=2048): 254.458
Elapsed time for mlp_h_to_4h (4x18624x74496, b=2048): 0.0874
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18624x74496, b=2048): 259.959
Elapsed time for mlp_4h_to_h (4x74496x18624, b=2048): 0.0861
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74496x18624, b=2048): 263.925

Attention duration (in seconds): 0.0962
Attention throughput (in TFLOP/s): 249.411
MLP duration (in seconds): 0.1736
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18656x55968, b=2048): 0.0671
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18656x55968, b=2048): 254.947
b: 64, m: 2048, n: 1166, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1166x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1166x2048): 159.951
b: 64, m: 2048, n: 2048, k: 1166,
Elapsed time for attention_prob_times_values (64x2048x2048x1166): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1166): 127.947
Elapsed time for attention_linear_projection (4x18656x18656, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x18656x18656, b=2048): 254.264
Elapsed time for mlp_h_to_4h (4x18656x74624, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18656x74624, b=2048): 257.955
Elapsed time for mlp_4h_to_h (4x74624x18656, b=2048): 0.0868
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74624x18656, b=2048): 262.896

Attention duration (in seconds): 0.0983
Attention throughput (in TFLOP/s): 244.691
MLP duration (in seconds): 0.1752
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2735
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 260.446
b: 64, m: 2048, n: 1168, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1168x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1168x2048): 164.053
b: 64, m: 2048, n: 2048, k: 1168,
Elapsed time for attention_prob_times_values (64x2048x2048x1168): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1168): 165.927
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 255.792
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0872
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 262.528
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0856
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 267.232

Attention duration (in seconds): 0.0959
Attention throughput (in TFLOP/s): 251.792
MLP duration (in seconds): 0.1728
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18720x56160, b=2048): 0.0666
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18720x56160, b=2048): 258.635
b: 64, m: 2048, n: 1170, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1170x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1170x2048): 160.236
b: 64, m: 2048, n: 2048, k: 1170,
Elapsed time for attention_prob_times_values (64x2048x2048x1170): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1170): 128.130
Elapsed time for attention_linear_projection (4x18720x18720, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x18720x18720, b=2048): 255.804
Elapsed time for mlp_h_to_4h (4x18720x74880, b=2048): 0.0878
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18720x74880, b=2048): 261.561
Elapsed time for mlp_4h_to_h (4x74880x18720, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74880x18720, b=2048): 265.285

Attention duration (in seconds): 0.0979
Attention throughput (in TFLOP/s): 247.507
MLP duration (in seconds): 0.1744
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2722
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18752x56256, b=2048): 0.0654
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18752x56256, b=2048): 264.200
b: 64, m: 2048, n: 1172, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1172x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1172x2048): 165.804
b: 64, m: 2048, n: 2048, k: 1172,
Elapsed time for attention_prob_times_values (64x2048x2048x1172): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1172): 149.018
Elapsed time for attention_linear_projection (4x18752x18752, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18752x18752, b=2048): 258.805
Elapsed time for mlp_h_to_4h (4x18752x75008, b=2048): 0.0870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18752x75008, b=2048): 264.911
Elapsed time for mlp_4h_to_h (4x75008x18752, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75008x18752, b=2048): 268.480

Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 253.961
MLP duration (in seconds): 0.1728
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2685
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18784x56352, b=2048): 0.1003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18784x56352, b=2048): 172.871
b: 64, m: 2048, n: 1174, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1174x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1174x2048): 160.258
b: 64, m: 2048, n: 2048, k: 1174,
Elapsed time for attention_prob_times_values (64x2048x2048x1174): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1174): 127.840
Elapsed time for attention_linear_projection (4x18784x18784, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x18784x18784, b=2048): 256.337
Elapsed time for mlp_h_to_4h (4x18784x75136, b=2048): 0.0883
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18784x75136, b=2048): 261.927
Elapsed time for mlp_4h_to_h (4x75136x18784, b=2048): 0.0871
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75136x18784, b=2048): 265.352

Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 185.097
MLP duration (in seconds): 0.1754
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 263.491
b: 64, m: 2048, n: 1176, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1176x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1176x2048): 164.551
b: 64, m: 2048, n: 2048, k: 1176,
Elapsed time for attention_prob_times_values (64x2048x2048x1176): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1176): 164.766
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 258.229
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0877
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 264.603
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0873
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 265.674

Attention duration (in seconds): 0.0962
Attention throughput (in TFLOP/s): 254.382
MLP duration (in seconds): 0.1750
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2712
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18848x56544, b=2048): 0.0681
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18848x56544, b=2048): 256.254
b: 64, m: 2048, n: 1178, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1178x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1178x2048): 160.950
b: 64, m: 2048, n: 2048, k: 1178,
Elapsed time for attention_prob_times_values (64x2048x2048x1178): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1178): 127.905
Elapsed time for attention_linear_projection (4x18848x18848, b=2048): 0.0227
Throughput (in TFLOP/s) for attention_linear_projection (4x18848x18848, b=2048): 256.900
Elapsed time for mlp_h_to_4h (4x18848x75392, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18848x75392, b=2048): 260.696
Elapsed time for mlp_4h_to_h (4x75392x18848, b=2048): 0.0885
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75392x18848, b=2048): 263.035

Attention duration (in seconds): 0.0997
Attention throughput (in TFLOP/s): 246.276
MLP duration (in seconds): 0.1778
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2775
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18880x56640, b=2048): 0.0668
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18880x56640, b=2048): 262.267
b: 64, m: 2048, n: 1180, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1180x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1180x2048): 166.926
b: 64, m: 2048, n: 2048, k: 1180,
Elapsed time for attention_prob_times_values (64x2048x2048x1180): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1180): 148.909
Elapsed time for attention_linear_projection (4x18880x18880, b=2048): 0.0227
Throughput (in TFLOP/s) for attention_linear_projection (4x18880x18880, b=2048): 257.702
Elapsed time for mlp_h_to_4h (4x18880x75520, b=2048): 0.0887
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18880x75520, b=2048): 263.244
Elapsed time for mlp_4h_to_h (4x75520x18880, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75520x18880, b=2048): 265.118

Attention duration (in seconds): 0.0975
Attention throughput (in TFLOP/s): 252.550
MLP duration (in seconds): 0.1769
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18912x56736, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18912x56736, b=2048): 256.889
b: 64, m: 2048, n: 1182, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1182x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1182x2048): 161.311
b: 64, m: 2048, n: 2048, k: 1182,
Elapsed time for attention_prob_times_values (64x2048x2048x1182): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1182): 127.951
Elapsed time for attention_linear_projection (4x18912x18912, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x18912x18912, b=2048): 257.030
Elapsed time for mlp_h_to_4h (4x18912x75648, b=2048): 0.0907
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18912x75648, b=2048): 258.516
Elapsed time for mlp_4h_to_h (4x75648x18912, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75648x18912, b=2048): 262.263

Attention duration (in seconds): 0.1001
Attention throughput (in TFLOP/s): 246.779
MLP duration (in seconds): 0.1800
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 263.368
b: 64, m: 2048, n: 1184, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1184x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1184x2048): 166.207
b: 64, m: 2048, n: 2048, k: 1184,
Elapsed time for attention_prob_times_values (64x2048x2048x1184): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1184): 167.323
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0227
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 259.022
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0889
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 264.430
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0891
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 263.959

Attention duration (in seconds): 0.0973
Attention throughput (in TFLOP/s): 254.785
MLP duration (in seconds): 0.1780
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18976x56928, b=2048): 0.0704
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18976x56928, b=2048): 251.539
b: 64, m: 2048, n: 1186, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1186x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1186x2048): 161.631
b: 64, m: 2048, n: 2048, k: 1186,
Elapsed time for attention_prob_times_values (64x2048x2048x1186): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1186): 128.649
Elapsed time for attention_linear_projection (4x18976x18976, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x18976x18976, b=2048): 253.235
Elapsed time for mlp_h_to_4h (4x18976x75904, b=2048): 0.0918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18976x75904, b=2048): 257.173
Elapsed time for mlp_4h_to_h (4x75904x18976, b=2048): 0.0897
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75904x18976, b=2048): 263.006

Attention duration (in seconds): 0.1025
Attention throughput (in TFLOP/s): 242.540
MLP duration (in seconds): 0.1815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19008x57024, b=2048): 0.0686
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19008x57024, b=2048): 258.821
b: 64, m: 2048, n: 1188, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1188x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1188x2048): 168.121
b: 64, m: 2048, n: 2048, k: 1188,
Elapsed time for attention_prob_times_values (64x2048x2048x1188): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1188): 149.641
Elapsed time for attention_linear_projection (4x19008x19008, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x19008x19008, b=2048): 254.815
Elapsed time for mlp_h_to_4h (4x19008x76032, b=2048): 0.0911
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19008x76032, b=2048): 259.865
Elapsed time for mlp_4h_to_h (4x76032x19008, b=2048): 0.0896
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76032x19008, b=2048): 264.332

Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 249.787
MLP duration (in seconds): 0.1807
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2806
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19040x57120, b=2048): 0.0721
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19040x57120, b=2048): 247.024
b: 64, m: 2048, n: 1190, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1190x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1190x2048): 162.206
b: 64, m: 2048, n: 2048, k: 1190,
Elapsed time for attention_prob_times_values (64x2048x2048x1190): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1190): 129.096
Elapsed time for attention_linear_projection (4x19040x19040, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x19040x19040, b=2048): 254.553
Elapsed time for mlp_h_to_4h (4x19040x76160, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19040x76160, b=2048): 256.149
Elapsed time for mlp_4h_to_h (4x76160x19040, b=2048): 0.0906
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76160x19040, b=2048): 262.336

Attention duration (in seconds): 0.1044
Attention throughput (in TFLOP/s): 239.914
MLP duration (in seconds): 0.1833
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0688
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 259.854
b: 64, m: 2048, n: 1192, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1192x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1192x2048): 166.129
b: 64, m: 2048, n: 2048, k: 1192,
Elapsed time for attention_prob_times_values (64x2048x2048x1192): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1192): 166.108
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 255.681
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 260.363
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0901
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 264.518

Attention duration (in seconds): 0.0998
Attention throughput (in TFLOP/s): 251.644
MLP duration (in seconds): 0.1817
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2815
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19104x57312, b=2048): 0.0706
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19104x57312, b=2048): 253.955
b: 64, m: 2048, n: 1194, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1194x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1194x2048): 162.860
b: 64, m: 2048, n: 2048, k: 1194,
Elapsed time for attention_prob_times_values (64x2048x2048x1194): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1194): 129.343
Elapsed time for attention_linear_projection (4x19104x19104, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_linear_projection (4x19104x19104, b=2048): 255.203
Elapsed time for mlp_h_to_4h (4x19104x76416, b=2048): 0.0926
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19104x76416, b=2048): 258.371
Elapsed time for mlp_4h_to_h (4x76416x19104, b=2048): 0.0907
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76416x19104, b=2048): 263.593

Attention duration (in seconds): 0.1030
Attention throughput (in TFLOP/s): 244.758
MLP duration (in seconds): 0.1833
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2863
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19136x57408, b=2048): 0.0692
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19136x57408, b=2048): 260.198
b: 64, m: 2048, n: 1196, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1196x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1196x2048): 169.030
b: 64, m: 2048, n: 2048, k: 1196,
Elapsed time for attention_prob_times_values (64x2048x2048x1196): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1196): 151.275
Elapsed time for attention_linear_projection (4x19136x19136, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_linear_projection (4x19136x19136, b=2048): 256.118
Elapsed time for mlp_h_to_4h (4x19136x76544, b=2048): 0.0918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19136x76544, b=2048): 261.288
Elapsed time for mlp_4h_to_h (4x76544x19136, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76544x19136, b=2048): 264.363

Attention duration (in seconds): 0.1006
Attention throughput (in TFLOP/s): 251.213
MLP duration (in seconds): 0.1826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2833
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19168x57504, b=2048): 0.0709
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19168x57504, b=2048): 254.644
b: 64, m: 2048, n: 1198, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1198x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1198x2048): 163.574
b: 64, m: 2048, n: 2048, k: 1198,
Elapsed time for attention_prob_times_values (64x2048x2048x1198): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1198): 129.707
Elapsed time for attention_linear_projection (4x19168x19168, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19168x19168, b=2048): 256.077
Elapsed time for mlp_h_to_4h (4x19168x76672, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19168x76672, b=2048): 259.467
Elapsed time for mlp_4h_to_h (4x76672x19168, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76672x19168, b=2048): 262.907

Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 245.508
MLP duration (in seconds): 0.1844
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0693
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 261.316
b: 64, m: 2048, n: 1200, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1200x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1200x2048): 168.087
b: 64, m: 2048, n: 2048, k: 1200,
Elapsed time for attention_prob_times_values (64x2048x2048x1200): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1200): 169.521
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 256.984
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 262.261
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0913
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 264.693

Attention duration (in seconds): 0.1005
Attention throughput (in TFLOP/s): 253.274
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19232x57696, b=2048): 0.0711
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19232x57696, b=2048): 255.524
b: 64, m: 2048, n: 1202, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1202x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1202x2048): 164.219
b: 64, m: 2048, n: 2048, k: 1202,
Elapsed time for attention_prob_times_values (64x2048x2048x1202): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1202): 130.234
Elapsed time for attention_linear_projection (4x19232x19232, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19232x19232, b=2048): 256.282
Elapsed time for mlp_h_to_4h (4x19232x76928, b=2048): 0.0931
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19232x76928, b=2048): 260.396
Elapsed time for mlp_4h_to_h (4x76928x19232, b=2048): 0.0924
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76928x19232, b=2048): 262.232

Attention duration (in seconds): 0.1037
Attention throughput (in TFLOP/s): 246.248
MLP duration (in seconds): 0.1855
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2892
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19264x57792, b=2048): 0.0696
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19264x57792, b=2048): 262.005
b: 64, m: 2048, n: 1204, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1204x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1204x2048): 170.460
b: 64, m: 2048, n: 2048, k: 1204,
Elapsed time for attention_prob_times_values (64x2048x2048x1204): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1204): 152.381
Elapsed time for attention_linear_projection (4x19264x19264, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19264x19264, b=2048): 258.002
Elapsed time for mlp_h_to_4h (4x19264x77056, b=2048): 0.0925
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19264x77056, b=2048): 263.007
Elapsed time for mlp_4h_to_h (4x77056x19264, b=2048): 0.0920
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77056x19264, b=2048): 264.217

Attention duration (in seconds): 0.1012
Attention throughput (in TFLOP/s): 253.049
MLP duration (in seconds): 0.1845
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2857
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19296x57888, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19296x57888, b=2048): 256.639
b: 64, m: 2048, n: 1206, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1206x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1206x2048): 164.327
b: 64, m: 2048, n: 2048, k: 1206,
Elapsed time for attention_prob_times_values (64x2048x2048x1206): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1206): 130.254
Elapsed time for attention_linear_projection (4x19296x19296, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x19296x19296, b=2048): 255.844
Elapsed time for mlp_h_to_4h (4x19296x77184, b=2048): 0.0939
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19296x77184, b=2048): 259.793
Elapsed time for mlp_4h_to_h (4x77184x19296, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77184x19296, b=2048): 262.904

Attention duration (in seconds): 0.1041
Attention throughput (in TFLOP/s): 246.925
MLP duration (in seconds): 0.1867
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2908
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 262.652
b: 64, m: 2048, n: 1208, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1208x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1208x2048): 168.505
b: 64, m: 2048, n: 2048, k: 1208,
Elapsed time for attention_prob_times_values (64x2048x2048x1208): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1208): 167.777
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 258.842
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0927
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 264.015
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0925
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 264.724

Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 254.563
MLP duration (in seconds): 0.1852
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2865
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19360x58080, b=2048): 0.0726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19360x58080, b=2048): 253.733
b: 64, m: 2048, n: 1210, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1210x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1210x2048): 164.753
b: 64, m: 2048, n: 2048, k: 1210,
Elapsed time for attention_prob_times_values (64x2048x2048x1210): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1210): 130.498
Elapsed time for attention_linear_projection (4x19360x19360, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19360x19360, b=2048): 253.853
Elapsed time for mlp_h_to_4h (4x19360x77440, b=2048): 0.0964
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19360x77440, b=2048): 254.753
Elapsed time for mlp_4h_to_h (4x77440x19360, b=2048): 0.0935
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77440x19360, b=2048): 262.603

Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 244.639
MLP duration (in seconds): 0.1900
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2957
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19392x58176, b=2048): 0.0714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19392x58176, b=2048): 259.020
b: 64, m: 2048, n: 1212, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1212x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1212x2048): 171.356
b: 64, m: 2048, n: 2048, k: 1212,
Elapsed time for attention_prob_times_values (64x2048x2048x1212): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1212): 152.793
Elapsed time for attention_linear_projection (4x19392x19392, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19392x19392, b=2048): 254.560
Elapsed time for mlp_h_to_4h (4x19392x77568, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19392x77568, b=2048): 259.968
Elapsed time for mlp_4h_to_h (4x77568x19392, b=2048): 0.0933
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77568x19392, b=2048): 264.266

Attention duration (in seconds): 0.1036
Attention throughput (in TFLOP/s): 250.400
MLP duration (in seconds): 0.1881
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2917
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19424x58272, b=2048): 0.0730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19424x58272, b=2048): 253.992
b: 64, m: 2048, n: 1214, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1214x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1214x2048): 165.849
b: 64, m: 2048, n: 2048, k: 1214,
Elapsed time for attention_prob_times_values (64x2048x2048x1214): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1214): 131.554
Elapsed time for attention_linear_projection (4x19424x19424, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19424x19424, b=2048): 253.530
Elapsed time for mlp_h_to_4h (4x19424x77696, b=2048): 0.0958
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19424x77696, b=2048): 258.045
Elapsed time for mlp_4h_to_h (4x77696x19424, b=2048): 0.0940
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77696x19424, b=2048): 263.148

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 244.920
MLP duration (in seconds): 0.1898
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2961
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0715
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 260.199
b: 64, m: 2048, n: 1216, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1216x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1216x2048): 171.094
b: 64, m: 2048, n: 2048, k: 1216,
Elapsed time for attention_prob_times_values (64x2048x2048x1216): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1216): 171.813
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 255.895
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0950
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 261.160
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 264.617

Attention duration (in seconds): 0.1034
Attention throughput (in TFLOP/s): 252.651
MLP duration (in seconds): 0.1887
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2921
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19488x58464, b=2048): 0.0734
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19488x58464, b=2048): 254.478
b: 64, m: 2048, n: 1218, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1218x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1218x2048): 166.012
b: 64, m: 2048, n: 2048, k: 1218,
Elapsed time for attention_prob_times_values (64x2048x2048x1218): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1218): 128.638
Elapsed time for attention_linear_projection (4x19488x19488, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19488x19488, b=2048): 254.942
Elapsed time for mlp_h_to_4h (4x19488x77952, b=2048): 0.0997
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19488x77952, b=2048): 249.552
Elapsed time for mlp_4h_to_h (4x77952x19488, b=2048): 0.0939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77952x19488, b=2048): 265.018

Attention duration (in seconds): 0.1068
Attention throughput (in TFLOP/s): 245.330
MLP duration (in seconds): 0.1937
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3004
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19520x58560, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19520x58560, b=2048): 262.817
b: 64, m: 2048, n: 1220, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1220x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1220x2048): 172.617
b: 64, m: 2048, n: 2048, k: 1220,
Elapsed time for attention_prob_times_values (64x2048x2048x1220): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1220): 150.465
Elapsed time for attention_linear_projection (4x19520x19520, b=2048): 0.0241
Throughput (in TFLOP/s) for attention_linear_projection (4x19520x19520, b=2048): 258.873
Elapsed time for mlp_h_to_4h (4x19520x78080, b=2048): 0.0946
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19520x78080, b=2048): 264.095
Elapsed time for mlp_4h_to_h (4x78080x19520, b=2048): 0.0929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78080x19520, b=2048): 268.888

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 253.868
MLP duration (in seconds): 0.1874
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2909
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19552x58656, b=2048): 0.0726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19552x58656, b=2048): 258.661
b: 64, m: 2048, n: 1222, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1222x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1222x2048): 166.326
b: 64, m: 2048, n: 2048, k: 1222,
Elapsed time for attention_prob_times_values (64x2048x2048x1222): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1222): 128.195
Elapsed time for attention_linear_projection (4x19552x19552, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19552x19552, b=2048): 256.782
Elapsed time for mlp_h_to_4h (4x19552x78208, b=2048): 0.1005
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19552x78208, b=2048): 249.323
Elapsed time for mlp_4h_to_h (4x78208x19552, b=2048): 0.0944
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78208x19552, b=2048): 265.429

Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 248.503
MLP duration (in seconds): 0.1949
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 264.192
b: 64, m: 2048, n: 1224, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1224x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1224x2048): 171.065
b: 64, m: 2048, n: 2048, k: 1224,
Elapsed time for attention_prob_times_values (64x2048x2048x1224): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1224): 167.063
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 259.639
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 265.032
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0942
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 266.706

Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 255.966
MLP duration (in seconds): 0.1891
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2924
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19616x58848, b=2048): 0.0734
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19616x58848, b=2048): 257.572
b: 64, m: 2048, n: 1226, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1226x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1226x2048): 166.961
b: 64, m: 2048, n: 2048, k: 1226,
Elapsed time for attention_prob_times_values (64x2048x2048x1226): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1226): 128.286
Elapsed time for attention_linear_projection (4x19616x19616, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x19616x19616, b=2048): 256.919
Elapsed time for mlp_h_to_4h (4x19616x78464, b=2048): 0.1181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19616x78464, b=2048): 213.585
Elapsed time for mlp_4h_to_h (4x78464x19616, b=2048): 0.0952
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78464x19616, b=2048): 264.866

Attention duration (in seconds): 0.1070
Attention throughput (in TFLOP/s): 247.888
MLP duration (in seconds): 0.2133
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19648x58944, b=2048): 0.0722
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19648x58944, b=2048): 262.746
b: 64, m: 2048, n: 1228, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1228x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1228x2048): 173.367
b: 64, m: 2048, n: 2048, k: 1228,
Elapsed time for attention_prob_times_values (64x2048x2048x1228): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1228): 150.291
Elapsed time for attention_linear_projection (4x19648x19648, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19648x19648, b=2048): 258.966
Elapsed time for mlp_h_to_4h (4x19648x78592, b=2048): 0.0960
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19648x78592, b=2048): 263.566
Elapsed time for mlp_4h_to_h (4x78592x19648, b=2048): 0.0949
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78592x19648, b=2048): 266.604

Attention duration (in seconds): 0.1048
Attention throughput (in TFLOP/s): 253.917
MLP duration (in seconds): 0.1909
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2957
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19680x59040, b=2048): 0.0738
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19680x59040, b=2048): 258.000
b: 64, m: 2048, n: 1230, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1230x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1230x2048): 167.557
b: 64, m: 2048, n: 2048, k: 1230,
Elapsed time for attention_prob_times_values (64x2048x2048x1230): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1230): 128.121
Elapsed time for attention_linear_projection (4x19680x19680, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19680x19680, b=2048): 257.665
Elapsed time for mlp_h_to_4h (4x19680x78720, b=2048): 0.0974
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19680x78720, b=2048): 260.647
Elapsed time for mlp_4h_to_h (4x78720x19680, b=2048): 0.0962
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78720x19680, b=2048): 263.792

Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 248.381
MLP duration (in seconds): 0.1936
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0721
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 264.722
b: 64, m: 2048, n: 1232, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1232x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1232x2048): 172.945
b: 64, m: 2048, n: 2048, k: 1232,
Elapsed time for attention_prob_times_values (64x2048x2048x1232): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1232): 169.180
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 261.519
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 266.321
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 268.749

Attention duration (in seconds): 0.1042
Attention throughput (in TFLOP/s): 257.022
MLP duration (in seconds): 0.1904
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2946
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19744x59232, b=2048): 0.0739
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19744x59232, b=2048): 259.412
b: 64, m: 2048, n: 1234, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1234x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1234x2048): 168.193
b: 64, m: 2048, n: 2048, k: 1234,
Elapsed time for attention_prob_times_values (64x2048x2048x1234): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1234): 128.794
Elapsed time for attention_linear_projection (4x19744x19744, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x19744x19744, b=2048): 258.832
Elapsed time for mlp_h_to_4h (4x19744x78976, b=2048): 0.0975
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19744x78976, b=2048): 261.945
Elapsed time for mlp_4h_to_h (4x78976x19744, b=2048): 0.0957
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78976x19744, b=2048): 267.032

Attention duration (in seconds): 0.1076
Attention throughput (in TFLOP/s): 249.697
MLP duration (in seconds): 0.1932
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19776x59328, b=2048): 0.0726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19776x59328, b=2048): 264.613
b: 64, m: 2048, n: 1236, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1236x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1236x2048): 174.628
b: 64, m: 2048, n: 2048, k: 1236,
Elapsed time for attention_prob_times_values (64x2048x2048x1236): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1236): 151.082
Elapsed time for attention_linear_projection (4x19776x19776, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x19776x19776, b=2048): 262.047
Elapsed time for mlp_h_to_4h (4x19776x79104, b=2048): 0.0965
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19776x79104, b=2048): 265.600
Elapsed time for mlp_4h_to_h (4x79104x19776, b=2048): 0.0954
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79104x19776, b=2048): 268.601

Attention duration (in seconds): 0.1053
Attention throughput (in TFLOP/s): 256.034
MLP duration (in seconds): 0.1919
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2972
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19808x59424, b=2048): 0.0801
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19808x59424, b=2048): 240.906
b: 64, m: 2048, n: 1238, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1238x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1238x2048): 168.148
b: 64, m: 2048, n: 2048, k: 1238,
Elapsed time for attention_prob_times_values (64x2048x2048x1238): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1238): 128.615
Elapsed time for attention_linear_projection (4x19808x19808, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19808x19808, b=2048): 259.388
Elapsed time for mlp_h_to_4h (4x19808x79232, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19808x79232, b=2048): 262.570
Elapsed time for mlp_4h_to_h (4x79232x19808, b=2048): 0.0973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79232x19808, b=2048): 264.262

Attention duration (in seconds): 0.1140
Attention throughput (in TFLOP/s): 237.310
MLP duration (in seconds): 0.1952
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 265.218
b: 64, m: 2048, n: 1240, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1240x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1240x2048): 173.087
b: 64, m: 2048, n: 2048, k: 1240,
Elapsed time for attention_prob_times_values (64x2048x2048x1240): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1240): 169.586
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 260.783
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.0973
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 264.994
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.0972
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 265.384

Attention duration (in seconds): 0.1055
Attention throughput (in TFLOP/s): 257.257
MLP duration (in seconds): 0.1946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19872x59616, b=2048): 0.0974
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19872x59616, b=2048): 199.334
b: 64, m: 2048, n: 1242, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1242x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1242x2048): 168.437
b: 64, m: 2048, n: 2048, k: 1242,
Elapsed time for attention_prob_times_values (64x2048x2048x1242): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1242): 127.898
Elapsed time for attention_linear_projection (4x19872x19872, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x19872x19872, b=2048): 255.007
Elapsed time for mlp_h_to_4h (4x19872x79488, b=2048): 0.1363
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19872x79488, b=2048): 189.840
Elapsed time for mlp_4h_to_h (4x79488x19872, b=2048): 0.0983
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79488x19872, b=2048): 263.338

Attention duration (in seconds): 0.1319
Attention throughput (in TFLOP/s): 206.291
MLP duration (in seconds): 0.2346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3665
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19904x59712, b=2048): 0.0747
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19904x59712, b=2048): 260.701
b: 64, m: 2048, n: 1244, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1244x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1244x2048): 175.803
b: 64, m: 2048, n: 2048, k: 1244,
Elapsed time for attention_prob_times_values (64x2048x2048x1244): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1244): 150.192
Elapsed time for attention_linear_projection (4x19904x19904, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_linear_projection (4x19904x19904, b=2048): 256.226
Elapsed time for mlp_h_to_4h (4x19904x79616, b=2048): 0.0994
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19904x79616, b=2048): 261.103
Elapsed time for mlp_4h_to_h (4x79616x19904, b=2048): 0.0982
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79616x19904, b=2048): 264.299

Attention duration (in seconds): 0.1083
Attention throughput (in TFLOP/s): 252.136
MLP duration (in seconds): 0.1977
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19936x59808, b=2048): 0.0963
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19936x59808, b=2048): 202.873
b: 64, m: 2048, n: 1246, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1246x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1246x2048): 169.123
b: 64, m: 2048, n: 2048, k: 1246,
Elapsed time for attention_prob_times_values (64x2048x2048x1246): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1246): 129.649
Elapsed time for attention_linear_projection (4x19936x19936, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x19936x19936, b=2048): 255.914
Elapsed time for mlp_h_to_4h (4x19936x79744, b=2048): 0.1546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19936x79744, b=2048): 168.489
Elapsed time for mlp_4h_to_h (4x79744x19936, b=2048): 0.0982
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79744x19936, b=2048): 265.262

Attention duration (in seconds): 0.1309
Attention throughput (in TFLOP/s): 209.279
MLP duration (in seconds): 0.2528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3836
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0743
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 263.625
b: 64, m: 2048, n: 1248, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1248x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1248x2048): 174.810
b: 64, m: 2048, n: 2048, k: 1248,
Elapsed time for attention_prob_times_values (64x2048x2048x1248): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1248): 170.649
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 259.693
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 264.666
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 269.515

Attention duration (in seconds): 0.1073
Attention throughput (in TFLOP/s): 256.125
MLP duration (in seconds): 0.1957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20000x60000, b=2048): 0.0925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20000x60000, b=2048): 212.601
b: 64, m: 2048, n: 1250, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1250x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1250x2048): 170.405
b: 64, m: 2048, n: 2048, k: 1250,
Elapsed time for attention_prob_times_values (64x2048x2048x1250): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1250): 129.862
Elapsed time for attention_linear_projection (4x20000x20000, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x20000x20000, b=2048): 256.859
Elapsed time for mlp_h_to_4h (4x20000x80000, b=2048): 0.1619
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20000x80000, b=2048): 161.920
Elapsed time for mlp_4h_to_h (4x80000x20000, b=2048): 0.0986
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80000x20000, b=2048): 265.996

Attention duration (in seconds): 0.1271
Attention throughput (in TFLOP/s): 216.814
MLP duration (in seconds): 0.2604
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3875
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20032x60096, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20032x60096, b=2048): 263.332
b: 64, m: 2048, n: 1252, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1252x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1252x2048): 176.656
b: 64, m: 2048, n: 2048, k: 1252,
Elapsed time for attention_prob_times_values (64x2048x2048x1252): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1252): 152.228
Elapsed time for attention_linear_projection (4x20032x20032, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x20032x20032, b=2048): 258.530
Elapsed time for mlp_h_to_4h (4x20032x80128, b=2048): 0.0994
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20032x80128, b=2048): 264.686
Elapsed time for mlp_4h_to_h (4x80128x20032, b=2048): 0.0988
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80128x20032, b=2048): 266.108

Attention duration (in seconds): 0.1086
Attention throughput (in TFLOP/s): 254.650
MLP duration (in seconds): 0.1982
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20064x60192, b=2048): 0.0827
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20064x60192, b=2048): 239.174
b: 64, m: 2048, n: 1254, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1254x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1254x2048): 170.342
b: 64, m: 2048, n: 2048, k: 1254,
Elapsed time for attention_prob_times_values (64x2048x2048x1254): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1254): 130.560
Elapsed time for attention_linear_projection (4x20064x20064, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x20064x20064, b=2048): 257.522
Elapsed time for mlp_h_to_4h (4x20064x80256, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20064x80256, b=2048): 198.061
Elapsed time for mlp_4h_to_h (4x80256x20064, b=2048): 0.0999
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80256x20064, b=2048): 263.992

Attention duration (in seconds): 0.1175
Attention throughput (in TFLOP/s): 236.090
MLP duration (in seconds): 0.2331
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3506
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0756
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 262.737
b: 64, m: 2048, n: 1256, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1256x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1256x2048): 175.016
b: 64, m: 2048, n: 2048, k: 1256,
Elapsed time for attention_prob_times_values (64x2048x2048x1256): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1256): 170.706
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 258.498
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1007
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 262.807
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.0999
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 264.916

Attention duration (in seconds): 0.1090
Attention throughput (in TFLOP/s): 255.302
MLP duration (in seconds): 0.2006
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20128x60384, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20128x60384, b=2048): 220.275
b: 64, m: 2048, n: 1258, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1258x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1258x2048): 170.762
b: 64, m: 2048, n: 2048, k: 1258,
Elapsed time for attention_prob_times_values (64x2048x2048x1258): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1258): 131.092
Elapsed time for attention_linear_projection (4x20128x20128, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_linear_projection (4x20128x20128, b=2048): 256.545
Elapsed time for mlp_h_to_4h (4x20128x80512, b=2048): 0.1333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20128x80512, b=2048): 199.248
Elapsed time for mlp_4h_to_h (4x80512x20128, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80512x20128, b=2048): 262.598

Attention duration (in seconds): 0.1254
Attention throughput (in TFLOP/s): 222.533
MLP duration (in seconds): 0.2344
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20160x60480, b=2048): 0.0761
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20160x60480, b=2048): 262.561
b: 64, m: 2048, n: 1260, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1260x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1260x2048): 178.366
b: 64, m: 2048, n: 2048, k: 1260,
Elapsed time for attention_prob_times_values (64x2048x2048x1260): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1260): 152.623
Elapsed time for attention_linear_projection (4x20160x20160, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_linear_projection (4x20160x20160, b=2048): 258.864
Elapsed time for mlp_h_to_4h (4x20160x80640, b=2048): 0.1012
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20160x80640, b=2048): 263.073
Elapsed time for mlp_4h_to_h (4x80640x20160, b=2048): 0.1005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80640x20160, b=2048): 265.128

Attention duration (in seconds): 0.1100
Attention throughput (in TFLOP/s): 254.366
MLP duration (in seconds): 0.2017
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20192x60576, b=2048): 0.0970
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20192x60576, b=2048): 206.559
b: 64, m: 2048, n: 1262, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1262x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1262x2048): 171.833
b: 64, m: 2048, n: 2048, k: 1262,
Elapsed time for attention_prob_times_values (64x2048x2048x1262): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1262): 131.857
Elapsed time for attention_linear_projection (4x20192x20192, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_linear_projection (4x20192x20192, b=2048): 258.065
Elapsed time for mlp_h_to_4h (4x20192x80768, b=2048): 0.1050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20192x80768, b=2048): 254.376
Elapsed time for mlp_4h_to_h (4x80768x20192, b=2048): 0.1014
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80768x20192, b=2048): 263.431

Attention duration (in seconds): 0.1320
Attention throughput (in TFLOP/s): 212.714
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3385
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0756
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 266.035
b: 64, m: 2048, n: 1264, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1264x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1264x2048): 177.305
b: 64, m: 2048, n: 2048, k: 1264,
Elapsed time for attention_prob_times_values (64x2048x2048x1264): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1264): 172.481
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 261.448
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1004
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 266.964
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.0997
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 268.904

Attention duration (in seconds): 0.1090
Attention throughput (in TFLOP/s): 258.461
MLP duration (in seconds): 0.2001
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20256x60768, b=2048): 0.0785
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20256x60768, b=2048): 256.928
b: 64, m: 2048, n: 1266, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1266x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1266x2048): 171.920
b: 64, m: 2048, n: 2048, k: 1266,
Elapsed time for attention_prob_times_values (64x2048x2048x1266): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1266): 131.755
Elapsed time for attention_linear_projection (4x20256x20256, b=2048): 0.0261
Throughput (in TFLOP/s) for attention_linear_projection (4x20256x20256, b=2048): 257.180
Elapsed time for mlp_h_to_4h (4x20256x81024, b=2048): 0.1164
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20256x81024, b=2048): 230.964
Elapsed time for mlp_4h_to_h (4x81024x20256, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81024x20256, b=2048): 266.126

Attention duration (in seconds): 0.1137
Attention throughput (in TFLOP/s): 248.355
MLP duration (in seconds): 0.2175
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20288x60864, b=2048): 0.0770
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20288x60864, b=2048): 262.648
b: 64, m: 2048, n: 1268, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1268x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1268x2048): 178.981
b: 64, m: 2048, n: 2048, k: 1268,
Elapsed time for attention_prob_times_values (64x2048x2048x1268): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1268): 152.444
Elapsed time for attention_linear_projection (4x20288x20288, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x20288x20288, b=2048): 258.919
Elapsed time for mlp_h_to_4h (4x20288x81152, b=2048): 0.1021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20288x81152, b=2048): 264.166
Elapsed time for mlp_4h_to_h (4x81152x20288, b=2048): 0.1001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81152x20288, b=2048): 269.593

Attention duration (in seconds): 0.1113
Attention throughput (in TFLOP/s): 254.498
MLP duration (in seconds): 0.2022
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20320x60960, b=2048): 0.0788
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20320x60960, b=2048): 257.509
b: 64, m: 2048, n: 1270, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1270x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1270x2048): 173.383
b: 64, m: 2048, n: 2048, k: 1270,
Elapsed time for attention_prob_times_values (64x2048x2048x1270): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1270): 131.569
Elapsed time for attention_linear_projection (4x20320x20320, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x20320x20320, b=2048): 257.313
Elapsed time for mlp_h_to_4h (4x20320x81280, b=2048): 0.1556
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20320x81280, b=2048): 173.929
Elapsed time for mlp_4h_to_h (4x81280x20320, b=2048): 0.1020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81280x20320, b=2048): 265.325

Attention duration (in seconds): 0.1142
Attention throughput (in TFLOP/s): 248.854
MLP duration (in seconds): 0.2576
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3718
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0773
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 263.232
b: 64, m: 2048, n: 1272, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1272x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1272x2048): 177.125
b: 64, m: 2048, n: 2048, k: 1272,
Elapsed time for attention_prob_times_values (64x2048x2048x1272): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1272): 171.207
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 259.043
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 264.188
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1012
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 268.300

Attention duration (in seconds): 0.1114
Attention throughput (in TFLOP/s): 255.971
MLP duration (in seconds): 0.2039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20384x61152, b=2048): 0.1108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20384x61152, b=2048): 184.403
b: 64, m: 2048, n: 1274, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1274x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1274x2048): 172.652
b: 64, m: 2048, n: 2048, k: 1274,
Elapsed time for attention_prob_times_values (64x2048x2048x1274): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1274): 132.600
Elapsed time for attention_linear_projection (4x20384x20384, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20384x20384, b=2048): 256.432
Elapsed time for mlp_h_to_4h (4x20384x81536, b=2048): 0.1472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20384x81536, b=2048): 185.000
Elapsed time for mlp_4h_to_h (4x81536x20384, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81536x20384, b=2048): 263.985

Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 195.320
MLP duration (in seconds): 0.2503
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3968
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20416x61248, b=2048): 0.0784
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20416x61248, b=2048): 261.275
b: 64, m: 2048, n: 1276, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1276x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1276x2048): 179.446
b: 64, m: 2048, n: 2048, k: 1276,
Elapsed time for attention_prob_times_values (64x2048x2048x1276): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1276): 153.940
Elapsed time for attention_linear_projection (4x20416x20416, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20416x20416, b=2048): 257.439
Elapsed time for mlp_h_to_4h (4x20416x81664, b=2048): 0.1042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20416x81664, b=2048): 262.052
Elapsed time for mlp_4h_to_h (4x81664x20416, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81664x20416, b=2048): 264.688

Attention duration (in seconds): 0.1132
Attention throughput (in TFLOP/s): 253.397
MLP duration (in seconds): 0.2074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20448x61344, b=2048): 0.0919
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20448x61344, b=2048): 223.647
b: 64, m: 2048, n: 1278, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1278x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1278x2048): 173.738
b: 64, m: 2048, n: 2048, k: 1278,
Elapsed time for attention_prob_times_values (64x2048x2048x1278): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1278): 132.447
Elapsed time for attention_linear_projection (4x20448x20448, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20448x20448, b=2048): 256.797
Elapsed time for mlp_h_to_4h (4x20448x81792, b=2048): 0.1190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20448x81792, b=2048): 230.300
Elapsed time for mlp_4h_to_h (4x81792x20448, b=2048): 0.1041
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81792x20448, b=2048): 263.165

Attention duration (in seconds): 0.1277
Attention throughput (in TFLOP/s): 225.329
MLP duration (in seconds): 0.2231
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0788
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 261.679
b: 64, m: 2048, n: 1280, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1280x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1280x2048): 179.684
b: 64, m: 2048, n: 2048, k: 1280,
Elapsed time for attention_prob_times_values (64x2048x2048x1280): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1280): 174.674
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 258.718
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 263.528
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1034
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 265.713

Attention duration (in seconds): 0.1131
Attention throughput (in TFLOP/s): 255.185
MLP duration (in seconds): 0.2078
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20512x61536, b=2048): 0.1199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20512x61536, b=2048): 172.486
b: 64, m: 2048, n: 1282, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1282x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1282x2048): 163.725
b: 64, m: 2048, n: 2048, k: 1282,
Elapsed time for attention_prob_times_values (64x2048x2048x1282): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1282): 129.896
Elapsed time for attention_linear_projection (4x20512x20512, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20512x20512, b=2048): 258.398
Elapsed time for mlp_h_to_4h (4x20512x82048, b=2048): 0.1469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20512x82048, b=2048): 187.754
Elapsed time for mlp_4h_to_h (4x82048x20512, b=2048): 0.1045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82048x20512, b=2048): 263.795

Attention duration (in seconds): 0.1561
Attention throughput (in TFLOP/s): 185.489
MLP duration (in seconds): 0.2514
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20544x61632, b=2048): 0.0791
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20544x61632, b=2048): 262.332
b: 64, m: 2048, n: 1284, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1284x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1284x2048): 170.861
b: 64, m: 2048, n: 2048, k: 1284,
Elapsed time for attention_prob_times_values (64x2048x2048x1284): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1284): 150.730
Elapsed time for attention_linear_projection (4x20544x20544, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20544x20544, b=2048): 259.365
Elapsed time for mlp_h_to_4h (4x20544x82176, b=2048): 0.1050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20544x82176, b=2048): 263.410
Elapsed time for mlp_4h_to_h (4x82176x20544, b=2048): 0.1043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82176x20544, b=2048): 265.178

Attention duration (in seconds): 0.1143
Attention throughput (in TFLOP/s): 253.949
MLP duration (in seconds): 0.2093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20576x61728, b=2048): 0.1172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20576x61728, b=2048): 177.513
b: 64, m: 2048, n: 1286, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1286x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1286x2048): 164.236
b: 64, m: 2048, n: 2048, k: 1286,
Elapsed time for attention_prob_times_values (64x2048x2048x1286): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1286): 129.921
Elapsed time for attention_linear_projection (4x20576x20576, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_linear_projection (4x20576x20576, b=2048): 257.904
Elapsed time for mlp_h_to_4h (4x20576x82304, b=2048): 0.1105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20576x82304, b=2048): 251.104
Elapsed time for mlp_4h_to_h (4x82304x20576, b=2048): 0.1055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82304x20576, b=2048): 263.110

Attention duration (in seconds): 0.1536
Attention throughput (in TFLOP/s): 189.577
MLP duration (in seconds): 0.2160
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 263.306
b: 64, m: 2048, n: 1288, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1288x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1288x2048): 168.643
b: 64, m: 2048, n: 2048, k: 1288,
Elapsed time for attention_prob_times_values (64x2048x2048x1288): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1288): 170.858
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 260.227
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 264.239
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 265.732

Attention duration (in seconds): 0.1142
Attention throughput (in TFLOP/s): 255.908
MLP duration (in seconds): 0.2101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20640x61920, b=2048): 0.1212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20640x61920, b=2048): 172.797
b: 64, m: 2048, n: 1290, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1290x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1290x2048): 165.028
b: 64, m: 2048, n: 2048, k: 1290,
Elapsed time for attention_prob_times_values (64x2048x2048x1290): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1290): 127.842
Elapsed time for attention_linear_projection (4x20640x20640, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_linear_projection (4x20640x20640, b=2048): 260.076
Elapsed time for mlp_h_to_4h (4x20640x82560, b=2048): 0.1331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20640x82560, b=2048): 209.690
Elapsed time for mlp_4h_to_h (4x82560x20640, b=2048): 0.1059
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82560x20640, b=2048): 263.681

Attention duration (in seconds): 0.1576
Attention throughput (in TFLOP/s): 185.905
MLP duration (in seconds): 0.2390
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20672x62016, b=2048): 0.0798
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20672x62016, b=2048): 263.234
b: 64, m: 2048, n: 1292, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1292x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1292x2048): 171.956
b: 64, m: 2048, n: 2048, k: 1292,
Elapsed time for attention_prob_times_values (64x2048x2048x1292): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1292): 151.662
Elapsed time for attention_linear_projection (4x20672x20672, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_linear_projection (4x20672x20672, b=2048): 261.249
Elapsed time for mlp_h_to_4h (4x20672x82688, b=2048): 0.1060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20672x82688, b=2048): 264.179
Elapsed time for mlp_4h_to_h (4x82688x20672, b=2048): 0.1054
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82688x20672, b=2048): 265.695

Attention duration (in seconds): 0.1152
Attention throughput (in TFLOP/s): 255.146
MLP duration (in seconds): 0.2114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20704x62112, b=2048): 0.1331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20704x62112, b=2048): 158.262
b: 64, m: 2048, n: 1294, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1294x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1294x2048): 165.277
b: 64, m: 2048, n: 2048, k: 1294,
Elapsed time for attention_prob_times_values (64x2048x2048x1294): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1294): 129.723
Elapsed time for attention_linear_projection (4x20704x20704, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x20704x20704, b=2048): 260.504
Elapsed time for mlp_h_to_4h (4x20704x82816, b=2048): 0.1198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20704x82816, b=2048): 234.559
Elapsed time for mlp_4h_to_h (4x82816x20704, b=2048): 0.1066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82816x20704, b=2048): 263.509

Attention duration (in seconds): 0.1696
Attention throughput (in TFLOP/s): 173.783
MLP duration (in seconds): 0.2264
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3960
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0800
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 264.175
b: 64, m: 2048, n: 1296, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1296x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1296x2048): 170.613
b: 64, m: 2048, n: 2048, k: 1296,
Elapsed time for attention_prob_times_values (64x2048x2048x1296): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1296): 175.002
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 261.828
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 264.927
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1059
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 266.003

Attention duration (in seconds): 0.1150
Attention throughput (in TFLOP/s): 257.223
MLP duration (in seconds): 0.2123
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20768x62304, b=2048): 0.1231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20768x62304, b=2048): 172.188
b: 64, m: 2048, n: 1298, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1298x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1298x2048): 165.948
b: 64, m: 2048, n: 2048, k: 1298,
Elapsed time for attention_prob_times_values (64x2048x2048x1298): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1298): 130.513
Elapsed time for attention_linear_projection (4x20768x20768, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20768x20768, b=2048): 256.485
Elapsed time for mlp_h_to_4h (4x20768x83072, b=2048): 0.1180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20768x83072, b=2048): 239.559
Elapsed time for mlp_4h_to_h (4x83072x20768, b=2048): 0.1074
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83072x20768, b=2048): 263.194

Attention duration (in seconds): 0.1602
Attention throughput (in TFLOP/s): 185.132
MLP duration (in seconds): 0.2254
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3856
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20800x62400, b=2048): 0.0814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20800x62400, b=2048): 261.189
b: 64, m: 2048, n: 1300, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1300x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1300x2048): 173.431
b: 64, m: 2048, n: 2048, k: 1300,
Elapsed time for attention_prob_times_values (64x2048x2048x1300): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1300): 152.992
Elapsed time for attention_linear_projection (4x20800x20800, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_linear_projection (4x20800x20800, b=2048): 257.811
Elapsed time for mlp_h_to_4h (4x20800x83200, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20800x83200, b=2048): 261.468
Elapsed time for mlp_4h_to_h (4x83200x20800, b=2048): 0.1068
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83200x20800, b=2048): 265.385

Attention duration (in seconds): 0.1175
Attention throughput (in TFLOP/s): 253.192
MLP duration (in seconds): 0.2153
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3328
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20832x62496, b=2048): 0.1260
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20832x62496, b=2048): 169.297
b: 64, m: 2048, n: 1302, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1302x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1302x2048): 166.762
b: 64, m: 2048, n: 2048, k: 1302,
Elapsed time for attention_prob_times_values (64x2048x2048x1302): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1302): 130.345
Elapsed time for attention_linear_projection (4x20832x20832, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20832x20832, b=2048): 257.555
Elapsed time for mlp_h_to_4h (4x20832x83328, b=2048): 0.1281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20832x83328, b=2048): 222.014
Elapsed time for mlp_4h_to_h (4x83328x20832, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83328x20832, b=2048): 263.507

Attention duration (in seconds): 0.1632
Attention throughput (in TFLOP/s): 182.885
MLP duration (in seconds): 0.2360
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3992
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0816
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 262.154
b: 64, m: 2048, n: 1304, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1304x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1304x2048): 171.526
b: 64, m: 2048, n: 2048, k: 1304,
Elapsed time for attention_prob_times_values (64x2048x2048x1304): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1304): 173.072
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 258.287
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1087
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 262.515
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1077
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 264.925

Attention duration (in seconds): 0.1174
Attention throughput (in TFLOP/s): 255.022
MLP duration (in seconds): 0.2164
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20896x62688, b=2048): 0.1255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20896x62688, b=2048): 170.987
b: 64, m: 2048, n: 1306, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1306x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1306x2048): 166.649
b: 64, m: 2048, n: 2048, k: 1306,
Elapsed time for attention_prob_times_values (64x2048x2048x1306): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1306): 131.300
Elapsed time for attention_linear_projection (4x20896x20896, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20896x20896, b=2048): 257.991
Elapsed time for mlp_h_to_4h (4x20896x83584, b=2048): 0.1499
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20896x83584, b=2048): 190.907
Elapsed time for mlp_4h_to_h (4x83584x20896, b=2048): 0.1083
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83584x20896, b=2048): 264.258

Attention duration (in seconds): 0.1628
Attention throughput (in TFLOP/s): 184.392
MLP duration (in seconds): 0.2582
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20928x62784, b=2048): 0.0814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20928x62784, b=2048): 264.376
b: 64, m: 2048, n: 1308, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1308x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1308x2048): 174.230
b: 64, m: 2048, n: 2048, k: 1308,
Elapsed time for attention_prob_times_values (64x2048x2048x1308): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1308): 155.190
Elapsed time for attention_linear_projection (4x20928x20928, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20928x20928, b=2048): 260.007
Elapsed time for mlp_h_to_4h (4x20928x83712, b=2048): 0.1080
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20928x83712, b=2048): 265.714
Elapsed time for mlp_4h_to_h (4x83712x20928, b=2048): 0.1067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83712x20928, b=2048): 269.050

Attention duration (in seconds): 0.1176
Attention throughput (in TFLOP/s): 256.059
MLP duration (in seconds): 0.2147
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20960x62880, b=2048): 0.1247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20960x62880, b=2048): 173.224
b: 64, m: 2048, n: 1310, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1310x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1310x2048): 167.330
b: 64, m: 2048, n: 2048, k: 1310,
Elapsed time for attention_prob_times_values (64x2048x2048x1310): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1310): 130.698
Elapsed time for attention_linear_projection (4x20960x20960, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20960x20960, b=2048): 259.797
Elapsed time for mlp_h_to_4h (4x20960x83840, b=2048): 0.1750
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20960x83840, b=2048): 164.565
Elapsed time for mlp_4h_to_h (4x83840x20960, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83840x20960, b=2048): 265.001

Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 186.469
MLP duration (in seconds): 0.2836
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4455
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0815
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 265.753
b: 64, m: 2048, n: 1312, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1312x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1312x2048): 172.266
b: 64, m: 2048, n: 2048, k: 1312,
Elapsed time for attention_prob_times_values (64x2048x2048x1312): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1312): 176.950
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 260.385
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 265.979
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 267.446

Attention duration (in seconds): 0.1173
Attention throughput (in TFLOP/s): 258.212
MLP duration (in seconds): 0.2166
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21024x63072, b=2048): 0.1255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21024x63072, b=2048): 173.055
b: 64, m: 2048, n: 1314, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1314x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1314x2048): 167.584
b: 64, m: 2048, n: 2048, k: 1314,
Elapsed time for attention_prob_times_values (64x2048x2048x1314): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1314): 131.646
Elapsed time for attention_linear_projection (4x21024x21024, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x21024x21024, b=2048): 260.563
Elapsed time for mlp_h_to_4h (4x21024x84096, b=2048): 0.1586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21024x84096, b=2048): 182.637
Elapsed time for mlp_4h_to_h (4x84096x21024, b=2048): 0.1090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84096x21024, b=2048): 265.863

Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 186.481
MLP duration (in seconds): 0.2676
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21056x63168, b=2048): 0.0820
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21056x63168, b=2048): 265.675
b: 64, m: 2048, n: 1316, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1316x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1316x2048): 174.152
b: 64, m: 2048, n: 2048, k: 1316,
Elapsed time for attention_prob_times_values (64x2048x2048x1316): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1316): 155.541
Elapsed time for attention_linear_projection (4x21056x21056, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x21056x21056, b=2048): 263.045
Elapsed time for mlp_h_to_4h (4x21056x84224, b=2048): 0.1090
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21056x84224, b=2048): 266.665
Elapsed time for mlp_4h_to_h (4x84224x21056, b=2048): 0.1078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84224x21056, b=2048): 269.471

Attention duration (in seconds): 0.1182
Attention throughput (in TFLOP/s): 257.690
MLP duration (in seconds): 0.2168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3350
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21088x63264, b=2048): 0.1326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21088x63264, b=2048): 164.818
b: 64, m: 2048, n: 1318, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1318x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1318x2048): 168.600
b: 64, m: 2048, n: 2048, k: 1318,
Elapsed time for attention_prob_times_values (64x2048x2048x1318): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1318): 131.043
Elapsed time for attention_linear_projection (4x21088x21088, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x21088x21088, b=2048): 260.660
Elapsed time for mlp_h_to_4h (4x21088x84352, b=2048): 0.1918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21088x84352, b=2048): 151.953
Elapsed time for mlp_4h_to_h (4x84352x21088, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84352x21088, b=2048): 264.750

Attention duration (in seconds): 0.1702
Attention throughput (in TFLOP/s): 179.583
MLP duration (in seconds): 0.3019
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4720
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 265.816
b: 64, m: 2048, n: 1320, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1320x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1320x2048): 172.622
b: 64, m: 2048, n: 2048, k: 1320,
Elapsed time for attention_prob_times_values (64x2048x2048x1320): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1320): 174.241
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 261.494
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1100
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 265.666
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1096
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 266.731

Attention duration (in seconds): 0.1186
Attention throughput (in TFLOP/s): 258.431
MLP duration (in seconds): 0.2196
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3382
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21152x63456, b=2048): 0.1428
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21152x63456, b=2048): 154.047
b: 64, m: 2048, n: 1322, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1322x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1322x2048): 168.853
b: 64, m: 2048, n: 2048, k: 1322,
Elapsed time for attention_prob_times_values (64x2048x2048x1322): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1322): 132.200
Elapsed time for attention_linear_projection (4x21152x21152, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_linear_projection (4x21152x21152, b=2048): 256.594
Elapsed time for mlp_h_to_4h (4x21152x84608, b=2048): 0.1833
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21152x84608, b=2048): 159.920
Elapsed time for mlp_4h_to_h (4x84608x21152, b=2048): 0.1102
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84608x21152, b=2048): 266.069

Attention duration (in seconds): 0.1809
Attention throughput (in TFLOP/s): 169.937
MLP duration (in seconds): 0.2936
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21184x63552, b=2048): 0.0837
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21184x63552, b=2048): 263.404
b: 64, m: 2048, n: 1324, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1324x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1324x2048): 175.324
b: 64, m: 2048, n: 2048, k: 1324,
Elapsed time for attention_prob_times_values (64x2048x2048x1324): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1324): 156.405
Elapsed time for attention_linear_projection (4x21184x21184, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_linear_projection (4x21184x21184, b=2048): 259.574
Elapsed time for mlp_h_to_4h (4x21184x84736, b=2048): 0.1113
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21184x84736, b=2048): 264.141
Elapsed time for mlp_4h_to_h (4x84736x21184, b=2048): 0.1095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84736x21184, b=2048): 268.540

Attention duration (in seconds): 0.1207
Attention throughput (in TFLOP/s): 255.516
MLP duration (in seconds): 0.2209
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3415
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21216x63648, b=2048): 0.1285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21216x63648, b=2048): 172.120
b: 64, m: 2048, n: 1326, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1326x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1326x2048): 169.153
b: 64, m: 2048, n: 2048, k: 1326,
Elapsed time for attention_prob_times_values (64x2048x2048x1326): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1326): 133.186
Elapsed time for attention_linear_projection (4x21216x21216, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_linear_projection (4x21216x21216, b=2048): 257.740
Elapsed time for mlp_h_to_4h (4x21216x84864, b=2048): 0.1523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21216x84864, b=2048): 193.704
Elapsed time for mlp_4h_to_h (4x84864x21216, b=2048): 0.1112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84864x21216, b=2048): 265.355

Attention duration (in seconds): 0.1667
Attention throughput (in TFLOP/s): 185.492
MLP duration (in seconds): 0.2635
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0841
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 263.887
b: 64, m: 2048, n: 1328, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1328x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1328x2048): 174.550
b: 64, m: 2048, n: 2048, k: 1328,
Elapsed time for attention_prob_times_values (64x2048x2048x1328): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1328): 177.198
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 258.850
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1119
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 264.389
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1104
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 268.073

Attention duration (in seconds): 0.1208
Attention throughput (in TFLOP/s): 256.786
MLP duration (in seconds): 0.2223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21280x63840, b=2048): 0.1344
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21280x63840, b=2048): 165.641
b: 64, m: 2048, n: 1330, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1330x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1330x2048): 169.769
b: 64, m: 2048, n: 2048, k: 1330,
Elapsed time for attention_prob_times_values (64x2048x2048x1330): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1330): 132.207
Elapsed time for attention_linear_projection (4x21280x21280, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x21280x21280, b=2048): 257.999
Elapsed time for mlp_h_to_4h (4x21280x85120, b=2048): 0.1744
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21280x85120, b=2048): 170.183
Elapsed time for mlp_4h_to_h (4x85120x21280, b=2048): 0.1124
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85120x21280, b=2048): 264.080

Attention duration (in seconds): 0.1727
Attention throughput (in TFLOP/s): 180.072
MLP duration (in seconds): 0.2868
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4595
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21312x63936, b=2048): 0.0848
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21312x63936, b=2048): 263.395
b: 64, m: 2048, n: 1332, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1332x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1332x2048): 176.643
b: 64, m: 2048, n: 2048, k: 1332,
Elapsed time for attention_prob_times_values (64x2048x2048x1332): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1332): 156.040
Elapsed time for attention_linear_projection (4x21312x21312, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x21312x21312, b=2048): 259.078
Elapsed time for mlp_h_to_4h (4x21312x85248, b=2048): 0.1127
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21312x85248, b=2048): 264.025
Elapsed time for mlp_4h_to_h (4x85248x21312, b=2048): 0.1120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85248x21312, b=2048): 265.723

Attention duration (in seconds): 0.1221
Attention throughput (in TFLOP/s): 255.475
MLP duration (in seconds): 0.2248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21344x64032, b=2048): 0.1380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21344x64032, b=2048): 162.234
b: 64, m: 2048, n: 1334, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1334x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1334x2048): 170.231
b: 64, m: 2048, n: 2048, k: 1334,
Elapsed time for attention_prob_times_values (64x2048x2048x1334): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1334): 133.519
Elapsed time for attention_linear_projection (4x21344x21344, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21344x21344, b=2048): 258.290
Elapsed time for mlp_h_to_4h (4x21344x85376, b=2048): 0.1600
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21344x85376, b=2048): 186.620
Elapsed time for mlp_4h_to_h (4x85376x21344, b=2048): 0.1123
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85376x21344, b=2048): 265.747

Attention duration (in seconds): 0.1765
Attention throughput (in TFLOP/s): 177.279
MLP duration (in seconds): 0.2723
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 265.645
b: 64, m: 2048, n: 1336, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1336x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1336x2048): 174.148
b: 64, m: 2048, n: 2048, k: 1336,
Elapsed time for attention_prob_times_values (64x2048x2048x1336): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1336): 175.009
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 261.529
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1125
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 266.225
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 269.209

Attention duration (in seconds): 0.1214
Attention throughput (in TFLOP/s): 258.510
MLP duration (in seconds): 0.2237
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3451
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21408x64224, b=2048): 0.1343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21408x64224, b=2048): 167.685
b: 64, m: 2048, n: 1338, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1338x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1338x2048): 170.481
b: 64, m: 2048, n: 2048, k: 1338,
Elapsed time for attention_prob_times_values (64x2048x2048x1338): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1338): 133.067
Elapsed time for attention_linear_projection (4x21408x21408, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21408x21408, b=2048): 259.413
Elapsed time for mlp_h_to_4h (4x21408x85632, b=2048): 0.1325
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21408x85632, b=2048): 226.660
Elapsed time for mlp_4h_to_h (4x85632x21408, b=2048): 0.1133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85632x21408, b=2048): 265.106

Attention duration (in seconds): 0.1729
Attention throughput (in TFLOP/s): 182.029
MLP duration (in seconds): 0.2458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21440x64320, b=2048): 0.0853
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21440x64320, b=2048): 264.930
b: 64, m: 2048, n: 1340, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1340x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1340x2048): 177.955
b: 64, m: 2048, n: 2048, k: 1340,
Elapsed time for attention_prob_times_values (64x2048x2048x1340): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1340): 157.255
Elapsed time for attention_linear_projection (4x21440x21440, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21440x21440, b=2048): 260.711
Elapsed time for mlp_h_to_4h (4x21440x85760, b=2048): 0.1132
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21440x85760, b=2048): 266.071
Elapsed time for mlp_4h_to_h (4x85760x21440, b=2048): 0.1130
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85760x21440, b=2048): 266.592

Attention duration (in seconds): 0.1228
Attention throughput (in TFLOP/s): 257.062
MLP duration (in seconds): 0.2262
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3490
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21472x64416, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21472x64416, b=2048): 163.058
b: 64, m: 2048, n: 1342, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1342x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1342x2048): 170.566
b: 64, m: 2048, n: 2048, k: 1342,
Elapsed time for attention_prob_times_values (64x2048x2048x1342): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1342): 132.094
Elapsed time for attention_linear_projection (4x21472x21472, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21472x21472, b=2048): 259.394
Elapsed time for mlp_h_to_4h (4x21472x85888, b=2048): 0.1798
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21472x85888, b=2048): 168.068
Elapsed time for mlp_4h_to_h (4x85888x21472, b=2048): 0.1145
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85888x21472, b=2048): 263.808

Attention duration (in seconds): 0.1778
Attention throughput (in TFLOP/s): 178.067
MLP duration (in seconds): 0.2943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0861
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 263.895
b: 64, m: 2048, n: 1344, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1344x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1344x2048): 176.283
b: 64, m: 2048, n: 2048, k: 1344,
Elapsed time for attention_prob_times_values (64x2048x2048x1344): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1344): 179.343
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 261.514
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 264.749
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1142
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 265.376

Attention duration (in seconds): 0.1232
Attention throughput (in TFLOP/s): 257.664
MLP duration (in seconds): 0.2287
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3519
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21536x64608, b=2048): 0.1326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21536x64608, b=2048): 171.904
b: 64, m: 2048, n: 1346, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1346x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1346x2048): 170.872
b: 64, m: 2048, n: 2048, k: 1346,
Elapsed time for attention_prob_times_values (64x2048x2048x1346): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1346): 129.399
Elapsed time for attention_linear_projection (4x21536x21536, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x21536x21536, b=2048): 256.784
Elapsed time for mlp_h_to_4h (4x21536x86144, b=2048): 0.1819
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21536x86144, b=2048): 167.136
Elapsed time for mlp_4h_to_h (4x86144x21536, b=2048): 0.1156
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86144x21536, b=2048): 262.947

Attention duration (in seconds): 0.1720
Attention throughput (in TFLOP/s): 185.101
MLP duration (in seconds): 0.2975
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21568x64704, b=2048): 0.0877
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21568x64704, b=2048): 260.778
b: 64, m: 2048, n: 1348, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1348x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1348x2048): 177.458
b: 64, m: 2048, n: 2048, k: 1348,
Elapsed time for attention_prob_times_values (64x2048x2048x1348): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1348): 154.924
Elapsed time for attention_linear_projection (4x21568x21568, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x21568x21568, b=2048): 257.510
Elapsed time for mlp_h_to_4h (4x21568x86272, b=2048): 0.1167
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21568x86272, b=2048): 261.261
Elapsed time for mlp_4h_to_h (4x86272x21568, b=2048): 0.1149
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86272x21568, b=2048): 265.416

Attention duration (in seconds): 0.1260
Attention throughput (in TFLOP/s): 253.391
MLP duration (in seconds): 0.2315
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3576
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21600x64800, b=2048): 0.1410
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21600x64800, b=2048): 162.690
b: 64, m: 2048, n: 1350, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1350x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1350x2048): 171.476
b: 64, m: 2048, n: 2048, k: 1350,
Elapsed time for attention_prob_times_values (64x2048x2048x1350): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1350): 129.967
Elapsed time for attention_linear_projection (4x21600x21600, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21600x21600, b=2048): 256.353
Elapsed time for mlp_h_to_4h (4x21600x86400, b=2048): 0.1693
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21600x86400, b=2048): 180.572
Elapsed time for mlp_4h_to_h (4x86400x21600, b=2048): 0.1162
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86400x21600, b=2048): 263.041

Attention duration (in seconds): 0.1806
Attention throughput (in TFLOP/s): 177.351
MLP duration (in seconds): 0.2856
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4662
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0879
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 261.713
b: 64, m: 2048, n: 1352, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1352x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1352x2048): 175.948
b: 64, m: 2048, n: 2048, k: 1352,
Elapsed time for attention_prob_times_values (64x2048x2048x1352): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1352): 173.255
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 258.396
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1169
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 262.241
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1158
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 264.918

Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 255.176
MLP duration (in seconds): 0.2327
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21664x64992, b=2048): 0.1446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21664x64992, b=2048): 159.519
b: 64, m: 2048, n: 1354, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1354x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1354x2048): 171.878
b: 64, m: 2048, n: 2048, k: 1354,
Elapsed time for attention_prob_times_values (64x2048x2048x1354): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1354): 130.822
Elapsed time for attention_linear_projection (4x21664x21664, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21664x21664, b=2048): 255.193
Elapsed time for mlp_h_to_4h (4x21664x86656, b=2048): 0.1672
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21664x86656, b=2048): 183.978
Elapsed time for mlp_4h_to_h (4x86656x21664, b=2048): 0.1157
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86656x21664, b=2048): 265.809

Attention duration (in seconds): 0.1845
Attention throughput (in TFLOP/s): 174.561
MLP duration (in seconds): 0.2829
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4674
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21696x65088, b=2048): 0.0876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21696x65088, b=2048): 264.260
b: 64, m: 2048, n: 1356, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1356x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1356x2048): 178.982
b: 64, m: 2048, n: 2048, k: 1356,
Elapsed time for attention_prob_times_values (64x2048x2048x1356): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1356): 156.949
Elapsed time for attention_linear_projection (4x21696x21696, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21696x21696, b=2048): 259.306
Elapsed time for mlp_h_to_4h (4x21696x86784, b=2048): 0.1164
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21696x86784, b=2048): 265.066
Elapsed time for mlp_4h_to_h (4x86784x21696, b=2048): 0.1149
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86784x21696, b=2048): 268.397

Attention duration (in seconds): 0.1260
Attention throughput (in TFLOP/s): 256.387
MLP duration (in seconds): 0.2313
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3573
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21728x65184, b=2048): 0.1460
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21728x65184, b=2048): 158.953
b: 64, m: 2048, n: 1358, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1358x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1358x2048): 171.804
b: 64, m: 2048, n: 2048, k: 1358,
Elapsed time for attention_prob_times_values (64x2048x2048x1358): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1358): 131.389
Elapsed time for attention_linear_projection (4x21728x21728, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x21728x21728, b=2048): 258.781
Elapsed time for mlp_h_to_4h (4x21728x86912, b=2048): 0.1884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21728x86912, b=2048): 164.205
Elapsed time for mlp_4h_to_h (4x86912x21728, b=2048): 0.1168
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86912x21728, b=2048): 264.822

Attention duration (in seconds): 0.1857
Attention throughput (in TFLOP/s): 174.494
MLP duration (in seconds): 0.3053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4909
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0882
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 263.881
b: 64, m: 2048, n: 1360, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1360x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1360x2048): 176.104
b: 64, m: 2048, n: 2048, k: 1360,
Elapsed time for attention_prob_times_values (64x2048x2048x1360): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1360): 174.598
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 259.674
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1172
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 264.770
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1170
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 265.334

Attention duration (in seconds): 0.1264
Attention throughput (in TFLOP/s): 257.054
MLP duration (in seconds): 0.2342
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3606
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21792x65376, b=2048): 0.1409
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21792x65376, b=2048): 165.640
b: 64, m: 2048, n: 1362, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1362x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1362x2048): 173.588
b: 64, m: 2048, n: 2048, k: 1362,
Elapsed time for attention_prob_times_values (64x2048x2048x1362): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1362): 132.585
Elapsed time for attention_linear_projection (4x21792x21792, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21792x21792, b=2048): 258.567
Elapsed time for mlp_h_to_4h (4x21792x87168, b=2048): 0.1905
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21792x87168, b=2048): 163.391
Elapsed time for mlp_4h_to_h (4x87168x21792, b=2048): 0.1181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87168x21792, b=2048): 263.611

Attention duration (in seconds): 0.1807
Attention throughput (in TFLOP/s): 180.288
MLP duration (in seconds): 0.3085
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4893
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21824x65472, b=2048): 0.0888
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21824x65472, b=2048): 263.570
b: 64, m: 2048, n: 1364, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1364x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1364x2048): 179.219
b: 64, m: 2048, n: 2048, k: 1364,
Elapsed time for attention_prob_times_values (64x2048x2048x1364): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1364): 157.341
Elapsed time for attention_linear_projection (4x21824x21824, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x21824x21824, b=2048): 260.690
Elapsed time for mlp_h_to_4h (4x21824x87296, b=2048): 0.1180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21824x87296, b=2048): 264.445
Elapsed time for mlp_4h_to_h (4x87296x21824, b=2048): 0.1169
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87296x21824, b=2048): 266.996

Attention duration (in seconds): 0.1275
Attention throughput (in TFLOP/s): 256.313
MLP duration (in seconds): 0.2349
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3624
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21856x65568, b=2048): 0.1306
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21856x65568, b=2048): 179.845
b: 64, m: 2048, n: 1366, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1366x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1366x2048): 173.626
b: 64, m: 2048, n: 2048, k: 1366,
Elapsed time for attention_prob_times_values (64x2048x2048x1366): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1366): 133.615
Elapsed time for attention_linear_projection (4x21856x21856, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21856x21856, b=2048): 260.210
Elapsed time for mlp_h_to_4h (4x21856x87424, b=2048): 0.1935
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21856x87424, b=2048): 161.766
Elapsed time for mlp_4h_to_h (4x87424x21856, b=2048): 0.1180
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87424x21856, b=2048): 265.389

Attention duration (in seconds): 0.1703
Attention throughput (in TFLOP/s): 192.391
MLP duration (in seconds): 0.3115
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 266.696
b: 64, m: 2048, n: 1368, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1368x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1368x2048): 176.198
b: 64, m: 2048, n: 2048, k: 1368,
Elapsed time for attention_prob_times_values (64x2048x2048x1368): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1368): 173.274
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 263.346
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1182
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 265.711
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.1172
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 267.811

Attention duration (in seconds): 0.1265
Attention throughput (in TFLOP/s): 259.795
MLP duration (in seconds): 0.2354
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3619
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21920x65760, b=2048): 0.1518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21920x65760, b=2048): 155.622
b: 64, m: 2048, n: 1370, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1370x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1370x2048): 172.692
b: 64, m: 2048, n: 2048, k: 1370,
Elapsed time for attention_prob_times_values (64x2048x2048x1370): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1370): 132.141
Elapsed time for attention_linear_projection (4x21920x21920, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_linear_projection (4x21920x21920, b=2048): 260.686
Elapsed time for mlp_h_to_4h (4x21920x87680, b=2048): 0.1774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21920x87680, b=2048): 177.496
Elapsed time for mlp_4h_to_h (4x87680x21920, b=2048): 0.1191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87680x21920, b=2048): 264.316

Attention duration (in seconds): 0.1918
Attention throughput (in TFLOP/s): 171.863
MLP duration (in seconds): 0.2965
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4883
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21952x65856, b=2048): 0.0894
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21952x65856, b=2048): 264.795
b: 64, m: 2048, n: 1372, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1372x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1372x2048): 180.840
b: 64, m: 2048, n: 2048, k: 1372,
Elapsed time for attention_prob_times_values (64x2048x2048x1372): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1372): 158.678
Elapsed time for attention_linear_projection (4x21952x21952, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21952x21952, b=2048): 262.037
Elapsed time for mlp_h_to_4h (4x21952x87808, b=2048): 0.1190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21952x87808, b=2048): 265.291
Elapsed time for mlp_4h_to_h (4x87808x21952, b=2048): 0.1194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87808x21952, b=2048): 264.532

Attention duration (in seconds): 0.1283
Attention throughput (in TFLOP/s): 257.642
MLP duration (in seconds): 0.2384
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3667
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21984x65952, b=2048): 0.1541
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21984x65952, b=2048): 154.145
b: 64, m: 2048, n: 1374, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1374x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1374x2048): 173.225
b: 64, m: 2048, n: 2048, k: 1374,
Elapsed time for attention_prob_times_values (64x2048x2048x1374): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1374): 132.663
Elapsed time for attention_linear_projection (4x21984x21984, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x21984x21984, b=2048): 259.433
Elapsed time for mlp_h_to_4h (4x21984x87936, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21984x87936, b=2048): 161.859
Elapsed time for mlp_4h_to_h (4x87936x21984, b=2048): 0.1204
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87936x21984, b=2048): 263.016

Attention duration (in seconds): 0.1944
Attention throughput (in TFLOP/s): 170.475
MLP duration (in seconds): 0.3161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0900
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 264.635
b: 64, m: 2048, n: 1376, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1376x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1376x2048): 178.545
b: 64, m: 2048, n: 2048, k: 1376,
Elapsed time for attention_prob_times_values (64x2048x2048x1376): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1376): 176.350
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 262.678
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 265.211
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.1198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 265.197

Attention duration (in seconds): 0.1286
Attention throughput (in TFLOP/s): 258.529
MLP duration (in seconds): 0.2396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3681
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22048x66144, b=2048): 0.1467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22048x66144, b=2048): 162.842
b: 64, m: 2048, n: 1378, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1378x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1378x2048): 173.690
b: 64, m: 2048, n: 2048, k: 1378,
Elapsed time for attention_prob_times_values (64x2048x2048x1378): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1378): 132.674
Elapsed time for attention_linear_projection (4x22048x22048, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x22048x22048, b=2048): 255.209
Elapsed time for mlp_h_to_4h (4x22048x88192, b=2048): 0.1880
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22048x88192, b=2048): 169.475
Elapsed time for mlp_4h_to_h (4x88192x22048, b=2048): 0.1213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88192x22048, b=2048): 262.739

Attention duration (in seconds): 0.1878
Attention throughput (in TFLOP/s): 177.543
MLP duration (in seconds): 0.3092
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4970
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22080x66240, b=2048): 0.0918
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22080x66240, b=2048): 261.112
b: 64, m: 2048, n: 1380, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1380x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1380x2048): 182.225
b: 64, m: 2048, n: 2048, k: 1380,
Elapsed time for attention_prob_times_values (64x2048x2048x1380): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1380): 158.748
Elapsed time for attention_linear_projection (4x22080x22080, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_linear_projection (4x22080x22080, b=2048): 259.051
Elapsed time for mlp_h_to_4h (4x22080x88320, b=2048): 0.1221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22080x88320, b=2048): 261.771
Elapsed time for mlp_4h_to_h (4x88320x22080, b=2048): 0.1206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88320x22080, b=2048): 264.911

Attention duration (in seconds): 0.1313
Attention throughput (in TFLOP/s): 254.549
MLP duration (in seconds): 0.2427
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3740
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22112x66336, b=2048): 0.1448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22112x66336, b=2048): 165.954
b: 64, m: 2048, n: 1382, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1382x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1382x2048): 175.323
b: 64, m: 2048, n: 2048, k: 1382,
Elapsed time for attention_prob_times_values (64x2048x2048x1382): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1382): 133.213
Elapsed time for attention_linear_projection (4x22112x22112, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_linear_projection (4x22112x22112, b=2048): 258.603
Elapsed time for mlp_h_to_4h (4x22112x88448, b=2048): 0.1880
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22112x88448, b=2048): 170.485
Elapsed time for mlp_4h_to_h (4x88448x22112, b=2048): 0.1204
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88448x22112, b=2048): 266.202

Attention duration (in seconds): 0.1856
Attention throughput (in TFLOP/s): 180.649
MLP duration (in seconds): 0.3083
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0909
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 265.099
b: 64, m: 2048, n: 1384, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1384x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1384x2048): 179.366
b: 64, m: 2048, n: 2048, k: 1384,
Elapsed time for attention_prob_times_values (64x2048x2048x1384): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1384): 175.291
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0307
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 261.619
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 265.323
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.1197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 268.504

Attention duration (in seconds): 0.1300
Attention throughput (in TFLOP/s): 258.617
MLP duration (in seconds): 0.2408
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3708
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22176x66528, b=2048): 0.1538
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22176x66528, b=2048): 157.209
b: 64, m: 2048, n: 1386, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1386x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1386x2048): 175.101
b: 64, m: 2048, n: 2048, k: 1386,
Elapsed time for attention_prob_times_values (64x2048x2048x1386): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1386): 133.679
Elapsed time for attention_linear_projection (4x22176x22176, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_linear_projection (4x22176x22176, b=2048): 259.794
Elapsed time for mlp_h_to_4h (4x22176x88704, b=2048): 0.1799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22176x88704, b=2048): 179.187
Elapsed time for mlp_4h_to_h (4x88704x22176, b=2048): 0.1212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88704x22176, b=2048): 265.976

Attention duration (in seconds): 0.1946
Attention throughput (in TFLOP/s): 173.277
MLP duration (in seconds): 0.3010
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4956
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22208x66624, b=2048): 0.0914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22208x66624, b=2048): 265.182
b: 64, m: 2048, n: 1388, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1388x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1388x2048): 182.266
b: 64, m: 2048, n: 2048, k: 1388,
Elapsed time for attention_prob_times_values (64x2048x2048x1388): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1388): 158.109
Elapsed time for attention_linear_projection (4x22208x22208, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_linear_projection (4x22208x22208, b=2048): 261.356
Elapsed time for mlp_h_to_4h (4x22208x88832, b=2048): 0.1215
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22208x88832, b=2048): 266.050
Elapsed time for mlp_4h_to_h (4x88832x22208, b=2048): 0.1208
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88832x22208, b=2048): 267.522

Attention duration (in seconds): 0.1311
Attention throughput (in TFLOP/s): 257.847
MLP duration (in seconds): 0.2423
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3734
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22240x66720, b=2048): 0.1568
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22240x66720, b=2048): 155.093
b: 64, m: 2048, n: 1390, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1390x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1390x2048): 176.179
b: 64, m: 2048, n: 2048, k: 1390,
Elapsed time for attention_prob_times_values (64x2048x2048x1390): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1390): 134.082
Elapsed time for attention_linear_projection (4x22240x22240, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x22240x22240, b=2048): 260.987
Elapsed time for mlp_h_to_4h (4x22240x88960, b=2048): 0.1897
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22240x88960, b=2048): 170.854
Elapsed time for mlp_4h_to_h (4x88960x22240, b=2048): 0.1222
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88960x22240, b=2048): 265.336

Attention duration (in seconds): 0.1976
Attention throughput (in TFLOP/s): 171.593
MLP duration (in seconds): 0.3119
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 265.102
b: 64, m: 2048, n: 1392, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1392x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1392x2048): 181.479
b: 64, m: 2048, n: 2048, k: 1392,
Elapsed time for attention_prob_times_values (64x2048x2048x1392): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1392): 178.777
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 261.337
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1224
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 265.627
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.1223
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 265.762

Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 258.842
MLP duration (in seconds): 0.2447
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22304x66912, b=2048): 0.1483
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22304x66912, b=2048): 164.873
b: 64, m: 2048, n: 1394, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1394x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1394x2048): 177.255
b: 64, m: 2048, n: 2048, k: 1394,
Elapsed time for attention_prob_times_values (64x2048x2048x1394): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1394): 135.337
Elapsed time for attention_linear_projection (4x22304x22304, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_linear_projection (4x22304x22304, b=2048): 258.423
Elapsed time for mlp_h_to_4h (4x22304x89216, b=2048): 0.2026
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22304x89216, b=2048): 160.926
Elapsed time for mlp_4h_to_h (4x89216x22304, b=2048): 0.1239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89216x22304, b=2048): 263.191

Attention duration (in seconds): 0.1896
Attention throughput (in TFLOP/s): 179.849
MLP duration (in seconds): 0.3265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22336x67008, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22336x67008, b=2048): 263.522
b: 64, m: 2048, n: 1396, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1396x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1396x2048): 182.116
b: 64, m: 2048, n: 2048, k: 1396,
Elapsed time for attention_prob_times_values (64x2048x2048x1396): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1396): 159.683
Elapsed time for attention_linear_projection (4x22336x22336, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x22336x22336, b=2048): 261.291
Elapsed time for mlp_h_to_4h (4x22336x89344, b=2048): 0.1236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22336x89344, b=2048): 264.542
Elapsed time for mlp_4h_to_h (4x89344x22336, b=2048): 0.1231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89344x22336, b=2048): 265.592

Attention duration (in seconds): 0.1331
Attention throughput (in TFLOP/s): 256.821
MLP duration (in seconds): 0.2467
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3798
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22368x67104, b=2048): 0.1518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22368x67104, b=2048): 161.999
b: 64, m: 2048, n: 1398, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1398x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1398x2048): 176.736
b: 64, m: 2048, n: 2048, k: 1398,
Elapsed time for attention_prob_times_values (64x2048x2048x1398): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1398): 135.073
Elapsed time for attention_linear_projection (4x22368x22368, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_linear_projection (4x22368x22368, b=2048): 258.887
Elapsed time for mlp_h_to_4h (4x22368x89472, b=2048): 0.1876
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22368x89472, b=2048): 174.755
Elapsed time for mlp_4h_to_h (4x89472x22368, b=2048): 0.1251
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89472x22368, b=2048): 262.184

Attention duration (in seconds): 0.1933
Attention throughput (in TFLOP/s): 177.422
MLP duration (in seconds): 0.3127
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 264.155
b: 64, m: 2048, n: 1400, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1400x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1400x2048): 181.880
b: 64, m: 2048, n: 2048, k: 1400,
Elapsed time for attention_prob_times_values (64x2048x2048x1400): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1400): 177.637
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 261.396
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1239
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 265.312
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1240
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 265.108

Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 258.202
MLP duration (in seconds): 0.2480
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3812
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22432x67296, b=2048): 0.1623
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22432x67296, b=2048): 152.433
b: 64, m: 2048, n: 1402, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1402x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1402x2048): 178.021
b: 64, m: 2048, n: 2048, k: 1402,
Elapsed time for attention_prob_times_values (64x2048x2048x1402): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1402): 136.420
Elapsed time for attention_linear_projection (4x22432x22432, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22432x22432, b=2048): 258.247
Elapsed time for mlp_h_to_4h (4x22432x89728, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22432x89728, b=2048): 160.439
Elapsed time for mlp_4h_to_h (4x89728x22432, b=2048): 0.1250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89728x22432, b=2048): 263.770

Attention duration (in seconds): 0.2039
Attention throughput (in TFLOP/s): 169.095
MLP duration (in seconds): 0.3306
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22464x67392, b=2048): 0.0949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22464x67392, b=2048): 261.238
b: 64, m: 2048, n: 1404, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1404x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1404x2048): 185.069
b: 64, m: 2048, n: 2048, k: 1404,
Elapsed time for attention_prob_times_values (64x2048x2048x1404): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1404): 162.146
Elapsed time for attention_linear_projection (4x22464x22464, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22464x22464, b=2048): 258.909
Elapsed time for mlp_h_to_4h (4x22464x89856, b=2048): 0.1263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22464x89856, b=2048): 261.802
Elapsed time for mlp_4h_to_h (4x89856x22464, b=2048): 0.1249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89856x22464, b=2048): 264.830

Attention duration (in seconds): 0.1356
Attention throughput (in TFLOP/s): 255.005
MLP duration (in seconds): 0.2512
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22496x67488, b=2048): 0.1530
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22496x67488, b=2048): 162.623
b: 64, m: 2048, n: 1406, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1406x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1406x2048): 178.993
b: 64, m: 2048, n: 2048, k: 1406,
Elapsed time for attention_prob_times_values (64x2048x2048x1406): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1406): 137.731
Elapsed time for attention_linear_projection (4x22496x22496, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x22496x22496, b=2048): 257.904
Elapsed time for mlp_h_to_4h (4x22496x89984, b=2048): 0.2126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22496x89984, b=2048): 155.964
Elapsed time for mlp_4h_to_h (4x89984x22496, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89984x22496, b=2048): 262.704

Attention duration (in seconds): 0.1948
Attention throughput (in TFLOP/s): 178.002
MLP duration (in seconds): 0.3389
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0950
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 262.506
b: 64, m: 2048, n: 1408, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1408x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1408x2048): 183.512
b: 64, m: 2048, n: 2048, k: 1408,
Elapsed time for attention_prob_times_values (64x2048x2048x1408): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1408): 180.359
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0320
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 259.563
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 246.483
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1245
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 267.160

Attention duration (in seconds): 0.1354
Attention throughput (in TFLOP/s): 256.863
MLP duration (in seconds): 0.2594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22560x67680, b=2048): 0.1627
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22560x67680, b=2048): 153.792
b: 64, m: 2048, n: 1410, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1410x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1410x2048): 169.155
b: 64, m: 2048, n: 2048, k: 1410,
Elapsed time for attention_prob_times_values (64x2048x2048x1410): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1410): 134.735
Elapsed time for attention_linear_projection (4x22560x22560, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_linear_projection (4x22560x22560, b=2048): 258.492
Elapsed time for mlp_h_to_4h (4x22560x90240, b=2048): 0.2105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22560x90240, b=2048): 158.474
Elapsed time for mlp_4h_to_h (4x90240x22560, b=2048): 0.1271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90240x22560, b=2048): 262.431

Attention duration (in seconds): 0.2050
Attention throughput (in TFLOP/s): 170.080
MLP duration (in seconds): 0.3376
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5426
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22592x67776, b=2048): 0.0957
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22592x67776, b=2048): 262.056
b: 64, m: 2048, n: 1412, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1412x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1412x2048): 176.152
b: 64, m: 2048, n: 2048, k: 1412,
Elapsed time for attention_prob_times_values (64x2048x2048x1412): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1412): 159.720
Elapsed time for attention_linear_projection (4x22592x22592, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x22592x22592, b=2048): 259.881
Elapsed time for mlp_h_to_4h (4x22592x90368, b=2048): 0.1270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22592x90368, b=2048): 263.485
Elapsed time for mlp_4h_to_h (4x90368x22592, b=2048): 0.1251
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90368x22592, b=2048): 267.307

Attention duration (in seconds): 0.1370
Attention throughput (in TFLOP/s): 255.300
MLP duration (in seconds): 0.2521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3890
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22624x67872, b=2048): 0.1599
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22624x67872, b=2048): 157.316
b: 64, m: 2048, n: 1414, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1414x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1414x2048): 170.170
b: 64, m: 2048, n: 2048, k: 1414,
Elapsed time for attention_prob_times_values (64x2048x2048x1414): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1414): 135.100
Elapsed time for attention_linear_projection (4x22624x22624, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x22624x22624, b=2048): 260.328
Elapsed time for mlp_h_to_4h (4x22624x90496, b=2048): 0.2158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22624x90496, b=2048): 155.410
Elapsed time for mlp_4h_to_h (4x90496x22624, b=2048): 0.1261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90496x22624, b=2048): 265.909

Attention duration (in seconds): 0.2022
Attention throughput (in TFLOP/s): 173.393
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0953
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 264.842
b: 64, m: 2048, n: 1416, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1416x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1416x2048): 174.133
b: 64, m: 2048, n: 2048, k: 1416,
Elapsed time for attention_prob_times_values (64x2048x2048x1416): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1416): 176.299
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 261.114
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1266
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 265.703
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 266.485

Attention duration (in seconds): 0.1361
Attention throughput (in TFLOP/s): 258.247
MLP duration (in seconds): 0.2528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3890
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22688x68064, b=2048): 0.1633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22688x68064, b=2048): 154.914
b: 64, m: 2048, n: 1418, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1418x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1418x2048): 170.642
b: 64, m: 2048, n: 2048, k: 1418,
Elapsed time for attention_prob_times_values (64x2048x2048x1418): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1418): 134.717
Elapsed time for attention_linear_projection (4x22688x22688, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_linear_projection (4x22688x22688, b=2048): 259.962
Elapsed time for mlp_h_to_4h (4x22688x90752, b=2048): 0.2242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22688x90752, b=2048): 150.436
Elapsed time for mlp_4h_to_h (4x90752x22688, b=2048): 0.1279
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90752x22688, b=2048): 263.817

Attention duration (in seconds): 0.2059
Attention throughput (in TFLOP/s): 171.253
MLP duration (in seconds): 0.3521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5580
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22720x68160, b=2048): 0.0958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22720x68160, b=2048): 264.771
b: 64, m: 2048, n: 1420, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1420x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1420x2048): 176.846
b: 64, m: 2048, n: 2048, k: 1420,
Elapsed time for attention_prob_times_values (64x2048x2048x1420): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1420): 160.006
Elapsed time for attention_linear_projection (4x22720x22720, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_linear_projection (4x22720x22720, b=2048): 260.845
Elapsed time for mlp_h_to_4h (4x22720x90880, b=2048): 0.1275
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22720x90880, b=2048): 265.291
Elapsed time for mlp_4h_to_h (4x90880x22720, b=2048): 0.1272
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90880x22720, b=2048): 266.040

Attention duration (in seconds): 0.1373
Attention throughput (in TFLOP/s): 257.449
MLP duration (in seconds): 0.2547
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3920
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22752x68256, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22752x68256, b=2048): 152.867
b: 64, m: 2048, n: 1422, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1422x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1422x2048): 171.114
b: 64, m: 2048, n: 2048, k: 1422,
Elapsed time for attention_prob_times_values (64x2048x2048x1422): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1422): 134.716
Elapsed time for attention_linear_projection (4x22752x22752, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_linear_projection (4x22752x22752, b=2048): 261.529
Elapsed time for mlp_h_to_4h (4x22752x91008, b=2048): 0.2044
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22752x91008, b=2048): 165.941
Elapsed time for mlp_4h_to_h (4x91008x22752, b=2048): 0.1284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91008x22752, b=2048): 264.170

Attention duration (in seconds): 0.2090
Attention throughput (in TFLOP/s): 169.625
MLP duration (in seconds): 0.3329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5419
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.0963
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 265.006
b: 64, m: 2048, n: 1424, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1424x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1424x2048): 175.002
b: 64, m: 2048, n: 2048, k: 1424,
Elapsed time for attention_prob_times_values (64x2048x2048x1424): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1424): 177.227
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 261.838
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 265.693
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 265.041

Attention duration (in seconds): 0.1374
Attention throughput (in TFLOP/s): 258.642
MLP duration (in seconds): 0.2564
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22816x68448, b=2048): 0.1572
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22816x68448, b=2048): 162.741
b: 64, m: 2048, n: 1426, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1426x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1426x2048): 170.357
b: 64, m: 2048, n: 2048, k: 1426,
Elapsed time for attention_prob_times_values (64x2048x2048x1426): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1426): 133.672
Elapsed time for attention_linear_projection (4x22816x22816, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_linear_projection (4x22816x22816, b=2048): 256.627
Elapsed time for mlp_h_to_4h (4x22816x91264, b=2048): 0.2198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22816x91264, b=2048): 155.233
Elapsed time for mlp_4h_to_h (4x91264x22816, b=2048): 0.1301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91264x22816, b=2048): 262.252

Attention duration (in seconds): 0.2007
Attention throughput (in TFLOP/s): 177.631
MLP duration (in seconds): 0.3499
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5505
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22848x68544, b=2048): 0.0984
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22848x68544, b=2048): 260.638
b: 64, m: 2048, n: 1428, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1428x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1428x2048): 177.774
b: 64, m: 2048, n: 2048, k: 1428,
Elapsed time for attention_prob_times_values (64x2048x2048x1428): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1428): 159.772
Elapsed time for attention_linear_projection (4x22848x22848, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_linear_projection (4x22848x22848, b=2048): 258.719
Elapsed time for mlp_h_to_4h (4x22848x91392, b=2048): 0.1307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22848x91392, b=2048): 261.724
Elapsed time for mlp_4h_to_h (4x91392x22848, b=2048): 0.1291
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91392x22848, b=2048): 264.963

Attention duration (in seconds): 0.1406
Attention throughput (in TFLOP/s): 254.204
MLP duration (in seconds): 0.2598
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4005
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22880x68640, b=2048): 0.1645
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22880x68640, b=2048): 156.434
b: 64, m: 2048, n: 1430, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1430x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1430x2048): 170.935
b: 64, m: 2048, n: 2048, k: 1430,
Elapsed time for attention_prob_times_values (64x2048x2048x1430): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1430): 133.696
Elapsed time for attention_linear_projection (4x22880x22880, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22880x22880, b=2048): 257.487
Elapsed time for mlp_h_to_4h (4x22880x91520, b=2048): 0.2178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22880x91520, b=2048): 157.498
Elapsed time for mlp_4h_to_h (4x91520x22880, b=2048): 0.1306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91520x22880, b=2048): 262.721

Attention duration (in seconds): 0.2080
Attention throughput (in TFLOP/s): 172.300
MLP duration (in seconds): 0.3484
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5564
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 261.956
b: 64, m: 2048, n: 1432, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1432x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1432x2048): 176.390
b: 64, m: 2048, n: 2048, k: 1432,
Elapsed time for attention_prob_times_values (64x2048x2048x1432): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1432): 177.575
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 259.908
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 262.845
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 265.549

Attention duration (in seconds): 0.1403
Attention throughput (in TFLOP/s): 256.210
MLP duration (in seconds): 0.2604
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22944x68832, b=2048): 0.1674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22944x68832, b=2048): 154.559
b: 64, m: 2048, n: 1434, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1434x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1434x2048): 171.204
b: 64, m: 2048, n: 2048, k: 1434,
Elapsed time for attention_prob_times_values (64x2048x2048x1434): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1434): 133.262
Elapsed time for attention_linear_projection (4x22944x22944, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x22944x22944, b=2048): 258.225
Elapsed time for mlp_h_to_4h (4x22944x91776, b=2048): 0.2195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22944x91776, b=2048): 157.179
Elapsed time for mlp_4h_to_h (4x91776x22944, b=2048): 0.1309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91776x22944, b=2048): 263.587

Attention duration (in seconds): 0.2111
Attention throughput (in TFLOP/s): 170.734
MLP duration (in seconds): 0.3504
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5615
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22976x68928, b=2048): 0.0986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22976x68928, b=2048): 263.272
b: 64, m: 2048, n: 1436, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1436x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1436x2048): 179.096
b: 64, m: 2048, n: 2048, k: 1436,
Elapsed time for attention_prob_times_values (64x2048x2048x1436): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1436): 160.420
Elapsed time for attention_linear_projection (4x22976x22976, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22976x22976, b=2048): 259.942
Elapsed time for mlp_h_to_4h (4x22976x91904, b=2048): 0.1307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22976x91904, b=2048): 264.619
Elapsed time for mlp_4h_to_h (4x91904x22976, b=2048): 0.1298
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91904x22976, b=2048): 266.451

Attention duration (in seconds): 0.1409
Attention throughput (in TFLOP/s): 256.408
MLP duration (in seconds): 0.2606
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23008x69024, b=2048): 0.1734
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23008x69024, b=2048): 150.069
b: 64, m: 2048, n: 1438, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1438x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1438x2048): 173.290
b: 64, m: 2048, n: 2048, k: 1438,
Elapsed time for attention_prob_times_values (64x2048x2048x1438): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1438): 133.838
Elapsed time for attention_linear_projection (4x23008x23008, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x23008x23008, b=2048): 258.522
Elapsed time for mlp_h_to_4h (4x23008x92032, b=2048): 0.2348
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23008x92032, b=2048): 147.730
Elapsed time for mlp_4h_to_h (4x92032x23008, b=2048): 0.1320
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92032x23008, b=2048): 262.744

Attention duration (in seconds): 0.2172
Attention throughput (in TFLOP/s): 166.870
MLP duration (in seconds): 0.3669
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.0994
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 262.364
b: 64, m: 2048, n: 1440, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1440x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1440x2048): 178.872
b: 64, m: 2048, n: 2048, k: 1440,
Elapsed time for attention_prob_times_values (64x2048x2048x1440): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1440): 181.150
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 261.119
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1320
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 263.579
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1311
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 265.274

Attention duration (in seconds): 0.1413
Attention throughput (in TFLOP/s): 257.066
MLP duration (in seconds): 0.2631
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23072x69216, b=2048): 0.1770
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23072x69216, b=2048): 147.781
b: 64, m: 2048, n: 1442, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1442x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1442x2048): 173.530
b: 64, m: 2048, n: 2048, k: 1442,
Elapsed time for attention_prob_times_values (64x2048x2048x1442): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1442): 133.901
Elapsed time for attention_linear_projection (4x23072x23072, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_linear_projection (4x23072x23072, b=2048): 258.679
Elapsed time for mlp_h_to_4h (4x23072x92288, b=2048): 0.2288
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23072x92288, b=2048): 152.483
Elapsed time for mlp_4h_to_h (4x92288x23072, b=2048): 0.1311
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92288x23072, b=2048): 266.020

Attention duration (in seconds): 0.2210
Attention throughput (in TFLOP/s): 164.855
MLP duration (in seconds): 0.3599
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5809
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23104x69312, b=2048): 0.0990
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23104x69312, b=2048): 265.081
b: 64, m: 2048, n: 1444, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1444x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1444x2048): 179.557
b: 64, m: 2048, n: 2048, k: 1444,
Elapsed time for attention_prob_times_values (64x2048x2048x1444): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1444): 160.494
Elapsed time for attention_linear_projection (4x23104x23104, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x23104x23104, b=2048): 262.090
Elapsed time for mlp_h_to_4h (4x23104x92416, b=2048): 0.1409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23104x92416, b=2048): 248.326
Elapsed time for mlp_4h_to_h (4x92416x23104, b=2048): 0.1304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92416x23104, b=2048): 268.186

Attention duration (in seconds): 0.1415
Attention throughput (in TFLOP/s): 258.196
MLP duration (in seconds): 0.2713
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23136x69408, b=2048): 0.1708
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23136x69408, b=2048): 154.071
b: 64, m: 2048, n: 1446, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1446x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1446x2048): 173.613
b: 64, m: 2048, n: 2048, k: 1446,
Elapsed time for attention_prob_times_values (64x2048x2048x1446): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1446): 132.935
Elapsed time for attention_linear_projection (4x23136x23136, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x23136x23136, b=2048): 261.507
Elapsed time for mlp_h_to_4h (4x23136x92544, b=2048): 0.2337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23136x92544, b=2048): 150.134
Elapsed time for mlp_4h_to_h (4x92544x23136, b=2048): 0.1322
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92544x23136, b=2048): 265.261

Attention duration (in seconds): 0.2146
Attention throughput (in TFLOP/s): 170.692
MLP duration (in seconds): 0.3659
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.0993
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 265.797
b: 64, m: 2048, n: 1448, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1448x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1448x2048): 178.302
b: 64, m: 2048, n: 2048, k: 1448,
Elapsed time for attention_prob_times_values (64x2048x2048x1448): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1448): 178.781
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 262.249
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1320
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 266.450
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1322
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 266.028

Attention duration (in seconds): 0.1415
Attention throughput (in TFLOP/s): 259.586
MLP duration (in seconds): 0.2643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23200x69600, b=2048): 0.1649
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23200x69600, b=2048): 160.479
b: 64, m: 2048, n: 1450, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1450x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1450x2048): 173.898
b: 64, m: 2048, n: 2048, k: 1450,
Elapsed time for attention_prob_times_values (64x2048x2048x1450): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1450): 131.865
Elapsed time for attention_linear_projection (4x23200x23200, b=2048): 0.0340
Throughput (in TFLOP/s) for attention_linear_projection (4x23200x23200, b=2048): 259.744
Elapsed time for mlp_h_to_4h (4x23200x92800, b=2048): 0.2273
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23200x92800, b=2048): 155.191
Elapsed time for mlp_4h_to_h (4x92800x23200, b=2048): 0.1339
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92800x23200, b=2048): 263.446

Attention duration (in seconds): 0.2092
Attention throughput (in TFLOP/s): 176.070
MLP duration (in seconds): 0.3612
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5704
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23232x69696, b=2048): 0.1007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23232x69696, b=2048): 263.346
b: 64, m: 2048, n: 1452, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1452x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1452x2048): 180.213
b: 64, m: 2048, n: 2048, k: 1452,
Elapsed time for attention_prob_times_values (64x2048x2048x1452): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1452): 159.657
Elapsed time for attention_linear_projection (4x23232x23232, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_linear_projection (4x23232x23232, b=2048): 261.611
Elapsed time for mlp_h_to_4h (4x23232x92928, b=2048): 0.1338
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23232x92928, b=2048): 264.385
Elapsed time for mlp_4h_to_h (4x92928x23232, b=2048): 0.1337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92928x23232, b=2048): 264.641

Attention duration (in seconds): 0.1437
Attention throughput (in TFLOP/s): 256.915
MLP duration (in seconds): 0.2674
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23264x69792, b=2048): 0.1726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23264x69792, b=2048): 154.140
b: 64, m: 2048, n: 1454, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1454x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1454x2048): 174.722
b: 64, m: 2048, n: 2048, k: 1454,
Elapsed time for attention_prob_times_values (64x2048x2048x1454): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1454): 133.659
Elapsed time for attention_linear_projection (4x23264x23264, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x23264x23264, b=2048): 260.087
Elapsed time for mlp_h_to_4h (4x23264x93056, b=2048): 0.2328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23264x93056, b=2048): 152.351
Elapsed time for mlp_4h_to_h (4x93056x23264, b=2048): 0.1351
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93056x23264, b=2048): 262.617

Attention duration (in seconds): 0.2170
Attention throughput (in TFLOP/s): 170.659
MLP duration (in seconds): 0.3679
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5849
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 264.893
b: 64, m: 2048, n: 1456, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1456x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1456x2048): 180.075
b: 64, m: 2048, n: 2048, k: 1456,
Elapsed time for attention_prob_times_values (64x2048x2048x1456): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1456): 181.701
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 264.253
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 267.215
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 268.110

Attention duration (in seconds): 0.1430
Attention throughput (in TFLOP/s): 259.664
MLP duration (in seconds): 0.2658
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23328x69984, b=2048): 0.1746
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23328x69984, b=2048): 153.217
b: 64, m: 2048, n: 1458, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1458x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1458x2048): 174.079
b: 64, m: 2048, n: 2048, k: 1458,
Elapsed time for attention_prob_times_values (64x2048x2048x1458): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1458): 133.449
Elapsed time for attention_linear_projection (4x23328x23328, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23328x23328, b=2048): 258.879
Elapsed time for mlp_h_to_4h (4x23328x93312, b=2048): 0.2456
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23328x93312, b=2048): 145.201
Elapsed time for mlp_4h_to_h (4x93312x23328, b=2048): 0.1345
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93312x23328, b=2048): 265.105

Attention duration (in seconds): 0.2194
Attention throughput (in TFLOP/s): 169.704
MLP duration (in seconds): 0.3802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5995
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23360x70080, b=2048): 0.1017
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23360x70080, b=2048): 263.693
b: 64, m: 2048, n: 1460, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1460x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1460x2048): 182.271
b: 64, m: 2048, n: 2048, k: 1460,
Elapsed time for attention_prob_times_values (64x2048x2048x1460): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1460): 162.088
Elapsed time for attention_linear_projection (4x23360x23360, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23360x23360, b=2048): 259.699
Elapsed time for mlp_h_to_4h (4x23360x93440, b=2048): 0.1356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23360x93440, b=2048): 263.737
Elapsed time for mlp_4h_to_h (4x93440x23360, b=2048): 0.1344
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93440x23360, b=2048): 266.106

Attention duration (in seconds): 0.1453
Attention throughput (in TFLOP/s): 256.954
MLP duration (in seconds): 0.2700
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23392x70176, b=2048): 0.1725
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23392x70176, b=2048): 155.934
b: 64, m: 2048, n: 1462, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1462x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1462x2048): 175.562
b: 64, m: 2048, n: 2048, k: 1462,
Elapsed time for attention_prob_times_values (64x2048x2048x1462): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1462): 132.034
Elapsed time for attention_linear_projection (4x23392x23392, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_linear_projection (4x23392x23392, b=2048): 259.674
Elapsed time for mlp_h_to_4h (4x23392x93568, b=2048): 0.2427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23392x93568, b=2048): 147.770
Elapsed time for mlp_4h_to_h (4x93568x23392, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93568x23392, b=2048): 265.822

Attention duration (in seconds): 0.2174
Attention throughput (in TFLOP/s): 172.157
MLP duration (in seconds): 0.3776
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 265.463
b: 64, m: 2048, n: 1464, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1464x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1464x2048): 180.351
b: 64, m: 2048, n: 2048, k: 1464,
Elapsed time for attention_prob_times_values (64x2048x2048x1464): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1464): 180.826
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 261.240
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.2361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 152.278
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 267.724

Attention duration (in seconds): 0.1447
Attention throughput (in TFLOP/s): 259.353
MLP duration (in seconds): 0.3705
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23456x70368, b=2048): 0.1774
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23456x70368, b=2048): 152.452
b: 64, m: 2048, n: 1466, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1466x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1466x2048): 174.802
b: 64, m: 2048, n: 2048, k: 1466,
Elapsed time for attention_prob_times_values (64x2048x2048x1466): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1466): 131.740
Elapsed time for attention_linear_projection (4x23456x23456, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23456x23456, b=2048): 260.830
Elapsed time for mlp_h_to_4h (4x23456x93824, b=2048): 0.2487
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23456x93824, b=2048): 144.983
Elapsed time for mlp_4h_to_h (4x93824x23456, b=2048): 0.1360
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93824x23456, b=2048): 265.061

Attention duration (in seconds): 0.2224
Attention throughput (in TFLOP/s): 169.188
MLP duration (in seconds): 0.3847
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23488x70464, b=2048): 0.1025
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23488x70464, b=2048): 264.524
b: 64, m: 2048, n: 1468, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1468x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1468x2048): 182.320
b: 64, m: 2048, n: 2048, k: 1468,
Elapsed time for attention_prob_times_values (64x2048x2048x1468): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1468): 162.097
Elapsed time for attention_linear_projection (4x23488x23488, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23488x23488, b=2048): 261.474
Elapsed time for mlp_h_to_4h (4x23488x93952, b=2048): 0.1363
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23488x93952, b=2048): 265.217
Elapsed time for mlp_4h_to_h (4x93952x23488, b=2048): 0.1358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93952x23488, b=2048): 266.153

Attention duration (in seconds): 0.1463
Attention throughput (in TFLOP/s): 257.969
MLP duration (in seconds): 0.2722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23520x70560, b=2048): 0.1767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23520x70560, b=2048): 153.852
b: 64, m: 2048, n: 1470, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1470x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1470x2048): 176.306
b: 64, m: 2048, n: 2048, k: 1470,
Elapsed time for attention_prob_times_values (64x2048x2048x1470): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1470): 135.429
Elapsed time for attention_linear_projection (4x23520x23520, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x23520x23520, b=2048): 259.062
Elapsed time for mlp_h_to_4h (4x23520x94080, b=2048): 0.2468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23520x94080, b=2048): 146.912
Elapsed time for mlp_4h_to_h (4x94080x23520, b=2048): 0.1374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94080x23520, b=2048): 263.794

Attention duration (in seconds): 0.2220
Attention throughput (in TFLOP/s): 170.400
MLP duration (in seconds): 0.3842
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 264.257
b: 64, m: 2048, n: 1472, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1472x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1472x2048): 182.857
b: 64, m: 2048, n: 2048, k: 1472,
Elapsed time for attention_prob_times_values (64x2048x2048x1472): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1472): 184.035
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0347
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 261.724
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1378
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 263.851
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 264.997

Attention duration (in seconds): 0.1465
Attention throughput (in TFLOP/s): 258.904
MLP duration (in seconds): 0.2750
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23584x70752, b=2048): 0.1802
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23584x70752, b=2048): 151.741
b: 64, m: 2048, n: 1474, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1474x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1474x2048): 176.899
b: 64, m: 2048, n: 2048, k: 1474,
Elapsed time for attention_prob_times_values (64x2048x2048x1474): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1474): 132.300
Elapsed time for attention_linear_projection (4x23584x23584, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_linear_projection (4x23584x23584, b=2048): 259.839
Elapsed time for mlp_h_to_4h (4x23584x94336, b=2048): 0.2455
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23584x94336, b=2048): 148.462
Elapsed time for mlp_4h_to_h (4x94336x23584, b=2048): 0.1389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94336x23584, b=2048): 262.361

Attention duration (in seconds): 0.2257
Attention throughput (in TFLOP/s): 168.523
MLP duration (in seconds): 0.3845
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23616x70848, b=2048): 0.1038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23616x70848, b=2048): 263.993
b: 64, m: 2048, n: 1476, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1476x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1476x2048): 183.486
b: 64, m: 2048, n: 2048, k: 1476,
Elapsed time for attention_prob_times_values (64x2048x2048x1476): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1476): 159.853
Elapsed time for attention_linear_projection (4x23616x23616, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_linear_projection (4x23616x23616, b=2048): 260.573
Elapsed time for mlp_h_to_4h (4x23616x94464, b=2048): 0.1380
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23616x94464, b=2048): 264.899
Elapsed time for mlp_4h_to_h (4x94464x23616, b=2048): 0.1379
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94464x23616, b=2048): 265.021

Attention duration (in seconds): 0.1482
Attention throughput (in TFLOP/s): 257.353
MLP duration (in seconds): 0.2759
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23648x70944, b=2048): 0.1776
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23648x70944, b=2048): 154.765
b: 64, m: 2048, n: 1478, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1478x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1478x2048): 176.663
b: 64, m: 2048, n: 2048, k: 1478,
Elapsed time for attention_prob_times_values (64x2048x2048x1478): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1478): 132.845
Elapsed time for attention_linear_projection (4x23648x23648, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_linear_projection (4x23648x23648, b=2048): 260.294
Elapsed time for mlp_h_to_4h (4x23648x94592, b=2048): 0.2580
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23648x94592, b=2048): 142.036
Elapsed time for mlp_4h_to_h (4x94592x23648, b=2048): 0.1388
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94592x23648, b=2048): 264.034

Attention duration (in seconds): 0.2233
Attention throughput (in TFLOP/s): 171.256
MLP duration (in seconds): 0.3968
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1033
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 266.937
b: 64, m: 2048, n: 1480, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1480x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1480x2048): 181.924
b: 64, m: 2048, n: 2048, k: 1480,
Elapsed time for attention_prob_times_values (64x2048x2048x1480): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1480): 179.070
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0348
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 264.002
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1497
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 245.437
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 267.226

Attention duration (in seconds): 0.1469
Attention throughput (in TFLOP/s): 261.058
MLP duration (in seconds): 0.2872
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4341
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23712x71136, b=2048): 0.1830
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23712x71136, b=2048): 151.056
b: 64, m: 2048, n: 1482, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1482x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1482x2048): 176.560
b: 64, m: 2048, n: 2048, k: 1482,
Elapsed time for attention_prob_times_values (64x2048x2048x1482): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1482): 133.497
Elapsed time for attention_linear_projection (4x23712x23712, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23712x23712, b=2048): 259.520
Elapsed time for mlp_h_to_4h (4x23712x94848, b=2048): 0.2537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23712x94848, b=2048): 145.253
Elapsed time for mlp_4h_to_h (4x94848x23712, b=2048): 0.1392
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94848x23712, b=2048): 264.716

Attention duration (in seconds): 0.2289
Attention throughput (in TFLOP/s): 167.919
MLP duration (in seconds): 0.3929
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23744x71232, b=2048): 0.1052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23744x71232, b=2048): 263.419
b: 64, m: 2048, n: 1484, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1484x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1484x2048): 184.023
b: 64, m: 2048, n: 2048, k: 1484,
Elapsed time for attention_prob_times_values (64x2048x2048x1484): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1484): 160.019
Elapsed time for attention_linear_projection (4x23744x23744, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23744x23744, b=2048): 260.277
Elapsed time for mlp_h_to_4h (4x23744x94976, b=2048): 0.1442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23744x94976, b=2048): 256.292
Elapsed time for mlp_4h_to_h (4x94976x23744, b=2048): 0.1378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94976x23744, b=2048): 268.169

Attention duration (in seconds): 0.1500
Attention throughput (in TFLOP/s): 256.952
MLP duration (in seconds): 0.2819
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23776x71328, b=2048): 0.1808
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23776x71328, b=2048): 153.641
b: 64, m: 2048, n: 1486, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1486x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1486x2048): 177.535
b: 64, m: 2048, n: 2048, k: 1486,
Elapsed time for attention_prob_times_values (64x2048x2048x1486): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1486): 133.170
Elapsed time for attention_linear_projection (4x23776x23776, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23776x23776, b=2048): 260.416
Elapsed time for mlp_h_to_4h (4x23776x95104, b=2048): 0.2532
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23776x95104, b=2048): 146.302
Elapsed time for mlp_4h_to_h (4x95104x23776, b=2048): 0.1397
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95104x23776, b=2048): 265.278

Attention duration (in seconds): 0.2269
Attention throughput (in TFLOP/s): 170.311
MLP duration (in seconds): 0.3929
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 264.411
b: 64, m: 2048, n: 1488, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1488x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1488x2048): 184.326
b: 64, m: 2048, n: 2048, k: 1488,
Elapsed time for attention_prob_times_values (64x2048x2048x1488): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1488): 182.082
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 261.417
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 264.438
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 266.515

Attention duration (in seconds): 0.1496
Attention throughput (in TFLOP/s): 258.966
MLP duration (in seconds): 0.2799
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23840x71520, b=2048): 0.1880
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23840x71520, b=2048): 148.626
b: 64, m: 2048, n: 1490, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1490x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1490x2048): 179.249
b: 64, m: 2048, n: 2048, k: 1490,
Elapsed time for attention_prob_times_values (64x2048x2048x1490): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1490): 134.439
Elapsed time for attention_linear_projection (4x23840x23840, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x23840x23840, b=2048): 257.968
Elapsed time for mlp_h_to_4h (4x23840x95360, b=2048): 0.2588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23840x95360, b=2048): 143.934
Elapsed time for mlp_4h_to_h (4x95360x23840, b=2048): 0.1412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95360x23840, b=2048): 263.829

Attention duration (in seconds): 0.2345
Attention throughput (in TFLOP/s): 165.682
MLP duration (in seconds): 0.4000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6344
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23872x71616, b=2048): 0.1064
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23872x71616, b=2048): 263.190
b: 64, m: 2048, n: 1492, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1492x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1492x2048): 185.895
b: 64, m: 2048, n: 2048, k: 1492,
Elapsed time for attention_prob_times_values (64x2048x2048x1492): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1492): 161.905
Elapsed time for attention_linear_projection (4x23872x23872, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x23872x23872, b=2048): 258.858
Elapsed time for mlp_h_to_4h (4x23872x95488, b=2048): 0.1507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23872x95488, b=2048): 247.769
Elapsed time for mlp_4h_to_h (4x95488x23872, b=2048): 0.1412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95488x23872, b=2048): 264.491

Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 256.663
MLP duration (in seconds): 0.2919
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23904x71712, b=2048): 0.1847
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23904x71712, b=2048): 152.101
b: 64, m: 2048, n: 1494, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1494x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1494x2048): 178.320
b: 64, m: 2048, n: 2048, k: 1494,
Elapsed time for attention_prob_times_values (64x2048x2048x1494): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1494): 133.855
Elapsed time for attention_linear_projection (4x23904x23904, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_linear_projection (4x23904x23904, b=2048): 258.825
Elapsed time for mlp_h_to_4h (4x23904x95616, b=2048): 0.2634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23904x95616, b=2048): 142.158
Elapsed time for mlp_4h_to_h (4x95616x23904, b=2048): 0.1426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95616x23904, b=2048): 262.676

Attention duration (in seconds): 0.2313
Attention throughput (in TFLOP/s): 168.826
MLP duration (in seconds): 0.4060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6373
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1063
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 264.856
b: 64, m: 2048, n: 1496, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1496x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1496x2048): 184.132
b: 64, m: 2048, n: 2048, k: 1496,
Elapsed time for attention_prob_times_values (64x2048x2048x1496): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1496): 180.163
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 262.116
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1927
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 194.806
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 265.543

Attention duration (in seconds): 0.1510
Attention throughput (in TFLOP/s): 259.372
MLP duration (in seconds): 0.3341
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4851
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23968x71904, b=2048): 0.1877
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23968x71904, b=2048): 150.428
b: 64, m: 2048, n: 1498, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1498x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1498x2048): 178.684
b: 64, m: 2048, n: 2048, k: 1498,
Elapsed time for attention_prob_times_values (64x2048x2048x1498): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1498): 134.038
Elapsed time for attention_linear_projection (4x23968x23968, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x23968x23968, b=2048): 259.045
Elapsed time for mlp_h_to_4h (4x23968x95872, b=2048): 0.2584
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23968x95872, b=2048): 145.707
Elapsed time for mlp_4h_to_h (4x95872x23968, b=2048): 0.1431
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95872x23968, b=2048): 263.155

Attention duration (in seconds): 0.2345
Attention throughput (in TFLOP/s): 167.377
MLP duration (in seconds): 0.4014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24000x72000, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24000x72000, b=2048): 263.679
b: 64, m: 2048, n: 1500, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1500x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1500x2048): 185.915
b: 64, m: 2048, n: 2048, k: 1500,
Elapsed time for attention_prob_times_values (64x2048x2048x1500): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1500): 161.690
Elapsed time for attention_linear_projection (4x24000x24000, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x24000x24000, b=2048): 261.431
Elapsed time for mlp_h_to_4h (4x24000x96000, b=2048): 0.1427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24000x96000, b=2048): 264.445
Elapsed time for mlp_4h_to_h (4x96000x24000, b=2048): 0.1425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96000x24000, b=2048): 264.920

Attention duration (in seconds): 0.1528
Attention throughput (in TFLOP/s): 257.619
MLP duration (in seconds): 0.2852
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4380
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24032x72096, b=2048): 0.1880
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24032x72096, b=2048): 150.963
b: 64, m: 2048, n: 1502, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1502x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1502x2048): 179.532
b: 64, m: 2048, n: 2048, k: 1502,
Elapsed time for attention_prob_times_values (64x2048x2048x1502): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1502): 134.755
Elapsed time for attention_linear_projection (4x24032x24032, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_linear_projection (4x24032x24032, b=2048): 259.744
Elapsed time for mlp_h_to_4h (4x24032x96128, b=2048): 0.2589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24032x96128, b=2048): 146.205
Elapsed time for mlp_4h_to_h (4x96128x24032, b=2048): 0.1440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96128x24032, b=2048): 262.827

Attention duration (in seconds): 0.2349
Attention throughput (in TFLOP/s): 167.963
MLP duration (in seconds): 0.4029
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6378
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 264.471
b: 64, m: 2048, n: 1504, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1504x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1504x2048): 186.318
b: 64, m: 2048, n: 2048, k: 1504,
Elapsed time for attention_prob_times_values (64x2048x2048x1504): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1504): 183.144
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 261.193
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 265.015
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 264.954

Attention duration (in seconds): 0.1527
Attention throughput (in TFLOP/s): 259.125
MLP duration (in seconds): 0.2864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4391
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24096x72288, b=2048): 0.1900
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24096x72288, b=2048): 150.240
b: 64, m: 2048, n: 1506, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1506x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1506x2048): 180.451
b: 64, m: 2048, n: 2048, k: 1506,
Elapsed time for attention_prob_times_values (64x2048x2048x1506): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1506): 134.947
Elapsed time for attention_linear_projection (4x24096x24096, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x24096x24096, b=2048): 260.745
Elapsed time for mlp_h_to_4h (4x24096x96384, b=2048): 0.2597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24096x96384, b=2048): 146.511
Elapsed time for mlp_4h_to_h (4x96384x24096, b=2048): 0.1447
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96384x24096, b=2048): 262.934

Attention duration (in seconds): 0.2369
Attention throughput (in TFLOP/s): 167.442
MLP duration (in seconds): 0.4044
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24128x72384, b=2048): 0.1083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24128x72384, b=2048): 264.122
b: 64, m: 2048, n: 1508, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1508x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1508x2048): 187.422
b: 64, m: 2048, n: 2048, k: 1508,
Elapsed time for attention_prob_times_values (64x2048x2048x1508): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1508): 162.958
Elapsed time for attention_linear_projection (4x24128x24128, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x24128x24128, b=2048): 262.457
Elapsed time for mlp_h_to_4h (4x24128x96512, b=2048): 0.1704
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24128x96512, b=2048): 223.844
Elapsed time for mlp_4h_to_h (4x96512x24128, b=2048): 0.1425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96512x24128, b=2048): 267.664

Attention duration (in seconds): 0.1540
Attention throughput (in TFLOP/s): 258.313
MLP duration (in seconds): 0.3130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4669
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24160x72480, b=2048): 0.1883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24160x72480, b=2048): 152.357
b: 64, m: 2048, n: 1510, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1510x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1510x2048): 180.747
b: 64, m: 2048, n: 2048, k: 1510,
Elapsed time for attention_prob_times_values (64x2048x2048x1510): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1510): 135.483
Elapsed time for attention_linear_projection (4x24160x24160, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_linear_projection (4x24160x24160, b=2048): 262.463
Elapsed time for mlp_h_to_4h (4x24160x96640, b=2048): 0.2690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24160x96640, b=2048): 142.198
Elapsed time for mlp_4h_to_h (4x96640x24160, b=2048): 0.1438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96640x24160, b=2048): 266.043

Attention duration (in seconds): 0.2352
Attention throughput (in TFLOP/s): 169.525
MLP duration (in seconds): 0.4128
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6480
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 267.381
b: 64, m: 2048, n: 1512, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1512x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1512x2048): 185.412
b: 64, m: 2048, n: 2048, k: 1512,
Elapsed time for attention_prob_times_values (64x2048x2048x1512): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1512): 181.160
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0360
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 266.390
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.2063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 185.926
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 267.310

Attention duration (in seconds): 0.1524
Attention throughput (in TFLOP/s): 262.259
MLP duration (in seconds): 0.3498
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24224x72672, b=2048): 0.1900
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24224x72672, b=2048): 151.818
b: 64, m: 2048, n: 1514, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1514x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1514x2048): 181.111
b: 64, m: 2048, n: 2048, k: 1514,
Elapsed time for attention_prob_times_values (64x2048x2048x1514): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1514): 135.185
Elapsed time for attention_linear_projection (4x24224x24224, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_linear_projection (4x24224x24224, b=2048): 259.916
Elapsed time for mlp_h_to_4h (4x24224x96896, b=2048): 0.2692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24224x96896, b=2048): 142.875
Elapsed time for mlp_4h_to_h (4x96896x24224, b=2048): 0.1448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96896x24224, b=2048): 265.536

Attention duration (in seconds): 0.2375
Attention throughput (in TFLOP/s): 168.789
MLP duration (in seconds): 0.4140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6515
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24256x72768, b=2048): 0.1092
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24256x72768, b=2048): 264.709
b: 64, m: 2048, n: 1516, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1516x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1516x2048): 188.084
b: 64, m: 2048, n: 2048, k: 1516,
Elapsed time for attention_prob_times_values (64x2048x2048x1516): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1516): 162.412
Elapsed time for attention_linear_projection (4x24256x24256, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x24256x24256, b=2048): 262.281
Elapsed time for mlp_h_to_4h (4x24256x97024, b=2048): 0.2315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24256x97024, b=2048): 166.594
Elapsed time for mlp_4h_to_h (4x97024x24256, b=2048): 0.1439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97024x24256, b=2048): 267.952

Attention duration (in seconds): 0.1553
Attention throughput (in TFLOP/s): 258.700
MLP duration (in seconds): 0.3754
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24288x72864, b=2048): 0.1974
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24288x72864, b=2048): 146.907
b: 64, m: 2048, n: 1518, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1518x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1518x2048): 181.473
b: 64, m: 2048, n: 2048, k: 1518,
Elapsed time for attention_prob_times_values (64x2048x2048x1518): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1518): 134.180
Elapsed time for attention_linear_projection (4x24288x24288, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_linear_projection (4x24288x24288, b=2048): 260.876
Elapsed time for mlp_h_to_4h (4x24288x97152, b=2048): 0.2689
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24288x97152, b=2048): 143.770
Elapsed time for mlp_4h_to_h (4x97152x24288, b=2048): 0.1456
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97152x24288, b=2048): 265.533

Attention duration (in seconds): 0.2450
Attention throughput (in TFLOP/s): 164.460
MLP duration (in seconds): 0.4145
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6595
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 265.570
b: 64, m: 2048, n: 1520, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1520x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1520x2048): 187.157
b: 64, m: 2048, n: 2048, k: 1520,
Elapsed time for attention_prob_times_values (64x2048x2048x1520): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1520): 183.978
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 263.419
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 250.462
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 267.766

Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 260.520
MLP duration (in seconds): 0.2995
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4546
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24352x73056, b=2048): 0.1955
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24352x73056, b=2048): 149.116
b: 64, m: 2048, n: 1522, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1522x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1522x2048): 182.689
b: 64, m: 2048, n: 2048, k: 1522,
Elapsed time for attention_prob_times_values (64x2048x2048x1522): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1522): 136.603
Elapsed time for attention_linear_projection (4x24352x24352, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x24352x24352, b=2048): 261.813
Elapsed time for mlp_h_to_4h (4x24352x97408, b=2048): 0.2680
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24352x97408, b=2048): 144.993
Elapsed time for mlp_4h_to_h (4x97408x24352, b=2048): 0.1464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97408x24352, b=2048): 265.403

Attention duration (in seconds): 0.2430
Attention throughput (in TFLOP/s): 166.634
MLP duration (in seconds): 0.4145
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6575
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24384x73152, b=2048): 0.1103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24384x73152, b=2048): 265.062
b: 64, m: 2048, n: 1524, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1524x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1524x2048): 189.599
b: 64, m: 2048, n: 2048, k: 1524,
Elapsed time for attention_prob_times_values (64x2048x2048x1524): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1524): 164.592
Elapsed time for attention_linear_projection (4x24384x24384, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x24384x24384, b=2048): 262.796
Elapsed time for mlp_h_to_4h (4x24384x97536, b=2048): 0.1467
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24384x97536, b=2048): 265.622
Elapsed time for mlp_4h_to_h (4x97536x24384, b=2048): 0.1462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97536x24384, b=2048): 266.529

Attention duration (in seconds): 0.1566
Attention throughput (in TFLOP/s): 259.257
MLP duration (in seconds): 0.2929
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24416x73248, b=2048): 0.1911
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24416x73248, b=2048): 153.347
b: 64, m: 2048, n: 1526, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1526x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1526x2048): 183.492
b: 64, m: 2048, n: 2048, k: 1526,
Elapsed time for attention_prob_times_values (64x2048x2048x1526): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1526): 136.359
Elapsed time for attention_linear_projection (4x24416x24416, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_linear_projection (4x24416x24416, b=2048): 262.387
Elapsed time for mlp_h_to_4h (4x24416x97664, b=2048): 0.2695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24416x97664, b=2048): 144.985
Elapsed time for mlp_4h_to_h (4x97664x24416, b=2048): 0.1473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97664x24416, b=2048): 265.294

Attention duration (in seconds): 0.2388
Attention throughput (in TFLOP/s): 170.482
MLP duration (in seconds): 0.4167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6555
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1104
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 266.142
b: 64, m: 2048, n: 1528, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1528x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1528x2048): 187.191
b: 64, m: 2048, n: 2048, k: 1528,
Elapsed time for attention_prob_times_values (64x2048x2048x1528): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1528): 182.330
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 263.452
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1876
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 208.767
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1466
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 267.122

Attention duration (in seconds): 0.1564
Attention throughput (in TFLOP/s): 260.881
MLP duration (in seconds): 0.3343
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4907
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24480x73440, b=2048): 0.1903
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24480x73440, b=2048): 154.776
b: 64, m: 2048, n: 1530, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1530x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1530x2048): 184.248
b: 64, m: 2048, n: 2048, k: 1530,
Elapsed time for attention_prob_times_values (64x2048x2048x1530): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1530): 135.865
Elapsed time for attention_linear_projection (4x24480x24480, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_linear_projection (4x24480x24480, b=2048): 263.057
Elapsed time for mlp_h_to_4h (4x24480x97920, b=2048): 0.2721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24480x97920, b=2048): 144.322
Elapsed time for mlp_4h_to_h (4x97920x24480, b=2048): 0.1492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97920x24480, b=2048): 263.276

Attention duration (in seconds): 0.2381
Attention throughput (in TFLOP/s): 171.819
MLP duration (in seconds): 0.4213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24512x73536, b=2048): 0.1117
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24512x73536, b=2048): 264.505
b: 64, m: 2048, n: 1532, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1532x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1532x2048): 190.594
b: 64, m: 2048, n: 2048, k: 1532,
Elapsed time for attention_prob_times_values (64x2048x2048x1532): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1532): 164.887
Elapsed time for attention_linear_projection (4x24512x24512, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_linear_projection (4x24512x24512, b=2048): 261.457
Elapsed time for mlp_h_to_4h (4x24512x98048, b=2048): 0.1473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24512x98048, b=2048): 267.285
Elapsed time for mlp_4h_to_h (4x98048x24512, b=2048): 0.1473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98048x24512, b=2048): 267.389

Attention duration (in seconds): 0.1586
Attention throughput (in TFLOP/s): 258.638
MLP duration (in seconds): 0.2946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24544x73632, b=2048): 0.1960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24544x73632, b=2048): 151.054
b: 64, m: 2048, n: 1534, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1534x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1534x2048): 183.992
b: 64, m: 2048, n: 2048, k: 1534,
Elapsed time for attention_prob_times_values (64x2048x2048x1534): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1534): 137.385
Elapsed time for attention_linear_projection (4x24544x24544, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x24544x24544, b=2048): 263.695
Elapsed time for mlp_h_to_4h (4x24544x98176, b=2048): 0.2808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24544x98176, b=2048): 140.602
Elapsed time for mlp_4h_to_h (4x98176x24544, b=2048): 0.1489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98176x24544, b=2048): 265.072

Attention duration (in seconds): 0.2439
Attention throughput (in TFLOP/s): 168.607
MLP duration (in seconds): 0.4297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6736
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1110
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 267.501
b: 64, m: 2048, n: 1536, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1536x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1536x2048): 190.356
b: 64, m: 2048, n: 2048, k: 1536,
Elapsed time for attention_prob_times_values (64x2048x2048x1536): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1536): 186.567
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 264.777
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.2184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 181.205
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1484
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 266.643

Attention duration (in seconds): 0.1571
Attention throughput (in TFLOP/s): 262.449
MLP duration (in seconds): 0.3669
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24608x73824, b=2048): 0.1972
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24608x73824, b=2048): 150.937
b: 64, m: 2048, n: 1538, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1538x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1538x2048): 174.639
b: 64, m: 2048, n: 2048, k: 1538,
Elapsed time for attention_prob_times_values (64x2048x2048x1538): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1538): 135.836
Elapsed time for attention_linear_projection (4x24608x24608, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x24608x24608, b=2048): 257.484
Elapsed time for mlp_h_to_4h (4x24608x98432, b=2048): 0.2801
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24608x98432, b=2048): 141.678
Elapsed time for mlp_4h_to_h (4x98432x24608, b=2048): 0.1508
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98432x24608, b=2048): 263.216

Attention duration (in seconds): 0.2465
Attention throughput (in TFLOP/s): 167.672
MLP duration (in seconds): 0.4309
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6774
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24640x73920, b=2048): 0.1140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24640x73920, b=2048): 261.794
b: 64, m: 2048, n: 1540, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1540x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1540x2048): 180.764
b: 64, m: 2048, n: 2048, k: 1540,
Elapsed time for attention_prob_times_values (64x2048x2048x1540): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1540): 161.495
Elapsed time for attention_linear_projection (4x24640x24640, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_linear_projection (4x24640x24640, b=2048): 258.732
Elapsed time for mlp_h_to_4h (4x24640x98560, b=2048): 0.1516
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24640x98560, b=2048): 262.412
Elapsed time for mlp_4h_to_h (4x98560x24640, b=2048): 0.1502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98560x24640, b=2048): 264.910

Attention duration (in seconds): 0.1621
Attention throughput (in TFLOP/s): 255.615
MLP duration (in seconds): 0.3018
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24672x74016, b=2048): 0.1975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24672x74016, b=2048): 151.481
b: 64, m: 2048, n: 1542, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1542x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1542x2048): 175.005
b: 64, m: 2048, n: 2048, k: 1542,
Elapsed time for attention_prob_times_values (64x2048x2048x1542): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1542): 134.799
Elapsed time for attention_linear_projection (4x24672x24672, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_linear_projection (4x24672x24672, b=2048): 258.360
Elapsed time for mlp_h_to_4h (4x24672x98688, b=2048): 0.2761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24672x98688, b=2048): 144.495
Elapsed time for mlp_4h_to_h (4x98688x24672, b=2048): 0.1522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98688x24672, b=2048): 262.036

Attention duration (in seconds): 0.2470
Attention throughput (in TFLOP/s): 168.221
MLP duration (in seconds): 0.4283
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 262.268
b: 64, m: 2048, n: 1544, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1544x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1544x2048): 178.187
b: 64, m: 2048, n: 2048, k: 1544,
Elapsed time for attention_prob_times_values (64x2048x2048x1544): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1544): 181.716
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 259.215
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 237.356
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1510
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 264.852

Attention duration (in seconds): 0.1622
Attention throughput (in TFLOP/s): 256.864
MLP duration (in seconds): 0.3195
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24736x74208, b=2048): 0.2012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24736x74208, b=2048): 149.511
b: 64, m: 2048, n: 1546, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1546x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1546x2048): 175.486
b: 64, m: 2048, n: 2048, k: 1546,
Elapsed time for attention_prob_times_values (64x2048x2048x1546): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1546): 134.876
Elapsed time for attention_linear_projection (4x24736x24736, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24736x24736, b=2048): 258.998
Elapsed time for mlp_h_to_4h (4x24736x98944, b=2048): 0.2843
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24736x98944, b=2048): 141.060
Elapsed time for mlp_4h_to_h (4x98944x24736, b=2048): 0.1529
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98944x24736, b=2048): 262.344

Attention duration (in seconds): 0.2507
Attention throughput (in TFLOP/s): 166.543
MLP duration (in seconds): 0.4371
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6879
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24768x74304, b=2048): 0.1148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24768x74304, b=2048): 262.657
b: 64, m: 2048, n: 1548, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1548x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1548x2048): 181.259
b: 64, m: 2048, n: 2048, k: 1548,
Elapsed time for attention_prob_times_values (64x2048x2048x1548): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1548): 161.282
Elapsed time for attention_linear_projection (4x24768x24768, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24768x24768, b=2048): 259.999
Elapsed time for mlp_h_to_4h (4x24768x99072, b=2048): 0.1533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24768x99072, b=2048): 262.332
Elapsed time for mlp_4h_to_h (4x99072x24768, b=2048): 0.1518
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99072x24768, b=2048): 264.762

Attention duration (in seconds): 0.1632
Attention throughput (in TFLOP/s): 256.540
MLP duration (in seconds): 0.3051
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4683
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24800x74400, b=2048): 0.2077
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24800x74400, b=2048): 145.525
b: 64, m: 2048, n: 1550, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1550x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1550x2048): 175.251
b: 64, m: 2048, n: 2048, k: 1550,
Elapsed time for attention_prob_times_values (64x2048x2048x1550): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1550): 134.979
Elapsed time for attention_linear_projection (4x24800x24800, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_linear_projection (4x24800x24800, b=2048): 259.732
Elapsed time for mlp_h_to_4h (4x24800x99200, b=2048): 0.2799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24800x99200, b=2048): 144.019
Elapsed time for mlp_4h_to_h (4x99200x24800, b=2048): 0.1530
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99200x24800, b=2048): 263.455

Attention duration (in seconds): 0.2574
Attention throughput (in TFLOP/s): 163.031
MLP duration (in seconds): 0.4329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 263.672
b: 64, m: 2048, n: 1552, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1552x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1552x2048): 180.060
b: 64, m: 2048, n: 2048, k: 1552,
Elapsed time for attention_prob_times_values (64x2048x2048x1552): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1552): 184.529
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 260.613
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1530
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 264.186
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1522
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 265.453

Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 258.374
MLP duration (in seconds): 0.3052
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4681
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24864x74592, b=2048): 0.2009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24864x74592, b=2048): 151.222
b: 64, m: 2048, n: 1554, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1554x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1554x2048): 175.447
b: 64, m: 2048, n: 2048, k: 1554,
Elapsed time for attention_prob_times_values (64x2048x2048x1554): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1554): 135.113
Elapsed time for attention_linear_projection (4x24864x24864, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24864x24864, b=2048): 260.213
Elapsed time for mlp_h_to_4h (4x24864x99456, b=2048): 0.2861
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24864x99456, b=2048): 141.599
Elapsed time for mlp_4h_to_h (4x99456x24864, b=2048): 0.1546
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99456x24864, b=2048): 262.066

Attention duration (in seconds): 0.2508
Attention throughput (in TFLOP/s): 168.201
MLP duration (in seconds): 0.4407
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24896x74688, b=2048): 0.1156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24896x74688, b=2048): 263.575
b: 64, m: 2048, n: 1556, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1556x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1556x2048): 182.082
b: 64, m: 2048, n: 2048, k: 1556,
Elapsed time for attention_prob_times_values (64x2048x2048x1556): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1556): 162.243
Elapsed time for attention_linear_projection (4x24896x24896, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24896x24896, b=2048): 261.219
Elapsed time for mlp_h_to_4h (4x24896x99584, b=2048): 0.1533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24896x99584, b=2048): 264.982
Elapsed time for mlp_4h_to_h (4x99584x24896, b=2048): 0.1534
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99584x24896, b=2048): 264.733

Attention duration (in seconds): 0.1642
Attention throughput (in TFLOP/s): 257.562
MLP duration (in seconds): 0.3067
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4709
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24928x74784, b=2048): 0.2008
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24928x74784, b=2048): 152.133
b: 64, m: 2048, n: 1558, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1558x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1558x2048): 176.111
b: 64, m: 2048, n: 2048, k: 1558,
Elapsed time for attention_prob_times_values (64x2048x2048x1558): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1558): 135.854
Elapsed time for attention_linear_projection (4x24928x24928, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_linear_projection (4x24928x24928, b=2048): 261.178
Elapsed time for mlp_h_to_4h (4x24928x99712, b=2048): 0.2845
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24928x99712, b=2048): 143.156
Elapsed time for mlp_4h_to_h (4x99712x24928, b=2048): 0.1552
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99712x24928, b=2048): 262.391

Attention duration (in seconds): 0.2507
Attention throughput (in TFLOP/s): 169.146
MLP duration (in seconds): 0.4397
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 264.682
b: 64, m: 2048, n: 1560, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1560x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1560x2048): 180.135
b: 64, m: 2048, n: 2048, k: 1560,
Elapsed time for attention_prob_times_values (64x2048x2048x1560): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1560): 182.702
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 262.215
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.2247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 181.694
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 267.362

Attention duration (in seconds): 0.1639
Attention throughput (in TFLOP/s): 259.403
MLP duration (in seconds): 0.3774
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24992x74976, b=2048): 0.2140
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24992x74976, b=2048): 143.446
b: 64, m: 2048, n: 1562, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1562x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1562x2048): 175.857
b: 64, m: 2048, n: 2048, k: 1562,
Elapsed time for attention_prob_times_values (64x2048x2048x1562): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1562): 135.594
Elapsed time for attention_linear_projection (4x24992x24992, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_linear_projection (4x24992x24992, b=2048): 259.629
Elapsed time for mlp_h_to_4h (4x24992x99968, b=2048): 0.2849
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24992x99968, b=2048): 143.699
Elapsed time for mlp_4h_to_h (4x99968x24992, b=2048): 0.1543
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99968x24992, b=2048): 265.234

Attention duration (in seconds): 0.2644
Attention throughput (in TFLOP/s): 161.168
MLP duration (in seconds): 0.4392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25024x75072, b=2048): 0.1168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25024x75072, b=2048): 263.516
b: 64, m: 2048, n: 1564, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1564x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1564x2048): 182.572
b: 64, m: 2048, n: 2048, k: 1564,
Elapsed time for attention_prob_times_values (64x2048x2048x1564): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1564): 163.676
Elapsed time for attention_linear_projection (4x25024x25024, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x25024x25024, b=2048): 261.141
Elapsed time for mlp_h_to_4h (4x25024x100096, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25024x100096, b=2048): 178.210
Elapsed time for mlp_4h_to_h (4x100096x25024, b=2048): 0.1541
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100096x25024, b=2048): 266.290

Attention duration (in seconds): 0.1658
Attention throughput (in TFLOP/s): 257.620
MLP duration (in seconds): 0.3844
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5502
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25056x75168, b=2048): 0.2050
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25056x75168, b=2048): 150.508
b: 64, m: 2048, n: 1566, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1566x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1566x2048): 176.263
b: 64, m: 2048, n: 2048, k: 1566,
Elapsed time for attention_prob_times_values (64x2048x2048x1566): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1566): 135.978
Elapsed time for attention_linear_projection (4x25056x25056, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_linear_projection (4x25056x25056, b=2048): 257.865
Elapsed time for mlp_h_to_4h (4x25056x100224, b=2048): 0.2889
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25056x100224, b=2048): 142.425
Elapsed time for mlp_4h_to_h (4x100224x25056, b=2048): 0.1572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100224x25056, b=2048): 261.802

Attention duration (in seconds): 0.2559
Attention throughput (in TFLOP/s): 167.373
MLP duration (in seconds): 0.4460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 262.226
b: 64, m: 2048, n: 1568, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1568x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1568x2048): 181.898
b: 64, m: 2048, n: 2048, k: 1568,
Elapsed time for attention_prob_times_values (64x2048x2048x1568): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1568): 185.735
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 261.111
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 262.847
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1558
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 264.744

Attention duration (in seconds): 0.1666
Attention throughput (in TFLOP/s): 257.650
MLP duration (in seconds): 0.3127
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4794
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25120x75360, b=2048): 0.2101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25120x75360, b=2048): 147.652
b: 64, m: 2048, n: 1570, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1570x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1570x2048): 176.819
b: 64, m: 2048, n: 2048, k: 1570,
Elapsed time for attention_prob_times_values (64x2048x2048x1570): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1570): 135.744
Elapsed time for attention_linear_projection (4x25120x25120, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x25120x25120, b=2048): 258.462
Elapsed time for mlp_h_to_4h (4x25120x100480, b=2048): 0.2959
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25120x100480, b=2048): 139.738
Elapsed time for mlp_4h_to_h (4x100480x25120, b=2048): 0.1576
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100480x25120, b=2048): 262.344

Attention duration (in seconds): 0.2610
Attention throughput (in TFLOP/s): 164.881
MLP duration (in seconds): 0.4536
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25152x75456, b=2048): 0.1186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25152x75456, b=2048): 262.113
b: 64, m: 2048, n: 1572, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1572x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1572x2048): 183.544
b: 64, m: 2048, n: 2048, k: 1572,
Elapsed time for attention_prob_times_values (64x2048x2048x1572): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1572): 163.980
Elapsed time for attention_linear_projection (4x25152x25152, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_linear_projection (4x25152x25152, b=2048): 259.533
Elapsed time for mlp_h_to_4h (4x25152x100608, b=2048): 0.2306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25152x100608, b=2048): 179.793
Elapsed time for mlp_4h_to_h (4x100608x25152, b=2048): 0.1563
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100608x25152, b=2048): 265.252

Attention duration (in seconds): 0.1683
Attention throughput (in TFLOP/s): 256.354
MLP duration (in seconds): 0.3869
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5552
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25184x75552, b=2048): 0.2081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25184x75552, b=2048): 149.812
b: 64, m: 2048, n: 1574, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1574x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1574x2048): 176.713
b: 64, m: 2048, n: 2048, k: 1574,
Elapsed time for attention_prob_times_values (64x2048x2048x1574): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1574): 135.208
Elapsed time for attention_linear_projection (4x25184x25184, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25184x25184, b=2048): 259.311
Elapsed time for mlp_h_to_4h (4x25184x100736, b=2048): 0.2984
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25184x100736, b=2048): 139.307
Elapsed time for mlp_4h_to_h (4x100736x25184, b=2048): 0.1586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100736x25184, b=2048): 262.147

Attention duration (in seconds): 0.2592
Attention throughput (in TFLOP/s): 166.885
MLP duration (in seconds): 0.4569
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 264.093
b: 64, m: 2048, n: 1576, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1576x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1576x2048): 181.852
b: 64, m: 2048, n: 2048, k: 1576,
Elapsed time for attention_prob_times_values (64x2048x2048x1576): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1576): 183.611
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 260.367
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.2022
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 206.079
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 264.790

Attention duration (in seconds): 0.1676
Attention throughput (in TFLOP/s): 258.708
MLP duration (in seconds): 0.3596
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25248x75744, b=2048): 0.2099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25248x75744, b=2048): 149.260
b: 64, m: 2048, n: 1578, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1578x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1578x2048): 177.534
b: 64, m: 2048, n: 2048, k: 1578,
Elapsed time for attention_prob_times_values (64x2048x2048x1578): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1578): 136.820
Elapsed time for attention_linear_projection (4x25248x25248, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x25248x25248, b=2048): 259.946
Elapsed time for mlp_h_to_4h (4x25248x100992, b=2048): 0.2991
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25248x100992, b=2048): 139.679
Elapsed time for mlp_4h_to_h (4x100992x25248, b=2048): 0.1594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100992x25248, b=2048): 262.141

Attention duration (in seconds): 0.2611
Attention throughput (in TFLOP/s): 166.517
MLP duration (in seconds): 0.4585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25280x75840, b=2048): 0.1192
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25280x75840, b=2048): 263.452
b: 64, m: 2048, n: 1580, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1580x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1580x2048): 184.296
b: 64, m: 2048, n: 2048, k: 1580,
Elapsed time for attention_prob_times_values (64x2048x2048x1580): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1580): 164.464
Elapsed time for attention_linear_projection (4x25280x25280, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25280x25280, b=2048): 261.048
Elapsed time for mlp_h_to_4h (4x25280x101120, b=2048): 0.2628
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25280x101120, b=2048): 159.371
Elapsed time for mlp_4h_to_h (4x101120x25280, b=2048): 0.1584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101120x25280, b=2048): 264.382

Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 257.708
MLP duration (in seconds): 0.4212
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25312x75936, b=2048): 0.2132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25312x75936, b=2048): 147.702
b: 64, m: 2048, n: 1582, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1582x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1582x2048): 177.957
b: 64, m: 2048, n: 2048, k: 1582,
Elapsed time for attention_prob_times_values (64x2048x2048x1582): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1582): 135.533
Elapsed time for attention_linear_projection (4x25312x25312, b=2048): 0.0403
Throughput (in TFLOP/s) for attention_linear_projection (4x25312x25312, b=2048): 260.616
Elapsed time for mlp_h_to_4h (4x25312x101248, b=2048): 0.3021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25312x101248, b=2048): 138.999
Elapsed time for mlp_4h_to_h (4x101248x25312, b=2048): 0.1595
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101248x25312, b=2048): 263.210

Attention duration (in seconds): 0.2645
Attention throughput (in TFLOP/s): 165.152
MLP duration (in seconds): 0.4616
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 266.930
b: 64, m: 2048, n: 1584, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1584x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1584x2048): 183.452
b: 64, m: 2048, n: 2048, k: 1584,
Elapsed time for attention_prob_times_values (64x2048x2048x1584): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1584): 186.668
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 264.597
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1956
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 215.235
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1568
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 268.501

Attention duration (in seconds): 0.1672
Attention throughput (in TFLOP/s): 261.875
MLP duration (in seconds): 0.3524
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25376x76128, b=2048): 0.2156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25376x76128, b=2048): 146.804
b: 64, m: 2048, n: 1586, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1586x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1586x2048): 177.759
b: 64, m: 2048, n: 2048, k: 1586,
Elapsed time for attention_prob_times_values (64x2048x2048x1586): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1586): 135.304
Elapsed time for attention_linear_projection (4x25376x25376, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25376x25376, b=2048): 263.260
Elapsed time for mlp_h_to_4h (4x25376x101504, b=2048): 0.3023
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25376x101504, b=2048): 139.605
Elapsed time for mlp_4h_to_h (4x101504x25376, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101504x25376, b=2048): 264.009

Attention duration (in seconds): 0.2668
Attention throughput (in TFLOP/s): 164.584
MLP duration (in seconds): 0.4621
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25408x76224, b=2048): 0.1194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25408x76224, b=2048): 265.743
b: 64, m: 2048, n: 1588, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1588x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1588x2048): 185.846
b: 64, m: 2048, n: 2048, k: 1588,
Elapsed time for attention_prob_times_values (64x2048x2048x1588): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1588): 165.205
Elapsed time for attention_linear_projection (4x25408x25408, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x25408x25408, b=2048): 264.669
Elapsed time for mlp_h_to_4h (4x25408x101632, b=2048): 0.2362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25408x101632, b=2048): 179.108
Elapsed time for mlp_4h_to_h (4x101632x25408, b=2048): 0.1581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101632x25408, b=2048): 267.546

Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 260.254
MLP duration (in seconds): 0.3943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25440x76320, b=2048): 0.2099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25440x76320, b=2048): 151.543
b: 64, m: 2048, n: 1590, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1590x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1590x2048): 178.083
b: 64, m: 2048, n: 2048, k: 1590,
Elapsed time for attention_prob_times_values (64x2048x2048x1590): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1590): 134.848
Elapsed time for attention_linear_projection (4x25440x25440, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25440x25440, b=2048): 264.118
Elapsed time for mlp_h_to_4h (4x25440x101760, b=2048): 0.2984
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25440x101760, b=2048): 142.120
Elapsed time for mlp_4h_to_h (4x101760x25440, b=2048): 0.1601
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101760x25440, b=2048): 264.850

Attention duration (in seconds): 0.2612
Attention throughput (in TFLOP/s): 168.930
MLP duration (in seconds): 0.4586
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 266.925
b: 64, m: 2048, n: 1592, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1592x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1592x2048): 183.332
b: 64, m: 2048, n: 2048, k: 1592,
Elapsed time for attention_prob_times_values (64x2048x2048x1592): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1592): 185.092
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 262.717
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.2259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 188.246
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1599
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 265.872

Attention duration (in seconds): 0.1692
Attention throughput (in TFLOP/s): 261.383
MLP duration (in seconds): 0.3858
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25504x76512, b=2048): 0.2206
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25504x76512, b=2048): 144.956
b: 64, m: 2048, n: 1594, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1594x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1594x2048): 178.753
b: 64, m: 2048, n: 2048, k: 1594,
Elapsed time for attention_prob_times_values (64x2048x2048x1594): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1594): 136.348
Elapsed time for attention_linear_projection (4x25504x25504, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x25504x25504, b=2048): 258.698
Elapsed time for mlp_h_to_4h (4x25504x102016, b=2048): 0.3012
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25504x102016, b=2048): 141.520
Elapsed time for mlp_4h_to_h (4x102016x25504, b=2048): 0.1629
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102016x25504, b=2048): 261.750

Attention duration (in seconds): 0.2728
Attention throughput (in TFLOP/s): 162.526
MLP duration (in seconds): 0.4641
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25536x76608, b=2048): 0.1221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25536x76608, b=2048): 262.598
b: 64, m: 2048, n: 1596, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1596x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1596x2048): 186.288
b: 64, m: 2048, n: 2048, k: 1596,
Elapsed time for attention_prob_times_values (64x2048x2048x1596): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1596): 164.081
Elapsed time for attention_linear_projection (4x25536x25536, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_linear_projection (4x25536x25536, b=2048): 259.714
Elapsed time for mlp_h_to_4h (4x25536x102144, b=2048): 0.1766
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25536x102144, b=2048): 241.941
Elapsed time for mlp_4h_to_h (4x102144x25536, b=2048): 0.1614
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102144x25536, b=2048): 264.854

Attention duration (in seconds): 0.1730
Attention throughput (in TFLOP/s): 256.910
MLP duration (in seconds): 0.3380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25568x76704, b=2048): 0.2168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25568x76704, b=2048): 148.209
b: 64, m: 2048, n: 1598, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1598x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1598x2048): 179.300
b: 64, m: 2048, n: 2048, k: 1598,
Elapsed time for attention_prob_times_values (64x2048x2048x1598): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1598): 135.747
Elapsed time for attention_linear_projection (4x25568x25568, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_linear_projection (4x25568x25568, b=2048): 259.272
Elapsed time for mlp_h_to_4h (4x25568x102272, b=2048): 0.3013
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25568x102272, b=2048): 142.180
Elapsed time for mlp_4h_to_h (4x102272x25568, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102272x25568, b=2048): 261.847

Attention duration (in seconds): 0.2692
Attention throughput (in TFLOP/s): 165.512
MLP duration (in seconds): 0.4649
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 263.047
b: 64, m: 2048, n: 1600, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1600x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1600x2048): 186.186
b: 64, m: 2048, n: 2048, k: 1600,
Elapsed time for attention_prob_times_values (64x2048x2048x1600): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1600): 188.830
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 260.280
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1666
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 257.791
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1611
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 266.530

Attention duration (in seconds): 0.1729
Attention throughput (in TFLOP/s): 258.383
MLP duration (in seconds): 0.3278
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5006
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25632x76896, b=2048): 0.2169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25632x76896, b=2048): 148.915
b: 64, m: 2048, n: 1602, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1602x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1602x2048): 179.830
b: 64, m: 2048, n: 2048, k: 1602,
Elapsed time for attention_prob_times_values (64x2048x2048x1602): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1602): 134.951
Elapsed time for attention_linear_projection (4x25632x25632, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25632x25632, b=2048): 262.579
Elapsed time for mlp_h_to_4h (4x25632x102528, b=2048): 0.2977
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25632x102528, b=2048): 144.637
Elapsed time for mlp_4h_to_h (4x102528x25632, b=2048): 0.1625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102528x25632, b=2048): 264.960

Attention duration (in seconds): 0.2690
Attention throughput (in TFLOP/s): 166.455
MLP duration (in seconds): 0.4602
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25664x76992, b=2048): 0.1219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25664x76992, b=2048): 265.480
b: 64, m: 2048, n: 1604, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1604x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1604x2048): 187.164
b: 64, m: 2048, n: 2048, k: 1604,
Elapsed time for attention_prob_times_values (64x2048x2048x1604): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1604): 161.729
Elapsed time for attention_linear_projection (4x25664x25664, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25664x25664, b=2048): 263.274
Elapsed time for mlp_h_to_4h (4x25664x102656, b=2048): 0.1626
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25664x102656, b=2048): 265.511
Elapsed time for mlp_4h_to_h (4x102656x25664, b=2048): 0.1620
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102656x25664, b=2048): 266.404

Attention duration (in seconds): 0.1729
Attention throughput (in TFLOP/s): 259.676
MLP duration (in seconds): 0.3246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4975
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25696x77088, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25696x77088, b=2048): 137.004
b: 64, m: 2048, n: 1606, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1606x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1606x2048): 180.423
b: 64, m: 2048, n: 2048, k: 1606,
Elapsed time for attention_prob_times_values (64x2048x2048x1606): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1606): 134.498
Elapsed time for attention_linear_projection (4x25696x25696, b=2048): 0.0416
Throughput (in TFLOP/s) for attention_linear_projection (4x25696x25696, b=2048): 260.262
Elapsed time for mlp_h_to_4h (4x25696x102784, b=2048): 0.3104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25696x102784, b=2048): 139.411
Elapsed time for mlp_4h_to_h (4x102784x25696, b=2048): 0.1653
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102784x25696, b=2048): 261.803

Attention duration (in seconds): 0.2896
Attention throughput (in TFLOP/s): 155.354
MLP duration (in seconds): 0.4757
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1228
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 264.984
b: 64, m: 2048, n: 1608, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1608x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1608x2048): 184.815
b: 64, m: 2048, n: 2048, k: 1608,
Elapsed time for attention_prob_times_values (64x2048x2048x1608): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1608): 183.429
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 263.116
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.2270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 191.064
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 265.146

Attention duration (in seconds): 0.1734
Attention throughput (in TFLOP/s): 260.166
MLP duration (in seconds): 0.3907
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25760x77280, b=2048): 0.2231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25760x77280, b=2048): 146.212
b: 64, m: 2048, n: 1610, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1610x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1610x2048): 180.890
b: 64, m: 2048, n: 2048, k: 1610,
Elapsed time for attention_prob_times_values (64x2048x2048x1610): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1610): 134.180
Elapsed time for attention_linear_projection (4x25760x25760, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x25760x25760, b=2048): 263.789
Elapsed time for mlp_h_to_4h (4x25760x103040, b=2048): 0.3028
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25760x103040, b=2048): 143.642
Elapsed time for mlp_4h_to_h (4x103040x25760, b=2048): 0.1639
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103040x25760, b=2048): 265.331

Attention duration (in seconds): 0.2755
Attention throughput (in TFLOP/s): 164.122
MLP duration (in seconds): 0.4667
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25792x77376, b=2048): 0.1226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25792x77376, b=2048): 266.729
b: 64, m: 2048, n: 1612, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1612x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1612x2048): 187.737
b: 64, m: 2048, n: 2048, k: 1612,
Elapsed time for attention_prob_times_values (64x2048x2048x1612): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1612): 159.802
Elapsed time for attention_linear_projection (4x25792x25792, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_linear_projection (4x25792x25792, b=2048): 264.194
Elapsed time for mlp_h_to_4h (4x25792x103168, b=2048): 0.1660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25792x103168, b=2048): 262.561
Elapsed time for mlp_4h_to_h (4x103168x25792, b=2048): 0.1632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103168x25792, b=2048): 267.156

Attention duration (in seconds): 0.1739
Attention throughput (in TFLOP/s): 260.703
MLP duration (in seconds): 0.3292
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25824x77472, b=2048): 0.2227
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25824x77472, b=2048): 147.207
b: 64, m: 2048, n: 1614, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1614x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1614x2048): 181.258
b: 64, m: 2048, n: 2048, k: 1614,
Elapsed time for attention_prob_times_values (64x2048x2048x1614): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1614): 135.244
Elapsed time for attention_linear_projection (4x25824x25824, b=2048): 0.0416
Throughput (in TFLOP/s) for attention_linear_projection (4x25824x25824, b=2048): 262.625
Elapsed time for mlp_h_to_4h (4x25824x103296, b=2048): 0.3042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25824x103296, b=2048): 143.673
Elapsed time for mlp_4h_to_h (4x103296x25824, b=2048): 0.1665
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103296x25824, b=2048): 262.557

Attention duration (in seconds): 0.2755
Attention throughput (in TFLOP/s): 164.951
MLP duration (in seconds): 0.4707
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1239
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 265.244
b: 64, m: 2048, n: 1616, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1616x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1616x2048): 186.879
b: 64, m: 2048, n: 2048, k: 1616,
Elapsed time for attention_prob_times_values (64x2048x2048x1616): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1616): 186.105
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 262.864
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 265.909
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1655
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 264.750

Attention duration (in seconds): 0.1749
Attention throughput (in TFLOP/s): 260.486
MLP duration (in seconds): 0.3303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25888x77664, b=2048): 0.2225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25888x77664, b=2048): 148.039
b: 64, m: 2048, n: 1618, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1618x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1618x2048): 181.671
b: 64, m: 2048, n: 2048, k: 1618,
Elapsed time for attention_prob_times_values (64x2048x2048x1618): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1618): 136.207
Elapsed time for attention_linear_projection (4x25888x25888, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_linear_projection (4x25888x25888, b=2048): 258.305
Elapsed time for mlp_h_to_4h (4x25888x103552, b=2048): 0.3061
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25888x103552, b=2048): 143.500
Elapsed time for mlp_4h_to_h (4x103552x25888, b=2048): 0.1672
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103552x25888, b=2048): 262.677

Attention duration (in seconds): 0.2762
Attention throughput (in TFLOP/s): 165.320
MLP duration (in seconds): 0.4733
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25920x77760, b=2048): 0.1263
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25920x77760, b=2048): 261.451
b: 64, m: 2048, n: 1620, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1620x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1620x2048): 188.943
b: 64, m: 2048, n: 2048, k: 1620,
Elapsed time for attention_prob_times_values (64x2048x2048x1620): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1620): 163.452
Elapsed time for attention_linear_projection (4x25920x25920, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x25920x25920, b=2048): 259.529
Elapsed time for mlp_h_to_4h (4x25920x103680, b=2048): 0.1676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25920x103680, b=2048): 262.770
Elapsed time for mlp_4h_to_h (4x103680x25920, b=2048): 0.1664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103680x25920, b=2048): 264.651

Attention duration (in seconds): 0.1786
Attention throughput (in TFLOP/s): 256.207
MLP duration (in seconds): 0.3339
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25952x77856, b=2048): 0.2255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25952x77856, b=2048): 146.830
b: 64, m: 2048, n: 1622, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1622x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1622x2048): 182.129
b: 64, m: 2048, n: 2048, k: 1622,
Elapsed time for attention_prob_times_values (64x2048x2048x1622): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1622): 136.142
Elapsed time for attention_linear_projection (4x25952x25952, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x25952x25952, b=2048): 259.093
Elapsed time for mlp_h_to_4h (4x25952x103808, b=2048): 0.3055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25952x103808, b=2048): 144.466
Elapsed time for mlp_4h_to_h (4x103808x25952, b=2048): 0.1689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103808x25952, b=2048): 261.349

Attention duration (in seconds): 0.2792
Attention throughput (in TFLOP/s): 164.313
MLP duration (in seconds): 0.4744
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7536
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1263
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 262.794
b: 64, m: 2048, n: 1624, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1624x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1624x2048): 186.931
b: 64, m: 2048, n: 2048, k: 1624,
Elapsed time for attention_prob_times_values (64x2048x2048x1624): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1624): 184.917
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 260.710
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1919
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 230.631
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 268.277

Attention duration (in seconds): 0.1781
Attention throughput (in TFLOP/s): 258.249
MLP duration (in seconds): 0.3568
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26016x78048, b=2048): 0.2308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26016x78048, b=2048): 144.168
b: 64, m: 2048, n: 1626, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1626x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1626x2048): 181.934
b: 64, m: 2048, n: 2048, k: 1626,
Elapsed time for attention_prob_times_values (64x2048x2048x1626): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1626): 136.529
Elapsed time for attention_linear_projection (4x26016x26016, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_linear_projection (4x26016x26016, b=2048): 261.943
Elapsed time for mlp_h_to_4h (4x26016x104064, b=2048): 0.3094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26016x104064, b=2048): 143.364
Elapsed time for mlp_4h_to_h (4x104064x26016, b=2048): 0.1678
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104064x26016, b=2048): 264.310

Attention duration (in seconds): 0.2843
Attention throughput (in TFLOP/s): 162.172
MLP duration (in seconds): 0.4772
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7615
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26048x78144, b=2048): 0.1256
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26048x78144, b=2048): 265.429
b: 64, m: 2048, n: 1628, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1628x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1628x2048): 189.925
b: 64, m: 2048, n: 2048, k: 1628,
Elapsed time for attention_prob_times_values (64x2048x2048x1628): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1628): 165.759
Elapsed time for attention_linear_projection (4x26048x26048, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_linear_projection (4x26048x26048, b=2048): 263.098
Elapsed time for mlp_h_to_4h (4x26048x104192, b=2048): 0.2488
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26048x104192, b=2048): 178.733
Elapsed time for mlp_4h_to_h (4x104192x26048, b=2048): 0.1663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104192x26048, b=2048): 267.439

Attention duration (in seconds): 0.1778
Attention throughput (in TFLOP/s): 259.964
MLP duration (in seconds): 0.4151
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5928
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26080x78240, b=2048): 0.2294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26080x78240, b=2048): 145.750
b: 64, m: 2048, n: 1630, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1630x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1630x2048): 182.900
b: 64, m: 2048, n: 2048, k: 1630,
Elapsed time for attention_prob_times_values (64x2048x2048x1630): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1630): 137.475
Elapsed time for attention_linear_projection (4x26080x26080, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x26080x26080, b=2048): 262.709
Elapsed time for mlp_h_to_4h (4x26080x104320, b=2048): 0.3131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26080x104320, b=2048): 142.360
Elapsed time for mlp_4h_to_h (4x104320x26080, b=2048): 0.1683
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104320x26080, b=2048): 264.843

Attention duration (in seconds): 0.2829
Attention throughput (in TFLOP/s): 163.727
MLP duration (in seconds): 0.4814
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 228.437
b: 64, m: 2048, n: 1632, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1632x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1632x2048): 188.962
b: 64, m: 2048, n: 2048, k: 1632,
Elapsed time for attention_prob_times_values (64x2048x2048x1632): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1632): 187.919
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 263.547
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.2089
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 213.946
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1673
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 267.145

Attention duration (in seconds): 0.1984
Attention throughput (in TFLOP/s): 234.064
MLP duration (in seconds): 0.3761
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5745
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26144x78432, b=2048): 0.2357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26144x78432, b=2048): 142.565
b: 64, m: 2048, n: 1634, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1634x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1634x2048): 182.295
b: 64, m: 2048, n: 2048, k: 1634,
Elapsed time for attention_prob_times_values (64x2048x2048x1634): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1634): 137.395
Elapsed time for attention_linear_projection (4x26144x26144, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x26144x26144, b=2048): 262.690
Elapsed time for mlp_h_to_4h (4x26144x104576, b=2048): 0.3076
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26144x104576, b=2048): 145.632
Elapsed time for mlp_4h_to_h (4x104576x26144, b=2048): 0.1713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104576x26144, b=2048): 261.534

Attention duration (in seconds): 0.2895
Attention throughput (in TFLOP/s): 160.802
MLP duration (in seconds): 0.4789
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7683
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26176x78528, b=2048): 0.1267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26176x78528, b=2048): 265.833
b: 64, m: 2048, n: 1636, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1636x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1636x2048): 190.749
b: 64, m: 2048, n: 2048, k: 1636,
Elapsed time for attention_prob_times_values (64x2048x2048x1636): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1636): 165.809
Elapsed time for attention_linear_projection (4x26176x26176, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_linear_projection (4x26176x26176, b=2048): 264.172
Elapsed time for mlp_h_to_4h (4x26176x104704, b=2048): 0.1683
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26176x104704, b=2048): 266.757
Elapsed time for mlp_4h_to_h (4x104704x26176, b=2048): 0.1689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104704x26176, b=2048): 265.940

Attention duration (in seconds): 0.1791
Attention throughput (in TFLOP/s): 260.549
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26208x78624, b=2048): 0.2418
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26208x78624, b=2048): 139.638
b: 64, m: 2048, n: 1638, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1638x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1638x2048): 184.082
b: 64, m: 2048, n: 2048, k: 1638,
Elapsed time for attention_prob_times_values (64x2048x2048x1638): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1638): 138.914
Elapsed time for attention_linear_projection (4x26208x26208, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_linear_projection (4x26208x26208, b=2048): 261.643
Elapsed time for mlp_h_to_4h (4x26208x104832, b=2048): 0.3110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26208x104832, b=2048): 144.736
Elapsed time for mlp_4h_to_h (4x104832x26208, b=2048): 0.1713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104832x26208, b=2048): 262.830

Attention duration (in seconds): 0.2959
Attention throughput (in TFLOP/s): 158.075
MLP duration (in seconds): 0.4823
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7782
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 265.198
b: 64, m: 2048, n: 1640, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1640x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1640x2048): 188.734
b: 64, m: 2048, n: 2048, k: 1640,
Elapsed time for attention_prob_times_values (64x2048x2048x1640): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1640): 185.641
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 265.060
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.2418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 186.649
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1688
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 267.252

Attention duration (in seconds): 0.1796
Attention throughput (in TFLOP/s): 261.078
MLP duration (in seconds): 0.4106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5902
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26272x78816, b=2048): 0.2339
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26272x78816, b=2048): 145.056
b: 64, m: 2048, n: 1642, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1642x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1642x2048): 184.725
b: 64, m: 2048, n: 2048, k: 1642,
Elapsed time for attention_prob_times_values (64x2048x2048x1642): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1642): 139.785
Elapsed time for attention_linear_projection (4x26272x26272, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_linear_projection (4x26272x26272, b=2048): 260.625
Elapsed time for mlp_h_to_4h (4x26272x105088, b=2048): 0.3090
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26272x105088, b=2048): 146.366
Elapsed time for mlp_4h_to_h (4x105088x26272, b=2048): 0.1719
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105088x26272, b=2048): 263.158

Attention duration (in seconds): 0.2883
Attention throughput (in TFLOP/s): 162.988
MLP duration (in seconds): 0.4809
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7693
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26304x78912, b=2048): 0.1287
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26304x78912, b=2048): 264.166
b: 64, m: 2048, n: 1644, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1644x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1644x2048): 192.410
b: 64, m: 2048, n: 2048, k: 1644,
Elapsed time for attention_prob_times_values (64x2048x2048x1644): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1644): 167.184
Elapsed time for attention_linear_projection (4x26304x26304, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_linear_projection (4x26304x26304, b=2048): 261.487
Elapsed time for mlp_h_to_4h (4x26304x105216, b=2048): 0.1714
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26304x105216, b=2048): 264.547
Elapsed time for mlp_4h_to_h (4x105216x26304, b=2048): 0.1706
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105216x26304, b=2048): 265.774

Attention duration (in seconds): 0.1820
Attention throughput (in TFLOP/s): 258.905
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26336x79008, b=2048): 0.2343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26336x79008, b=2048): 145.482
b: 64, m: 2048, n: 1646, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1646x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1646x2048): 186.320
b: 64, m: 2048, n: 2048, k: 1646,
Elapsed time for attention_prob_times_values (64x2048x2048x1646): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1646): 139.100
Elapsed time for attention_linear_projection (4x26336x26336, b=2048): 0.0438
Throughput (in TFLOP/s) for attention_linear_projection (4x26336x26336, b=2048): 259.309
Elapsed time for mlp_h_to_4h (4x26336x105344, b=2048): 0.3169
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26336x105344, b=2048): 143.430
Elapsed time for mlp_4h_to_h (4x105344x26336, b=2048): 0.1741
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105344x26336, b=2048): 261.148

Attention duration (in seconds): 0.2893
Attention throughput (in TFLOP/s): 163.257
MLP duration (in seconds): 0.4910
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1299
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 263.170
b: 64, m: 2048, n: 1648, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1648x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1648x2048): 191.218
b: 64, m: 2048, n: 2048, k: 1648,
Elapsed time for attention_prob_times_values (64x2048x2048x1648): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1648): 189.858
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 261.736
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1717
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 265.356
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 265.237

Attention duration (in seconds): 0.1827
Attention throughput (in TFLOP/s): 259.135
MLP duration (in seconds): 0.3435
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26400x79200, b=2048): 0.2317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26400x79200, b=2048): 147.840
b: 64, m: 2048, n: 1650, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1650x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1650x2048): 186.304
b: 64, m: 2048, n: 2048, k: 1650,
Elapsed time for attention_prob_times_values (64x2048x2048x1650): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1650): 140.238
Elapsed time for attention_linear_projection (4x26400x26400, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x26400x26400, b=2048): 261.919
Elapsed time for mlp_h_to_4h (4x26400x105600, b=2048): 0.3190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26400x105600, b=2048): 143.207
Elapsed time for mlp_4h_to_h (4x105600x26400, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105600x26400, b=2048): 264.547

Attention duration (in seconds): 0.2864
Attention throughput (in TFLOP/s): 165.677
MLP duration (in seconds): 0.4916
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7780
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26432x79296, b=2048): 0.1291
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26432x79296, b=2048): 265.914
b: 64, m: 2048, n: 1652, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1652x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1652x2048): 193.175
b: 64, m: 2048, n: 2048, k: 1652,
Elapsed time for attention_prob_times_values (64x2048x2048x1652): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1652): 166.345
Elapsed time for attention_linear_projection (4x26432x26432, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x26432x26432, b=2048): 262.806
Elapsed time for mlp_h_to_4h (4x26432x105728, b=2048): 0.1883
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26432x105728, b=2048): 243.210
Elapsed time for mlp_4h_to_h (4x105728x26432, b=2048): 0.1713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105728x26432, b=2048): 267.315

Attention duration (in seconds): 0.1826
Attention throughput (in TFLOP/s): 260.437
MLP duration (in seconds): 0.3595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26464x79392, b=2048): 0.2416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26464x79392, b=2048): 142.457
b: 64, m: 2048, n: 1654, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1654x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1654x2048): 186.532
b: 64, m: 2048, n: 2048, k: 1654,
Elapsed time for attention_prob_times_values (64x2048x2048x1654): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1654): 140.361
Elapsed time for attention_linear_projection (4x26464x26464, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_linear_projection (4x26464x26464, b=2048): 261.186
Elapsed time for mlp_h_to_4h (4x26464x105856, b=2048): 0.3256
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26464x105856, b=2048): 140.968
Elapsed time for mlp_4h_to_h (4x105856x26464, b=2048): 0.1758
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105856x26464, b=2048): 261.084

Attention duration (in seconds): 0.2967
Attention throughput (in TFLOP/s): 160.702
MLP duration (in seconds): 0.5014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7980
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1300
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 265.528
b: 64, m: 2048, n: 1656, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1656x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1656x2048): 190.536
b: 64, m: 2048, n: 2048, k: 1656,
Elapsed time for attention_prob_times_values (64x2048x2048x1656): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1656): 187.339
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 262.019
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1960
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 234.789
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1734
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 265.351

Attention duration (in seconds): 0.1833
Attention throughput (in TFLOP/s): 260.753
MLP duration (in seconds): 0.3693
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5526
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26528x79584, b=2048): 0.2461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26528x79584, b=2048): 140.554
b: 64, m: 2048, n: 1658, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1658x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1658x2048): 186.964
b: 64, m: 2048, n: 2048, k: 1658,
Elapsed time for attention_prob_times_values (64x2048x2048x1658): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1658): 140.351
Elapsed time for attention_linear_projection (4x26528x26528, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_linear_projection (4x26528x26528, b=2048): 260.525
Elapsed time for mlp_h_to_4h (4x26528x106112, b=2048): 0.3227
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26528x106112, b=2048): 142.902
Elapsed time for mlp_4h_to_h (4x106112x26528, b=2048): 0.1765
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106112x26528, b=2048): 261.272

Attention duration (in seconds): 0.3015
Attention throughput (in TFLOP/s): 158.895
MLP duration (in seconds): 0.4993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26560x79680, b=2048): 0.1314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26560x79680, b=2048): 263.938
b: 64, m: 2048, n: 1660, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1660x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1660x2048): 193.839
b: 64, m: 2048, n: 2048, k: 1660,
Elapsed time for attention_prob_times_values (64x2048x2048x1660): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1660): 165.522
Elapsed time for attention_linear_projection (4x26560x26560, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26560x26560, b=2048): 261.322
Elapsed time for mlp_h_to_4h (4x26560x106240, b=2048): 0.1747
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26560x106240, b=2048): 264.613
Elapsed time for mlp_4h_to_h (4x106240x26560, b=2048): 0.1737
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106240x26560, b=2048): 266.184

Attention duration (in seconds): 0.1856
Attention throughput (in TFLOP/s): 258.723
MLP duration (in seconds): 0.3484
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26592x79776, b=2048): 0.2398
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26592x79776, b=2048): 144.972
b: 64, m: 2048, n: 1662, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1662x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1662x2048): 186.891
b: 64, m: 2048, n: 2048, k: 1662,
Elapsed time for attention_prob_times_values (64x2048x2048x1662): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1662): 141.120
Elapsed time for attention_linear_projection (4x26592x26592, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_linear_projection (4x26592x26592, b=2048): 263.834
Elapsed time for mlp_h_to_4h (4x26592x106368, b=2048): 0.3161
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26592x106368, b=2048): 146.618
Elapsed time for mlp_4h_to_h (4x106368x26592, b=2048): 0.1754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106368x26592, b=2048): 264.287

Attention duration (in seconds): 0.2948
Attention throughput (in TFLOP/s): 163.276
MLP duration (in seconds): 0.4914
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7862
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 267.666
b: 64, m: 2048, n: 1664, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1664x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1664x2048): 193.204
b: 64, m: 2048, n: 2048, k: 1664,
Elapsed time for attention_prob_times_values (64x2048x2048x1664): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1664): 191.339
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 264.624
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.2957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 157.102
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 267.109

Attention duration (in seconds): 0.1833
Attention throughput (in TFLOP/s): 263.116
MLP duration (in seconds): 0.4696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6530
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26656x79968, b=2048): 0.2479
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26656x79968, b=2048): 140.864
b: 64, m: 2048, n: 1666, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1666x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1666x2048): 177.275
b: 64, m: 2048, n: 2048, k: 1666,
Elapsed time for attention_prob_times_values (64x2048x2048x1666): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1666): 138.903
Elapsed time for attention_linear_projection (4x26656x26656, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26656x26656, b=2048): 262.032
Elapsed time for mlp_h_to_4h (4x26656x106624, b=2048): 0.3248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26656x106624, b=2048): 143.383
Elapsed time for mlp_4h_to_h (4x106624x26656, b=2048): 0.1781
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106624x26656, b=2048): 261.436

Attention duration (in seconds): 0.3038
Attention throughput (in TFLOP/s): 159.145
MLP duration (in seconds): 0.5029
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26688x80064, b=2048): 0.1318
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26688x80064, b=2048): 265.626
b: 64, m: 2048, n: 1668, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1668x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1668x2048): 184.190
b: 64, m: 2048, n: 2048, k: 1668,
Elapsed time for attention_prob_times_values (64x2048x2048x1668): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1668): 163.518
Elapsed time for attention_linear_projection (4x26688x26688, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26688x26688, b=2048): 262.742
Elapsed time for mlp_h_to_4h (4x26688x106752, b=2048): 0.1749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26688x106752, b=2048): 266.942
Elapsed time for mlp_4h_to_h (4x106752x26688, b=2048): 0.1748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106752x26688, b=2048): 266.993

Attention duration (in seconds): 0.1865
Attention throughput (in TFLOP/s): 259.819
MLP duration (in seconds): 0.3497
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26720x80160, b=2048): 0.2513
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26720x80160, b=2048): 139.641
b: 64, m: 2048, n: 1670, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1670x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1670x2048): 177.533
b: 64, m: 2048, n: 2048, k: 1670,
Elapsed time for attention_prob_times_values (64x2048x2048x1670): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1670): 139.190
Elapsed time for attention_linear_projection (4x26720x26720, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26720x26720, b=2048): 264.785
Elapsed time for mlp_h_to_4h (4x26720x106880, b=2048): 0.3192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26720x106880, b=2048): 146.567
Elapsed time for mlp_4h_to_h (4x106880x26720, b=2048): 0.1772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106880x26720, b=2048): 264.099

Attention duration (in seconds): 0.3070
Attention throughput (in TFLOP/s): 158.264
MLP duration (in seconds): 0.4964
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 267.729
b: 64, m: 2048, n: 1672, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1672x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1672x2048): 181.341
b: 64, m: 2048, n: 2048, k: 1672,
Elapsed time for attention_prob_times_values (64x2048x2048x1672): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1672): 185.651
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0441
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 265.618
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.2944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 159.335
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1755
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 267.217

Attention duration (in seconds): 0.1853
Attention throughput (in TFLOP/s): 262.777
MLP duration (in seconds): 0.4699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6552
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26784x80352, b=2048): 0.2460
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26784x80352, b=2048): 143.330
b: 64, m: 2048, n: 1674, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1674x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1674x2048): 178.076
b: 64, m: 2048, n: 2048, k: 1674,
Elapsed time for attention_prob_times_values (64x2048x2048x1674): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1674): 138.758
Elapsed time for attention_linear_projection (4x26784x26784, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_linear_projection (4x26784x26784, b=2048): 259.669
Elapsed time for mlp_h_to_4h (4x26784x107136, b=2048): 0.3279
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26784x107136, b=2048): 143.359
Elapsed time for mlp_4h_to_h (4x107136x26784, b=2048): 0.1805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107136x26784, b=2048): 260.502

Attention duration (in seconds): 0.3028
Attention throughput (in TFLOP/s): 161.202
MLP duration (in seconds): 0.5084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26816x80448, b=2048): 0.1345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26816x80448, b=2048): 262.858
b: 64, m: 2048, n: 1676, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1676x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1676x2048): 184.404
b: 64, m: 2048, n: 2048, k: 1676,
Elapsed time for attention_prob_times_values (64x2048x2048x1676): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1676): 161.878
Elapsed time for attention_linear_projection (4x26816x26816, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26816x26816, b=2048): 261.469
Elapsed time for mlp_h_to_4h (4x26816x107264, b=2048): 0.1929
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26816x107264, b=2048): 244.346
Elapsed time for mlp_4h_to_h (4x107264x26816, b=2048): 0.1771
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107264x26816, b=2048): 266.153

Attention duration (in seconds): 0.1900
Attention throughput (in TFLOP/s): 257.558
MLP duration (in seconds): 0.3699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5599
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26848x80544, b=2048): 0.2430
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26848x80544, b=2048): 145.786
b: 64, m: 2048, n: 1678, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1678x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1678x2048): 178.266
b: 64, m: 2048, n: 2048, k: 1678,
Elapsed time for attention_prob_times_values (64x2048x2048x1678): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1678): 138.712
Elapsed time for attention_linear_projection (4x26848x26848, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_linear_projection (4x26848x26848, b=2048): 260.074
Elapsed time for mlp_h_to_4h (4x26848x107392, b=2048): 0.3331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26848x107392, b=2048): 141.814
Elapsed time for mlp_4h_to_h (4x107392x26848, b=2048): 0.1805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107392x26848, b=2048): 261.678

Attention duration (in seconds): 0.3000
Attention throughput (in TFLOP/s): 163.480
MLP duration (in seconds): 0.5136
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1340
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 264.951
b: 64, m: 2048, n: 1680, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1680x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1680x2048): 183.571
b: 64, m: 2048, n: 2048, k: 1680,
Elapsed time for attention_prob_times_values (64x2048x2048x1680): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1680): 188.849
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 261.768
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 198.623
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 265.459

Attention duration (in seconds): 0.1890
Attention throughput (in TFLOP/s): 260.149
MLP duration (in seconds): 0.4168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26912x80736, b=2048): 0.2452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26912x80736, b=2048): 145.165
b: 64, m: 2048, n: 1682, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1682x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1682x2048): 178.649
b: 64, m: 2048, n: 2048, k: 1682,
Elapsed time for attention_prob_times_values (64x2048x2048x1682): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1682): 138.145
Elapsed time for attention_linear_projection (4x26912x26912, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x26912x26912, b=2048): 259.992
Elapsed time for mlp_h_to_4h (4x26912x107648, b=2048): 0.3326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26912x107648, b=2048): 142.708
Elapsed time for mlp_4h_to_h (4x107648x26912, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107648x26912, b=2048): 260.317

Attention duration (in seconds): 0.3025
Attention throughput (in TFLOP/s): 162.900
MLP duration (in seconds): 0.5149
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26944x80832, b=2048): 0.1354
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26944x80832, b=2048): 263.557
b: 64, m: 2048, n: 1684, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1684x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1684x2048): 185.793
b: 64, m: 2048, n: 2048, k: 1684,
Elapsed time for attention_prob_times_values (64x2048x2048x1684): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1684): 162.212
Elapsed time for attention_linear_projection (4x26944x26944, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_linear_projection (4x26944x26944, b=2048): 261.433
Elapsed time for mlp_h_to_4h (4x26944x107776, b=2048): 0.3056
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26944x107776, b=2048): 155.682
Elapsed time for mlp_4h_to_h (4x107776x26944, b=2048): 0.1795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107776x26944, b=2048): 265.062

Attention duration (in seconds): 0.1913
Attention throughput (in TFLOP/s): 258.122
MLP duration (in seconds): 0.4851
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26976x80928, b=2048): 0.2471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26976x80928, b=2048): 144.775
b: 64, m: 2048, n: 1686, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1686x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1686x2048): 179.888
b: 64, m: 2048, n: 2048, k: 1686,
Elapsed time for attention_prob_times_values (64x2048x2048x1686): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1686): 139.789
Elapsed time for attention_linear_projection (4x26976x26976, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x26976x26976, b=2048): 260.566
Elapsed time for mlp_h_to_4h (4x26976x107904, b=2048): 0.3340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26976x107904, b=2048): 142.779
Elapsed time for mlp_4h_to_h (4x107904x26976, b=2048): 0.1833
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107904x26976, b=2048): 260.200

Attention duration (in seconds): 0.3043
Attention throughput (in TFLOP/s): 162.660
MLP duration (in seconds): 0.5173
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1355
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 264.592
b: 64, m: 2048, n: 1688, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1688x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1688x2048): 183.324
b: 64, m: 2048, n: 2048, k: 1688,
Elapsed time for attention_prob_times_values (64x2048x2048x1688): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1688): 186.949
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 262.288
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.2949
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 162.103
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1800
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 265.603

Attention duration (in seconds): 0.1909
Attention throughput (in TFLOP/s): 259.965
MLP duration (in seconds): 0.4749
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6657
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27040x81120, b=2048): 0.2499
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27040x81120, b=2048): 143.792
b: 64, m: 2048, n: 1690, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1690x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1690x2048): 179.211
b: 64, m: 2048, n: 2048, k: 1690,
Elapsed time for attention_prob_times_values (64x2048x2048x1690): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1690): 138.792
Elapsed time for attention_linear_projection (4x27040x27040, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x27040x27040, b=2048): 261.393
Elapsed time for mlp_h_to_4h (4x27040x108160, b=2048): 0.3368
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27040x108160, b=2048): 142.266
Elapsed time for mlp_4h_to_h (4x108160x27040, b=2048): 0.1840
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108160x27040, b=2048): 260.432

Attention duration (in seconds): 0.3074
Attention throughput (in TFLOP/s): 161.804
MLP duration (in seconds): 0.5208
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27072x81216, b=2048): 0.1360
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27072x81216, b=2048): 264.799
b: 64, m: 2048, n: 1692, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1692x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1692x2048): 186.128
b: 64, m: 2048, n: 2048, k: 1692,
Elapsed time for attention_prob_times_values (64x2048x2048x1692): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1692): 162.123
Elapsed time for attention_linear_projection (4x27072x27072, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x27072x27072, b=2048): 262.689
Elapsed time for mlp_h_to_4h (4x27072x108288, b=2048): 0.2165
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27072x108288, b=2048): 221.854
Elapsed time for mlp_4h_to_h (4x108288x27072, b=2048): 0.1807
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108288x27072, b=2048): 265.875

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 259.307
MLP duration (in seconds): 0.3971
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5894
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27104x81312, b=2048): 0.2557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27104x81312, b=2048): 141.212
b: 64, m: 2048, n: 1694, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1694x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1694x2048): 179.407
b: 64, m: 2048, n: 2048, k: 1694,
Elapsed time for attention_prob_times_values (64x2048x2048x1694): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1694): 138.283
Elapsed time for attention_linear_projection (4x27104x27104, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_linear_projection (4x27104x27104, b=2048): 262.085
Elapsed time for mlp_h_to_4h (4x27104x108416, b=2048): 0.3417
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27104x108416, b=2048): 140.891
Elapsed time for mlp_4h_to_h (4x108416x27104, b=2048): 0.1829
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108416x27104, b=2048): 263.261

Attention duration (in seconds): 0.3133
Attention throughput (in TFLOP/s): 159.488
MLP duration (in seconds): 0.5246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1620
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 223.471
b: 64, m: 2048, n: 1696, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1696x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1696x2048): 184.611
b: 64, m: 2048, n: 2048, k: 1696,
Elapsed time for attention_prob_times_values (64x2048x2048x1696): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1696): 189.569
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 265.605
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.3105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 155.441
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.1800
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 268.136

Attention duration (in seconds): 0.2171
Attention throughput (in TFLOP/s): 230.653
MLP duration (in seconds): 0.4904
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27168x81504, b=2048): 0.2574
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27168x81504, b=2048): 140.939
b: 64, m: 2048, n: 1698, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1698x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1698x2048): 179.535
b: 64, m: 2048, n: 2048, k: 1698,
Elapsed time for attention_prob_times_values (64x2048x2048x1698): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1698): 136.927
Elapsed time for attention_linear_projection (4x27168x27168, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_linear_projection (4x27168x27168, b=2048): 260.936
Elapsed time for mlp_h_to_4h (4x27168x108672, b=2048): 0.3440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27168x108672, b=2048): 140.601
Elapsed time for mlp_4h_to_h (4x108672x27168, b=2048): 0.1838
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108672x27168, b=2048): 263.233

Attention duration (in seconds): 0.3155
Attention throughput (in TFLOP/s): 159.103
MLP duration (in seconds): 0.5278
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8433
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27200x81600, b=2048): 0.1855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27200x81600, b=2048): 196.060
b: 64, m: 2048, n: 1700, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1700x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1700x2048): 186.525
b: 64, m: 2048, n: 2048, k: 1700,
Elapsed time for attention_prob_times_values (64x2048x2048x1700): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1700): 161.597
Elapsed time for attention_linear_projection (4x27200x27200, b=2048): 0.0462
Throughput (in TFLOP/s) for attention_linear_projection (4x27200x27200, b=2048): 262.272
Elapsed time for mlp_h_to_4h (4x27200x108800, b=2048): 0.2388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27200x108800, b=2048): 203.009
Elapsed time for mlp_4h_to_h (4x108800x27200, b=2048): 0.1821
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108800x27200, b=2048): 266.200

Attention duration (in seconds): 0.2422
Attention throughput (in TFLOP/s): 207.697
MLP duration (in seconds): 0.4210
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27232x81696, b=2048): 0.2603
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27232x81696, b=2048): 140.032
b: 64, m: 2048, n: 1702, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1702x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1702x2048): 180.670
b: 64, m: 2048, n: 2048, k: 1702,
Elapsed time for attention_prob_times_values (64x2048x2048x1702): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1702): 138.410
Elapsed time for attention_linear_projection (4x27232x27232, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_linear_projection (4x27232x27232, b=2048): 260.118
Elapsed time for mlp_h_to_4h (4x27232x108928, b=2048): 0.3465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27232x108928, b=2048): 140.246
Elapsed time for mlp_4h_to_h (4x108928x27232, b=2048): 0.1864
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108928x27232, b=2048): 260.763

Attention duration (in seconds): 0.3187
Attention throughput (in TFLOP/s): 158.246
MLP duration (in seconds): 0.5329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8516
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1381
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 264.612
b: 64, m: 2048, n: 1704, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1704x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1704x2048): 185.267
b: 64, m: 2048, n: 2048, k: 1704,
Elapsed time for attention_prob_times_values (64x2048x2048x1704): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1704): 188.193
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 262.111
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.2361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 206.293
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.1816
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 268.235

Attention duration (in seconds): 0.1943
Attention throughput (in TFLOP/s): 260.087
MLP duration (in seconds): 0.4178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27296x81888, b=2048): 0.2632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27296x81888, b=2048): 139.140
b: 64, m: 2048, n: 1706, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1706x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1706x2048): 180.780
b: 64, m: 2048, n: 2048, k: 1706,
Elapsed time for attention_prob_times_values (64x2048x2048x1706): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1706): 136.784
Elapsed time for attention_linear_projection (4x27296x27296, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_linear_projection (4x27296x27296, b=2048): 262.422
Elapsed time for mlp_h_to_4h (4x27296x109184, b=2048): 0.3472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27296x109184, b=2048): 140.623
Elapsed time for mlp_4h_to_h (4x109184x27296, b=2048): 0.1857
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109184x27296, b=2048): 262.897

Attention duration (in seconds): 0.3215
Attention throughput (in TFLOP/s): 157.586
MLP duration (in seconds): 0.5330
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27328x81984, b=2048): 0.1382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27328x81984, b=2048): 265.540
b: 64, m: 2048, n: 1708, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1708x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1708x2048): 187.266
b: 64, m: 2048, n: 2048, k: 1708,
Elapsed time for attention_prob_times_values (64x2048x2048x1708): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1708): 159.575
Elapsed time for attention_linear_projection (4x27328x27328, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_linear_projection (4x27328x27328, b=2048): 262.662
Elapsed time for mlp_h_to_4h (4x27328x109312, b=2048): 0.2587
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27328x109312, b=2048): 189.204
Elapsed time for mlp_4h_to_h (4x109312x27328, b=2048): 0.1842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109312x27328, b=2048): 265.686

Attention duration (in seconds): 0.1955
Attention throughput (in TFLOP/s): 259.778
MLP duration (in seconds): 0.4429
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6384
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27360x82080, b=2048): 0.2593
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27360x82080, b=2048): 141.906
b: 64, m: 2048, n: 1710, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1710x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1710x2048): 180.668
b: 64, m: 2048, n: 2048, k: 1710,
Elapsed time for attention_prob_times_values (64x2048x2048x1710): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1710): 138.276
Elapsed time for attention_linear_projection (4x27360x27360, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27360x27360, b=2048): 260.567
Elapsed time for mlp_h_to_4h (4x27360x109440, b=2048): 0.3542
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27360x109440, b=2048): 138.517
Elapsed time for mlp_4h_to_h (4x109440x27360, b=2048): 0.1860
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109440x27360, b=2048): 263.776

Attention duration (in seconds): 0.3181
Attention throughput (in TFLOP/s): 160.009
MLP duration (in seconds): 0.5402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 266.346
b: 64, m: 2048, n: 1712, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1712x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1712x2048): 186.353
b: 64, m: 2048, n: 2048, k: 1712,
Elapsed time for attention_prob_times_values (64x2048x2048x1712): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1712): 191.357
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 264.580
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.3331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 147.603
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.1840
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 267.252

Attention duration (in seconds): 0.1947
Attention throughput (in TFLOP/s): 262.047
MLP duration (in seconds): 0.5171
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27424x82272, b=2048): 0.2553
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27424x82272, b=2048): 144.797
b: 64, m: 2048, n: 1714, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1714x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1714x2048): 181.423
b: 64, m: 2048, n: 2048, k: 1714,
Elapsed time for attention_prob_times_values (64x2048x2048x1714): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1714): 139.034
Elapsed time for attention_linear_projection (4x27424x27424, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_linear_projection (4x27424x27424, b=2048): 263.628
Elapsed time for mlp_h_to_4h (4x27424x109696, b=2048): 0.3449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27424x109696, b=2048): 142.921
Elapsed time for mlp_4h_to_h (4x109696x27424, b=2048): 0.1875
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109696x27424, b=2048): 262.831

Attention duration (in seconds): 0.3137
Attention throughput (in TFLOP/s): 162.971
MLP duration (in seconds): 0.5324
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27456x82368, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27456x82368, b=2048): 266.579
b: 64, m: 2048, n: 1716, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1716x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1716x2048): 188.465
b: 64, m: 2048, n: 2048, k: 1716,
Elapsed time for attention_prob_times_values (64x2048x2048x1716): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1716): 162.671
Elapsed time for attention_linear_projection (4x27456x27456, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_linear_projection (4x27456x27456, b=2048): 264.790
Elapsed time for mlp_h_to_4h (4x27456x109824, b=2048): 0.3215
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27456x109824, b=2048): 153.644
Elapsed time for mlp_4h_to_h (4x109824x27456, b=2048): 0.1844
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109824x27456, b=2048): 267.858

Attention duration (in seconds): 0.1962
Attention throughput (in TFLOP/s): 261.208
MLP duration (in seconds): 0.5060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27488x82464, b=2048): 0.2588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27488x82464, b=2048): 143.496
b: 64, m: 2048, n: 1718, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1718x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1718x2048): 181.812
b: 64, m: 2048, n: 2048, k: 1718,
Elapsed time for attention_prob_times_values (64x2048x2048x1718): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1718): 139.108
Elapsed time for attention_linear_projection (4x27488x27488, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_linear_projection (4x27488x27488, b=2048): 264.219
Elapsed time for mlp_h_to_4h (4x27488x109952, b=2048): 0.3492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27488x109952, b=2048): 141.788
Elapsed time for mlp_4h_to_h (4x109952x27488, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109952x27488, b=2048): 263.092

Attention duration (in seconds): 0.3174
Attention throughput (in TFLOP/s): 161.839
MLP duration (in seconds): 0.5375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 267.758
b: 64, m: 2048, n: 1720, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1720x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1720x2048): 186.233
b: 64, m: 2048, n: 2048, k: 1720,
Elapsed time for attention_prob_times_values (64x2048x2048x1720): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1720): 189.486
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 265.434
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.2649
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 187.367
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.1855
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 267.633

Attention duration (in seconds): 0.1956
Attention throughput (in TFLOP/s): 263.186
MLP duration (in seconds): 0.4504
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27552x82656, b=2048): 0.2596
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27552x82656, b=2048): 143.703
b: 64, m: 2048, n: 1722, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1722x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1722x2048): 182.518
b: 64, m: 2048, n: 2048, k: 1722,
Elapsed time for attention_prob_times_values (64x2048x2048x1722): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1722): 139.012
Elapsed time for attention_linear_projection (4x27552x27552, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_linear_projection (4x27552x27552, b=2048): 264.424
Elapsed time for mlp_h_to_4h (4x27552x110208, b=2048): 0.3461
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27552x110208, b=2048): 143.749
Elapsed time for mlp_4h_to_h (4x110208x27552, b=2048): 0.1895
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110208x27552, b=2048): 262.485

Attention duration (in seconds): 0.3184
Attention throughput (in TFLOP/s): 162.056
MLP duration (in seconds): 0.5356
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8540
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27584x82752, b=2048): 0.1405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27584x82752, b=2048): 266.240
b: 64, m: 2048, n: 1724, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1724x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1724x2048): 189.787
b: 64, m: 2048, n: 2048, k: 1724,
Elapsed time for attention_prob_times_values (64x2048x2048x1724): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1724): 163.919
Elapsed time for attention_linear_projection (4x27584x27584, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_linear_projection (4x27584x27584, b=2048): 265.743
Elapsed time for mlp_h_to_4h (4x27584x110336, b=2048): 0.2125
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27584x110336, b=2048): 234.638
Elapsed time for mlp_4h_to_h (4x110336x27584, b=2048): 0.1873
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110336x27584, b=2048): 266.290

Attention duration (in seconds): 0.1979
Attention throughput (in TFLOP/s): 261.319
MLP duration (in seconds): 0.3998
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5977
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27616x82848, b=2048): 0.2594
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27616x82848, b=2048): 144.499
b: 64, m: 2048, n: 1726, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1726x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1726x2048): 183.184
b: 64, m: 2048, n: 2048, k: 1726,
Elapsed time for attention_prob_times_values (64x2048x2048x1726): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1726): 140.129
Elapsed time for attention_linear_projection (4x27616x27616, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x27616x27616, b=2048): 262.574
Elapsed time for mlp_h_to_4h (4x27616x110464, b=2048): 0.3465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27616x110464, b=2048): 144.231
Elapsed time for mlp_4h_to_h (4x110464x27616, b=2048): 0.1902
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110464x27616, b=2048): 262.738

Attention duration (in seconds): 0.3187
Attention throughput (in TFLOP/s): 162.655
MLP duration (in seconds): 0.5368
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8554
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1525
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 246.307
b: 64, m: 2048, n: 1728, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1728x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1728x2048): 188.962
b: 64, m: 2048, n: 2048, k: 1728,
Elapsed time for attention_prob_times_values (64x2048x2048x1728): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1728): 193.318
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 267.400
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.3176
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 157.735
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.1870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 267.913

Attention duration (in seconds): 0.2091
Attention throughput (in TFLOP/s): 248.470
MLP duration (in seconds): 0.5046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27680x83040, b=2048): 0.2604
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27680x83040, b=2048): 144.600
b: 64, m: 2048, n: 1730, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1730x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1730x2048): 182.979
b: 64, m: 2048, n: 2048, k: 1730,
Elapsed time for attention_prob_times_values (64x2048x2048x1730): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1730): 138.270
Elapsed time for attention_linear_projection (4x27680x27680, b=2048): 0.0479
Throughput (in TFLOP/s) for attention_linear_projection (4x27680x27680, b=2048): 262.228
Elapsed time for mlp_h_to_4h (4x27680x110720, b=2048): 0.3516
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27680x110720, b=2048): 142.812
Elapsed time for mlp_4h_to_h (4x110720x27680, b=2048): 0.1919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110720x27680, b=2048): 261.608

Attention duration (in seconds): 0.3201
Attention throughput (in TFLOP/s): 162.667
MLP duration (in seconds): 0.5435
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27712x83136, b=2048): 0.1425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27712x83136, b=2048): 264.852
b: 64, m: 2048, n: 1732, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1732x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1732x2048): 191.070
b: 64, m: 2048, n: 2048, k: 1732,
Elapsed time for attention_prob_times_values (64x2048x2048x1732): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1732): 160.393
Elapsed time for attention_linear_projection (4x27712x27712, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_linear_projection (4x27712x27712, b=2048): 263.373
Elapsed time for mlp_h_to_4h (4x27712x110848, b=2048): 0.1904
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27712x110848, b=2048): 264.291
Elapsed time for mlp_4h_to_h (4x110848x27712, b=2048): 0.1897
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110848x27712, b=2048): 265.266

Attention duration (in seconds): 0.2010
Attention throughput (in TFLOP/s): 259.700
MLP duration (in seconds): 0.3802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5811
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27744x83232, b=2048): 0.2700
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27744x83232, b=2048): 140.147
b: 64, m: 2048, n: 1734, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1734x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1734x2048): 183.851
b: 64, m: 2048, n: 2048, k: 1734,
Elapsed time for attention_prob_times_values (64x2048x2048x1734): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1734): 137.477
Elapsed time for attention_linear_projection (4x27744x27744, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_linear_projection (4x27744x27744, b=2048): 260.525
Elapsed time for mlp_h_to_4h (4x27744x110976, b=2048): 0.3598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27744x110976, b=2048): 140.188
Elapsed time for mlp_4h_to_h (4x110976x27744, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110976x27744, b=2048): 260.046

Attention duration (in seconds): 0.3302
Attention throughput (in TFLOP/s): 158.409
MLP duration (in seconds): 0.5538
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1429
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 265.424
b: 64, m: 2048, n: 1736, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1736x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1736x2048): 188.519
b: 64, m: 2048, n: 2048, k: 1736,
Elapsed time for attention_prob_times_values (64x2048x2048x1736): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1736): 188.482
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0479
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 263.695
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.2447
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 206.614
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.1902
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 265.827

Attention duration (in seconds): 0.2007
Attention throughput (in TFLOP/s): 261.221
MLP duration (in seconds): 0.4349
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27808x83424, b=2048): 0.2663
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27808x83424, b=2048): 142.715
b: 64, m: 2048, n: 1738, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1738x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1738x2048): 183.998
b: 64, m: 2048, n: 2048, k: 1738,
Elapsed time for attention_prob_times_values (64x2048x2048x1738): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1738): 138.565
Elapsed time for attention_linear_projection (4x27808x27808, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_linear_projection (4x27808x27808, b=2048): 263.719
Elapsed time for mlp_h_to_4h (4x27808x111232, b=2048): 0.3613
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27808x111232, b=2048): 140.282
Elapsed time for mlp_4h_to_h (4x111232x27808, b=2048): 0.1926
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111232x27808, b=2048): 263.121

Attention duration (in seconds): 0.3262
Attention throughput (in TFLOP/s): 161.094
MLP duration (in seconds): 0.5539
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8800
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27840x83520, b=2048): 0.1427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27840x83520, b=2048): 267.046
b: 64, m: 2048, n: 1740, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1740x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1740x2048): 191.352
b: 64, m: 2048, n: 2048, k: 1740,
Elapsed time for attention_prob_times_values (64x2048x2048x1740): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1740): 160.579
Elapsed time for attention_linear_projection (4x27840x27840, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_linear_projection (4x27840x27840, b=2048): 264.534
Elapsed time for mlp_h_to_4h (4x27840x111360, b=2048): 0.2732
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27840x111360, b=2048): 185.924
Elapsed time for mlp_4h_to_h (4x111360x27840, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111360x27840, b=2048): 267.311

Attention duration (in seconds): 0.2014
Attention throughput (in TFLOP/s): 261.536
MLP duration (in seconds): 0.4632
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6646
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27872x83616, b=2048): 0.2695
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27872x83616, b=2048): 141.685
b: 64, m: 2048, n: 1742, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1742x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1742x2048): 184.508
b: 64, m: 2048, n: 2048, k: 1742,
Elapsed time for attention_prob_times_values (64x2048x2048x1742): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1742): 137.980
Elapsed time for attention_linear_projection (4x27872x27872, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_linear_projection (4x27872x27872, b=2048): 261.637
Elapsed time for mlp_h_to_4h (4x27872x111488, b=2048): 0.3588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27872x111488, b=2048): 141.890
Elapsed time for mlp_4h_to_h (4x111488x27872, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111488x27872, b=2048): 262.711

Attention duration (in seconds): 0.3300
Attention throughput (in TFLOP/s): 159.950
MLP duration (in seconds): 0.5526
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8826
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1676
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 228.338
b: 64, m: 2048, n: 1744, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1744x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1744x2048): 190.453
b: 64, m: 2048, n: 2048, k: 1744,
Elapsed time for attention_prob_times_values (64x2048x2048x1744): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1744): 191.465
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 265.356
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.3283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 155.423
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.1899
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 268.738

Attention duration (in seconds): 0.2255
Attention throughput (in TFLOP/s): 234.605
MLP duration (in seconds): 0.5182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27936x83808, b=2048): 0.2713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27936x83808, b=2048): 141.383
b: 64, m: 2048, n: 1746, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1746x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1746x2048): 184.437
b: 64, m: 2048, n: 2048, k: 1746,
Elapsed time for attention_prob_times_values (64x2048x2048x1746): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1746): 138.996
Elapsed time for attention_linear_projection (4x27936x27936, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_linear_projection (4x27936x27936, b=2048): 264.535
Elapsed time for mlp_h_to_4h (4x27936x111744, b=2048): 0.3626
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27936x111744, b=2048): 141.043
Elapsed time for mlp_4h_to_h (4x111744x27936, b=2048): 0.1947
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111744x27936, b=2048): 262.756

Attention duration (in seconds): 0.3315
Attention throughput (in TFLOP/s): 159.953
MLP duration (in seconds): 0.5573
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8887
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27968x83904, b=2048): 0.1427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27968x83904, b=2048): 269.511
b: 64, m: 2048, n: 1748, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1748x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1748x2048): 192.242
b: 64, m: 2048, n: 2048, k: 1748,
Elapsed time for attention_prob_times_values (64x2048x2048x1748): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1748): 163.204
Elapsed time for attention_linear_projection (4x27968x27968, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_linear_projection (4x27968x27968, b=2048): 266.484
Elapsed time for mlp_h_to_4h (4x27968x111872, b=2048): 0.3152
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27968x111872, b=2048): 162.651
Elapsed time for mlp_4h_to_h (4x111872x27968, b=2048): 0.1918
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111872x27968, b=2048): 267.277

Attention duration (in seconds): 0.2014
Attention throughput (in TFLOP/s): 263.879
MLP duration (in seconds): 0.5070
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28000x84000, b=2048): 0.2701
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28000x84000, b=2048): 142.664
b: 64, m: 2048, n: 1750, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1750x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1750x2048): 185.303
b: 64, m: 2048, n: 2048, k: 1750,
Elapsed time for attention_prob_times_values (64x2048x2048x1750): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1750): 139.191
Elapsed time for attention_linear_projection (4x28000x28000, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_linear_projection (4x28000x28000, b=2048): 265.614
Elapsed time for mlp_h_to_4h (4x28000x112000, b=2048): 0.3700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28000x112000, b=2048): 138.852
Elapsed time for mlp_4h_to_h (4x112000x28000, b=2048): 0.1963
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112000x28000, b=2048): 261.734

Attention duration (in seconds): 0.3303
Attention throughput (in TFLOP/s): 161.250
MLP duration (in seconds): 0.5663
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8966
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1444
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 267.534
b: 64, m: 2048, n: 1752, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1752x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1752x2048): 190.799
b: 64, m: 2048, n: 2048, k: 1752,
Elapsed time for attention_prob_times_values (64x2048x2048x1752): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1752): 190.137
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 265.004
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 231.399
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 265.687

Attention duration (in seconds): 0.2028
Attention throughput (in TFLOP/s): 263.175
MLP duration (in seconds): 0.4164
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28064x84192, b=2048): 0.2737
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28064x84192, b=2048): 141.458
b: 64, m: 2048, n: 1754, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1754x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1754x2048): 185.822
b: 64, m: 2048, n: 2048, k: 1754,
Elapsed time for attention_prob_times_values (64x2048x2048x1754): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1754): 139.190
Elapsed time for attention_linear_projection (4x28064x28064, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_linear_projection (4x28064x28064, b=2048): 259.574
Elapsed time for mlp_h_to_4h (4x28064x112256, b=2048): 0.3719
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28064x112256, b=2048): 138.789
Elapsed time for mlp_4h_to_h (4x112256x28064, b=2048): 0.3026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112256x28064, b=2048): 170.554

Attention duration (in seconds): 0.3352
Attention throughput (in TFLOP/s): 159.600
MLP duration (in seconds): 0.6745
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28096x84288, b=2048): 0.1477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28096x84288, b=2048): 262.710
b: 64, m: 2048, n: 1756, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1756x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1756x2048): 194.126
b: 64, m: 2048, n: 2048, k: 1756,
Elapsed time for attention_prob_times_values (64x2048x2048x1756): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1756): 163.058
Elapsed time for attention_linear_projection (4x28096x28096, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_linear_projection (4x28096x28096, b=2048): 260.705
Elapsed time for mlp_h_to_4h (4x28096x112384, b=2048): 0.2612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28096x112384, b=2048): 198.093
Elapsed time for mlp_4h_to_h (4x112384x28096, b=2048): 0.1951
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112384x28096, b=2048): 265.114

Attention duration (in seconds): 0.2079
Attention throughput (in TFLOP/s): 257.859
MLP duration (in seconds): 0.4563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28128x84384, b=2048): 0.2775
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28128x84384, b=2048): 140.136
b: 64, m: 2048, n: 1758, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1758x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1758x2048): 186.404
b: 64, m: 2048, n: 2048, k: 1758,
Elapsed time for attention_prob_times_values (64x2048x2048x1758): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1758): 140.938
Elapsed time for attention_linear_projection (4x28128x28128, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_linear_projection (4x28128x28128, b=2048): 259.641
Elapsed time for mlp_h_to_4h (4x28128x112512, b=2048): 0.3826
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28128x112512, b=2048): 135.511
Elapsed time for mlp_4h_to_h (4x112512x28128, b=2048): 0.3191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112512x28128, b=2048): 162.486

Attention duration (in seconds): 0.3392
Attention throughput (in TFLOP/s): 158.432
MLP duration (in seconds): 0.7017
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0409
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.2307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 168.986
b: 64, m: 2048, n: 1760, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1760x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1760x2048): 192.247
b: 64, m: 2048, n: 2048, k: 1760,
Elapsed time for attention_prob_times_values (64x2048x2048x1760): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1760): 192.574
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 263.610
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.3639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 142.792
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.1945
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 267.185

Attention duration (in seconds): 0.2898
Attention throughput (in TFLOP/s): 185.875
MLP duration (in seconds): 0.5585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28192x84576, b=2048): 0.2759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28192x84576, b=2048): 141.596
b: 64, m: 2048, n: 1762, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1762x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1762x2048): 186.758
b: 64, m: 2048, n: 2048, k: 1762,
Elapsed time for attention_prob_times_values (64x2048x2048x1762): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1762): 140.553
Elapsed time for attention_linear_projection (4x28192x28192, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x28192x28192, b=2048): 260.557
Elapsed time for mlp_h_to_4h (4x28192x112768, b=2048): 0.3799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28192x112768, b=2048): 137.097
Elapsed time for mlp_4h_to_h (4x112768x28192, b=2048): 0.2007
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112768x28192, b=2048): 259.483

Attention duration (in seconds): 0.3377
Attention throughput (in TFLOP/s): 159.859
MLP duration (in seconds): 0.5807
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28224x84672, b=2048): 0.1475
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28224x84672, b=2048): 265.438
b: 64, m: 2048, n: 1764, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1764x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1764x2048): 194.371
b: 64, m: 2048, n: 2048, k: 1764,
Elapsed time for attention_prob_times_values (64x2048x2048x1764): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1764): 163.943
Elapsed time for attention_linear_projection (4x28224x28224, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_linear_projection (4x28224x28224, b=2048): 261.807
Elapsed time for mlp_h_to_4h (4x28224x112896, b=2048): 0.3423
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28224x112896, b=2048): 152.507
Elapsed time for mlp_4h_to_h (4x112896x28224, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112896x28224, b=2048): 265.194

Attention duration (in seconds): 0.2080
Attention throughput (in TFLOP/s): 260.085
MLP duration (in seconds): 0.5392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28256x84768, b=2048): 0.2735
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28256x84768, b=2048): 143.473
b: 64, m: 2048, n: 1766, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1766x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1766x2048): 187.332
b: 64, m: 2048, n: 2048, k: 1766,
Elapsed time for attention_prob_times_values (64x2048x2048x1766): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1766): 141.393
Elapsed time for attention_linear_projection (4x28256x28256, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_linear_projection (4x28256x28256, b=2048): 263.854
Elapsed time for mlp_h_to_4h (4x28256x113024, b=2048): 0.3752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28256x113024, b=2048): 139.449
Elapsed time for mlp_4h_to_h (4x113024x28256, b=2048): 0.1998
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113024x28256, b=2048): 261.831

Attention duration (in seconds): 0.3349
Attention throughput (in TFLOP/s): 161.917
MLP duration (in seconds): 0.5751
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 267.094
b: 64, m: 2048, n: 1768, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1768x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1768x2048): 192.198
b: 64, m: 2048, n: 2048, k: 1768,
Elapsed time for attention_prob_times_values (64x2048x2048x1768): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1768): 190.825
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 265.107
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.3532
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 148.489
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.1970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 266.172

Attention duration (in seconds): 0.2066
Attention throughput (in TFLOP/s): 262.992
MLP duration (in seconds): 0.5502
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7568
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28320x84960, b=2048): 0.2804
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28320x84960, b=2048): 140.594
b: 64, m: 2048, n: 1770, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1770x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1770x2048): 188.155
b: 64, m: 2048, n: 2048, k: 1770,
Elapsed time for attention_prob_times_values (64x2048x2048x1770): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1770): 141.527
Elapsed time for attention_linear_projection (4x28320x28320, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x28320x28320, b=2048): 261.289
Elapsed time for mlp_h_to_4h (4x28320x113280, b=2048): 0.3709
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28320x113280, b=2048): 141.704
Elapsed time for mlp_4h_to_h (4x113280x28320, b=2048): 0.2654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113280x28320, b=2048): 198.055

Attention duration (in seconds): 0.3424
Attention throughput (in TFLOP/s): 159.039
MLP duration (in seconds): 0.6363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9788
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28352x85056, b=2048): 0.1491
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28352x85056, b=2048): 265.070
b: 64, m: 2048, n: 1772, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1772x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1772x2048): 195.425
b: 64, m: 2048, n: 2048, k: 1772,
Elapsed time for attention_prob_times_values (64x2048x2048x1772): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1772): 164.979
Elapsed time for attention_linear_projection (4x28352x28352, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_linear_projection (4x28352x28352, b=2048): 262.777
Elapsed time for mlp_h_to_4h (4x28352x113408, b=2048): 0.2884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28352x113408, b=2048): 182.676
Elapsed time for mlp_4h_to_h (4x113408x28352, b=2048): 0.1984
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113408x28352, b=2048): 265.587

Attention duration (in seconds): 0.2098
Attention throughput (in TFLOP/s): 260.156
MLP duration (in seconds): 0.4867
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6965
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28384x85152, b=2048): 0.2762
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28384x85152, b=2048): 143.383
b: 64, m: 2048, n: 1774, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1774x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1774x2048): 188.367
b: 64, m: 2048, n: 2048, k: 1774,
Elapsed time for attention_prob_times_values (64x2048x2048x1774): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1774): 141.736
Elapsed time for attention_linear_projection (4x28384x28384, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_linear_projection (4x28384x28384, b=2048): 260.919
Elapsed time for mlp_h_to_4h (4x28384x113536, b=2048): 0.3800
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28384x113536, b=2048): 138.957
Elapsed time for mlp_4h_to_h (4x113536x28384, b=2048): 0.2019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113536x28384, b=2048): 261.470

Attention duration (in seconds): 0.3385
Attention throughput (in TFLOP/s): 161.586
MLP duration (in seconds): 0.5819
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 268.586
b: 64, m: 2048, n: 1776, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1776x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1776x2048): 193.741
b: 64, m: 2048, n: 2048, k: 1776,
Elapsed time for attention_prob_times_values (64x2048x2048x1776): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1776): 193.684
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 266.074
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.3544
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 149.336
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.1975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 267.930

Attention duration (in seconds): 0.2073
Attention throughput (in TFLOP/s): 264.428
MLP duration (in seconds): 0.5519
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7592
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28448x85344, b=2048): 0.2793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28448x85344, b=2048): 142.443
b: 64, m: 2048, n: 1778, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1778x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1778x2048): 188.578
b: 64, m: 2048, n: 2048, k: 1778,
Elapsed time for attention_prob_times_values (64x2048x2048x1778): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1778): 142.795
Elapsed time for attention_linear_projection (4x28448x28448, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28448x28448, b=2048): 261.512
Elapsed time for mlp_h_to_4h (4x28448x113792, b=2048): 0.3811
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28448x113792, b=2048): 139.157
Elapsed time for mlp_4h_to_h (4x113792x28448, b=2048): 0.3070
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113792x28448, b=2048): 172.752

Attention duration (in seconds): 0.3417
Attention throughput (in TFLOP/s): 160.801
MLP duration (in seconds): 0.6882
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28480x85440, b=2048): 0.1509
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28480x85440, b=2048): 264.164
b: 64, m: 2048, n: 1780, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1780x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1780x2048): 196.307
b: 64, m: 2048, n: 2048, k: 1780,
Elapsed time for attention_prob_times_values (64x2048x2048x1780): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1780): 165.930
Elapsed time for attention_linear_projection (4x28480x28480, b=2048): 0.0505
Throughput (in TFLOP/s) for attention_linear_projection (4x28480x28480, b=2048): 262.902
Elapsed time for mlp_h_to_4h (4x28480x113920, b=2048): 0.2905
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28480x113920, b=2048): 183.007
Elapsed time for mlp_4h_to_h (4x113920x28480, b=2048): 0.2000
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113920x28480, b=2048): 265.736

Attention duration (in seconds): 0.2121
Attention throughput (in TFLOP/s): 259.638
MLP duration (in seconds): 0.4905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28512x85536, b=2048): 0.2810
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28512x85536, b=2048): 142.205
b: 64, m: 2048, n: 1782, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1782x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1782x2048): 189.270
b: 64, m: 2048, n: 2048, k: 1782,
Elapsed time for attention_prob_times_values (64x2048x2048x1782): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1782): 143.959
Elapsed time for attention_linear_projection (4x28512x28512, b=2048): 0.0514
Throughput (in TFLOP/s) for attention_linear_projection (4x28512x28512, b=2048): 259.218
Elapsed time for mlp_h_to_4h (4x28512x114048, b=2048): 0.4052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28512x114048, b=2048): 131.498
Elapsed time for mlp_4h_to_h (4x114048x28512, b=2048): 0.2053
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114048x28512, b=2048): 259.552

Attention duration (in seconds): 0.3441
Attention throughput (in TFLOP/s): 160.404
MLP duration (in seconds): 0.6104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9545
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 263.559
b: 64, m: 2048, n: 1784, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1784x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1784x2048): 193.638
b: 64, m: 2048, n: 2048, k: 1784,
Elapsed time for attention_prob_times_values (64x2048x2048x1784): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1784): 191.798
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 263.139
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.3641
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 146.652
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 264.785

Attention duration (in seconds): 0.2126
Attention throughput (in TFLOP/s): 260.146
MLP duration (in seconds): 0.5658
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7784
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28576x85728, b=2048): 0.2862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28576x85728, b=2048): 140.225
b: 64, m: 2048, n: 1786, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1786x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1786x2048): 189.829
b: 64, m: 2048, n: 2048, k: 1786,
Elapsed time for attention_prob_times_values (64x2048x2048x1786): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1786): 144.142
Elapsed time for attention_linear_projection (4x28576x28576, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_linear_projection (4x28576x28576, b=2048): 258.376
Elapsed time for mlp_h_to_4h (4x28576x114304, b=2048): 0.3962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28576x114304, b=2048): 135.067
Elapsed time for mlp_4h_to_h (4x114304x28576, b=2048): 0.3229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114304x28576, b=2048): 165.730

Attention duration (in seconds): 0.3497
Attention throughput (in TFLOP/s): 158.510
MLP duration (in seconds): 0.7191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0688
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28608x85824, b=2048): 0.1525
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28608x85824, b=2048): 263.795
b: 64, m: 2048, n: 1788, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1788x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1788x2048): 196.968
b: 64, m: 2048, n: 2048, k: 1788,
Elapsed time for attention_prob_times_values (64x2048x2048x1788): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1788): 167.459
Elapsed time for attention_linear_projection (4x28608x28608, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28608x28608, b=2048): 261.453
Elapsed time for mlp_h_to_4h (4x28608x114432, b=2048): 0.3411
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28608x114432, b=2048): 157.229
Elapsed time for mlp_4h_to_h (4x114432x28608, b=2048): 0.2017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114432x28608, b=2048): 265.913

Attention duration (in seconds): 0.2144
Attention throughput (in TFLOP/s): 259.140
MLP duration (in seconds): 0.5428
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7572
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28640x85920, b=2048): 0.2858
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28640x85920, b=2048): 141.051
b: 64, m: 2048, n: 1790, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1790x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1790x2048): 190.993
b: 64, m: 2048, n: 2048, k: 1790,
Elapsed time for attention_prob_times_values (64x2048x2048x1790): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1790): 145.687
Elapsed time for attention_linear_projection (4x28640x28640, b=2048): 0.0520
Throughput (in TFLOP/s) for attention_linear_projection (4x28640x28640, b=2048): 258.610
Elapsed time for mlp_h_to_4h (4x28640x114560, b=2048): 0.3855
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28640x114560, b=2048): 139.448
Elapsed time for mlp_4h_to_h (4x114560x28640, b=2048): 0.2067
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114560x28640, b=2048): 260.069

Attention duration (in seconds): 0.3494
Attention throughput (in TFLOP/s): 159.341
MLP duration (in seconds): 0.5922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9416
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1530
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 264.014
b: 64, m: 2048, n: 1792, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1792x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1792x2048): 196.066
b: 64, m: 2048, n: 2048, k: 1792,
Elapsed time for attention_prob_times_values (64x2048x2048x1792): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1792): 196.209
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 262.318
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.3614
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 149.079
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 265.809

Attention duration (in seconds): 0.2142
Attention throughput (in TFLOP/s): 260.499
MLP duration (in seconds): 0.5641
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7783
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28704x86112, b=2048): 0.2854
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28704x86112, b=2048): 141.882
b: 64, m: 2048, n: 1794, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1794x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1794x2048): 181.929
b: 64, m: 2048, n: 2048, k: 1794,
Elapsed time for attention_prob_times_values (64x2048x2048x1794): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1794): 143.344
Elapsed time for attention_linear_projection (4x28704x28704, b=2048): 0.0517
Throughput (in TFLOP/s) for attention_linear_projection (4x28704x28704, b=2048): 261.239
Elapsed time for mlp_h_to_4h (4x28704x114816, b=2048): 0.3891
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28704x114816, b=2048): 138.762
Elapsed time for mlp_4h_to_h (4x114816x28704, b=2048): 0.2065
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114816x28704, b=2048): 261.541

Attention duration (in seconds): 0.3491
Attention throughput (in TFLOP/s): 160.183
MLP duration (in seconds): 0.5956
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28736x86208, b=2048): 0.1521
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28736x86208, b=2048): 266.776
b: 64, m: 2048, n: 1796, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1796x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1796x2048): 187.633
b: 64, m: 2048, n: 2048, k: 1796,
Elapsed time for attention_prob_times_values (64x2048x2048x1796): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1796): 165.077
Elapsed time for attention_linear_projection (4x28736x28736, b=2048): 0.0509
Throughput (in TFLOP/s) for attention_linear_projection (4x28736x28736, b=2048): 265.564
Elapsed time for mlp_h_to_4h (4x28736x114944, b=2048): 0.3492
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28736x114944, b=2048): 154.959
Elapsed time for mlp_4h_to_h (4x114944x28736, b=2048): 0.2021
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114944x28736, b=2048): 267.718

Attention duration (in seconds): 0.2141
Attention throughput (in TFLOP/s): 261.813
MLP duration (in seconds): 0.5514
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7654
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28768x86304, b=2048): 0.2871
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28768x86304, b=2048): 141.680
b: 64, m: 2048, n: 1798, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1798x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1798x2048): 181.176
b: 64, m: 2048, n: 2048, k: 1798,
Elapsed time for attention_prob_times_values (64x2048x2048x1798): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1798): 142.536
Elapsed time for attention_linear_projection (4x28768x28768, b=2048): 0.0514
Throughput (in TFLOP/s) for attention_linear_projection (4x28768x28768, b=2048): 263.818
Elapsed time for mlp_h_to_4h (4x28768x115072, b=2048): 0.3870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28768x115072, b=2048): 140.157
Elapsed time for mlp_4h_to_h (4x115072x28768, b=2048): 0.3285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115072x28768, b=2048): 165.116

Attention duration (in seconds): 0.3506
Attention throughput (in TFLOP/s): 160.201
MLP duration (in seconds): 0.7155
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0661
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1595
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 255.563
b: 64, m: 2048, n: 1800, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1800x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1800x2048): 184.641
b: 64, m: 2048, n: 2048, k: 1800,
Elapsed time for attention_prob_times_values (64x2048x2048x1800): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1800): 189.936
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 265.762
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.3659
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 148.558
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2034
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 267.238

Attention duration (in seconds): 0.2210
Attention throughput (in TFLOP/s): 254.732
MLP duration (in seconds): 0.5693
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28832x86496, b=2048): 0.2881
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28832x86496, b=2048): 141.813
b: 64, m: 2048, n: 1802, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1802x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1802x2048): 181.790
b: 64, m: 2048, n: 2048, k: 1802,
Elapsed time for attention_prob_times_values (64x2048x2048x1802): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1802): 142.242
Elapsed time for attention_linear_projection (4x28832x28832, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_linear_projection (4x28832x28832, b=2048): 263.815
Elapsed time for mlp_h_to_4h (4x28832x115328, b=2048): 0.3888
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28832x115328, b=2048): 140.131
Elapsed time for mlp_4h_to_h (4x115328x28832, b=2048): 0.2560
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115328x28832, b=2048): 212.795

Attention duration (in seconds): 0.3519
Attention throughput (in TFLOP/s): 160.326
MLP duration (in seconds): 0.6448
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28864x86592, b=2048): 0.1531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28864x86592, b=2048): 267.389
b: 64, m: 2048, n: 1804, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1804x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1804x2048): 188.530
b: 64, m: 2048, n: 2048, k: 1804,
Elapsed time for attention_prob_times_values (64x2048x2048x1804): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1804): 164.697
Elapsed time for attention_linear_projection (4x28864x28864, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28864x28864, b=2048): 266.005
Elapsed time for mlp_h_to_4h (4x28864x115456, b=2048): 0.3714
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28864x115456, b=2048): 147.003
Elapsed time for mlp_4h_to_h (4x115456x28864, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115456x28864, b=2048): 266.928

Attention duration (in seconds): 0.2155
Attention throughput (in TFLOP/s): 262.377
MLP duration (in seconds): 0.5760
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28896x86688, b=2048): 0.2915
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28896x86688, b=2048): 140.777
b: 64, m: 2048, n: 1806, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1806x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1806x2048): 181.309
b: 64, m: 2048, n: 2048, k: 1806,
Elapsed time for attention_prob_times_values (64x2048x2048x1806): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1806): 141.788
Elapsed time for attention_linear_projection (4x28896x28896, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_linear_projection (4x28896x28896, b=2048): 264.014
Elapsed time for mlp_h_to_4h (4x28896x115584, b=2048): 0.4035
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28896x115584, b=2048): 135.618
Elapsed time for mlp_4h_to_h (4x115584x28896, b=2048): 0.3177
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115584x28896, b=2048): 172.242

Attention duration (in seconds): 0.3555
Attention throughput (in TFLOP/s): 159.367
MLP duration (in seconds): 0.7212
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.2541
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 161.850
b: 64, m: 2048, n: 1808, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1808x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1808x2048): 186.712
b: 64, m: 2048, n: 2048, k: 1808,
Elapsed time for attention_prob_times_values (64x2048x2048x1808): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1808): 193.234
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 265.958
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.3816
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 143.727
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 266.485

Attention duration (in seconds): 0.3159
Attention throughput (in TFLOP/s): 179.747
MLP duration (in seconds): 0.5874
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28960x86880, b=2048): 0.2876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28960x86880, b=2048): 143.343
b: 64, m: 2048, n: 1810, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1810x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1810x2048): 181.872
b: 64, m: 2048, n: 2048, k: 1810,
Elapsed time for attention_prob_times_values (64x2048x2048x1810): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1810): 141.490
Elapsed time for attention_linear_projection (4x28960x28960, b=2048): 0.0529
Throughput (in TFLOP/s) for attention_linear_projection (4x28960x28960, b=2048): 259.564
Elapsed time for mlp_h_to_4h (4x28960x115840, b=2048): 0.4060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28960x115840, b=2048): 135.380
Elapsed time for mlp_4h_to_h (4x115840x28960, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115840x28960, b=2048): 199.138

Attention duration (in seconds): 0.3527
Attention throughput (in TFLOP/s): 161.333
MLP duration (in seconds): 0.6820
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28992x86976, b=2048): 0.1559
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28992x86976, b=2048): 264.937
b: 64, m: 2048, n: 1812, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1812x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1812x2048): 188.822
b: 64, m: 2048, n: 2048, k: 1812,
Elapsed time for attention_prob_times_values (64x2048x2048x1812): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1812): 161.832
Elapsed time for attention_linear_projection (4x28992x28992, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28992x28992, b=2048): 261.928
Elapsed time for mlp_h_to_4h (4x28992x115968, b=2048): 0.3132
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28992x115968, b=2048): 175.882
Elapsed time for mlp_4h_to_h (4x115968x28992, b=2048): 0.2068
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115968x28992, b=2048): 266.330

Attention duration (in seconds): 0.2197
Attention throughput (in TFLOP/s): 259.610
MLP duration (in seconds): 0.5200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7397
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29024x87072, b=2048): 0.2912
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29024x87072, b=2048): 142.191
b: 64, m: 2048, n: 1814, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1814x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1814x2048): 182.160
b: 64, m: 2048, n: 2048, k: 1814,
Elapsed time for attention_prob_times_values (64x2048x2048x1814): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1814): 140.665
Elapsed time for attention_linear_projection (4x29024x29024, b=2048): 0.0525
Throughput (in TFLOP/s) for attention_linear_projection (4x29024x29024, b=2048): 262.724
Elapsed time for mlp_h_to_4h (4x29024x116096, b=2048): 0.3941
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29024x116096, b=2048): 140.072
Elapsed time for mlp_4h_to_h (4x116096x29024, b=2048): 0.2696
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116096x29024, b=2048): 204.743

Attention duration (in seconds): 0.3560
Attention throughput (in TFLOP/s): 160.549
MLP duration (in seconds): 0.6638
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.2625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 158.098
b: 64, m: 2048, n: 1816, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1816x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1816x2048): 186.767
b: 64, m: 2048, n: 2048, k: 1816,
Elapsed time for attention_prob_times_values (64x2048x2048x1816): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1816): 191.418
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0523
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 264.433
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.3677
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 150.473
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 266.284

Attention duration (in seconds): 0.3251
Attention throughput (in TFLOP/s): 176.190
MLP duration (in seconds): 0.5755
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9006
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29088x87264, b=2048): 0.2948
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29088x87264, b=2048): 141.095
b: 64, m: 2048, n: 1818, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1818x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1818x2048): 182.635
b: 64, m: 2048, n: 2048, k: 1818,
Elapsed time for attention_prob_times_values (64x2048x2048x1818): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1818): 139.667
Elapsed time for attention_linear_projection (4x29088x29088, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_linear_projection (4x29088x29088, b=2048): 260.621
Elapsed time for mlp_h_to_4h (4x29088x116352, b=2048): 0.3940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29088x116352, b=2048): 140.730
Elapsed time for mlp_4h_to_h (4x116352x29088, b=2048): 0.3201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116352x29088, b=2048): 173.223

Attention duration (in seconds): 0.3603
Attention throughput (in TFLOP/s): 159.330
MLP duration (in seconds): 0.7141
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29120x87360, b=2048): 0.1882
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29120x87360, b=2048): 221.515
b: 64, m: 2048, n: 1820, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1820x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1820x2048): 189.577
b: 64, m: 2048, n: 2048, k: 1820,
Elapsed time for attention_prob_times_values (64x2048x2048x1820): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1820): 164.662
Elapsed time for attention_linear_projection (4x29120x29120, b=2048): 0.0525
Throughput (in TFLOP/s) for attention_linear_projection (4x29120x29120, b=2048): 264.842
Elapsed time for mlp_h_to_4h (4x29120x116480, b=2048): 0.3618
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29120x116480, b=2048): 153.593
Elapsed time for mlp_4h_to_h (4x116480x29120, b=2048): 0.2073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116480x29120, b=2048): 268.131

Attention duration (in seconds): 0.2517
Attention throughput (in TFLOP/s): 228.551
MLP duration (in seconds): 0.5691
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29152x87456, b=2048): 0.2998
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29152x87456, b=2048): 139.343
b: 64, m: 2048, n: 1822, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1822x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1822x2048): 182.866
b: 64, m: 2048, n: 2048, k: 1822,
Elapsed time for attention_prob_times_values (64x2048x2048x1822): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1822): 140.295
Elapsed time for attention_linear_projection (4x29152x29152, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x29152x29152, b=2048): 263.751
Elapsed time for mlp_h_to_4h (4x29152x116608, b=2048): 0.4013
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29152x116608, b=2048): 138.793
Elapsed time for mlp_4h_to_h (4x116608x29152, b=2048): 0.3470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116608x29152, b=2048): 160.489

Attention duration (in seconds): 0.3649
Attention throughput (in TFLOP/s): 157.998
MLP duration (in seconds): 0.7483
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1573
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 266.104
b: 64, m: 2048, n: 1824, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1824x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1824x2048): 187.659
b: 64, m: 2048, n: 2048, k: 1824,
Elapsed time for attention_prob_times_values (64x2048x2048x1824): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1824): 194.345
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0529
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 263.771
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2789
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 200.129
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 265.787

Attention duration (in seconds): 0.2205
Attention throughput (in TFLOP/s): 262.048
MLP duration (in seconds): 0.4889
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29216x87648, b=2048): 0.2953
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29216x87648, b=2048): 142.083
b: 64, m: 2048, n: 1826, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1826x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1826x2048): 182.478
b: 64, m: 2048, n: 2048, k: 1826,
Elapsed time for attention_prob_times_values (64x2048x2048x1826): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1826): 141.168
Elapsed time for attention_linear_projection (4x29216x29216, b=2048): 0.0538
Throughput (in TFLOP/s) for attention_linear_projection (4x29216x29216, b=2048): 260.024
Elapsed time for mlp_h_to_4h (4x29216x116864, b=2048): 0.3991
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29216x116864, b=2048): 140.172
Elapsed time for mlp_4h_to_h (4x116864x29216, b=2048): 0.2699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116864x29216, b=2048): 207.228

Attention duration (in seconds): 0.3614
Attention throughput (in TFLOP/s): 160.219
MLP duration (in seconds): 0.6690
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29248x87744, b=2048): 0.1571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29248x87744, b=2048): 267.678
b: 64, m: 2048, n: 1828, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1828x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1828x2048): 190.155
b: 64, m: 2048, n: 2048, k: 1828,
Elapsed time for attention_prob_times_values (64x2048x2048x1828): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1828): 162.122
Elapsed time for attention_linear_projection (4x29248x29248, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x29248x29248, b=2048): 266.211
Elapsed time for mlp_h_to_4h (4x29248x116992, b=2048): 0.3752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29248x116992, b=2048): 149.424
Elapsed time for mlp_4h_to_h (4x116992x29248, b=2048): 0.2093
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116992x29248, b=2048): 267.835

Attention duration (in seconds): 0.2209
Attention throughput (in TFLOP/s): 262.626
MLP duration (in seconds): 0.5845
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29280x87840, b=2048): 0.2942
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29280x87840, b=2048): 143.231
b: 64, m: 2048, n: 1830, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1830x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1830x2048): 182.765
b: 64, m: 2048, n: 2048, k: 1830,
Elapsed time for attention_prob_times_values (64x2048x2048x1830): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1830): 140.311
Elapsed time for attention_linear_projection (4x29280x29280, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_linear_projection (4x29280x29280, b=2048): 263.977
Elapsed time for mlp_h_to_4h (4x29280x117120, b=2048): 0.4012
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29280x117120, b=2048): 140.039
Elapsed time for mlp_4h_to_h (4x117120x29280, b=2048): 0.2283
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117120x29280, b=2048): 246.081

Attention duration (in seconds): 0.3598
Attention throughput (in TFLOP/s): 161.622
MLP duration (in seconds): 0.6295
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9893
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1577
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 267.753
b: 64, m: 2048, n: 1832, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1832x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1832x2048): 187.735
b: 64, m: 2048, n: 2048, k: 1832,
Elapsed time for attention_prob_times_values (64x2048x2048x1832): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1832): 192.393
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 265.048
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.3772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 149.262
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 265.488

Attention duration (in seconds): 0.2212
Attention throughput (in TFLOP/s): 263.466
MLP duration (in seconds): 0.5893
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29344x88032, b=2048): 0.2968
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29344x88032, b=2048): 142.593
b: 64, m: 2048, n: 1834, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1834x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1834x2048): 183.384
b: 64, m: 2048, n: 2048, k: 1834,
Elapsed time for attention_prob_times_values (64x2048x2048x1834): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1834): 139.299
Elapsed time for attention_linear_projection (4x29344x29344, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x29344x29344, b=2048): 261.579
Elapsed time for mlp_h_to_4h (4x29344x117376, b=2048): 0.4220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29344x117376, b=2048): 133.715
Elapsed time for mlp_4h_to_h (4x117376x29344, b=2048): 0.2164
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117376x29344, b=2048): 260.724

Attention duration (in seconds): 0.3632
Attention throughput (in TFLOP/s): 160.802
MLP duration (in seconds): 0.6385
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29376x88128, b=2048): 0.1601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29376x88128, b=2048): 264.856
b: 64, m: 2048, n: 1836, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1836x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1836x2048): 190.899
b: 64, m: 2048, n: 2048, k: 1836,
Elapsed time for attention_prob_times_values (64x2048x2048x1836): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1836): 165.471
Elapsed time for attention_linear_projection (4x29376x29376, b=2048): 0.0537
Throughput (in TFLOP/s) for attention_linear_projection (4x29376x29376, b=2048): 263.461
Elapsed time for mlp_h_to_4h (4x29376x117504, b=2048): 0.3782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29376x117504, b=2048): 149.535
Elapsed time for mlp_4h_to_h (4x117504x29376, b=2048): 0.2120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117504x29376, b=2048): 266.819

Attention duration (in seconds): 0.2249
Attention throughput (in TFLOP/s): 260.193
MLP duration (in seconds): 0.5902
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29408x88224, b=2048): 0.2995
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29408x88224, b=2048): 141.932
b: 64, m: 2048, n: 1838, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1838x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1838x2048): 183.792
b: 64, m: 2048, n: 2048, k: 1838,
Elapsed time for attention_prob_times_values (64x2048x2048x1838): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1838): 140.675
Elapsed time for attention_linear_projection (4x29408x29408, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_linear_projection (4x29408x29408, b=2048): 262.299
Elapsed time for mlp_h_to_4h (4x29408x117632, b=2048): 0.4031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29408x117632, b=2048): 140.587
Elapsed time for mlp_4h_to_h (4x117632x29408, b=2048): 0.2634
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117632x29408, b=2048): 215.171

Attention duration (in seconds): 0.3659
Attention throughput (in TFLOP/s): 160.293
MLP duration (in seconds): 0.6666
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.2682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 158.845
b: 64, m: 2048, n: 1840, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1840x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1840x2048): 189.764
b: 64, m: 2048, n: 2048, k: 1840,
Elapsed time for attention_prob_times_values (64x2048x2048x1840): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1840): 195.587
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0538
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 264.013
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.3918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 144.985
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 267.797

Attention duration (in seconds): 0.3322
Attention throughput (in TFLOP/s): 176.914
MLP duration (in seconds): 0.6039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9361
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29472x88416, b=2048): 0.3024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29472x88416, b=2048): 141.173
b: 64, m: 2048, n: 1842, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1842x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1842x2048): 184.274
b: 64, m: 2048, n: 2048, k: 1842,
Elapsed time for attention_prob_times_values (64x2048x2048x1842): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1842): 140.656
Elapsed time for attention_linear_projection (4x29472x29472, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_linear_projection (4x29472x29472, b=2048): 261.100
Elapsed time for mlp_h_to_4h (4x29472x117888, b=2048): 0.4096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29472x117888, b=2048): 138.987
Elapsed time for mlp_4h_to_h (4x117888x29472, b=2048): 0.2914
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117888x29472, b=2048): 195.338

Attention duration (in seconds): 0.3693
Attention throughput (in TFLOP/s): 159.488
MLP duration (in seconds): 0.7010
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0703
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29504x88512, b=2048): 0.2611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29504x88512, b=2048): 163.873
b: 64, m: 2048, n: 1844, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1844x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1844x2048): 191.563
b: 64, m: 2048, n: 2048, k: 1844,
Elapsed time for attention_prob_times_values (64x2048x2048x1844): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1844): 164.500
Elapsed time for attention_linear_projection (4x29504x29504, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_linear_projection (4x29504x29504, b=2048): 264.235
Elapsed time for mlp_h_to_4h (4x29504x118016, b=2048): 0.3852
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29504x118016, b=2048): 148.099
Elapsed time for mlp_4h_to_h (4x118016x29504, b=2048): 0.2134
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118016x29504, b=2048): 267.290

Attention duration (in seconds): 0.3263
Attention throughput (in TFLOP/s): 180.927
MLP duration (in seconds): 0.5986
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29536x88608, b=2048): 0.3008
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29536x88608, b=2048): 142.541
b: 64, m: 2048, n: 1846, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1846x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1846x2048): 184.526
b: 64, m: 2048, n: 2048, k: 1846,
Elapsed time for attention_prob_times_values (64x2048x2048x1846): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1846): 141.316
Elapsed time for attention_linear_projection (4x29536x29536, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29536x29536, b=2048): 261.710
Elapsed time for mlp_h_to_4h (4x29536x118144, b=2048): 0.4131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29536x118144, b=2048): 138.396
Elapsed time for mlp_4h_to_h (4x118144x29536, b=2048): 0.2852
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118144x29536, b=2048): 200.489

Attention duration (in seconds): 0.3678
Attention throughput (in TFLOP/s): 160.825
MLP duration (in seconds): 0.6983
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0661
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 266.739
b: 64, m: 2048, n: 1848, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1848x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1848x2048): 189.790
b: 64, m: 2048, n: 2048, k: 1848,
Elapsed time for attention_prob_times_values (64x2048x2048x1848): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1848): 193.676
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 264.616
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.3143
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 182.311
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2143
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 267.376

Attention duration (in seconds): 0.2256
Attention throughput (in TFLOP/s): 262.787
MLP duration (in seconds): 0.5286
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29600x88800, b=2048): 0.3059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29600x88800, b=2048): 140.786
b: 64, m: 2048, n: 1850, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1850x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1850x2048): 185.304
b: 64, m: 2048, n: 2048, k: 1850,
Elapsed time for attention_prob_times_values (64x2048x2048x1850): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1850): 141.709
Elapsed time for attention_linear_projection (4x29600x29600, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_linear_projection (4x29600x29600, b=2048): 262.014
Elapsed time for mlp_h_to_4h (4x29600x118400, b=2048): 0.4083
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29600x118400, b=2048): 140.646
Elapsed time for mlp_4h_to_h (4x118400x29600, b=2048): 0.2841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118400x29600, b=2048): 202.146

Attention duration (in seconds): 0.3730
Attention throughput (in TFLOP/s): 159.247
MLP duration (in seconds): 0.6923
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0654
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29632x88896, b=2048): 0.1615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29632x88896, b=2048): 267.173
b: 64, m: 2048, n: 1852, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1852x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1852x2048): 192.803
b: 64, m: 2048, n: 2048, k: 1852,
Elapsed time for attention_prob_times_values (64x2048x2048x1852): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1852): 164.374
Elapsed time for attention_linear_projection (4x29632x29632, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_linear_projection (4x29632x29632, b=2048): 265.787
Elapsed time for mlp_h_to_4h (4x29632x118528, b=2048): 0.3651
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29632x118528, b=2048): 157.633
Elapsed time for mlp_4h_to_h (4x118528x29632, b=2048): 0.2154
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118528x29632, b=2048): 267.150

Attention duration (in seconds): 0.2269
Attention throughput (in TFLOP/s): 262.411
MLP duration (in seconds): 0.5805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29664x88992, b=2048): 0.3056
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29664x88992, b=2048): 141.508
b: 64, m: 2048, n: 1854, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1854x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1854x2048): 185.499
b: 64, m: 2048, n: 2048, k: 1854,
Elapsed time for attention_prob_times_values (64x2048x2048x1854): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1854): 142.321
Elapsed time for attention_linear_projection (4x29664x29664, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_linear_projection (4x29664x29664, b=2048): 262.175
Elapsed time for mlp_h_to_4h (4x29664x118656, b=2048): 0.4264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29664x118656, b=2048): 135.253
Elapsed time for mlp_4h_to_h (4x118656x29664, b=2048): 0.2219
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118656x29664, b=2048): 259.922

Attention duration (in seconds): 0.3730
Attention throughput (in TFLOP/s): 159.945
MLP duration (in seconds): 0.6482
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1617
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 268.042
b: 64, m: 2048, n: 1856, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1856x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1856x2048): 191.678
b: 64, m: 2048, n: 2048, k: 1856,
Elapsed time for attention_prob_times_values (64x2048x2048x1856): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1856): 197.278
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 266.885
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.3206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 180.292
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2160
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 267.593

Attention duration (in seconds): 0.2261
Attention throughput (in TFLOP/s): 264.428
MLP duration (in seconds): 0.5365
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29728x89184, b=2048): 0.3082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29728x89184, b=2048): 140.936
b: 64, m: 2048, n: 1858, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1858x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1858x2048): 185.693
b: 64, m: 2048, n: 2048, k: 1858,
Elapsed time for attention_prob_times_values (64x2048x2048x1858): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1858): 140.219
Elapsed time for attention_linear_projection (4x29728x29728, b=2048): 0.0553
Throughput (in TFLOP/s) for attention_linear_projection (4x29728x29728, b=2048): 261.823
Elapsed time for mlp_h_to_4h (4x29728x118912, b=2048): 0.4618
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29728x118912, b=2048): 125.431
Elapsed time for mlp_4h_to_h (4x118912x29728, b=2048): 0.2504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118912x29728, b=2048): 231.269

Attention duration (in seconds): 0.3760
Attention throughput (in TFLOP/s): 159.342
MLP duration (in seconds): 0.7122
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29760x89280, b=2048): 0.1642
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29760x89280, b=2048): 265.090
b: 64, m: 2048, n: 1860, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1860x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1860x2048): 193.573
b: 64, m: 2048, n: 2048, k: 1860,
Elapsed time for attention_prob_times_values (64x2048x2048x1860): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1860): 163.690
Elapsed time for attention_linear_projection (4x29760x29760, b=2048): 0.0553
Throughput (in TFLOP/s) for attention_linear_projection (4x29760x29760, b=2048): 262.538
Elapsed time for mlp_h_to_4h (4x29760x119040, b=2048): 0.4157
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29760x119040, b=2048): 139.637
Elapsed time for mlp_4h_to_h (4x119040x29760, b=2048): 0.2178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119040x29760, b=2048): 266.455

Attention duration (in seconds): 0.2307
Attention throughput (in TFLOP/s): 260.199
MLP duration (in seconds): 0.6335
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29792x89376, b=2048): 0.3106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29792x89376, b=2048): 140.463
b: 64, m: 2048, n: 1862, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1862x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1862x2048): 185.648
b: 64, m: 2048, n: 2048, k: 1862,
Elapsed time for attention_prob_times_values (64x2048x2048x1862): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1862): 137.376
Elapsed time for attention_linear_projection (4x29792x29792, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29792x29792, b=2048): 259.946
Elapsed time for mlp_h_to_4h (4x29792x119168, b=2048): 0.4410
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29792x119168, b=2048): 131.902
Elapsed time for mlp_4h_to_h (4x119168x29792, b=2048): 0.3387
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119168x29792, b=2048): 171.740

Attention duration (in seconds): 0.3792
Attention throughput (in TFLOP/s): 158.673
MLP duration (in seconds): 0.7797
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1589
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1640
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 266.512
b: 64, m: 2048, n: 1864, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1864x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1864x2048): 190.841
b: 64, m: 2048, n: 2048, k: 1864,
Elapsed time for attention_prob_times_values (64x2048x2048x1864): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1864): 192.099
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 264.270
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.4021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 144.956
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2183
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 267.009

Attention duration (in seconds): 0.2296
Attention throughput (in TFLOP/s): 262.557
MLP duration (in seconds): 0.6205
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8501
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29856x89568, b=2048): 0.3115
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29856x89568, b=2048): 140.659
b: 64, m: 2048, n: 1866, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1866x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1866x2048): 186.071
b: 64, m: 2048, n: 2048, k: 1866,
Elapsed time for attention_prob_times_values (64x2048x2048x1866): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1866): 139.268
Elapsed time for attention_linear_projection (4x29856x29856, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_linear_projection (4x29856x29856, b=2048): 262.215
Elapsed time for mlp_h_to_4h (4x29856x119424, b=2048): 0.4306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29856x119424, b=2048): 135.668
Elapsed time for mlp_4h_to_h (4x119424x29856, b=2048): 0.2785
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119424x29856, b=2048): 209.754

Attention duration (in seconds): 0.3798
Attention throughput (in TFLOP/s): 159.104
MLP duration (in seconds): 0.7091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0889
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29888x89664, b=2048): 0.1708
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29888x89664, b=2048): 257.008
b: 64, m: 2048, n: 1868, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1868x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1868x2048): 194.460
b: 64, m: 2048, n: 2048, k: 1868,
Elapsed time for attention_prob_times_values (64x2048x2048x1868): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1868): 164.904
Elapsed time for attention_linear_projection (4x29888x29888, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29888x29888, b=2048): 264.341
Elapsed time for mlp_h_to_4h (4x29888x119552, b=2048): 0.3966
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29888x119552, b=2048): 147.618
Elapsed time for mlp_4h_to_h (4x119552x29888, b=2048): 0.2192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119552x29888, b=2048): 267.027

Attention duration (in seconds): 0.2374
Attention throughput (in TFLOP/s): 255.000
MLP duration (in seconds): 0.6158
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8533
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29920x89760, b=2048): 0.3108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29920x89760, b=2048): 141.573
b: 64, m: 2048, n: 1870, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1870x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1870x2048): 186.776
b: 64, m: 2048, n: 2048, k: 1870,
Elapsed time for attention_prob_times_values (64x2048x2048x1870): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1870): 139.961
Elapsed time for attention_linear_projection (4x29920x29920, b=2048): 0.0563
Throughput (in TFLOP/s) for attention_linear_projection (4x29920x29920, b=2048): 260.325
Elapsed time for mlp_h_to_4h (4x29920x119680, b=2048): 0.4225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29920x119680, b=2048): 138.844
Elapsed time for mlp_4h_to_h (4x119680x29920, b=2048): 0.2473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119680x29920, b=2048): 237.271

Attention duration (in seconds): 0.3797
Attention throughput (in TFLOP/s): 159.804
MLP duration (in seconds): 0.6698
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1654
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 266.672
b: 64, m: 2048, n: 1872, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1872x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1872x2048): 192.483
b: 64, m: 2048, n: 2048, k: 1872,
Elapsed time for attention_prob_times_values (64x2048x2048x1872): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1872): 194.633
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 265.335
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.3905
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 150.571
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 266.644

Attention duration (in seconds): 0.2311
Attention throughput (in TFLOP/s): 263.066
MLP duration (in seconds): 0.6110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8421
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29984x89952, b=2048): 0.3180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29984x89952, b=2048): 138.952
b: 64, m: 2048, n: 1874, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1874x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1874x2048): 186.662
b: 64, m: 2048, n: 2048, k: 1874,
Elapsed time for attention_prob_times_values (64x2048x2048x1874): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1874): 140.158
Elapsed time for attention_linear_projection (4x29984x29984, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_linear_projection (4x29984x29984, b=2048): 261.275
Elapsed time for mlp_h_to_4h (4x29984x119936, b=2048): 0.4263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29984x119936, b=2048): 138.226
Elapsed time for mlp_4h_to_h (4x119936x29984, b=2048): 0.2292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119936x29984, b=2048): 257.063

Attention duration (in seconds): 0.3870
Attention throughput (in TFLOP/s): 157.460
MLP duration (in seconds): 0.6555
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30016x90048, b=2048): 0.1938
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30016x90048, b=2048): 228.452
b: 64, m: 2048, n: 1876, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1876x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1876x2048): 194.968
b: 64, m: 2048, n: 2048, k: 1876,
Elapsed time for attention_prob_times_values (64x2048x2048x1876): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1876): 164.635
Elapsed time for attention_linear_projection (4x30016x30016, b=2048): 0.0561
Throughput (in TFLOP/s) for attention_linear_projection (4x30016x30016, b=2048): 262.943
Elapsed time for mlp_h_to_4h (4x30016x120064, b=2048): 0.3254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30016x120064, b=2048): 181.447
Elapsed time for mlp_4h_to_h (4x120064x30016, b=2048): 0.2223
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120064x30016, b=2048): 265.612

Attention duration (in seconds): 0.2613
Attention throughput (in TFLOP/s): 233.707
MLP duration (in seconds): 0.5477
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30048x90144, b=2048): 0.3143
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30048x90144, b=2048): 141.203
b: 64, m: 2048, n: 1878, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1878x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1878x2048): 187.925
b: 64, m: 2048, n: 2048, k: 1878,
Elapsed time for attention_prob_times_values (64x2048x2048x1878): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1878): 141.057
Elapsed time for attention_linear_projection (4x30048x30048, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x30048x30048, b=2048): 259.834
Elapsed time for mlp_h_to_4h (4x30048x120192, b=2048): 0.4416
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30048x120192, b=2048): 133.986
Elapsed time for mlp_4h_to_h (4x120192x30048, b=2048): 0.3050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120192x30048, b=2048): 193.982

Attention duration (in seconds): 0.3837
Attention throughput (in TFLOP/s): 159.454
MLP duration (in seconds): 0.7467
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.2487
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 178.846
b: 64, m: 2048, n: 1880, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1880x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1880x2048): 192.988
b: 64, m: 2048, n: 2048, k: 1880,
Elapsed time for attention_prob_times_values (64x2048x2048x1880): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1880): 193.243
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 263.571
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.3733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 158.839
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 266.099

Attention duration (in seconds): 0.3154
Attention throughput (in TFLOP/s): 194.430
MLP duration (in seconds): 0.5962
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30112x90336, b=2048): 0.3159
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30112x90336, b=2048): 141.075
b: 64, m: 2048, n: 1882, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1882x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1882x2048): 188.167
b: 64, m: 2048, n: 2048, k: 1882,
Elapsed time for attention_prob_times_values (64x2048x2048x1882): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1882): 140.532
Elapsed time for attention_linear_projection (4x30112x30112, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_linear_projection (4x30112x30112, b=2048): 259.825
Elapsed time for mlp_h_to_4h (4x30112x120448, b=2048): 0.4400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30112x120448, b=2048): 135.065
Elapsed time for mlp_4h_to_h (4x120448x30112, b=2048): 0.2296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120448x30112, b=2048): 258.774

Attention duration (in seconds): 0.3857
Attention throughput (in TFLOP/s): 159.327
MLP duration (in seconds): 0.6696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30144x90432, b=2048): 0.2783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30144x90432, b=2048): 160.492
b: 64, m: 2048, n: 1884, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1884x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1884x2048): 195.899
b: 64, m: 2048, n: 2048, k: 1884,
Elapsed time for attention_prob_times_values (64x2048x2048x1884): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1884): 164.849
Elapsed time for attention_linear_projection (4x30144x30144, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_linear_projection (4x30144x30144, b=2048): 263.771
Elapsed time for mlp_h_to_4h (4x30144x120576, b=2048): 0.4167
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30144x120576, b=2048): 142.919
Elapsed time for mlp_4h_to_h (4x120576x30144, b=2048): 0.2243
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120576x30144, b=2048): 265.489

Attention duration (in seconds): 0.3460
Attention throughput (in TFLOP/s): 177.944
MLP duration (in seconds): 0.6410
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9870
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30176x90528, b=2048): 0.3184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30176x90528, b=2048): 140.556
b: 64, m: 2048, n: 1886, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1886x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1886x2048): 188.408
b: 64, m: 2048, n: 2048, k: 1886,
Elapsed time for attention_prob_times_values (64x2048x2048x1886): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1886): 140.733
Elapsed time for attention_linear_projection (4x30176x30176, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_linear_projection (4x30176x30176, b=2048): 260.642
Elapsed time for mlp_h_to_4h (4x30176x120704, b=2048): 0.4380
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30176x120704, b=2048): 136.250
Elapsed time for mlp_4h_to_h (4x120704x30176, b=2048): 0.2958
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120704x30176, b=2048): 201.780

Attention duration (in seconds): 0.3882
Attention throughput (in TFLOP/s): 158.926
MLP duration (in seconds): 0.7337
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 250.187
b: 64, m: 2048, n: 1888, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1888x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1888x2048): 194.741
b: 64, m: 2048, n: 2048, k: 1888,
Elapsed time for attention_prob_times_values (64x2048x2048x1888): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1888): 196.578
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0566
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 264.228
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.4162
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 143.702
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2247
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 266.122

Attention duration (in seconds): 0.2462
Attention throughput (in TFLOP/s): 251.119
MLP duration (in seconds): 0.6409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8871
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30240x90720, b=2048): 0.3215
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30240x90720, b=2048): 139.786
b: 64, m: 2048, n: 1890, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1890x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1890x2048): 188.090
b: 64, m: 2048, n: 2048, k: 1890,
Elapsed time for attention_prob_times_values (64x2048x2048x1890): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1890): 140.098
Elapsed time for attention_linear_projection (4x30240x30240, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x30240x30240, b=2048): 259.698
Elapsed time for mlp_h_to_4h (4x30240x120960, b=2048): 0.4327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30240x120960, b=2048): 138.489
Elapsed time for mlp_4h_to_h (4x120960x30240, b=2048): 0.2310
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120960x30240, b=2048): 259.472

Attention duration (in seconds): 0.3919
Attention throughput (in TFLOP/s): 158.110
MLP duration (in seconds): 0.6637
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0556
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30272x90816, b=2048): 0.1705
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30272x90816, b=2048): 264.193
b: 64, m: 2048, n: 1892, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1892x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1892x2048): 197.104
b: 64, m: 2048, n: 2048, k: 1892,
Elapsed time for attention_prob_times_values (64x2048x2048x1892): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1892): 163.811
Elapsed time for attention_linear_projection (4x30272x30272, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_linear_projection (4x30272x30272, b=2048): 261.633
Elapsed time for mlp_h_to_4h (4x30272x121088, b=2048): 0.3277
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30272x121088, b=2048): 183.256
Elapsed time for mlp_4h_to_h (4x121088x30272, b=2048): 0.2258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121088x30272, b=2048): 265.963

Attention duration (in seconds): 0.2392
Attention throughput (in TFLOP/s): 259.532
MLP duration (in seconds): 0.5535
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7928
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30304x90912, b=2048): 0.3222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30304x90912, b=2048): 140.083
b: 64, m: 2048, n: 1894, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1894x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1894x2048): 188.679
b: 64, m: 2048, n: 2048, k: 1894,
Elapsed time for attention_prob_times_values (64x2048x2048x1894): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1894): 141.157
Elapsed time for attention_linear_projection (4x30304x30304, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x30304x30304, b=2048): 262.577
Elapsed time for mlp_h_to_4h (4x30304x121216, b=2048): 0.4361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30304x121216, b=2048): 138.010
Elapsed time for mlp_4h_to_h (4x121216x30304, b=2048): 0.2310
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121216x30304, b=2048): 260.518

Attention duration (in seconds): 0.3921
Attention throughput (in TFLOP/s): 158.671
MLP duration (in seconds): 0.6671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0592
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.2999
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 150.827
b: 64, m: 2048, n: 1896, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1896x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1896x2048): 194.657
b: 64, m: 2048, n: 2048, k: 1896,
Elapsed time for attention_prob_times_values (64x2048x2048x1896): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1896): 194.471
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 264.643
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.4108
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 146.816
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 267.563

Attention duration (in seconds): 0.3673
Attention throughput (in TFLOP/s): 169.726
MLP duration (in seconds): 0.6362
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30368x91104, b=2048): 0.3194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30368x91104, b=2048): 141.901
b: 64, m: 2048, n: 1898, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1898x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1898x2048): 188.502
b: 64, m: 2048, n: 2048, k: 1898,
Elapsed time for attention_prob_times_values (64x2048x2048x1898): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1898): 140.280
Elapsed time for attention_linear_projection (4x30368x30368, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_linear_projection (4x30368x30368, b=2048): 261.384
Elapsed time for mlp_h_to_4h (4x30368x121472, b=2048): 0.4369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30368x121472, b=2048): 138.342
Elapsed time for mlp_4h_to_h (4x121472x30368, b=2048): 0.2327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121472x30368, b=2048): 259.693

Attention duration (in seconds): 0.3899
Attention throughput (in TFLOP/s): 160.230
MLP duration (in seconds): 0.6696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0595
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30400x91200, b=2048): 0.1723
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30400x91200, b=2048): 263.610
b: 64, m: 2048, n: 1900, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1900x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1900x2048): 197.883
b: 64, m: 2048, n: 2048, k: 1900,
Elapsed time for attention_prob_times_values (64x2048x2048x1900): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1900): 165.362
Elapsed time for attention_linear_projection (4x30400x30400, b=2048): 0.0571
Throughput (in TFLOP/s) for attention_linear_projection (4x30400x30400, b=2048): 265.327
Elapsed time for mlp_h_to_4h (4x30400x121600, b=2048): 0.4093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30400x121600, b=2048): 147.958
Elapsed time for mlp_4h_to_h (4x121600x30400, b=2048): 0.2263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121600x30400, b=2048): 267.630

Attention duration (in seconds): 0.2407
Attention throughput (in TFLOP/s): 260.092
MLP duration (in seconds): 0.6356
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30432x91296, b=2048): 0.3216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30432x91296, b=2048): 141.556
b: 64, m: 2048, n: 1902, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1902x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1902x2048): 188.368
b: 64, m: 2048, n: 2048, k: 1902,
Elapsed time for attention_prob_times_values (64x2048x2048x1902): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1902): 140.272
Elapsed time for attention_linear_projection (4x30432x30432, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x30432x30432, b=2048): 263.058
Elapsed time for mlp_h_to_4h (4x30432x121728, b=2048): 0.4349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30432x121728, b=2048): 139.561
Elapsed time for mlp_4h_to_h (4x121728x30432, b=2048): 0.2675
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121728x30432, b=2048): 226.909

Attention duration (in seconds): 0.3920
Attention throughput (in TFLOP/s): 160.060
MLP duration (in seconds): 0.7024
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0943
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1739
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 262.259
b: 64, m: 2048, n: 1904, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1904x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1904x2048): 196.671
b: 64, m: 2048, n: 2048, k: 1904,
Elapsed time for attention_prob_times_values (64x2048x2048x1904): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1904): 197.741
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 265.138
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.3876
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 156.925
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 266.162

Attention duration (in seconds): 0.2416
Attention throughput (in TFLOP/s): 260.151
MLP duration (in seconds): 0.6161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8577
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30496x91488, b=2048): 0.3278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30496x91488, b=2048): 139.436
b: 64, m: 2048, n: 1906, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1906x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1906x2048): 189.381
b: 64, m: 2048, n: 2048, k: 1906,
Elapsed time for attention_prob_times_values (64x2048x2048x1906): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1906): 140.885
Elapsed time for attention_linear_projection (4x30496x30496, b=2048): 0.0586
Throughput (in TFLOP/s) for attention_linear_projection (4x30496x30496, b=2048): 260.130
Elapsed time for mlp_h_to_4h (4x30496x121984, b=2048): 0.4413
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30496x121984, b=2048): 138.122
Elapsed time for mlp_4h_to_h (4x121984x30496, b=2048): 0.2344
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121984x30496, b=2048): 260.070

Attention duration (in seconds): 0.3991
Attention throughput (in TFLOP/s): 157.854
MLP duration (in seconds): 0.6756
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0747
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30528x91584, b=2048): 0.1714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30528x91584, b=2048): 267.319
b: 64, m: 2048, n: 1908, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1908x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1908x2048): 199.066
b: 64, m: 2048, n: 2048, k: 1908,
Elapsed time for attention_prob_times_values (64x2048x2048x1908): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1908): 164.677
Elapsed time for attention_linear_projection (4x30528x30528, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_linear_projection (4x30528x30528, b=2048): 263.511
Elapsed time for mlp_h_to_4h (4x30528x122112, b=2048): 0.3896
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30528x122112, b=2048): 156.752
Elapsed time for mlp_4h_to_h (4x122112x30528, b=2048): 0.2295
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122112x30528, b=2048): 266.178

Attention duration (in seconds): 0.2407
Attention throughput (in TFLOP/s): 262.290
MLP duration (in seconds): 0.6191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8598
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30560x91680, b=2048): 0.3220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30560x91680, b=2048): 142.578
b: 64, m: 2048, n: 1910, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1910x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1910x2048): 189.318
b: 64, m: 2048, n: 2048, k: 1910,
Elapsed time for attention_prob_times_values (64x2048x2048x1910): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1910): 139.686
Elapsed time for attention_linear_projection (4x30560x30560, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30560x30560, b=2048): 262.282
Elapsed time for mlp_h_to_4h (4x30560x122240, b=2048): 0.4381
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30560x122240, b=2048): 139.720
Elapsed time for mlp_4h_to_h (4x122240x30560, b=2048): 0.2369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122240x30560, b=2048): 258.368

Attention duration (in seconds): 0.3931
Attention throughput (in TFLOP/s): 160.935
MLP duration (in seconds): 0.6749
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1722
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 267.200
b: 64, m: 2048, n: 1912, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1912x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1912x2048): 196.005
b: 64, m: 2048, n: 2048, k: 1912,
Elapsed time for attention_prob_times_values (64x2048x2048x1912): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1912): 195.453
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0580
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 264.201
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.3797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 161.523
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2320
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 264.373

Attention duration (in seconds): 0.2407
Attention throughput (in TFLOP/s): 263.362
MLP duration (in seconds): 0.6117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30624x91872, b=2048): 0.3358
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30624x91872, b=2048): 137.279
b: 64, m: 2048, n: 1914, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1914x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1914x2048): 190.470
b: 64, m: 2048, n: 2048, k: 1914,
Elapsed time for attention_prob_times_values (64x2048x2048x1914): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1914): 140.444
Elapsed time for attention_linear_projection (4x30624x30624, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_linear_projection (4x30624x30624, b=2048): 261.214
Elapsed time for mlp_h_to_4h (4x30624x122496, b=2048): 0.4454
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30624x122496, b=2048): 137.989
Elapsed time for mlp_4h_to_h (4x122496x30624, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122496x30624, b=2048): 262.571

Attention duration (in seconds): 0.4073
Attention throughput (in TFLOP/s): 155.938
MLP duration (in seconds): 0.6795
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30656x91968, b=2048): 0.2633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30656x91968, b=2048): 175.467
b: 64, m: 2048, n: 1916, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1916x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1916x2048): 197.503
b: 64, m: 2048, n: 2048, k: 1916,
Elapsed time for attention_prob_times_values (64x2048x2048x1916): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1916): 162.289
Elapsed time for attention_linear_projection (4x30656x30656, b=2048): 0.0584
Throughput (in TFLOP/s) for attention_linear_projection (4x30656x30656, b=2048): 263.432
Elapsed time for mlp_h_to_4h (4x30656x122624, b=2048): 0.4203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30656x122624, b=2048): 146.550
Elapsed time for mlp_4h_to_h (4x122624x30656, b=2048): 0.2305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122624x30656, b=2048): 267.212

Attention duration (in seconds): 0.3333
Attention throughput (in TFLOP/s): 190.989
MLP duration (in seconds): 0.6508
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30688x92064, b=2048): 0.3331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30688x92064, b=2048): 138.982
b: 64, m: 2048, n: 1918, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1918x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1918x2048): 188.807
b: 64, m: 2048, n: 2048, k: 1918,
Elapsed time for attention_prob_times_values (64x2048x2048x1918): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1918): 140.793
Elapsed time for attention_linear_projection (4x30688x30688, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30688x30688, b=2048): 259.855
Elapsed time for mlp_h_to_4h (4x30688x122752, b=2048): 0.4452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30688x122752, b=2048): 138.617
Elapsed time for mlp_4h_to_h (4x122752x30688, b=2048): 0.2372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122752x30688, b=2048): 260.238

Attention duration (in seconds): 0.4052
Attention throughput (in TFLOP/s): 157.397
MLP duration (in seconds): 0.6824
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1766
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 262.617
b: 64, m: 2048, n: 1920, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1920x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1920x2048): 198.488
b: 64, m: 2048, n: 2048, k: 1920,
Elapsed time for attention_prob_times_values (64x2048x2048x1920): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1920): 199.873
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 262.563
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.4097
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 150.962
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2324
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 266.114

Attention duration (in seconds): 0.2459
Attention throughput (in TFLOP/s): 259.934
MLP duration (in seconds): 0.6421
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8880
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30752x92256, b=2048): 0.3341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30752x92256, b=2048): 139.147
b: 64, m: 2048, n: 1922, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1922x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1922x2048): 183.785
b: 64, m: 2048, n: 2048, k: 1922,
Elapsed time for attention_prob_times_values (64x2048x2048x1922): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1922): 140.240
Elapsed time for attention_linear_projection (4x30752x30752, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x30752x30752, b=2048): 260.328
Elapsed time for mlp_h_to_4h (4x30752x123008, b=2048): 0.4448
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30752x123008, b=2048): 139.339
Elapsed time for mlp_4h_to_h (4x123008x30752, b=2048): 0.2374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123008x30752, b=2048): 261.066

Attention duration (in seconds): 0.4065
Attention throughput (in TFLOP/s): 157.524
MLP duration (in seconds): 0.6822
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0887
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30784x92352, b=2048): 0.1751
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30784x92352, b=2048): 265.951
b: 64, m: 2048, n: 1924, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1924x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1924x2048): 191.880
b: 64, m: 2048, n: 2048, k: 1924,
Elapsed time for attention_prob_times_values (64x2048x2048x1924): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1924): 162.380
Elapsed time for attention_linear_projection (4x30784x30784, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x30784x30784, b=2048): 263.053
Elapsed time for mlp_h_to_4h (4x30784x123136, b=2048): 0.2811
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30784x123136, b=2048): 220.935
Elapsed time for mlp_4h_to_h (4x123136x30784, b=2048): 0.2343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123136x30784, b=2048): 265.076

Attention duration (in seconds): 0.2459
Attention throughput (in TFLOP/s): 260.955
MLP duration (in seconds): 0.5154
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30816x92448, b=2048): 0.3351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30816x92448, b=2048): 139.288
b: 64, m: 2048, n: 1926, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1926x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1926x2048): 184.332
b: 64, m: 2048, n: 2048, k: 1926,
Elapsed time for attention_prob_times_values (64x2048x2048x1926): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1926): 138.490
Elapsed time for attention_linear_projection (4x30816x30816, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30816x30816, b=2048): 260.257
Elapsed time for mlp_h_to_4h (4x30816x123264, b=2048): 0.4502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30816x123264, b=2048): 138.244
Elapsed time for mlp_4h_to_h (4x123264x30816, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123264x30816, b=2048): 260.845

Attention duration (in seconds): 0.4080
Attention throughput (in TFLOP/s): 157.619
MLP duration (in seconds): 0.6888
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1948
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 240.056
b: 64, m: 2048, n: 1928, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1928x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1928x2048): 189.130
b: 64, m: 2048, n: 2048, k: 1928,
Elapsed time for attention_prob_times_values (64x2048x2048x1928): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1928): 194.336
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0586
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 265.988
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.3931
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 158.660
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 266.722

Attention duration (in seconds): 0.2643
Attention throughput (in TFLOP/s): 243.832
MLP duration (in seconds): 0.6269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30880x92640, b=2048): 0.3371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30880x92640, b=2048): 139.060
b: 64, m: 2048, n: 1930, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1930x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1930x2048): 184.636
b: 64, m: 2048, n: 2048, k: 1930,
Elapsed time for attention_prob_times_values (64x2048x2048x1930): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1930): 138.305
Elapsed time for attention_linear_projection (4x30880x30880, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x30880x30880, b=2048): 261.685
Elapsed time for mlp_h_to_4h (4x30880x123520, b=2048): 0.4467
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30880x123520, b=2048): 139.915
Elapsed time for mlp_4h_to_h (4x123520x30880, b=2048): 0.2394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123520x30880, b=2048): 261.065

Attention duration (in seconds): 0.4099
Attention throughput (in TFLOP/s): 157.532
MLP duration (in seconds): 0.6860
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0959
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30912x92736, b=2048): 0.2335
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30912x92736, b=2048): 201.171
b: 64, m: 2048, n: 1932, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1932x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1932x2048): 191.896
b: 64, m: 2048, n: 2048, k: 1932,
Elapsed time for attention_prob_times_values (64x2048x2048x1932): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1932): 163.632
Elapsed time for attention_linear_projection (4x30912x30912, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30912x30912, b=2048): 263.407
Elapsed time for mlp_h_to_4h (4x30912x123648, b=2048): 0.4207
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30912x123648, b=2048): 148.872
Elapsed time for mlp_4h_to_h (4x123648x30912, b=2048): 0.2351
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123648x30912, b=2048): 266.374

Attention duration (in seconds): 0.3046
Attention throughput (in TFLOP/s): 212.367
MLP duration (in seconds): 0.6557
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9604
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30944x92832, b=2048): 0.3383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30944x92832, b=2048): 139.110
b: 64, m: 2048, n: 1934, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1934x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1934x2048): 184.283
b: 64, m: 2048, n: 2048, k: 1934,
Elapsed time for attention_prob_times_values (64x2048x2048x1934): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1934): 138.711
Elapsed time for attention_linear_projection (4x30944x30944, b=2048): 0.0599
Throughput (in TFLOP/s) for attention_linear_projection (4x30944x30944, b=2048): 261.741
Elapsed time for mlp_h_to_4h (4x30944x123776, b=2048): 0.4589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30944x123776, b=2048): 136.742
Elapsed time for mlp_4h_to_h (4x123776x30944, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123776x30944, b=2048): 262.964

Attention duration (in seconds): 0.4114
Attention throughput (in TFLOP/s): 157.589
MLP duration (in seconds): 0.6976
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1761
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 267.814
b: 64, m: 2048, n: 1936, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1936x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1936x2048): 191.289
b: 64, m: 2048, n: 2048, k: 1936,
Elapsed time for attention_prob_times_values (64x2048x2048x1936): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1936): 197.548
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0593
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 264.918
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.4184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 150.286
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 265.116

Attention duration (in seconds): 0.2461
Attention throughput (in TFLOP/s): 263.925
MLP duration (in seconds): 0.6556
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31008x93024, b=2048): 0.3457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31008x93024, b=2048): 136.723
b: 64, m: 2048, n: 1938, k: 2048,
Elapsed time for attention_key_query_prob (64x2048x1938x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1938x2048): 185.056
b: 64, m: 2048, n: 2048, k: 1938,
Elapsed time for attention_prob_times_values (64x2048x2048x1938): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1938): 140.765
Elapsed time for attention_linear_projection (4x31008x31008, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_linear_projection (4x31008x31008, b=2048): 259.440
Elapsed time for mlp_h_to_4h (4x31008x124032, b=2048): 0.4565
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31008x124032, b=2048): 138.026
Elapsed time for mlp_4h_to_h (4x124032x31008, b=2048): 0.2417
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124032x31008, b=2048): 260.702

Attention duration (in seconds): 0.4194
Attention throughput (in TFLOP/s): 155.209
MLP duration (in seconds): 0.6982
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
