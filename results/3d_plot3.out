1.13.1 

num_attention_heads: 32, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1x2048): 0.922
Elapsed time for attention_prob_times_values (128x2048x2048x1): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1): 1.396

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 1.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x2x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x2x2048): 1.202
Elapsed time for attention_prob_times_values (128x2048x2048x2): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x2): 2.726

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 1.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x3x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x3x2048): 2.781
Elapsed time for attention_prob_times_values (128x2048x2048x3): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x3): 3.225

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 3.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x4x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x4x2048): 2.394
Elapsed time for attention_prob_times_values (128x2048x2048x4): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x4): 5.360

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 3.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x5x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x5x2048): 4.642
Elapsed time for attention_prob_times_values (128x2048x2048x5): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x5): 5.057

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 5.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x6x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x6x2048): 3.623
Elapsed time for attention_prob_times_values (128x2048x2048x6): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x6): 7.601

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 5.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x7x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x7x2048): 6.469
Elapsed time for attention_prob_times_values (128x2048x2048x7): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x7): 6.818

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 8.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x8x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x8x2048): 7.063
Elapsed time for attention_prob_times_values (128x2048x2048x8): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x8): 10.353

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 10.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x9x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x9x2048): 6.172
Elapsed time for attention_prob_times_values (128x2048x2048x9): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x9): 8.640

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 9.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x10x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x10x2048): 5.897
Elapsed time for attention_prob_times_values (128x2048x2048x10): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x10): 12.538

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 10.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x11x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x11x2048): 7.431
Elapsed time for attention_prob_times_values (128x2048x2048x11): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x11): 10.373

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 11.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x12x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x12x2048): 7.103
Elapsed time for attention_prob_times_values (128x2048x2048x12): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x12): 15.570

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 13.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x13x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x13x2048): 8.753
Elapsed time for attention_prob_times_values (128x2048x2048x13): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x13): 11.165

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 13.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x14x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x14x2048): 8.261
Elapsed time for attention_prob_times_values (128x2048x2048x14): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x14): 17.706

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 16.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x15x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x15x2048): 10.001
Elapsed time for attention_prob_times_values (128x2048x2048x15): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x15): 14.395

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 17.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x16x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x16x2048): 19.028
Elapsed time for attention_prob_times_values (128x2048x2048x16): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x16): 21.162

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 30.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x17x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x17x2048): 11.254
Elapsed time for attention_prob_times_values (128x2048x2048x17): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x17): 15.708

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 20.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x18x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x18x2048): 10.562
Elapsed time for attention_prob_times_values (128x2048x2048x18): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x18): 21.739

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 22.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x19x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x19x2048): 12.957
Elapsed time for attention_prob_times_values (128x2048x2048x19): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x19): 17.224

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 23.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x20x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x20x2048): 11.710
Elapsed time for attention_prob_times_values (128x2048x2048x20): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x20): 24.536

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 25.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x21x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x21x2048): 13.764
Elapsed time for attention_prob_times_values (128x2048x2048x21): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x21): 18.802

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 26.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x22x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x22x2048): 12.960
Elapsed time for attention_prob_times_values (128x2048x2048x22): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x22): 27.280

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 29.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x23x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x23x2048): 14.661
Elapsed time for attention_prob_times_values (128x2048x2048x23): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x23): 21.620

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 30.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x24x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x24x2048): 28.885
Elapsed time for attention_prob_times_values (128x2048x2048x24): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x24): 29.048

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 50.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x25x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x25x2048): 11.835
Elapsed time for attention_prob_times_values (128x2048x2048x25): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x25): 22.055

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 27.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x26x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x26x2048): 14.771
Elapsed time for attention_prob_times_values (128x2048x2048x26): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x26): 28.841

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 35.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x27x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x27x2048): 17.610
Elapsed time for attention_prob_times_values (128x2048x2048x27): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x27): 22.904

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 36.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x28x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x28x2048): 15.539
Elapsed time for attention_prob_times_values (128x2048x2048x28): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x28): 31.044

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 38.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x29x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x29x2048): 17.086
Elapsed time for attention_prob_times_values (128x2048x2048x29): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x29): 24.213

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 38.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x30x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x30x2048): 17.400
Elapsed time for attention_prob_times_values (128x2048x2048x30): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x30): 31.768

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 43.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x31x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x31x2048): 1.970
Elapsed time for attention_prob_times_values (128x2048x2048x31): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x31): 27.215

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 7.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x32x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x32x2048): 38.420
Elapsed time for attention_prob_times_values (128x2048x2048x32): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x32): 41.401

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 79.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x33x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x33x2048): 18.045
Elapsed time for attention_prob_times_values (128x2048x2048x33): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x33): 23.313

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 41.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x34x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x34x2048): 14.660
Elapsed time for attention_prob_times_values (128x2048x2048x34): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x34): 36.667

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 43.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x35x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x35x2048): 18.801
Elapsed time for attention_prob_times_values (128x2048x2048x35): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x35): 21.107

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 41.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x36x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x36x2048): 15.047
Elapsed time for attention_prob_times_values (128x2048x2048x36): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x36): 39.124

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 46.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x37x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x37x2048): 19.428
Elapsed time for attention_prob_times_values (128x2048x2048x37): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x37): 25.286

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 47.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x38x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x38x2048): 16.186
Elapsed time for attention_prob_times_values (128x2048x2048x38): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x38): 39.901

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 50.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x39x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x39x2048): 20.524
Elapsed time for attention_prob_times_values (128x2048x2048x39): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x39): 23.713

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 48.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x40x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x40x2048): 38.807
Elapsed time for attention_prob_times_values (128x2048x2048x40): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x40): 50.026

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 98.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x41x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x41x2048): 21.803
Elapsed time for attention_prob_times_values (128x2048x2048x41): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x41): 26.530

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 54.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x42x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x42x2048): 16.804
Elapsed time for attention_prob_times_values (128x2048x2048x42): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x42): 45.100

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 56.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x43x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x43x2048): 22.066
Elapsed time for attention_prob_times_values (128x2048x2048x43): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x43): 29.853

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 59.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x44x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x44x2048): 17.272
Elapsed time for attention_prob_times_values (128x2048x2048x44): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x44): 47.013

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 59.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x45x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x45x2048): 21.940
Elapsed time for attention_prob_times_values (128x2048x2048x45): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x45): 30.140

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 61.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x46x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x46x2048): 18.093
Elapsed time for attention_prob_times_values (128x2048x2048x46): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x46): 48.722

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 64.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x47x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x47x2048): 24.516
Elapsed time for attention_prob_times_values (128x2048x2048x47): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x47): 29.893

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 66.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x48x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x48x2048): 44.784
Elapsed time for attention_prob_times_values (128x2048x2048x48): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x48): 60.115

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 128.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x49x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x49x2048): 25.220
Elapsed time for attention_prob_times_values (128x2048x2048x49): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x49): 32.640

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 72.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x50x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x50x2048): 19.555
Elapsed time for attention_prob_times_values (128x2048x2048x50): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x50): 53.576

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 73.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x51x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x51x2048): 25.796
Elapsed time for attention_prob_times_values (128x2048x2048x51): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x51): 35.152

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 77.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x52x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x52x2048): 19.813
Elapsed time for attention_prob_times_values (128x2048x2048x52): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x52): 48.506

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 73.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x53x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x53x2048): 26.262
Elapsed time for attention_prob_times_values (128x2048x2048x53): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x53): 35.509

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 80.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x54x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x54x2048): 19.598
Elapsed time for attention_prob_times_values (128x2048x2048x54): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x54): 56.914

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 78.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x55x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x55x2048): 26.060
Elapsed time for attention_prob_times_values (128x2048x2048x55): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x55): 34.986

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 81.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x56x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x56x2048): 47.567
Elapsed time for attention_prob_times_values (128x2048x2048x56): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x56): 68.421

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 154.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x57x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x57x2048): 26.886
Elapsed time for attention_prob_times_values (128x2048x2048x57): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x57): 35.883

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 85.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x58x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x58x2048): 21.349
Elapsed time for attention_prob_times_values (128x2048x2048x58): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x58): 57.879

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 87.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x59x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x59x2048): 29.167
Elapsed time for attention_prob_times_values (128x2048x2048x59): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x59): 40.101

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 96.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x60x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x60x2048): 20.459
Elapsed time for attention_prob_times_values (128x2048x2048x60): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x60): 62.914

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 88.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x61x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x61x2048): 29.751
Elapsed time for attention_prob_times_values (128x2048x2048x61): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x61): 38.034

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 97.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x62x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x62x2048): 22.233
Elapsed time for attention_prob_times_values (128x2048x2048x62): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x62): 61.707

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 96.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x63x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x63x2048): 28.376
Elapsed time for attention_prob_times_values (128x2048x2048x63): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x63): 40.194

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 98.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x64x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x64x2048): 57.064
Elapsed time for attention_prob_times_values (128x2048x2048x64): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x64): 78.601

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 198.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x65x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x65x2048): 25.965
Elapsed time for attention_prob_times_values (128x2048x2048x65): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x65): 37.588

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 93.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x66x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x66x2048): 19.358
Elapsed time for attention_prob_times_values (128x2048x2048x66): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x66): 67.248

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 92.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x67x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x67x2048): 27.619
Elapsed time for attention_prob_times_values (128x2048x2048x67): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x67): 39.469

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 100.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x68x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x68x2048): 18.835
Elapsed time for attention_prob_times_values (128x2048x2048x68): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x68): 68.773

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 92.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x69x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x69x2048): 27.434
Elapsed time for attention_prob_times_values (128x2048x2048x69): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x69): 41.104

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 103.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x70x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x70x2048): 19.641
Elapsed time for attention_prob_times_values (128x2048x2048x70): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x70): 70.416

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 97.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x71x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x71x2048): 27.045
Elapsed time for attention_prob_times_values (128x2048x2048x71): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x71): 41.521

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 105.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x72x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x72x2048): 66.858
Elapsed time for attention_prob_times_values (128x2048x2048x72): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x72): 77.704

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 233.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x73x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x73x2048): 28.160
Elapsed time for attention_prob_times_values (128x2048x2048x73): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x73): 42.575

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 111.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x74x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x74x2048): 47.514
Elapsed time for attention_prob_times_values (128x2048x2048x74): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x74): 73.053

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 190.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x75x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x75x2048): 27.672
Elapsed time for attention_prob_times_values (128x2048x2048x75): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x75): 43.348

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 112.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x76x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x76x2048): 49.749
Elapsed time for attention_prob_times_values (128x2048x2048x76): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x76): 74.037

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 200.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x77x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x77x2048): 29.787
Elapsed time for attention_prob_times_values (128x2048x2048x77): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x77): 45.001

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 122.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x78x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x78x2048): 52.697
Elapsed time for attention_prob_times_values (128x2048x2048x78): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x78): 77.477

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 215.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x79x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x79x2048): 29.592
Elapsed time for attention_prob_times_values (128x2048x2048x79): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x79): 46.284

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 125.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x80x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x80x2048): 77.398
Elapsed time for attention_prob_times_values (128x2048x2048x80): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x80): 88.675

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 289.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x81x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x81x2048): 29.502
Elapsed time for attention_prob_times_values (128x2048x2048x81): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x81): 47.228

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 128.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x82x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x82x2048): 49.444
Elapsed time for attention_prob_times_values (128x2048x2048x82): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x82): 76.824

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 214.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x83x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x83x2048): 31.026
Elapsed time for attention_prob_times_values (128x2048x2048x83): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x83): 24.191

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 97.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x84x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x84x2048): 54.167
Elapsed time for attention_prob_times_values (128x2048x2048x84): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x84): 76.875

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 230.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x85x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x85x2048): 31.452
Elapsed time for attention_prob_times_values (128x2048x2048x85): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x85): 48.340

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 139.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x86x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x86x2048): 55.544
Elapsed time for attention_prob_times_values (128x2048x2048x86): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x86): 83.942

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 246.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x87x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x87x2048): 31.628
Elapsed time for attention_prob_times_values (128x2048x2048x87): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x87): 49.951

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 144.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x88x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x88x2048): 75.331
Elapsed time for attention_prob_times_values (128x2048x2048x88): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x88): 97.784

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 319.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x89x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x89x2048): 30.975
Elapsed time for attention_prob_times_values (128x2048x2048x89): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x89): 51.552

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 146.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x90x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x90x2048): 58.760
Elapsed time for attention_prob_times_values (128x2048x2048x90): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x90): 84.110

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 263.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x91x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x91x2048): 32.093
Elapsed time for attention_prob_times_values (128x2048x2048x91): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x91): 52.128

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 152.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x92x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x92x2048): 60.196
Elapsed time for attention_prob_times_values (128x2048x2048x92): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x92): 86.807

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 275.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x93x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x93x2048): 33.357
Elapsed time for attention_prob_times_values (128x2048x2048x93): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x93): 51.746

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 158.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x94x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x94x2048): 60.227
Elapsed time for attention_prob_times_values (128x2048x2048x94): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x94): 92.110

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 286.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x95x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x95x2048): 34.148
Elapsed time for attention_prob_times_values (128x2048x2048x95): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x95): 52.787

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 164.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x96x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x96x2048): 92.106
Elapsed time for attention_prob_times_values (128x2048x2048x96): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x96): 107.043

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 396.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x97x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x97x2048): 31.912
Elapsed time for attention_prob_times_values (128x2048x2048x97): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x97): 53.773

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 161.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x98x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x98x2048): 56.685
Elapsed time for attention_prob_times_values (128x2048x2048x98): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x98): 94.772

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 288.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x99x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x99x2048): 31.172
Elapsed time for attention_prob_times_values (128x2048x2048x99): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x99): 57.190

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 165.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x100x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x100x2048): 58.359
Elapsed time for attention_prob_times_values (128x2048x2048x100): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x100): 96.748

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 300.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x101x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x101x2048): 30.604
Elapsed time for attention_prob_times_values (128x2048x2048x101): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x101): 57.252

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 165.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x102x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x102x2048): 61.086
Elapsed time for attention_prob_times_values (128x2048x2048x102): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x102): 8.764

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 64.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x103x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x103x2048): 30.580
Elapsed time for attention_prob_times_values (128x2048x2048x103): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x103): 58.874

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 169.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x104x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x104x2048): 71.236
Elapsed time for attention_prob_times_values (128x2048x2048x104): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x104): 99.506

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 352.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x105x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x105x2048): 30.402
Elapsed time for attention_prob_times_values (128x2048x2048x105): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x105): 57.528

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 170.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x106x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x106x2048): 63.146
Elapsed time for attention_prob_times_values (128x2048x2048x106): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x106): 80.367

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 304.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x107x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x107x2048): 31.430
Elapsed time for attention_prob_times_values (128x2048x2048x107): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x107): 61.473

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 180.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x108x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x108x2048): 58.376
Elapsed time for attention_prob_times_values (128x2048x2048x108): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x108): 98.839

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 321.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x109x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x109x2048): 31.695
Elapsed time for attention_prob_times_values (128x2048x2048x109): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x109): 60.485

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 183.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x110x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x110x2048): 63.245
Elapsed time for attention_prob_times_values (128x2048x2048x110): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x110): 106.377

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 352.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x111x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x111x2048): 23.668
Elapsed time for attention_prob_times_values (128x2048x2048x111): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x111): 7.196

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 49.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x112x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x112x2048): 94.582
Elapsed time for attention_prob_times_values (128x2048x2048x112): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x112): 116.195

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 469.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x113x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x113x2048): 33.260
Elapsed time for attention_prob_times_values (128x2048x2048x113): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x113): 46.143

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 175.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x114x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x114x2048): 59.081
Elapsed time for attention_prob_times_values (128x2048x2048x114): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x114): 106.605

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 346.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x115x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x115x2048): 33.587
Elapsed time for attention_prob_times_values (128x2048x2048x115): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x115): 66.060

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 204.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x116x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x116x2048): 68.326
Elapsed time for attention_prob_times_values (128x2048x2048x116): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x116): 110.401

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 390.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x117x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x117x2048): 34.639
Elapsed time for attention_prob_times_values (128x2048x2048x117): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x117): 66.860

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 212.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x118x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x118x2048): 67.836
Elapsed time for attention_prob_times_values (128x2048x2048x118): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x118): 111.503

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 395.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x119x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x119x2048): 32.685
Elapsed time for attention_prob_times_values (128x2048x2048x119): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x119): 61.814

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 201.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x120x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x120x2048): 86.802
Elapsed time for attention_prob_times_values (128x2048x2048x120): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x120): 128.460

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 492.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x121x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x121x2048): 33.673
Elapsed time for attention_prob_times_values (128x2048x2048x121): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x121): 68.202

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 215.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x122x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x122x2048): 70.405
Elapsed time for attention_prob_times_values (128x2048x2048x122): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x122): 109.910

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 413.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x123x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x123x2048): 34.619
Elapsed time for attention_prob_times_values (128x2048x2048x123): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x123): 64.292

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 217.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x124x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x124x2048): 73.000
Elapsed time for attention_prob_times_values (128x2048x2048x124): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x124): 103.244

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 416.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x125x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x125x2048): 33.457
Elapsed time for attention_prob_times_values (128x2048x2048x125): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x125): 58.948

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 209.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x126x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x126x2048): 74.459
Elapsed time for attention_prob_times_values (128x2048x2048x126): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x126): 117.998

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 450.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x127x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x127x2048): 36.179
Elapsed time for attention_prob_times_values (128x2048x2048x127): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x127): 71.414

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 238.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x128x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x128x2048): 104.318
Elapsed time for attention_prob_times_values (128x2048x2048x128): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x128): 136.570

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 591.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x129x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x129x2048): 33.570
Elapsed time for attention_prob_times_values (128x2048x2048x129): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x129): 53.402

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 207.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x130x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x130x2048): 68.038
Elapsed time for attention_prob_times_values (128x2048x2048x130): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x130): 91.709

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 395.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x131x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x131x2048): 34.146
Elapsed time for attention_prob_times_values (128x2048x2048x131): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x131): 56.231

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 216.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x132x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x132x2048): 15.152
Elapsed time for attention_prob_times_values (128x2048x2048x132): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x132): 90.968

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 133.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x133x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x133x2048): 34.017
Elapsed time for attention_prob_times_values (128x2048x2048x133): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x133): 18.848

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 125.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x134x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x134x2048): 72.429
Elapsed time for attention_prob_times_values (128x2048x2048x134): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x134): 90.450

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 417.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x135x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x135x2048): 32.537
Elapsed time for attention_prob_times_values (128x2048x2048x135): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x135): 52.781

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 210.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x136x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x136x2048): 94.901
Elapsed time for attention_prob_times_values (128x2048x2048x136): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x136): 115.804

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 547.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x137x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x137x2048): 33.245
Elapsed time for attention_prob_times_values (128x2048x2048x137): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x137): 58.245

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 223.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x138x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x138x2048): 70.713
Elapsed time for attention_prob_times_values (128x2048x2048x138): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x138): 92.885

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 426.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x139x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x139x2048): 34.331
Elapsed time for attention_prob_times_values (128x2048x2048x139): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x139): 59.836

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 233.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x140x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x140x2048): 74.125
Elapsed time for attention_prob_times_values (128x2048x2048x140): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x140): 92.899

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 443.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x141x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x141x2048): 34.598
Elapsed time for attention_prob_times_values (128x2048x2048x141): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x141): 57.456

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 233.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x142x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x142x2048): 73.263
Elapsed time for attention_prob_times_values (128x2048x2048x142): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x142): 95.265

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 450.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x143x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x143x2048): 34.327
Elapsed time for attention_prob_times_values (128x2048x2048x143): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x143): 58.857

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 237.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x144x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x144x2048): 106.893
Elapsed time for attention_prob_times_values (128x2048x2048x144): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x144): 133.825

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 653.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x145x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x145x2048): 56.188
Elapsed time for attention_prob_times_values (128x2048x2048x145): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x145): 58.841

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 317.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x146x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x146x2048): 77.347
Elapsed time for attention_prob_times_values (128x2048x2048x146): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x146): 96.709

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 478.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x147x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x147x2048): 46.488
Elapsed time for attention_prob_times_values (128x2048x2048x147): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x147): 52.559

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 275.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x148x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x148x2048): 76.825
Elapsed time for attention_prob_times_values (128x2048x2048x148): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x148): 95.369

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 478.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x149x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x149x2048): 54.107
Elapsed time for attention_prob_times_values (128x2048x2048x149): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x149): 59.410

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 320.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x150x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x150x2048): 79.729
Elapsed time for attention_prob_times_values (128x2048x2048x150): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x150): 98.275

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 500.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x151x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x151x2048): 57.660
Elapsed time for attention_prob_times_values (128x2048x2048x151): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x151): 57.147

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 328.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x152x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x152x2048): 104.863
Elapsed time for attention_prob_times_values (128x2048x2048x152): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x152): 120.286

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 644.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x153x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x153x2048): 57.378
Elapsed time for attention_prob_times_values (128x2048x2048x153): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x153): 58.434

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 334.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x154x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x154x2048): 80.496
Elapsed time for attention_prob_times_values (128x2048x2048x154): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x154): 94.016

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 504.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x155x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x155x2048): 56.236
Elapsed time for attention_prob_times_values (128x2048x2048x155): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x155): 62.227

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 345.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x156x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x156x2048): 81.315
Elapsed time for attention_prob_times_values (128x2048x2048x156): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x156): 101.424

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 530.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x157x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x157x2048): 56.989
Elapsed time for attention_prob_times_values (128x2048x2048x157): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x157): 59.492

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 343.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x158x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x158x2048): 83.069
Elapsed time for attention_prob_times_values (128x2048x2048x158): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x158): 98.283

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 534.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x159x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x159x2048): 58.024
Elapsed time for attention_prob_times_values (128x2048x2048x159): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x159): 60.479

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 353.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x160x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x160x2048): 102.238
Elapsed time for attention_prob_times_values (128x2048x2048x160): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x160): 147.629

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 724.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x161x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x161x2048): 55.982
Elapsed time for attention_prob_times_values (128x2048x2048x161): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x161): 62.539

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 356.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x162x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x162x2048): 68.332
Elapsed time for attention_prob_times_values (128x2048x2048x162): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x162): 105.446

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 502.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x163x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x163x2048): 57.670
Elapsed time for attention_prob_times_values (128x2048x2048x163): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x163): 63.917

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 369.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x164x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x164x2048): 73.859
Elapsed time for attention_prob_times_values (128x2048x2048x164): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x164): 92.776

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 503.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x165x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x165x2048): 58.009
Elapsed time for attention_prob_times_values (128x2048x2048x165): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x165): 64.944

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 377.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x166x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x166x2048): 76.029
Elapsed time for attention_prob_times_values (128x2048x2048x166): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x166): 106.314

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 548.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x167x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x167x2048): 58.167
Elapsed time for attention_prob_times_values (128x2048x2048x167): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x167): 64.080

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 379.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x168x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x168x2048): 106.775
Elapsed time for attention_prob_times_values (128x2048x2048x168): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x168): 131.196

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 735.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x169x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x169x2048): 58.969
Elapsed time for attention_prob_times_values (128x2048x2048x169): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x169): 60.757

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 375.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x170x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x170x2048): 79.585
Elapsed time for attention_prob_times_values (128x2048x2048x170): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x170): 107.304

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 576.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x171x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x171x2048): 59.422
Elapsed time for attention_prob_times_values (128x2048x2048x171): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x171): 60.941

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 381.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x172x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x172x2048): 83.562
Elapsed time for attention_prob_times_values (128x2048x2048x172): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x172): 110.565

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 606.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x173x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x173x2048): 59.850
Elapsed time for attention_prob_times_values (128x2048x2048x173): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x173): 68.016

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 407.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x174x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x174x2048): 84.053
Elapsed time for attention_prob_times_values (128x2048x2048x174): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x174): 109.983

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 613.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x175x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x175x2048): 58.719
Elapsed time for attention_prob_times_values (128x2048x2048x175): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x175): 64.295

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 397.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x176x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x176x2048): 123.791
Elapsed time for attention_prob_times_values (128x2048x2048x176): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x176): 159.419

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 905.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x177x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x177x2048): 60.925
Elapsed time for attention_prob_times_values (128x2048x2048x177): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x177): 67.417

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 418.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x178x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x178x2048): 63.072
Elapsed time for attention_prob_times_values (128x2048x2048x178): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x178): 113.547

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 532.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x179x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x179x2048): 60.791
Elapsed time for attention_prob_times_values (128x2048x2048x179): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x179): 66.607

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 419.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x180x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x180x2048): 82.982
Elapsed time for attention_prob_times_values (128x2048x2048x180): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x180): 109.488

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 625.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x181x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x181x2048): 59.600
Elapsed time for attention_prob_times_values (128x2048x2048x181): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x181): 67.946

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 422.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x182x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x182x2048): 84.570
Elapsed time for attention_prob_times_values (128x2048x2048x182): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x182): 115.396

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 652.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x183x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x183x2048): 58.035
Elapsed time for attention_prob_times_values (128x2048x2048x183): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x183): 69.106

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 423.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x184x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x184x2048): 109.771
Elapsed time for attention_prob_times_values (128x2048x2048x184): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x184): 163.735

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 887.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x185x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x185x2048): 61.267
Elapsed time for attention_prob_times_values (128x2048x2048x185): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x185): 66.504

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 432.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x186x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x186x2048): 87.148
Elapsed time for attention_prob_times_values (128x2048x2048x186): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x186): 100.068

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 634.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x187x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x187x2048): 60.341
Elapsed time for attention_prob_times_values (128x2048x2048x187): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x187): 70.445

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 444.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x188x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x188x2048): 86.228
Elapsed time for attention_prob_times_values (128x2048x2048x188): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x188): 114.000

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 675.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x189x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x189x2048): 62.864
Elapsed time for attention_prob_times_values (128x2048x2048x189): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x189): 73.144

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 466.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x190x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x190x2048): 90.281
Elapsed time for attention_prob_times_values (128x2048x2048x190): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x190): 112.991

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 696.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x191x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x191x2048): 60.419
Elapsed time for attention_prob_times_values (128x2048x2048x191): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x191): 51.065

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 385.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x192x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x192x2048): 135.256
Elapsed time for attention_prob_times_values (128x2048x2048x192): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x192): 163.272

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 1035.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x193x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x193x2048): 59.416
Elapsed time for attention_prob_times_values (128x2048x2048x193): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x193): 70.655

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 453.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x194x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x194x2048): 81.373
Elapsed time for attention_prob_times_values (128x2048x2048x194): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x194): 116.711

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 677.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x195x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x195x2048): 58.414
Elapsed time for attention_prob_times_values (128x2048x2048x195): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x195): 69.440

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 450.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x196x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x196x2048): 84.957
Elapsed time for attention_prob_times_values (128x2048x2048x196): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x196): 114.966

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 696.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x197x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x197x2048): 61.246
Elapsed time for attention_prob_times_values (128x2048x2048x197): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x197): 74.593

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 481.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x198x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x198x2048): 88.745
Elapsed time for attention_prob_times_values (128x2048x2048x198): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x198): 112.932

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 714.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x199x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x199x2048): 60.604
Elapsed time for attention_prob_times_values (128x2048x2048x199): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x199): 70.275

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 469.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x200x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x200x2048): 115.729
Elapsed time for attention_prob_times_values (128x2048x2048x200): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x200): 172.618

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 1004.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x201x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x201x2048): 61.325
Elapsed time for attention_prob_times_values (128x2048x2048x201): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x201): 71.294

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 480.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x202x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x202x2048): 86.140
Elapsed time for attention_prob_times_values (128x2048x2048x202): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x202): 113.305

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 715.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x203x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x203x2048): 62.752
Elapsed time for attention_prob_times_values (128x2048x2048x203): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x203): 75.307

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 502.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x204x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x204x2048): 88.493
Elapsed time for attention_prob_times_values (128x2048x2048x204): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x204): 115.245

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 738.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x205x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x205x2048): 63.650
Elapsed time for attention_prob_times_values (128x2048x2048x205): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x205): 73.682

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 505.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x206x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x206x2048): 86.892
Elapsed time for attention_prob_times_values (128x2048x2048x206): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x206): 118.698

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 746.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x207x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x207x2048): 62.013
Elapsed time for attention_prob_times_values (128x2048x2048x207): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x207): 75.199

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 507.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x208x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x208x2048): 135.212
Elapsed time for attention_prob_times_values (128x2048x2048x208): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x208): 182.141

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 1164.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x209x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x209x2048): 64.087
Elapsed time for attention_prob_times_values (128x2048x2048x209): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x209): 75.138

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 520.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x210x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x210x2048): 91.661
Elapsed time for attention_prob_times_values (128x2048x2048x210): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x210): 117.939

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 780.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x211x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x211x2048): 63.056
Elapsed time for attention_prob_times_values (128x2048x2048x211): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x211): 45.369

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 400.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x212x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x212x2048): 91.637
Elapsed time for attention_prob_times_values (128x2048x2048x212): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x212): 122.689

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 799.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x213x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x213x2048): 64.785
Elapsed time for attention_prob_times_values (128x2048x2048x213): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x213): 76.247

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 536.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x214x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x214x2048): 91.718
Elapsed time for attention_prob_times_values (128x2048x2048x214): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x214): 123.719

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 809.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x215x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x215x2048): 63.327
Elapsed time for attention_prob_times_values (128x2048x2048x215): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x215): 75.694

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 532.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x216x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x216x2048): 118.242
Elapsed time for attention_prob_times_values (128x2048x2048x216): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x216): 178.557

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 1102.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x217x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x217x2048): 64.156
Elapsed time for attention_prob_times_values (128x2048x2048x217): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x217): 77.073

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 544.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x218x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x218x2048): 95.924
Elapsed time for attention_prob_times_values (128x2048x2048x218): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x218): 126.096

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 851.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x219x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x219x2048): 65.613
Elapsed time for attention_prob_times_values (128x2048x2048x219): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x219): 75.283

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 549.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x220x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x220x2048): 96.409
Elapsed time for attention_prob_times_values (128x2048x2048x220): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x220): 122.729

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 850.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x221x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x221x2048): 67.005
Elapsed time for attention_prob_times_values (128x2048x2048x221): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x221): 79.886

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 576.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x222x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x222x2048): 97.895
Elapsed time for attention_prob_times_values (128x2048x2048x222): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x222): 128.674

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 882.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x223x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x223x2048): 67.029
Elapsed time for attention_prob_times_values (128x2048x2048x223): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x223): 79.999

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 581.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x224x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x224x2048): 147.810
Elapsed time for attention_prob_times_values (128x2048x2048x224): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x224): 182.424

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 1306.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x225x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x225x2048): 66.208
Elapsed time for attention_prob_times_values (128x2048x2048x225): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x225): 78.357

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 576.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x226x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x226x2048): 89.905
Elapsed time for attention_prob_times_values (128x2048x2048x226): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x226): 131.162

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 860.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x227x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x227x2048): 64.683
Elapsed time for attention_prob_times_values (128x2048x2048x227): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x227): 82.638

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 587.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x228x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x228x2048): 82.310
Elapsed time for attention_prob_times_values (128x2048x2048x228): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x228): 109.680

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 764.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x229x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x229x2048): 65.891
Elapsed time for attention_prob_times_values (128x2048x2048x229): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x229): 78.847

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 585.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x230x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x230x2048): 91.886
Elapsed time for attention_prob_times_values (128x2048x2048x230): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x230): 122.034

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 858.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x231x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x231x2048): 66.335
Elapsed time for attention_prob_times_values (128x2048x2048x231): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x231): 79.017

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 592.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x232x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x232x2048): 125.551
Elapsed time for attention_prob_times_values (128x2048x2048x232): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x232): 199.358

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1271.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x233x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x233x2048): 63.968
Elapsed time for attention_prob_times_values (128x2048x2048x233): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x233): 82.172

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 595.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x234x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x234x2048): 93.650
Elapsed time for attention_prob_times_values (128x2048x2048x234): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x234): 134.952

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 919.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x235x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x235x2048): 67.570
Elapsed time for attention_prob_times_values (128x2048x2048x235): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x235): 84.769

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 627.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x236x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x236x2048): 97.090
Elapsed time for attention_prob_times_values (128x2048x2048x236): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x236): 133.406

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 941.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x237x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x237x2048): 68.280
Elapsed time for attention_prob_times_values (128x2048x2048x237): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x237): 83.958

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 633.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x238x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x238x2048): 94.329
Elapsed time for attention_prob_times_values (128x2048x2048x238): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x238): 136.682

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 941.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x239x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x239x2048): 66.463
Elapsed time for attention_prob_times_values (128x2048x2048x239): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x239): 85.568

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 633.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x240x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x240x2048): 143.981
Elapsed time for attention_prob_times_values (128x2048x2048x240): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x240): 182.363

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1367.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x241x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x241x2048): 65.960
Elapsed time for attention_prob_times_values (128x2048x2048x241): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x241): 85.624

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 635.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x242x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x242x2048): 97.119
Elapsed time for attention_prob_times_values (128x2048x2048x242): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x242): 131.357

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 956.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x243x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x243x2048): 58.014
Elapsed time for attention_prob_times_values (128x2048x2048x243): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x243): 89.162

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 604.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x244x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x244x2048): 98.290
Elapsed time for attention_prob_times_values (128x2048x2048x244): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x244): 134.964

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 981.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x245x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x245x2048): 68.892
Elapsed time for attention_prob_times_values (128x2048x2048x245): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x245): 86.303

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 663.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x246x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x246x2048): 100.407
Elapsed time for attention_prob_times_values (128x2048x2048x246): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x246): 139.832

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 1015.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x247x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x247x2048): 68.607
Elapsed time for attention_prob_times_values (128x2048x2048x247): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x247): 89.270

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 676.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x248x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x248x2048): 126.445
Elapsed time for attention_prob_times_values (128x2048x2048x248): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x248): 204.073

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 1366.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x249x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x249x2048): 67.538
Elapsed time for attention_prob_times_values (128x2048x2048x249): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x249): 63.937

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 576.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x250x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x250x2048): 102.504
Elapsed time for attention_prob_times_values (128x2048x2048x250): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x250): 138.846

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 1039.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x251x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x251x2048): 70.203
Elapsed time for attention_prob_times_values (128x2048x2048x251): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x251): 86.251

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 684.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x252x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x252x2048): 103.787
Elapsed time for attention_prob_times_values (128x2048x2048x252): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x252): 142.130

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 1064.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x253x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x253x2048): 67.842
Elapsed time for attention_prob_times_values (128x2048x2048x253): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x253): 91.284

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 693.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x254x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x254x2048): 103.223
Elapsed time for attention_prob_times_values (128x2048x2048x254): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x254): 149.904

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 1092.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x255x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x255x2048): 71.202
Elapsed time for attention_prob_times_values (128x2048x2048x255): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x255): 93.779

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 725.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x256x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x256x2048): 151.124
Elapsed time for attention_prob_times_values (128x2048x2048x256): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x256): 219.020

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 1609.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x257x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x257x2048): 70.207
Elapsed time for attention_prob_times_values (128x2048x2048x257): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x257): 54.344

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 553.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x258x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x258x2048): 101.736
Elapsed time for attention_prob_times_values (128x2048x2048x258): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x258): 94.496

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 887.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x259x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x259x2048): 69.024
Elapsed time for attention_prob_times_values (128x2048x2048x259): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x259): 55.065

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 557.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x260x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x260x2048): 101.679
Elapsed time for attention_prob_times_values (128x2048x2048x260): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x260): 94.959

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 896.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x261x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x261x2048): 69.026
Elapsed time for attention_prob_times_values (128x2048x2048x261): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x261): 55.380

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 562.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x262x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x262x2048): 101.326
Elapsed time for attention_prob_times_values (128x2048x2048x262): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x262): 95.026

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 901.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x263x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x263x2048): 67.544
Elapsed time for attention_prob_times_values (128x2048x2048x263): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x263): 54.037

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 553.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x264x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x264x2048): 134.970
Elapsed time for attention_prob_times_values (128x2048x2048x264): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x264): 131.158

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 1230.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x265x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x265x2048): 69.789
Elapsed time for attention_prob_times_values (128x2048x2048x265): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x265): 55.437

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 573.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x266x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x266x2048): 101.815
Elapsed time for attention_prob_times_values (128x2048x2048x266): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x266): 94.112

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 910.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x267x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x267x2048): 69.290
Elapsed time for attention_prob_times_values (128x2048x2048x267): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x267): 56.179

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 579.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x268x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x268x2048): 102.433
Elapsed time for attention_prob_times_values (128x2048x2048x268): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x268): 96.403

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 931.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x269x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x269x2048): 69.231
Elapsed time for attention_prob_times_values (128x2048x2048x269): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x269): 57.470

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 590.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x270x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x270x2048): 101.551
Elapsed time for attention_prob_times_values (128x2048x2048x270): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x270): 97.029

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 936.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x271x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x271x2048): 68.829
Elapsed time for attention_prob_times_values (128x2048x2048x271): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x271): 57.130

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 591.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x272x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x272x2048): 150.934
Elapsed time for attention_prob_times_values (128x2048x2048x272): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x272): 134.776

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 1352.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x273x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x273x2048): 71.200
Elapsed time for attention_prob_times_values (128x2048x2048x273): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x273): 56.802

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 602.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x274x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x274x2048): 104.248
Elapsed time for attention_prob_times_values (128x2048x2048x274): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x274): 96.791

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 959.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x275x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x275x2048): 70.038
Elapsed time for attention_prob_times_values (128x2048x2048x275): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x275): 56.216

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 598.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x276x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x276x2048): 100.941
Elapsed time for attention_prob_times_values (128x2048x2048x276): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x276): 95.578

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 945.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x277x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x277x2048): 69.413
Elapsed time for attention_prob_times_values (128x2048x2048x277): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x277): 57.848

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 609.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x278x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x278x2048): 104.065
Elapsed time for attention_prob_times_values (128x2048x2048x278): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x278): 91.628

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 944.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x279x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x279x2048): 66.464
Elapsed time for attention_prob_times_values (128x2048x2048x279): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x279): 56.536

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 593.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x280x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x280x2048): 135.636
Elapsed time for attention_prob_times_values (128x2048x2048x280): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x280): 129.215

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 1290.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x281x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x281x2048): 72.220
Elapsed time for attention_prob_times_values (128x2048x2048x281): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x281): 56.777

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 621.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x282x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x282x2048): 107.392
Elapsed time for attention_prob_times_values (128x2048x2048x282): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x282): 97.356

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1002.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x283x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x283x2048): 70.345
Elapsed time for attention_prob_times_values (128x2048x2048x283): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x283): 58.336

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 627.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x284x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x284x2048): 107.274
Elapsed time for attention_prob_times_values (128x2048x2048x284): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x284): 94.623

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 992.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x285x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x285x2048): 73.652
Elapsed time for attention_prob_times_values (128x2048x2048x285): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x285): 58.443

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 645.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x286x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x286x2048): 106.282
Elapsed time for attention_prob_times_values (128x2048x2048x286): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x286): 96.597

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1005.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x287x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x287x2048): 60.620
Elapsed time for attention_prob_times_values (128x2048x2048x287): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x287): 58.220

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 592.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x288x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x288x2048): 162.862
Elapsed time for attention_prob_times_values (128x2048x2048x288): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x288): 143.939

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 1528.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x289x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x289x2048): 72.139
Elapsed time for attention_prob_times_values (128x2048x2048x289): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x289): 14.564

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 243.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x290x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x290x2048): 100.962
Elapsed time for attention_prob_times_values (128x2048x2048x290): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x290): 100.627

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 1014.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x291x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x291x2048): 70.090
Elapsed time for attention_prob_times_values (128x2048x2048x291): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x291): 58.711

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 644.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x292x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x292x2048): 100.793
Elapsed time for attention_prob_times_values (128x2048x2048x292): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x292): 98.906

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1010.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x293x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x293x2048): 66.064
Elapsed time for attention_prob_times_values (128x2048x2048x293): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x293): 55.274

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 611.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x294x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x294x2048): 102.824
Elapsed time for attention_prob_times_values (128x2048x2048x294): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x294): 98.869

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1026.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x295x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x295x2048): 70.294
Elapsed time for attention_prob_times_values (128x2048x2048x295): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x295): 58.713

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 653.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x296x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x296x2048): 135.186
Elapsed time for attention_prob_times_values (128x2048x2048x296): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x296): 141.559

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 1417.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x297x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x297x2048): 70.906
Elapsed time for attention_prob_times_values (128x2048x2048x297): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x297): 58.647

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 660.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x298x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x298x2048): 103.014
Elapsed time for attention_prob_times_values (128x2048x2048x298): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x298): 100.620

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1049.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x299x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x299x2048): 68.465
Elapsed time for attention_prob_times_values (128x2048x2048x299): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x299): 60.303

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 663.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x300x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x300x2048): 106.175
Elapsed time for attention_prob_times_values (128x2048x2048x300): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x300): 101.098

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 1074.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x301x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x301x2048): 18.001
Elapsed time for attention_prob_times_values (128x2048x2048x301): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x301): 61.545

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 289.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x302x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x302x2048): 105.205
Elapsed time for attention_prob_times_values (128x2048x2048x302): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x302): 103.123

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 1087.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x303x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x303x2048): 69.775
Elapsed time for attention_prob_times_values (128x2048x2048x303): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x303): 57.223

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 658.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x304x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x304x2048): 154.299
Elapsed time for attention_prob_times_values (128x2048x2048x304): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x304): 19.389

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 361.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x305x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x305x2048): 69.244
Elapsed time for attention_prob_times_values (128x2048x2048x305): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x305): 60.490

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 680.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x306x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x306x2048): 105.102
Elapsed time for attention_prob_times_values (128x2048x2048x306): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x306): 101.931

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1093.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x307x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x307x2048): 70.119
Elapsed time for attention_prob_times_values (128x2048x2048x307): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x307): 61.611

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 694.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x308x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x308x2048): 106.610
Elapsed time for attention_prob_times_values (128x2048x2048x308): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x308): 102.309

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1109.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x309x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x309x2048): 68.569
Elapsed time for attention_prob_times_values (128x2048x2048x309): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x309): 62.029

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 694.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x310x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x310x2048): 107.625
Elapsed time for attention_prob_times_values (128x2048x2048x310): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x310): 92.317

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 1062.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x311x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x311x2048): 68.806
Elapsed time for attention_prob_times_values (128x2048x2048x311): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x311): 60.971

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 692.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x312x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x312x2048): 134.013
Elapsed time for attention_prob_times_values (128x2048x2048x312): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x312): 150.587

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 1524.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x313x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x313x2048): 68.792
Elapsed time for attention_prob_times_values (128x2048x2048x313): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x313): 61.129

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 697.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x314x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x314x2048): 109.887
Elapsed time for attention_prob_times_values (128x2048x2048x314): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x314): 107.123

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 1173.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x315x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x315x2048): 69.763
Elapsed time for attention_prob_times_values (128x2048x2048x315): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x315): 62.655

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 715.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x316x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x316x2048): 108.092
Elapsed time for attention_prob_times_values (128x2048x2048x316): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x316): 108.902

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1179.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x317x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x317x2048): 70.414
Elapsed time for attention_prob_times_values (128x2048x2048x317): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x317): 62.751

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 723.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x318x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x318x2048): 112.228
Elapsed time for attention_prob_times_values (128x2048x2048x318): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x318): 40.765

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 654.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x319x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x319x2048): 71.714
Elapsed time for attention_prob_times_values (128x2048x2048x319): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x319): 63.451

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 738.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x320x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x320x2048): 167.634
Elapsed time for attention_prob_times_values (128x2048x2048x320): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x320): 153.005

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 1759.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x321x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x321x2048): 19.929
Elapsed time for attention_prob_times_values (128x2048x2048x321): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x321): 61.981

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 332.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x322x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x322x2048): 107.443
Elapsed time for attention_prob_times_values (128x2048x2048x322): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x322): 109.496

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 1199.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x323x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x323x2048): 67.927
Elapsed time for attention_prob_times_values (128x2048x2048x323): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x323): 63.625

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 728.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x324x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x324x2048): 96.634
Elapsed time for attention_prob_times_values (128x2048x2048x324): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x324): 102.161

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 1104.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x325x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x325x2048): 69.908
Elapsed time for attention_prob_times_values (128x2048x2048x325): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x325): 63.404

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 741.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x326x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x326x2048): 106.443
Elapsed time for attention_prob_times_values (128x2048x2048x326): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x326): 106.606

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1191.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x327x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x327x2048): 70.642
Elapsed time for attention_prob_times_values (128x2048x2048x327): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x327): 62.534

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 744.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x328x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x328x2048): 139.370
Elapsed time for attention_prob_times_values (128x2048x2048x328): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x328): 153.537

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 1643.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x329x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x329x2048): 69.950
Elapsed time for attention_prob_times_values (128x2048x2048x329): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x329): 63.005

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 747.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x330x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x330x2048): 109.496
Elapsed time for attention_prob_times_values (128x2048x2048x330): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x330): 110.415

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 1243.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x331x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x331x2048): 70.995
Elapsed time for attention_prob_times_values (128x2048x2048x331): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x331): 65.872

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 775.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x332x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x332x2048): 106.762
Elapsed time for attention_prob_times_values (128x2048x2048x332): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x332): 110.111

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1233.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x333x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x333x2048): 68.295
Elapsed time for attention_prob_times_values (128x2048x2048x333): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x333): 65.398

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 762.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x334x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x334x2048): 110.433
Elapsed time for attention_prob_times_values (128x2048x2048x334): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x334): 109.868

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 1259.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x335x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x335x2048): 73.682
Elapsed time for attention_prob_times_values (128x2048x2048x335): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x335): 64.370

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 788.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x336x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x336x2048): 150.403
Elapsed time for attention_prob_times_values (128x2048x2048x336): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x336): 166.580

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 1817.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x337x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x337x2048): 73.160
Elapsed time for attention_prob_times_values (128x2048x2048x337): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x337): 66.305

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 802.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x338x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x338x2048): 111.788
Elapsed time for attention_prob_times_values (128x2048x2048x338): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x338): 108.335

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1272.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x339x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x339x2048): 73.340
Elapsed time for attention_prob_times_values (128x2048x2048x339): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x339): 67.749

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 816.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x340x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x340x2048): 113.189
Elapsed time for attention_prob_times_values (128x2048x2048x340): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x340): 109.726

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1295.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x341x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x341x2048): 74.022
Elapsed time for attention_prob_times_values (128x2048x2048x341): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x341): 67.883

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 825.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x342x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x342x2048): 111.248
Elapsed time for attention_prob_times_values (128x2048x2048x342): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x342): 110.466

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1295.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x343x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x343x2048): 73.946
Elapsed time for attention_prob_times_values (128x2048x2048x343): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x343): 66.944

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 823.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x344x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x344x2048): 179.227
Elapsed time for attention_prob_times_values (128x2048x2048x344): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x344): 160.360

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 1988.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x345x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x345x2048): 75.527
Elapsed time for attention_prob_times_values (128x2048x2048x345): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x345): 65.617

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 827.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x346x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x346x2048): 110.805
Elapsed time for attention_prob_times_values (128x2048x2048x346): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x346): 114.166

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1328.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x347x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x347x2048): 73.607
Elapsed time for attention_prob_times_values (128x2048x2048x347): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x347): 69.425

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 846.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x348x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x348x2048): 113.990
Elapsed time for attention_prob_times_values (128x2048x2048x348): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x348): 113.651

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1351.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x349x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x349x2048): 73.937
Elapsed time for attention_prob_times_values (128x2048x2048x349): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x349): 69.201

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 851.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x350x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x350x2048): 116.012
Elapsed time for attention_prob_times_values (128x2048x2048x350): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x350): 111.863

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1359.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x351x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x351x2048): 76.027
Elapsed time for attention_prob_times_values (128x2048x2048x351): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x351): 69.020

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 865.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x352x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x352x2048): 196.002
Elapsed time for attention_prob_times_values (128x2048x2048x352): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x352): 166.958

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 2163.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x353x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x353x2048): 72.659
Elapsed time for attention_prob_times_values (128x2048x2048x353): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x353): 67.601

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 842.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x354x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x354x2048): 110.760
Elapsed time for attention_prob_times_values (128x2048x2048x354): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x354): 114.952

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 1360.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x355x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x355x2048): 71.643
Elapsed time for attention_prob_times_values (128x2048x2048x355): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x355): 70.048

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 856.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x356x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x356x2048): 107.236
Elapsed time for attention_prob_times_values (128x2048x2048x356): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x356): 117.758

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 1361.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x357x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x357x2048): 72.008
Elapsed time for attention_prob_times_values (128x2048x2048x357): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x357): 69.173

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 857.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x358x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x358x2048): 108.938
Elapsed time for attention_prob_times_values (128x2048x2048x358): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x358): 116.151

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 1370.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x359x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x359x2048): 71.271
Elapsed time for attention_prob_times_values (128x2048x2048x359): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x359): 67.279

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 845.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x360x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x360x2048): 171.747
Elapsed time for attention_prob_times_values (128x2048x2048x360): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x360): 174.408

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 2120.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x361x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x361x2048): 70.804
Elapsed time for attention_prob_times_values (128x2048x2048x361): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x361): 68.328

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 854.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x362x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x362x2048): 110.807
Elapsed time for attention_prob_times_values (128x2048x2048x362): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x362): 116.201

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1396.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x363x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x363x2048): 71.454
Elapsed time for attention_prob_times_values (128x2048x2048x363): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x363): 70.795

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 877.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x364x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x364x2048): 108.070
Elapsed time for attention_prob_times_values (128x2048x2048x364): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x364): 120.653

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1410.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x365x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x365x2048): 62.806
Elapsed time for attention_prob_times_values (128x2048x2048x365): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x365): 69.494

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 818.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x366x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x366x2048): 109.939
Elapsed time for attention_prob_times_values (128x2048x2048x366): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x366): 117.897

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1415.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x367x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x367x2048): 73.718
Elapsed time for attention_prob_times_values (128x2048x2048x367): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x367): 70.195

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 896.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x368x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x368x2048): 165.204
Elapsed time for attention_prob_times_values (128x2048x2048x368): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x368): 174.363

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2120.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x369x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x369x2048): 72.878
Elapsed time for attention_prob_times_values (128x2048x2048x369): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x369): 70.867

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 900.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x370x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x370x2048): 100.237
Elapsed time for attention_prob_times_values (128x2048x2048x370): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x370): 120.977

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1377.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x371x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x371x2048): 69.171
Elapsed time for attention_prob_times_values (128x2048x2048x371): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x371): 71.648

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 886.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x372x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x372x2048): 108.003
Elapsed time for attention_prob_times_values (128x2048x2048x372): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x372): 119.173

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1430.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x373x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x373x2048): 73.719
Elapsed time for attention_prob_times_values (128x2048x2048x373): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x373): 72.954

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 928.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x374x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x374x2048): 111.494
Elapsed time for attention_prob_times_values (128x2048x2048x374): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x374): 121.693

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1476.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x375x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x375x2048): 74.497
Elapsed time for attention_prob_times_values (128x2048x2048x375): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x375): 71.166

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 925.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x376x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x376x2048): 175.550
Elapsed time for attention_prob_times_values (128x2048x2048x376): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x376): 174.519

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 2231.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x377x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x377x2048): 73.215
Elapsed time for attention_prob_times_values (128x2048x2048x377): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x377): 73.240

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 935.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x378x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x378x2048): 112.367
Elapsed time for attention_prob_times_values (128x2048x2048x378): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x378): 124.697

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1514.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x379x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x379x2048): 75.565
Elapsed time for attention_prob_times_values (128x2048x2048x379): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x379): 73.845

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 959.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x380x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x380x2048): 112.184
Elapsed time for attention_prob_times_values (128x2048x2048x380): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x380): 126.956

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1533.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x381x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x381x2048): 74.600
Elapsed time for attention_prob_times_values (128x2048x2048x381): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x381): 76.088

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 972.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x382x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x382x2048): 115.617
Elapsed time for attention_prob_times_values (128x2048x2048x382): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x382): 125.009

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 1554.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x383x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x383x2048): 77.107
Elapsed time for attention_prob_times_values (128x2048x2048x383): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x383): 73.612

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 976.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x384x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x384x2048): 190.293
Elapsed time for attention_prob_times_values (128x2048x2048x384): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x384): 186.698

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 2450.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x385x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x385x2048): 74.515
Elapsed time for attention_prob_times_values (128x2048x2048x385): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x385): 68.890

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 932.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x386x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x386x2048): 105.754
Elapsed time for attention_prob_times_values (128x2048x2048x386): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x386): 125.534

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1499.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x387x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x387x2048): 73.833
Elapsed time for attention_prob_times_values (128x2048x2048x387): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x387): 73.971

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 967.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x388x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x388x2048): 106.568
Elapsed time for attention_prob_times_values (128x2048x2048x388): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x388): 124.850

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1509.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x389x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x389x2048): 72.956
Elapsed time for attention_prob_times_values (128x2048x2048x389): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x389): 62.367

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 884.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x390x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x390x2048): 95.389
Elapsed time for attention_prob_times_values (128x2048x2048x390): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x390): 122.997

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1416.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x391x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x391x2048): 74.580
Elapsed time for attention_prob_times_values (128x2048x2048x391): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x391): 71.941

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 968.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x392x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x392x2048): 171.034
Elapsed time for attention_prob_times_values (128x2048x2048x392): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x392): 178.216

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 2312.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x393x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x393x2048): 73.450
Elapsed time for attention_prob_times_values (128x2048x2048x393): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x393): 72.428

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 968.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x394x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x394x2048): 107.966
Elapsed time for attention_prob_times_values (128x2048x2048x394): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x394): 122.399

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1527.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x395x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x395x2048): 75.266
Elapsed time for attention_prob_times_values (128x2048x2048x395): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x395): 73.823

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 994.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x396x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x396x2048): 110.285
Elapsed time for attention_prob_times_values (128x2048x2048x396): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x396): 121.157

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1544.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x397x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x397x2048): 76.535
Elapsed time for attention_prob_times_values (128x2048x2048x397): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x397): 74.525

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1012.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x398x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x398x2048): 108.566
Elapsed time for attention_prob_times_values (128x2048x2048x398): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x398): 125.582

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1564.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x399x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x399x2048): 77.521
Elapsed time for attention_prob_times_values (128x2048x2048x399): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x399): 76.148

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1034.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x400x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x400x2048): 25.019
Elapsed time for attention_prob_times_values (128x2048x2048x400): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x400): 188.179

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 596.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x401x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x401x2048): 76.072
Elapsed time for attention_prob_times_values (128x2048x2048x401): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x401): 74.169

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1016.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x402x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x402x2048): 108.410
Elapsed time for attention_prob_times_values (128x2048x2048x402): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x402): 122.179

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1558.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x403x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x403x2048): 76.911
Elapsed time for attention_prob_times_values (128x2048x2048x403): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x403): 76.255

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1041.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x404x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x404x2048): 109.824
Elapsed time for attention_prob_times_values (128x2048x2048x404): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x404): 127.163

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1605.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x405x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x405x2048): 76.435
Elapsed time for attention_prob_times_values (128x2048x2048x405): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x405): 76.068

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1041.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x406x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x406x2048): 112.348
Elapsed time for attention_prob_times_values (128x2048x2048x406): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x406): 71.341

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1194.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x407x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x407x2048): 79.028
Elapsed time for attention_prob_times_values (128x2048x2048x407): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x407): 75.403

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1058.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x408x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x408x2048): 180.907
Elapsed time for attention_prob_times_values (128x2048x2048x408): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x408): 187.880

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 2534.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x409x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x409x2048): 77.475
Elapsed time for attention_prob_times_values (128x2048x2048x409): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x409): 74.598

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1047.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x410x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x410x2048): 110.620
Elapsed time for attention_prob_times_values (128x2048x2048x410): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x410): 127.343

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1635.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x411x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x411x2048): 79.411
Elapsed time for attention_prob_times_values (128x2048x2048x411): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x411): 77.440

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1085.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x412x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x412x2048): 113.471
Elapsed time for attention_prob_times_values (128x2048x2048x412): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x412): 126.189

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1657.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x413x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x413x2048): 78.108
Elapsed time for attention_prob_times_values (128x2048x2048x413): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x413): 77.516

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1082.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x414x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x414x2048): 111.934
Elapsed time for attention_prob_times_values (128x2048x2048x414): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x414): 122.898

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1632.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x415x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x415x2048): 79.603
Elapsed time for attention_prob_times_values (128x2048x2048x415): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x415): 76.815

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1092.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x416x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x416x2048): 187.800
Elapsed time for attention_prob_times_values (128x2048x2048x416): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x416): 197.293

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 2694.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x417x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x417x2048): 32.727
Elapsed time for attention_prob_times_values (128x2048x2048x417): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x417): 77.751

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 646.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x418x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x418x2048): 111.941
Elapsed time for attention_prob_times_values (128x2048x2048x418): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x418): 125.668

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1665.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x419x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x419x2048): 75.963
Elapsed time for attention_prob_times_values (128x2048x2048x419): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x419): 79.070

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1092.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x420x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x420x2048): 115.378
Elapsed time for attention_prob_times_values (128x2048x2048x420): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x420): 126.084

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1701.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x421x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x421x2048): 76.440
Elapsed time for attention_prob_times_values (128x2048x2048x421): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x421): 79.288

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1101.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x422x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x422x2048): 115.078
Elapsed time for attention_prob_times_values (128x2048x2048x422): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x422): 126.701

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1711.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x423x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x423x2048): 75.828
Elapsed time for attention_prob_times_values (128x2048x2048x423): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x423): 78.110

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1094.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x424x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x424x2048): 175.686
Elapsed time for attention_prob_times_values (128x2048x2048x424): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x424): 190.819

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 2606.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x425x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x425x2048): 76.241
Elapsed time for attention_prob_times_values (128x2048x2048x425): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x425): 77.993

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1101.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x426x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x426x2048): 116.699
Elapsed time for attention_prob_times_values (128x2048x2048x426): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x426): 128.554

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1750.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x427x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x427x2048): 75.734
Elapsed time for attention_prob_times_values (128x2048x2048x427): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x427): 80.871

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1121.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x428x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x428x2048): 115.991
Elapsed time for attention_prob_times_values (128x2048x2048x428): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x428): 132.550

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1778.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x429x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x429x2048): 77.695
Elapsed time for attention_prob_times_values (128x2048x2048x429): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x429): 79.698

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1133.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x430x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x430x2048): 116.624
Elapsed time for attention_prob_times_values (128x2048x2048x430): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x430): 130.936

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1781.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x431x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x431x2048): 75.980
Elapsed time for attention_prob_times_values (128x2048x2048x431): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x431): 79.928

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1127.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x432x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x432x2048): 194.867
Elapsed time for attention_prob_times_values (128x2048x2048x432): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x432): 203.425

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2886.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x433x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x433x2048): 77.071
Elapsed time for attention_prob_times_values (128x2048x2048x433): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x433): 79.145

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1134.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x434x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x434x2048): 119.934
Elapsed time for attention_prob_times_values (128x2048x2048x434): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x434): 133.581

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1840.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x435x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x435x2048): 77.658
Elapsed time for attention_prob_times_values (128x2048x2048x435): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x435): 82.668

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1168.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x436x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x436x2048): 120.464
Elapsed time for attention_prob_times_values (128x2048x2048x436): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x436): 131.934

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1841.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x437x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x437x2048): 78.163
Elapsed time for attention_prob_times_values (128x2048x2048x437): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x437): 82.807

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1178.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x438x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x438x2048): 117.241
Elapsed time for attention_prob_times_values (128x2048x2048x438): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x438): 135.731

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1847.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x439x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x439x2048): 77.545
Elapsed time for attention_prob_times_values (128x2048x2048x439): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x439): 81.782

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1171.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x440x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x440x2048): 179.394
Elapsed time for attention_prob_times_values (128x2048x2048x440): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x440): 188.776

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 2713.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x441x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x441x2048): 77.898
Elapsed time for attention_prob_times_values (128x2048x2048x441): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x441): 80.191

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1168.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x442x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x442x2048): 122.505
Elapsed time for attention_prob_times_values (128x2048x2048x442): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x442): 136.604

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1913.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x443x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x443x2048): 80.441
Elapsed time for attention_prob_times_values (128x2048x2048x443): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x443): 83.473

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1216.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x444x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x444x2048): 120.632
Elapsed time for attention_prob_times_values (128x2048x2048x444): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x444): 137.865

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1914.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x445x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x445x2048): 81.987
Elapsed time for attention_prob_times_values (128x2048x2048x445): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x445): 85.412

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1247.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x446x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x446x2048): 123.333
Elapsed time for attention_prob_times_values (128x2048x2048x446): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x446): 138.620

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1949.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x447x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x447x2048): 80.428
Elapsed time for attention_prob_times_values (128x2048x2048x447): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x447): 83.135

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1223.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x448x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x448x2048): 192.999
Elapsed time for attention_prob_times_values (128x2048x2048x448): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x448): 213.708

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 3042.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x449x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x449x2048): 79.837
Elapsed time for attention_prob_times_values (128x2048x2048x449): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x449): 82.860

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1222.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x450x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x450x2048): 115.352
Elapsed time for attention_prob_times_values (128x2048x2048x450): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x450): 136.344

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1882.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x451x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x451x2048): 80.536
Elapsed time for attention_prob_times_values (128x2048x2048x451): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x451): 85.245

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1250.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x452x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x452x2048): 114.509
Elapsed time for attention_prob_times_values (128x2048x2048x452): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x452): 134.525

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1871.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x453x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x453x2048): 79.562
Elapsed time for attention_prob_times_values (128x2048x2048x453): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x453): 80.583

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1213.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x454x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x454x2048): 115.935
Elapsed time for attention_prob_times_values (128x2048x2048x454): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x454): 135.847

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1900.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x455x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x455x2048): 80.032
Elapsed time for attention_prob_times_values (128x2048x2048x455): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x455): 81.926

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1232.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x456x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x456x2048): 178.764
Elapsed time for attention_prob_times_values (128x2048x2048x456): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x456): 208.365

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 2934.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x457x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x457x2048): 79.208
Elapsed time for attention_prob_times_values (128x2048x2048x457): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x457): 83.965

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1245.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x458x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x458x2048): 116.943
Elapsed time for attention_prob_times_values (128x2048x2048x458): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x458): 135.407

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1921.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x459x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x459x2048): 80.821
Elapsed time for attention_prob_times_values (128x2048x2048x459): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x459): 86.027

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1278.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x460x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x460x2048): 117.641
Elapsed time for attention_prob_times_values (128x2048x2048x460): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x460): 138.313

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1954.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x461x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x461x2048): 79.267
Elapsed time for attention_prob_times_values (128x2048x2048x461): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x461): 86.720

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1276.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x462x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x462x2048): 115.586
Elapsed time for attention_prob_times_values (128x2048x2048x462): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x462): 138.655

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1946.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x463x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x463x2048): 79.556
Elapsed time for attention_prob_times_values (128x2048x2048x463): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x463): 83.507

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1260.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x464x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x464x2048): 191.414
Elapsed time for attention_prob_times_values (128x2048x2048x464): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x464): 215.652

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 3143.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x465x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x465x2048): 80.570
Elapsed time for attention_prob_times_values (128x2048x2048x465): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x465): 83.908

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1276.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x466x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x466x2048): 101.351
Elapsed time for attention_prob_times_values (128x2048x2048x466): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x466): 136.900

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1812.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x467x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x467x2048): 79.860
Elapsed time for attention_prob_times_values (128x2048x2048x467): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x467): 86.182

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1292.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x468x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x468x2048): 119.031
Elapsed time for attention_prob_times_values (128x2048x2048x468): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x468): 134.918

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1976.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x469x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x469x2048): 80.034
Elapsed time for attention_prob_times_values (128x2048x2048x469): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x469): 84.963

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1290.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x470x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x470x2048): 119.587
Elapsed time for attention_prob_times_values (128x2048x2048x470): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x470): 137.877

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2009.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x471x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x471x2048): 78.958
Elapsed time for attention_prob_times_values (128x2048x2048x471): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x471): 85.333

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1289.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x472x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x472x2048): 180.373
Elapsed time for attention_prob_times_values (128x2048x2048x472): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x472): 213.145

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 3077.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x473x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x473x2048): 80.336
Elapsed time for attention_prob_times_values (128x2048x2048x473): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x473): 85.079

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1304.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x474x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x474x2048): 117.009
Elapsed time for attention_prob_times_values (128x2048x2048x474): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x474): 142.428

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2031.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x475x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x475x2048): 71.548
Elapsed time for attention_prob_times_values (128x2048x2048x475): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x475): 82.925

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1217.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x476x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x476x2048): 118.496
Elapsed time for attention_prob_times_values (128x2048x2048x476): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x476): 139.865

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2036.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x477x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x477x2048): 79.493
Elapsed time for attention_prob_times_values (128x2048x2048x477): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x477): 88.437

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1331.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x478x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x478x2048): 119.576
Elapsed time for attention_prob_times_values (128x2048x2048x478): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x478): 144.488

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2085.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x479x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x479x2048): 81.117
Elapsed time for attention_prob_times_values (128x2048x2048x479): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x479): 87.220

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1342.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x480x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x480x2048): 205.312
Elapsed time for attention_prob_times_values (128x2048x2048x480): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x480): 215.569

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 3365.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x481x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x481x2048): 79.311
Elapsed time for attention_prob_times_values (128x2048x2048x481): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x481): 88.137

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1338.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x482x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x482x2048): 119.659
Elapsed time for attention_prob_times_values (128x2048x2048x482): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x482): 140.701

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2077.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x483x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x483x2048): 77.112
Elapsed time for attention_prob_times_values (128x2048x2048x483): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x483): 89.296

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1331.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x484x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x484x2048): 120.421
Elapsed time for attention_prob_times_values (128x2048x2048x484): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x484): 142.123

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2102.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x485x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x485x2048): 81.006
Elapsed time for attention_prob_times_values (128x2048x2048x485): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x485): 88.884

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1369.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x486x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x486x2048): 119.702
Elapsed time for attention_prob_times_values (128x2048x2048x486): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x486): 146.788

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2134.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x487x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x487x2048): 79.011
Elapsed time for attention_prob_times_values (128x2048x2048x487): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x487): 89.120

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1358.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x488x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x488x2048): 186.535
Elapsed time for attention_prob_times_values (128x2048x2048x488): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x488): 211.812

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 3223.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x489x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x489x2048): 80.915
Elapsed time for attention_prob_times_values (128x2048x2048x489): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x489): 87.374

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1367.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x490x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x490x2048): 124.859
Elapsed time for attention_prob_times_values (128x2048x2048x490): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x490): 145.613

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2193.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x491x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x491x2048): 80.162
Elapsed time for attention_prob_times_values (128x2048x2048x491): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x491): 92.514

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1403.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x492x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x492x2048): 125.362
Elapsed time for attention_prob_times_values (128x2048x2048x492): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x492): 144.040

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2195.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x493x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x493x2048): 81.002
Elapsed time for attention_prob_times_values (128x2048x2048x493): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x493): 90.025

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1399.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x494x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x494x2048): 123.489
Elapsed time for attention_prob_times_values (128x2048x2048x494): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x494): 143.054

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2178.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x495x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x495x2048): 46.600
Elapsed time for attention_prob_times_values (128x2048x2048x495): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x495): 76.116

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 952.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x496x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x496x2048): 189.545
Elapsed time for attention_prob_times_values (128x2048x2048x496): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x496): 228.637

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3419.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x497x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x497x2048): 80.948
Elapsed time for attention_prob_times_values (128x2048x2048x497): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x497): 91.876

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1422.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x498x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x498x2048): 125.156
Elapsed time for attention_prob_times_values (128x2048x2048x498): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x498): 151.817

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2272.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x499x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x499x2048): 82.978
Elapsed time for attention_prob_times_values (128x2048x2048x499): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x499): 94.143

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1463.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x500x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x500x2048): 124.265
Elapsed time for attention_prob_times_values (128x2048x2048x500): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x500): 146.888

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2238.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x501x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x501x2048): 80.996
Elapsed time for attention_prob_times_values (128x2048x2048x501): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x501): 92.109

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1435.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x502x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x502x2048): 124.962
Elapsed time for attention_prob_times_values (128x2048x2048x502): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x502): 152.345

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2291.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x503x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x503x2048): 82.202
Elapsed time for attention_prob_times_values (128x2048x2048x503): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x503): 91.531

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1448.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x504x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x504x2048): 192.504
Elapsed time for attention_prob_times_values (128x2048x2048x504): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x504): 216.399

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 3412.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x505x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x505x2048): 81.831
Elapsed time for attention_prob_times_values (128x2048x2048x505): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x505): 93.697

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1466.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x506x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x506x2048): 128.551
Elapsed time for attention_prob_times_values (128x2048x2048x506): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x506): 152.399

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2344.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x507x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x507x2048): 84.552
Elapsed time for attention_prob_times_values (128x2048x2048x507): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x507): 93.679

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1497.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x508x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x508x2048): 128.515
Elapsed time for attention_prob_times_values (128x2048x2048x508): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x508): 154.458

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2367.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x509x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x509x2048): 76.672
Elapsed time for attention_prob_times_values (128x2048x2048x509): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x509): 92.243

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1415.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x510x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x510x2048): 127.136
Elapsed time for attention_prob_times_values (128x2048x2048x510): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x510): 122.075

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2109.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x511x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x511x2048): 85.099
Elapsed time for attention_prob_times_values (128x2048x2048x511): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x511): 95.230

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1525.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x512x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x512x2048): 202.107
Elapsed time for attention_prob_times_values (128x2048x2048x512): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x512): 228.913

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3649.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x513x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x513x2048): 84.267
Elapsed time for attention_prob_times_values (128x2048x2048x513): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x513): 71.085

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1313.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x514x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x514x2048): 123.670
Elapsed time for attention_prob_times_values (128x2048x2048x514): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x514): 118.123

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 2061.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x515x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x515x2048): 83.140
Elapsed time for attention_prob_times_values (128x2048x2048x515): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x515): 72.421

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1323.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x516x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x516x2048): 120.582
Elapsed time for attention_prob_times_values (128x2048x2048x516): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x516): 113.542

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2002.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x517x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x517x2048): 84.674
Elapsed time for attention_prob_times_values (128x2048x2048x517): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x517): 73.159

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1346.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x518x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x518x2048): 121.617
Elapsed time for attention_prob_times_values (128x2048x2048x518): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x518): 118.691

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 2064.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x519x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x519x2048): 82.307
Elapsed time for attention_prob_times_values (128x2048x2048x519): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x519): 70.836

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1311.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x520x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x520x2048): 193.144
Elapsed time for attention_prob_times_values (128x2048x2048x520): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x520): 160.710

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3026.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x521x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x521x2048): 81.088
Elapsed time for attention_prob_times_values (128x2048x2048x521): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x521): 71.031

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1308.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x522x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x522x2048): 124.208
Elapsed time for attention_prob_times_values (128x2048x2048x522): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x522): 117.245

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 2088.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x523x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x523x2048): 84.341
Elapsed time for attention_prob_times_values (128x2048x2048x523): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x523): 73.155

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1358.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x524x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x524x2048): 123.348
Elapsed time for attention_prob_times_values (128x2048x2048x524): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x524): 116.919

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2085.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x525x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x525x2048): 82.738
Elapsed time for attention_prob_times_values (128x2048x2048x525): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x525): 73.846

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1358.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x526x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x526x2048): 124.026
Elapsed time for attention_prob_times_values (128x2048x2048x526): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x526): 116.556

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2095.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x527x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x527x2048): 79.420
Elapsed time for attention_prob_times_values (128x2048x2048x527): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x527): 72.024

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1319.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x528x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x528x2048): 202.771
Elapsed time for attention_prob_times_values (128x2048x2048x528): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x528): 173.367

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 3271.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x529x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x529x2048): 82.668
Elapsed time for attention_prob_times_values (128x2048x2048x529): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x529): 71.552

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1344.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x530x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x530x2048): 123.202
Elapsed time for attention_prob_times_values (128x2048x2048x530): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x530): 118.275

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2119.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x531x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x531x2048): 82.779
Elapsed time for attention_prob_times_values (128x2048x2048x531): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x531): 73.338

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1368.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x532x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x532x2048): 125.697
Elapsed time for attention_prob_times_values (128x2048x2048x532): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x532): 117.562

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2141.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x533x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x533x2048): 82.026
Elapsed time for attention_prob_times_values (128x2048x2048x533): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x533): 73.407

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1367.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x534x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x534x2048): 124.298
Elapsed time for attention_prob_times_values (128x2048x2048x534): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x534): 116.681

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2129.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x535x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x535x2048): 82.146
Elapsed time for attention_prob_times_values (128x2048x2048x535): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x535): 71.716

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1356.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x536x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x536x2048): 190.734
Elapsed time for attention_prob_times_values (128x2048x2048x536): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x536): 169.375

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3184.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x537x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x537x2048): 83.616
Elapsed time for attention_prob_times_values (128x2048x2048x537): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x537): 71.091

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1366.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x538x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x538x2048): 125.801
Elapsed time for attention_prob_times_values (128x2048x2048x538): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x538): 118.766

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2176.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x539x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x539x2048): 83.394
Elapsed time for attention_prob_times_values (128x2048x2048x539): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x539): 73.791

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1397.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x540x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x540x2048): 123.574
Elapsed time for attention_prob_times_values (128x2048x2048x540): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x540): 121.451

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2189.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x541x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x541x2048): 82.077
Elapsed time for attention_prob_times_values (128x2048x2048x541): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x541): 74.436

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1397.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x542x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x542x2048): 126.428
Elapsed time for attention_prob_times_values (128x2048x2048x542): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x542): 120.102

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2209.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x543x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x543x2048): 82.821
Elapsed time for attention_prob_times_values (128x2048x2048x543): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x543): 72.725

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1391.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x544x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x544x2048): 202.476
Elapsed time for attention_prob_times_values (128x2048x2048x544): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x544): 174.151

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 3370.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x545x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x545x2048): 81.102
Elapsed time for attention_prob_times_values (128x2048x2048x545): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x545): 72.628

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1381.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x546x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x546x2048): 127.818
Elapsed time for attention_prob_times_values (128x2048x2048x546): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x546): 97.683

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 2000.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x547x2048): 0.0219
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x547x2048): 26.877
Elapsed time for attention_prob_times_values (128x2048x2048x547): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x547): 75.101

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 716.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x548x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x548x2048): 127.914
Elapsed time for attention_prob_times_values (128x2048x2048x548): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x548): 120.642

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2250.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x549x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x549x2048): 80.681
Elapsed time for attention_prob_times_values (128x2048x2048x549): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x549): 75.579

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1417.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x550x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x550x2048): 123.762
Elapsed time for attention_prob_times_values (128x2048x2048x550): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x550): 118.618

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2203.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x551x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x551x2048): 82.016
Elapsed time for attention_prob_times_values (128x2048x2048x551): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x551): 73.600

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1413.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x552x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x552x2048): 195.886
Elapsed time for attention_prob_times_values (128x2048x2048x552): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x552): 175.614

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3379.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x553x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x553x2048): 81.399
Elapsed time for attention_prob_times_values (128x2048x2048x553): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x553): 72.404

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1401.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x554x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x554x2048): 125.705
Elapsed time for attention_prob_times_values (128x2048x2048x554): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x554): 120.392

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2252.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x555x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x555x2048): 81.314
Elapsed time for attention_prob_times_values (128x2048x2048x555): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x555): 74.932

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1430.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x556x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x556x2048): 129.319
Elapsed time for attention_prob_times_values (128x2048x2048x556): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x556): 120.715

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 2294.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x557x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x557x2048): 82.055
Elapsed time for attention_prob_times_values (128x2048x2048x557): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x557): 75.571

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1448.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x558x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x558x2048): 126.645
Elapsed time for attention_prob_times_values (128x2048x2048x558): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x558): 118.869

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2261.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x559x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x559x2048): 81.916
Elapsed time for attention_prob_times_values (128x2048x2048x559): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x559): 74.397

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1440.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x560x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x560x2048): 198.615
Elapsed time for attention_prob_times_values (128x2048x2048x560): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x560): 176.229

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3454.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x561x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x561x2048): 82.582
Elapsed time for attention_prob_times_values (128x2048x2048x561): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x561): 71.895

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1424.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x562x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x562x2048): 127.181
Elapsed time for attention_prob_times_values (128x2048x2048x562): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x562): 119.982

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2292.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x563x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x563x2048): 82.411
Elapsed time for attention_prob_times_values (128x2048x2048x563): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x563): 75.830

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1468.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x564x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x564x2048): 124.438
Elapsed time for attention_prob_times_values (128x2048x2048x564): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x564): 122.494

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2299.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x565x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x565x2048): 83.724
Elapsed time for attention_prob_times_values (128x2048x2048x565): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x565): 77.007

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1496.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x566x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x566x2048): 126.328
Elapsed time for attention_prob_times_values (128x2048x2048x566): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x566): 122.121

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2320.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x567x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x567x2048): 76.986
Elapsed time for attention_prob_times_values (128x2048x2048x567): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x567): 73.521

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1407.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x568x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x568x2048): 190.999
Elapsed time for attention_prob_times_values (128x2048x2048x568): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x568): 173.509

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 3409.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x569x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x569x2048): 80.894
Elapsed time for attention_prob_times_values (128x2048x2048x569): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x569): 70.228

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1412.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x570x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x570x2048): 130.142
Elapsed time for attention_prob_times_values (128x2048x2048x570): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x570): 121.190

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2361.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x571x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x571x2048): 82.801
Elapsed time for attention_prob_times_values (128x2048x2048x571): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x571): 77.247

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1506.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x572x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x572x2048): 130.698
Elapsed time for attention_prob_times_values (128x2048x2048x572): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x572): 121.027

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2372.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x573x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x573x2048): 82.597
Elapsed time for attention_prob_times_values (128x2048x2048x573): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x573): 77.036

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1507.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x574x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x574x2048): 132.201
Elapsed time for attention_prob_times_values (128x2048x2048x574): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x574): 124.756

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 2431.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x575x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x575x2048): 84.137
Elapsed time for attention_prob_times_values (128x2048x2048x575): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x575): 75.276

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1507.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x576x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x576x2048): 201.936
Elapsed time for attention_prob_times_values (128x2048x2048x576): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x576): 183.664

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3654.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x577x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x577x2048): 82.542
Elapsed time for attention_prob_times_values (128x2048x2048x577): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x577): 75.158

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1497.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x578x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x578x2048): 125.281
Elapsed time for attention_prob_times_values (128x2048x2048x578): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x578): 126.658

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2401.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x579x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x579x2048): 82.404
Elapsed time for attention_prob_times_values (128x2048x2048x579): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x579): 77.623

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1526.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x580x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x580x2048): 124.605
Elapsed time for attention_prob_times_values (128x2048x2048x580): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x580): 103.025

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 2157.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x581x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x581x2048): 81.661
Elapsed time for attention_prob_times_values (128x2048x2048x581): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x581): 77.133

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1519.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x582x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x582x2048): 122.384
Elapsed time for attention_prob_times_values (128x2048x2048x582): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x582): 125.148

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2374.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x583x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x583x2048): 81.759
Elapsed time for attention_prob_times_values (128x2048x2048x583): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x583): 75.202

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1505.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x584x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x584x2048): 198.558
Elapsed time for attention_prob_times_values (128x2048x2048x584): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x584): 187.223

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 3709.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x585x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x585x2048): 81.132
Elapsed time for attention_prob_times_values (128x2048x2048x585): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x585): 75.698

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1510.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x586x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x586x2048): 124.059
Elapsed time for attention_prob_times_values (128x2048x2048x586): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x586): 125.594

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2410.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x587x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x587x2048): 78.803
Elapsed time for attention_prob_times_values (128x2048x2048x587): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x587): 77.962

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1516.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x588x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x588x2048): 111.102
Elapsed time for attention_prob_times_values (128x2048x2048x588): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x588): 29.915

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 913.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x589x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x589x2048): 76.019
Elapsed time for attention_prob_times_values (128x2048x2048x589): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x589): 73.003

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1445.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x590x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x590x2048): 122.206
Elapsed time for attention_prob_times_values (128x2048x2048x590): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x590): 120.395

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2357.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x591x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x591x2048): 82.953
Elapsed time for attention_prob_times_values (128x2048x2048x591): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x591): 76.229

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1546.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x592x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x592x2048): 203.165
Elapsed time for attention_prob_times_values (128x2048x2048x592): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x592): 187.044

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 3798.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x593x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x593x2048): 83.666
Elapsed time for attention_prob_times_values (128x2048x2048x593): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x593): 75.035

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1545.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x594x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x594x2048): 127.394
Elapsed time for attention_prob_times_values (128x2048x2048x594): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x594): 123.987

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2458.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x595x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x595x2048): 82.697
Elapsed time for attention_prob_times_values (128x2048x2048x595): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x595): 80.008

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1593.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x596x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x596x2048): 128.170
Elapsed time for attention_prob_times_values (128x2048x2048x596): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x596): 121.543

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2448.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x597x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x597x2048): 83.209
Elapsed time for attention_prob_times_values (128x2048x2048x597): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x597): 78.204

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1584.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x598x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x598x2048): 125.931
Elapsed time for attention_prob_times_values (128x2048x2048x598): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x598): 120.537

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2425.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x599x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x599x2048): 83.199
Elapsed time for attention_prob_times_values (128x2048x2048x599): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x599): 77.557

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1583.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x600x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x600x2048): 193.011
Elapsed time for attention_prob_times_values (128x2048x2048x600): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x600): 187.976

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 3761.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x601x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x601x2048): 83.162
Elapsed time for attention_prob_times_values (128x2048x2048x601): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x601): 55.363

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1314.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x602x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x602x2048): 125.989
Elapsed time for attention_prob_times_values (128x2048x2048x602): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x602): 124.617

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2482.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x603x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x603x2048): 82.919
Elapsed time for attention_prob_times_values (128x2048x2048x603): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x603): 76.755

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1581.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x604x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x604x2048): 125.371
Elapsed time for attention_prob_times_values (128x2048x2048x604): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x604): 126.590

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2503.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x605x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x605x2048): 81.812
Elapsed time for attention_prob_times_values (128x2048x2048x605): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x605): 79.525

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1605.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x606x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x606x2048): 129.131
Elapsed time for attention_prob_times_values (128x2048x2048x606): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x606): 122.643

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2508.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x607x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x607x2048): 84.173
Elapsed time for attention_prob_times_values (128x2048x2048x607): 0.0354
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x607): 18.389

Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 602.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0432
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x608x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x608x2048): 212.444
Elapsed time for attention_prob_times_values (128x2048x2048x608): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x608): 193.854

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 4054.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x609x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x609x2048): 83.754
Elapsed time for attention_prob_times_values (128x2048x2048x609): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x609): 78.086

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1618.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x610x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x610x2048): 127.533
Elapsed time for attention_prob_times_values (128x2048x2048x610): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x610): 125.060

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2533.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x611x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x611x2048): 81.329
Elapsed time for attention_prob_times_values (128x2048x2048x611): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x611): 79.414

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1614.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x612x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x612x2048): 126.182
Elapsed time for attention_prob_times_values (128x2048x2048x612): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x612): 127.032

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2547.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x613x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x613x2048): 80.725
Elapsed time for attention_prob_times_values (128x2048x2048x613): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x613): 78.848

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1607.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x614x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x614x2048): 131.102
Elapsed time for attention_prob_times_values (128x2048x2048x614): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x614): 130.768

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2643.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x615x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x615x2048): 80.986
Elapsed time for attention_prob_times_values (128x2048x2048x615): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x615): 77.963

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1606.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x616x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x616x2048): 166.480
Elapsed time for attention_prob_times_values (128x2048x2048x616): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x616): 194.259

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 3630.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x617x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x617x2048): 81.428
Elapsed time for attention_prob_times_values (128x2048x2048x617): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x617): 78.961

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1626.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x618x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x618x2048): 128.395
Elapsed time for attention_prob_times_values (128x2048x2048x618): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x618): 130.717

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2631.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x619x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x619x2048): 80.715
Elapsed time for attention_prob_times_values (128x2048x2048x619): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x619): 79.868

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1633.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x620x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x620x2048): 132.246
Elapsed time for attention_prob_times_values (128x2048x2048x620): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x620): 131.140

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2683.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x621x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x621x2048): 81.757
Elapsed time for attention_prob_times_values (128x2048x2048x621): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x621): 77.949

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1628.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x622x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x622x2048): 131.755
Elapsed time for attention_prob_times_values (128x2048x2048x622): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x622): 129.453

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2669.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x623x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x623x2048): 82.768
Elapsed time for attention_prob_times_values (128x2048x2048x623): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x623): 78.740

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1651.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x624x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x624x2048): 203.302
Elapsed time for attention_prob_times_values (128x2048x2048x624): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x624): 201.856

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 4152.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x625x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x625x2048): 81.180
Elapsed time for attention_prob_times_values (128x2048x2048x625): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x625): 79.347

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1647.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x626x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x626x2048): 131.593
Elapsed time for attention_prob_times_values (128x2048x2048x626): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x626): 130.310

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2692.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x627x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x627x2048): 82.217
Elapsed time for attention_prob_times_values (128x2048x2048x627): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x627): 81.240

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1683.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x628x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x628x2048): 126.953
Elapsed time for attention_prob_times_values (128x2048x2048x628): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x628): 130.696

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 2656.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x629x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x629x2048): 81.263
Elapsed time for attention_prob_times_values (128x2048x2048x629): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x629): 80.611

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1671.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x630x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x630x2048): 134.736
Elapsed time for attention_prob_times_values (128x2048x2048x630): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x630): 131.960

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2758.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x631x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x631x2048): 80.650
Elapsed time for attention_prob_times_values (128x2048x2048x631): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x631): 79.617

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1660.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x632x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x632x2048): 193.703
Elapsed time for attention_prob_times_values (128x2048x2048x632): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x632): 191.038

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 3991.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x633x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x633x2048): 80.152
Elapsed time for attention_prob_times_values (128x2048x2048x633): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x633): 80.963

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1674.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x634x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x634x2048): 108.323
Elapsed time for attention_prob_times_values (128x2048x2048x634): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x634): 122.083

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 2389.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x635x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x635x2048): 80.519
Elapsed time for attention_prob_times_values (128x2048x2048x635): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x635): 81.788

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1691.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x636x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x636x2048): 128.472
Elapsed time for attention_prob_times_values (128x2048x2048x636): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x636): 131.958

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 2717.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x637x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x637x2048): 81.558
Elapsed time for attention_prob_times_values (128x2048x2048x637): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x637): 82.358

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1713.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x638x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x638x2048): 134.612
Elapsed time for attention_prob_times_values (128x2048x2048x638): 0.0268
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x638): 25.549

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 899.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x639x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x639x2048): 81.937
Elapsed time for attention_prob_times_values (128x2048x2048x639): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x639): 81.888

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1717.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x640x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x640x2048): 218.224
Elapsed time for attention_prob_times_values (128x2048x2048x640): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x640): 208.139

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 4474.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x641x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x641x2048): 80.651
Elapsed time for attention_prob_times_values (128x2048x2048x641): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x641): 81.275

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1702.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x642x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x642x2048): 128.967
Elapsed time for attention_prob_times_values (128x2048x2048x642): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x642): 135.577

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2784.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x643x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x643x2048): 79.350
Elapsed time for attention_prob_times_values (128x2048x2048x643): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x643): 82.655

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1707.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x644x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x644x2048): 129.548
Elapsed time for attention_prob_times_values (128x2048x2048x644): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x644): 132.829

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 2770.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x645x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x645x2048): 80.177
Elapsed time for attention_prob_times_values (128x2048x2048x645): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x645): 82.766

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1723.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x646x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x646x2048): 129.057
Elapsed time for attention_prob_times_values (128x2048x2048x646): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x646): 132.852

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 2774.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x647x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x647x2048): 79.230
Elapsed time for attention_prob_times_values (128x2048x2048x647): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x647): 79.969

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1688.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x648x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x648x2048): 203.453
Elapsed time for attention_prob_times_values (128x2048x2048x648): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x648): 203.652

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 4325.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x649x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x649x2048): 80.614
Elapsed time for attention_prob_times_values (128x2048x2048x649): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x649): 81.319

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1723.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x650x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x650x2048): 130.527
Elapsed time for attention_prob_times_values (128x2048x2048x650): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x650): 132.862

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 2806.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x651x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x651x2048): 81.361
Elapsed time for attention_prob_times_values (128x2048x2048x651): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x651): 83.140

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1755.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x652x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x652x2048): 130.435
Elapsed time for attention_prob_times_values (128x2048x2048x652): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x652): 133.665

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 2822.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x653x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x653x2048): 81.715
Elapsed time for attention_prob_times_values (128x2048x2048x653): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x653): 79.642

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1726.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x654x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x654x2048): 128.472
Elapsed time for attention_prob_times_values (128x2048x2048x654): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x654): 132.399

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 2795.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x655x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x655x2048): 82.608
Elapsed time for attention_prob_times_values (128x2048x2048x655): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x655): 81.277

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1759.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x656x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x656x2048): 192.441
Elapsed time for attention_prob_times_values (128x2048x2048x656): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x656): 211.630

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 4333.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x657x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x657x2048): 84.820
Elapsed time for attention_prob_times_values (128x2048x2048x657): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x657): 82.747

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1803.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x658x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x658x2048): 131.297
Elapsed time for attention_prob_times_values (128x2048x2048x658): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x658): 132.536

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 2844.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x659x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x659x2048): 83.267
Elapsed time for attention_prob_times_values (128x2048x2048x659): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x659): 82.585

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1790.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x660x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x660x2048): 129.577
Elapsed time for attention_prob_times_values (128x2048x2048x660): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x660): 132.998

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 2838.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x661x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x661x2048): 84.568
Elapsed time for attention_prob_times_values (128x2048x2048x661): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x661): 83.012

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1814.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x662x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x662x2048): 132.007
Elapsed time for attention_prob_times_values (128x2048x2048x662): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x662): 134.557

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 2890.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x663x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x663x2048): 84.620
Elapsed time for attention_prob_times_values (128x2048x2048x663): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x663): 81.260

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1800.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x664x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x664x2048): 204.625
Elapsed time for attention_prob_times_values (128x2048x2048x664): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x664): 208.666

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 4494.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x665x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x665x2048): 86.368
Elapsed time for attention_prob_times_values (128x2048x2048x665): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x665): 82.552

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1838.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x666x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x666x2048): 132.120
Elapsed time for attention_prob_times_values (128x2048x2048x666): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x666): 134.599

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 2908.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x667x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x667x2048): 85.169
Elapsed time for attention_prob_times_values (128x2048x2048x667): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x667): 83.467

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1841.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x668x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x668x2048): 129.055
Elapsed time for attention_prob_times_values (128x2048x2048x668): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x668): 132.587

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 2861.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x669x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x669x2048): 85.948
Elapsed time for attention_prob_times_values (128x2048x2048x669): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x669): 84.360

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1865.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x670x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x670x2048): 129.910
Elapsed time for attention_prob_times_values (128x2048x2048x670): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x670): 133.762

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 2891.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x671x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x671x2048): 88.479
Elapsed time for attention_prob_times_values (128x2048x2048x671): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x671): 84.244

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1896.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x672x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x672x2048): 216.885
Elapsed time for attention_prob_times_values (128x2048x2048x672): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x672): 203.717

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 4622.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x673x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x673x2048): 85.397
Elapsed time for attention_prob_times_values (128x2048x2048x673): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x673): 81.119

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1833.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x674x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x674x2048): 132.366
Elapsed time for attention_prob_times_values (128x2048x2048x674): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x674): 133.203

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 2929.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x675x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x675x2048): 86.455
Elapsed time for attention_prob_times_values (128x2048x2048x675): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x675): 84.501

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1888.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x676x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x676x2048): 132.045
Elapsed time for attention_prob_times_values (128x2048x2048x676): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x676): 137.032

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 2975.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x677x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x677x2048): 85.323
Elapsed time for attention_prob_times_values (128x2048x2048x677): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x677): 82.831

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1862.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x678x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x678x2048): 133.169
Elapsed time for attention_prob_times_values (128x2048x2048x678): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x678): 135.300

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 2978.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x679x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x679x2048): 85.621
Elapsed time for attention_prob_times_values (128x2048x2048x679): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x679): 81.582

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1856.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x680x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x680x2048): 203.255
Elapsed time for attention_prob_times_values (128x2048x2048x680): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x680): 182.180

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 4275.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x681x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x681x2048): 85.612
Elapsed time for attention_prob_times_values (128x2048x2048x681): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x681): 84.331

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1893.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x682x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x682x2048): 132.436
Elapsed time for attention_prob_times_values (128x2048x2048x682): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x682): 137.021

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3005.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x683x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x683x2048): 86.496
Elapsed time for attention_prob_times_values (128x2048x2048x683): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x683): 87.239

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1940.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x684x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x684x2048): 135.883
Elapsed time for attention_prob_times_values (128x2048x2048x684): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x684): 138.423

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3068.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x685x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x685x2048): 86.006
Elapsed time for attention_prob_times_values (128x2048x2048x685): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x685): 86.727

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1935.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x686x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x686x2048): 136.618
Elapsed time for attention_prob_times_values (128x2048x2048x686): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x686): 136.799

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3067.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x687x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x687x2048): 85.844
Elapsed time for attention_prob_times_values (128x2048x2048x687): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x687): 84.845

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1917.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x688x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x688x2048): 208.301
Elapsed time for attention_prob_times_values (128x2048x2048x688): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x688): 219.735

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 4811.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x689x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x689x2048): 86.528
Elapsed time for attention_prob_times_values (128x2048x2048x689): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x689): 84.939

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1931.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x690x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x690x2048): 135.179
Elapsed time for attention_prob_times_values (128x2048x2048x690): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x690): 138.172

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3083.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x691x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x691x2048): 86.157
Elapsed time for attention_prob_times_values (128x2048x2048x691): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x691): 88.118

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1968.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x692x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x692x2048): 137.642
Elapsed time for attention_prob_times_values (128x2048x2048x692): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x692): 136.959

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3106.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x693x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x693x2048): 86.418
Elapsed time for attention_prob_times_values (128x2048x2048x693): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x693): 86.897

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1963.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x694x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x694x2048): 134.615
Elapsed time for attention_prob_times_values (128x2048x2048x694): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x694): 137.882

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3090.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x695x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x695x2048): 85.547
Elapsed time for attention_prob_times_values (128x2048x2048x695): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x695): 85.398

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1941.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x696x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x696x2048): 201.836
Elapsed time for attention_prob_times_values (128x2048x2048x696): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x696): 210.879

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 4692.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x697x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x697x2048): 86.339
Elapsed time for attention_prob_times_values (128x2048x2048x697): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x697): 85.390

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1956.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x698x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x698x2048): 135.672
Elapsed time for attention_prob_times_values (128x2048x2048x698): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x698): 138.353

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3125.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x699x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x699x2048): 87.635
Elapsed time for attention_prob_times_values (128x2048x2048x699): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x699): 87.968

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2005.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x700x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x700x2048): 136.131
Elapsed time for attention_prob_times_values (128x2048x2048x700): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x700): 141.801

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3177.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x701x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x701x2048): 87.501
Elapsed time for attention_prob_times_values (128x2048x2048x701): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x701): 88.131

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2011.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x702x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x702x2048): 137.154
Elapsed time for attention_prob_times_values (128x2048x2048x702): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x702): 135.143

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3122.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x703x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x703x2048): 87.324
Elapsed time for attention_prob_times_values (128x2048x2048x703): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x703): 87.360

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2006.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x704x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x704x2048): 130.271
Elapsed time for attention_prob_times_values (128x2048x2048x704): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x704): 218.567

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 3754.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x705x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x705x2048): 86.396
Elapsed time for attention_prob_times_values (128x2048x2048x705): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x705): 87.805

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2005.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x706x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x706x2048): 128.752
Elapsed time for attention_prob_times_values (128x2048x2048x706): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x706): 136.919

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3060.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x707x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x707x2048): 86.846
Elapsed time for attention_prob_times_values (128x2048x2048x707): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x707): 87.819

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2016.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x708x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x708x2048): 130.097
Elapsed time for attention_prob_times_values (128x2048x2048x708): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x708): 137.295

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3089.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x709x2048): 0.0318
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x709x2048): 23.907
Elapsed time for attention_prob_times_values (128x2048x2048x709): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x709): 87.130

Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 868.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0406
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x710x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x710x2048): 130.337
Elapsed time for attention_prob_times_values (128x2048x2048x710): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x710): 136.275

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3089.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x711x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x711x2048): 44.360
Elapsed time for attention_prob_times_values (128x2048x2048x711): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x711): 84.587

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1351.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x712x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x712x2048): 191.322
Elapsed time for attention_prob_times_values (128x2048x2048x712): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x712): 219.267

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 4750.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x713x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x713x2048): 85.505
Elapsed time for attention_prob_times_values (128x2048x2048x713): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x713): 76.216

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1876.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x714x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x714x2048): 132.749
Elapsed time for attention_prob_times_values (128x2048x2048x714): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x714): 136.722

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3140.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x715x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x715x2048): 79.734
Elapsed time for attention_prob_times_values (128x2048x2048x715): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x715): 86.657

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1938.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x716x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x716x2048): 131.465
Elapsed time for attention_prob_times_values (128x2048x2048x716): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x716): 136.403

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3129.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x717x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x717x2048): 84.739
Elapsed time for attention_prob_times_values (128x2048x2048x717): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x717): 87.597

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2016.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x718x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x718x2048): 133.680
Elapsed time for attention_prob_times_values (128x2048x2048x718): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x718): 139.517

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3200.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x719x2048): 0.0229
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x719x2048): 33.765
Elapsed time for attention_prob_times_values (128x2048x2048x719): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x719): 85.807

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1137.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x720x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x720x2048): 214.627
Elapsed time for attention_prob_times_values (128x2048x2048x720): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x720): 220.090

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 5107.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x721x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x721x2048): 84.183
Elapsed time for attention_prob_times_values (128x2048x2048x721): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x721): 85.356

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1994.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x722x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x722x2048): 133.646
Elapsed time for attention_prob_times_values (128x2048x2048x722): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x722): 141.781

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3242.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x723x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x723x2048): 84.231
Elapsed time for attention_prob_times_values (128x2048x2048x723): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x723): 87.770

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2028.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x724x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x724x2048): 133.900
Elapsed time for attention_prob_times_values (128x2048x2048x724): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x724): 136.370

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3192.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x725x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x725x2048): 81.982
Elapsed time for attention_prob_times_values (128x2048x2048x725): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x725): 86.914

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1996.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x726x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x726x2048): 134.508
Elapsed time for attention_prob_times_values (128x2048x2048x726): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x726): 138.902

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3237.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x727x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x727x2048): 84.059
Elapsed time for attention_prob_times_values (128x2048x2048x727): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x727): 86.637

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2023.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x728x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x728x2048): 199.989
Elapsed time for attention_prob_times_values (128x2048x2048x728): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x728): 223.309

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 5011.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x729x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x729x2048): 84.595
Elapsed time for attention_prob_times_values (128x2048x2048x729): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x729): 85.840

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2026.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x730x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x730x2048): 131.827
Elapsed time for attention_prob_times_values (128x2048x2048x730): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x730): 140.084

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3234.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x731x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x731x2048): 84.494
Elapsed time for attention_prob_times_values (128x2048x2048x731): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x731): 87.978

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2055.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x732x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x732x2048): 133.020
Elapsed time for attention_prob_times_values (128x2048x2048x732): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x732): 135.435

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 3204.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x733x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x733x2048): 60.342
Elapsed time for attention_prob_times_values (128x2048x2048x733): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x733): 88.313

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1713.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x734x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x734x2048): 131.930
Elapsed time for attention_prob_times_values (128x2048x2048x734): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x734): 144.503

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3301.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x735x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x735x2048): 87.637
Elapsed time for attention_prob_times_values (128x2048x2048x735): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x735): 86.744

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2089.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x736x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x736x2048): 215.573
Elapsed time for attention_prob_times_values (128x2048x2048x736): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x736): 232.493

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 5369.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x737x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x737x2048): 83.099
Elapsed time for attention_prob_times_values (128x2048x2048x737): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x737): 87.813

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2052.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x738x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x738x2048): 132.623
Elapsed time for attention_prob_times_values (128x2048x2048x738): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x738): 143.645

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3318.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x739x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x739x2048): 60.772
Elapsed time for attention_prob_times_values (128x2048x2048x739): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x739): 88.390

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1735.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x740x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x740x2048): 133.826
Elapsed time for attention_prob_times_values (128x2048x2048x740): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x740): 142.543

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3330.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x741x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x741x2048): 84.614
Elapsed time for attention_prob_times_values (128x2048x2048x741): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x741): 88.266

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2087.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x742x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x742x2048): 135.296
Elapsed time for attention_prob_times_values (128x2048x2048x742): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x742): 138.463

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 3310.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x743x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x743x2048): 70.696
Elapsed time for attention_prob_times_values (128x2048x2048x743): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x743): 86.202

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1881.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x744x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x744x2048): 193.894
Elapsed time for attention_prob_times_values (128x2048x2048x744): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x744): 225.544

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 5056.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x745x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x745x2048): 84.723
Elapsed time for attention_prob_times_values (128x2048x2048x745): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x745): 86.755

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2081.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x746x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x746x2048): 128.061
Elapsed time for attention_prob_times_values (128x2048x2048x746): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x746): 141.783

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 3271.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x747x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x747x2048): 85.625
Elapsed time for attention_prob_times_values (128x2048x2048x747): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x747): 89.549

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2131.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x748x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x748x2048): 135.812
Elapsed time for attention_prob_times_values (128x2048x2048x748): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x748): 145.673

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3426.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x749x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x749x2048): 84.826
Elapsed time for attention_prob_times_values (128x2048x2048x749): 0.0354
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x749): 22.717

Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 874.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0449
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x750x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x750x2048): 139.638
Elapsed time for attention_prob_times_values (128x2048x2048x750): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x750): 143.421

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 3458.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x751x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x751x2048): 85.364
Elapsed time for attention_prob_times_values (128x2048x2048x751): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x751): 89.179

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2134.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x752x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x752x2048): 202.373
Elapsed time for attention_prob_times_values (128x2048x2048x752): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x752): 233.695

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 5314.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x753x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x753x2048): 85.743
Elapsed time for attention_prob_times_values (128x2048x2048x753): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x753): 89.285

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2145.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x754x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x754x2048): 137.178
Elapsed time for attention_prob_times_values (128x2048x2048x754): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x754): 148.889

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3507.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x755x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x755x2048): 85.248
Elapsed time for attention_prob_times_values (128x2048x2048x755): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x755): 90.482

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2159.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x756x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x756x2048): 140.761
Elapsed time for attention_prob_times_values (128x2048x2048x756): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x756): 147.922

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3552.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x757x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x757x2048): 86.251
Elapsed time for attention_prob_times_values (128x2048x2048x757): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x757): 88.326

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2151.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x758x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x758x2048): 138.561
Elapsed time for attention_prob_times_values (128x2048x2048x758): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x758): 144.149

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 3488.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x759x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x759x2048): 85.039
Elapsed time for attention_prob_times_values (128x2048x2048x759): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x759): 89.805

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2159.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x760x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x760x2048): 195.630
Elapsed time for attention_prob_times_values (128x2048x2048x760): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x760): 225.789

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 5188.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x761x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x761x2048): 84.619
Elapsed time for attention_prob_times_values (128x2048x2048x761): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x761): 87.212

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2128.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x762x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x762x2048): 140.449
Elapsed time for attention_prob_times_values (128x2048x2048x762): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x762): 148.573

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 3582.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x763x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x763x2048): 85.100
Elapsed time for attention_prob_times_values (128x2048x2048x763): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x763): 91.545

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2191.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x764x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x764x2048): 141.513
Elapsed time for attention_prob_times_values (128x2048x2048x764): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x764): 150.798

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 3631.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x765x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x765x2048): 85.908
Elapsed time for attention_prob_times_values (128x2048x2048x765): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x765): 92.326

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2216.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x766x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x766x2048): 142.954
Elapsed time for attention_prob_times_values (128x2048x2048x766): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x766): 152.247

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 3677.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x767x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x767x2048): 86.324
Elapsed time for attention_prob_times_values (128x2048x2048x767): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x767): 92.308

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2227.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x768x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x768x2048): 208.284
Elapsed time for attention_prob_times_values (128x2048x2048x768): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x768): 234.048

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 5510.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x769x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x769x2048): 85.865
Elapsed time for attention_prob_times_values (128x2048x2048x769): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x769): 77.123

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2034.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x770x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x770x2048): 134.707
Elapsed time for attention_prob_times_values (128x2048x2048x770): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x770): 123.316

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3227.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x771x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x771x2048): 84.626
Elapsed time for attention_prob_times_values (128x2048x2048x771): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x771): 77.862

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2035.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x772x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x772x2048): 134.109
Elapsed time for attention_prob_times_values (128x2048x2048x772): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x772): 127.327

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3282.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x773x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x773x2048): 81.764
Elapsed time for attention_prob_times_values (128x2048x2048x773): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x773): 78.384

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2013.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x774x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x774x2048): 134.100
Elapsed time for attention_prob_times_values (128x2048x2048x774): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x774): 125.664

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3267.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x775x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x775x2048): 84.380
Elapsed time for attention_prob_times_values (128x2048x2048x775): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x775): 64.897

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1850.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x776x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x776x2048): 189.903
Elapsed time for attention_prob_times_values (128x2048x2048x776): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x776): 182.926

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 4705.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x777x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x777x2048): 81.822
Elapsed time for attention_prob_times_values (128x2048x2048x777): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x777): 75.751

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1988.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x778x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x778x2048): 135.475
Elapsed time for attention_prob_times_values (128x2048x2048x778): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x778): 120.298

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3225.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x779x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x779x2048): 79.305
Elapsed time for attention_prob_times_values (128x2048x2048x779): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x779): 75.639

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1962.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x780x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x780x2048): 135.295
Elapsed time for attention_prob_times_values (128x2048x2048x780): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x780): 127.725

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3334.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x781x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x781x2048): 85.581
Elapsed time for attention_prob_times_values (128x2048x2048x781): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x781): 75.646

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2040.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x782x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x782x2048): 136.161
Elapsed time for attention_prob_times_values (128x2048x2048x782): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x782): 125.582

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3323.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x783x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x783x2048): 85.966
Elapsed time for attention_prob_times_values (128x2048x2048x783): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x783): 76.808

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2066.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x784x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x784x2048): 202.177
Elapsed time for attention_prob_times_values (128x2048x2048x784): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x784): 182.302

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 4889.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x785x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x785x2048): 86.874
Elapsed time for attention_prob_times_values (128x2048x2048x785): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x785): 75.599

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2064.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x786x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x786x2048): 136.599
Elapsed time for attention_prob_times_values (128x2048x2048x786): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x786): 125.811

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3348.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x787x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x787x2048): 87.521
Elapsed time for attention_prob_times_values (128x2048x2048x787): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x787): 78.310

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2115.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x788x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x788x2048): 136.002
Elapsed time for attention_prob_times_values (128x2048x2048x788): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x788): 122.674

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3305.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x789x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x789x2048): 86.351
Elapsed time for attention_prob_times_values (128x2048x2048x789): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x789): 78.376

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2108.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x790x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x790x2048): 136.060
Elapsed time for attention_prob_times_values (128x2048x2048x790): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x790): 123.993

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3332.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x791x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x791x2048): 86.726
Elapsed time for attention_prob_times_values (128x2048x2048x791): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x791): 76.215

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2086.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x792x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x792x2048): 194.591
Elapsed time for attention_prob_times_values (128x2048x2048x792): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x792): 178.120

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 4789.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x793x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x793x2048): 86.564
Elapsed time for attention_prob_times_values (128x2048x2048x793): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x793): 76.376

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2092.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x794x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x794x2048): 135.665
Elapsed time for attention_prob_times_values (128x2048x2048x794): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x794): 127.623

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3394.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x795x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x795x2048): 86.278
Elapsed time for attention_prob_times_values (128x2048x2048x795): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x795): 78.644

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2126.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x796x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x796x2048): 139.240
Elapsed time for attention_prob_times_values (128x2048x2048x796): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x796): 125.402

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3414.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x797x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x797x2048): 86.920
Elapsed time for attention_prob_times_values (128x2048x2048x797): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x797): 80.061

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2159.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x798x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x798x2048): 140.041
Elapsed time for attention_prob_times_values (128x2048x2048x798): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x798): 129.332

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3487.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x799x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x799x2048): 86.921
Elapsed time for attention_prob_times_values (128x2048x2048x799): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x799): 78.272

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2139.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x800x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x800x2048): 208.972
Elapsed time for attention_prob_times_values (128x2048x2048x800): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x800): 183.231

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 5076.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x801x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x801x2048): 86.747
Elapsed time for attention_prob_times_values (128x2048x2048x801): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x801): 77.949

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2137.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x802x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x802x2048): 137.827
Elapsed time for attention_prob_times_values (128x2048x2048x802): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x802): 125.308

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3421.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x803x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x803x2048): 85.955
Elapsed time for attention_prob_times_values (128x2048x2048x803): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x803): 79.109

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2149.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x804x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x804x2048): 138.413
Elapsed time for attention_prob_times_values (128x2048x2048x804): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x804): 126.177

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3448.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x805x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x805x2048): 84.270
Elapsed time for attention_prob_times_values (128x2048x2048x805): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x805): 78.039

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2119.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x806x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x806x2048): 138.145
Elapsed time for attention_prob_times_values (128x2048x2048x806): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x806): 125.576

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 3445.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x807x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x807x2048): 85.934
Elapsed time for attention_prob_times_values (128x2048x2048x807): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x807): 76.764

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2126.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x808x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x808x2048): 200.778
Elapsed time for attention_prob_times_values (128x2048x2048x808): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x808): 181.166

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 4999.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x809x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x809x2048): 84.731
Elapsed time for attention_prob_times_values (128x2048x2048x809): 0.0291
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x809): 29.820

Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 1159.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0394
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x810x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x810x2048): 139.212
Elapsed time for attention_prob_times_values (128x2048x2048x810): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x810): 130.245

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3541.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x811x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x811x2048): 85.820
Elapsed time for attention_prob_times_values (128x2048x2048x811): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x811): 80.275

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2185.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x812x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x812x2048): 141.401
Elapsed time for attention_prob_times_values (128x2048x2048x812): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x812): 128.579

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3552.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x813x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x813x2048): 84.818
Elapsed time for attention_prob_times_values (128x2048x2048x813): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x813): 79.525

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2167.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x814x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x814x2048): 138.924
Elapsed time for attention_prob_times_values (128x2048x2048x814): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x814): 128.368

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3527.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x815x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x815x2048): 87.688
Elapsed time for attention_prob_times_values (128x2048x2048x815): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x815): 78.589

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2193.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x816x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x816x2048): 203.361
Elapsed time for attention_prob_times_values (128x2048x2048x816): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x816): 187.593

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5171.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x817x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x817x2048): 88.414
Elapsed time for attention_prob_times_values (128x2048x2048x817): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x817): 50.022

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1695.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x818x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x818x2048): 140.480
Elapsed time for attention_prob_times_values (128x2048x2048x818): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x818): 130.193

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3589.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x819x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x819x2048): 88.660
Elapsed time for attention_prob_times_values (128x2048x2048x819): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x819): 81.628

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2260.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x820x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x820x2048): 139.351
Elapsed time for attention_prob_times_values (128x2048x2048x820): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x820): 123.535

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3487.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x821x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x821x2048): 90.497
Elapsed time for attention_prob_times_values (128x2048x2048x821): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x821): 79.362

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2254.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x822x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x822x2048): 139.950
Elapsed time for attention_prob_times_values (128x2048x2048x822): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x822): 129.182

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3585.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x823x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x823x2048): 88.264
Elapsed time for attention_prob_times_values (128x2048x2048x823): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x823): 78.982

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2227.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x824x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x824x2048): 174.914
Elapsed time for attention_prob_times_values (128x2048x2048x824): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x824): 188.479

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 4853.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x825x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x825x2048): 87.932
Elapsed time for attention_prob_times_values (128x2048x2048x825): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x825): 80.974

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2257.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x826x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x826x2048): 142.500
Elapsed time for attention_prob_times_values (128x2048x2048x826): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x826): 127.966

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 3615.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x827x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x827x2048): 90.530
Elapsed time for attention_prob_times_values (128x2048x2048x827): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x827): 84.350

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2344.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x828x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x828x2048): 140.940
Elapsed time for attention_prob_times_values (128x2048x2048x828): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x828): 132.770

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3674.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x829x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x829x2048): 90.496
Elapsed time for attention_prob_times_values (128x2048x2048x829): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x829): 82.586

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2323.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x830x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x830x2048): 143.885
Elapsed time for attention_prob_times_values (128x2048x2048x830): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x830): 134.712

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3748.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x831x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x831x2048): 88.448
Elapsed time for attention_prob_times_values (128x2048x2048x831): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x831): 82.842

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2307.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x832x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x832x2048): 213.845
Elapsed time for attention_prob_times_values (128x2048x2048x832): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x832): 198.338

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 5556.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x833x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x833x2048): 89.515
Elapsed time for attention_prob_times_values (128x2048x2048x833): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x833): 81.326

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2303.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x834x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x834x2048): 137.729
Elapsed time for attention_prob_times_values (128x2048x2048x834): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x834): 99.544

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 3127.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x835x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x835x2048): 88.754
Elapsed time for attention_prob_times_values (128x2048x2048x835): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x835): 82.208

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2312.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x836x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x836x2048): 137.189
Elapsed time for attention_prob_times_values (128x2048x2048x836): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x836): 131.990

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3649.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x837x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x837x2048): 88.069
Elapsed time for attention_prob_times_values (128x2048x2048x837): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x837): 82.750

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2317.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x838x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x838x2048): 137.990
Elapsed time for attention_prob_times_values (128x2048x2048x838): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x838): 133.423

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3688.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x839x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x839x2048): 88.269
Elapsed time for attention_prob_times_values (128x2048x2048x839): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x839): 81.295

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2303.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x840x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x840x2048): 198.626
Elapsed time for attention_prob_times_values (128x2048x2048x840): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x840): 184.457

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 5212.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x841x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x841x2048): 82.940
Elapsed time for attention_prob_times_values (128x2048x2048x841): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x841): 81.119

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2237.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x842x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x842x2048): 138.512
Elapsed time for attention_prob_times_values (128x2048x2048x842): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x842): 132.108

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3693.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x843x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x843x2048): 88.827
Elapsed time for attention_prob_times_values (128x2048x2048x843): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x843): 84.570

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2369.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x844x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x844x2048): 139.385
Elapsed time for attention_prob_times_values (128x2048x2048x844): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x844): 131.188

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3700.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x845x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x845x2048): 89.112
Elapsed time for attention_prob_times_values (128x2048x2048x845): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x845): 84.803

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2381.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x846x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x846x2048): 139.547
Elapsed time for attention_prob_times_values (128x2048x2048x846): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x846): 129.615

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3687.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x847x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x847x2048): 88.468
Elapsed time for attention_prob_times_values (128x2048x2048x847): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x847): 82.132

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2339.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x848x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x848x2048): 206.871
Elapsed time for attention_prob_times_values (128x2048x2048x848): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x848): 193.360

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 5496.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x849x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x849x2048): 88.149
Elapsed time for attention_prob_times_values (128x2048x2048x849): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x849): 80.085

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2310.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x850x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x850x2048): 136.936
Elapsed time for attention_prob_times_values (128x2048x2048x850): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x850): 130.624

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 3685.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x851x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x851x2048): 88.921
Elapsed time for attention_prob_times_values (128x2048x2048x851): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x851): 83.575

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2377.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x852x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x852x2048): 139.840
Elapsed time for attention_prob_times_values (128x2048x2048x852): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x852): 130.225

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3725.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x853x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x853x2048): 88.357
Elapsed time for attention_prob_times_values (128x2048x2048x853): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x853): 85.463

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2402.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x854x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x854x2048): 139.028
Elapsed time for attention_prob_times_values (128x2048x2048x854): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x854): 134.690

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3788.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x855x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x855x2048): 88.950
Elapsed time for attention_prob_times_values (128x2048x2048x855): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x855): 82.298

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2369.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x856x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x856x2048): 198.082
Elapsed time for attention_prob_times_values (128x2048x2048x856): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x856): 187.751

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 5349.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x857x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x857x2048): 88.419
Elapsed time for attention_prob_times_values (128x2048x2048x857): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x857): 83.209

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2381.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x858x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x858x2048): 137.307
Elapsed time for attention_prob_times_values (128x2048x2048x858): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x858): 134.053

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3773.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x859x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x859x2048): 88.171
Elapsed time for attention_prob_times_values (128x2048x2048x859): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x859): 85.564

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2418.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x860x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x860x2048): 140.386
Elapsed time for attention_prob_times_values (128x2048x2048x860): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x860): 133.307

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3812.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x861x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x861x2048): 88.757
Elapsed time for attention_prob_times_values (128x2048x2048x861): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x861): 84.500

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2416.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x862x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x862x2048): 139.916
Elapsed time for attention_prob_times_values (128x2048x2048x862): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x862): 137.090

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3869.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x863x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x863x2048): 90.883
Elapsed time for attention_prob_times_values (128x2048x2048x863): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x863): 84.868

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2454.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x864x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x864x2048): 214.351
Elapsed time for attention_prob_times_values (128x2048x2048x864): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x864): 194.536

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 5710.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x865x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x865x2048): 90.028
Elapsed time for attention_prob_times_values (128x2048x2048x865): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x865): 84.524

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2444.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x866x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x866x2048): 141.319
Elapsed time for attention_prob_times_values (128x2048x2048x866): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x866): 135.689

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3885.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x867x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x867x2048): 87.240
Elapsed time for attention_prob_times_values (128x2048x2048x867): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x867): 87.420

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2453.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x868x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x868x2048): 142.021
Elapsed time for attention_prob_times_values (128x2048x2048x868): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x868): 137.452

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3929.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x869x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x869x2048): 86.765
Elapsed time for attention_prob_times_values (128x2048x2048x869): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x869): 87.053

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2447.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x870x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x870x2048): 142.644
Elapsed time for attention_prob_times_values (128x2048x2048x870): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x870): 137.812

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3951.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x871x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x871x2048): 88.515
Elapsed time for attention_prob_times_values (128x2048x2048x871): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x871): 83.110

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2419.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x872x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x872x2048): 199.895
Elapsed time for attention_prob_times_values (128x2048x2048x872): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x872): 201.423

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 5668.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x873x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x873x2048): 87.018
Elapsed time for attention_prob_times_values (128x2048x2048x873): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x873): 83.056

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2403.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x874x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x874x2048): 141.715
Elapsed time for attention_prob_times_values (128x2048x2048x874): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x874): 136.914

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3943.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x875x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x875x2048): 87.370
Elapsed time for attention_prob_times_values (128x2048x2048x875): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x875): 88.247

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2488.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x876x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x876x2048): 142.868
Elapsed time for attention_prob_times_values (128x2048x2048x876): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x876): 130.968

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 3877.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x877x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x877x2048): 89.139
Elapsed time for attention_prob_times_values (128x2048x2048x877): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x877): 86.895

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2499.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x878x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x878x2048): 144.381
Elapsed time for attention_prob_times_values (128x2048x2048x878): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x878): 138.146

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 4015.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x879x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x879x2048): 89.540
Elapsed time for attention_prob_times_values (128x2048x2048x879): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x879): 86.071

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2498.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x880x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x880x2048): 185.776
Elapsed time for attention_prob_times_values (128x2048x2048x880): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x880): 204.035

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 5542.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x881x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x881x2048): 89.874
Elapsed time for attention_prob_times_values (128x2048x2048x881): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x881): 85.589

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2501.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x882x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x882x2048): 143.909
Elapsed time for attention_prob_times_values (128x2048x2048x882): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x882): 138.278

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 4028.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x883x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x883x2048): 90.141
Elapsed time for attention_prob_times_values (128x2048x2048x883): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x883): 89.333

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2565.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x884x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x884x2048): 145.092
Elapsed time for attention_prob_times_values (128x2048x2048x884): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x884): 139.348

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 4069.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x885x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x885x2048): 90.362
Elapsed time for attention_prob_times_values (128x2048x2048x885): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x885): 87.936

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2554.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x886x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x886x2048): 146.585
Elapsed time for attention_prob_times_values (128x2048x2048x886): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x886): 138.712

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 4089.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x887x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x887x2048): 89.123
Elapsed time for attention_prob_times_values (128x2048x2048x887): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x887): 85.978

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2513.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x888x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x888x2048): 203.129
Elapsed time for attention_prob_times_values (128x2048x2048x888): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x888): 204.751

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 5863.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x889x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x889x2048): 90.663
Elapsed time for attention_prob_times_values (128x2048x2048x889): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x889): 87.513

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2563.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x890x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x890x2048): 144.502
Elapsed time for attention_prob_times_values (128x2048x2048x890): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x890): 138.257

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4071.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x891x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x891x2048): 91.730
Elapsed time for attention_prob_times_values (128x2048x2048x891): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x891): 90.902

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2633.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x892x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x892x2048): 142.606
Elapsed time for attention_prob_times_values (128x2048x2048x892): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x892): 140.673

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4089.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x893x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x893x2048): 90.398
Elapsed time for attention_prob_times_values (128x2048x2048x893): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x893): 90.242

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2610.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x894x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x894x2048): 147.093
Elapsed time for attention_prob_times_values (128x2048x2048x894): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x894): 136.403

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4096.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x895x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x895x2048): 92.224
Elapsed time for attention_prob_times_values (128x2048x2048x895): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x895): 89.108

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2625.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x896x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x896x2048): 212.649
Elapsed time for attention_prob_times_values (128x2048x2048x896): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x896): 198.496

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 5954.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x897x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x897x2048): 91.376
Elapsed time for attention_prob_times_values (128x2048x2048x897): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x897): 88.635

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2612.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x898x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x898x2048): 136.000
Elapsed time for attention_prob_times_values (128x2048x2048x898): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x898): 139.919

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 4008.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x899x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x899x2048): 90.840
Elapsed time for attention_prob_times_values (128x2048x2048x899): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x899): 88.093

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2602.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x900x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x900x2048): 139.825
Elapsed time for attention_prob_times_values (128x2048x2048x900): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x900): 140.212

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4078.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x901x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x901x2048): 88.770
Elapsed time for attention_prob_times_values (128x2048x2048x901): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x901): 88.935

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2590.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x902x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x902x2048): 140.582
Elapsed time for attention_prob_times_values (128x2048x2048x902): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x902): 140.611

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4103.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x903x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x903x2048): 86.765
Elapsed time for attention_prob_times_values (128x2048x2048x903): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x903): 87.198

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2541.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x904x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x904x2048): 199.005
Elapsed time for attention_prob_times_values (128x2048x2048x904): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x904): 207.357

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 5940.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x905x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x905x2048): 89.144
Elapsed time for attention_prob_times_values (128x2048x2048x905): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x905): 86.065

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2564.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x906x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x906x2048): 141.039
Elapsed time for attention_prob_times_values (128x2048x2048x906): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x906): 140.201

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4121.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x907x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x907x2048): 89.838
Elapsed time for attention_prob_times_values (128x2048x2048x907): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x907): 86.834

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2591.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x908x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x908x2048): 142.548
Elapsed time for attention_prob_times_values (128x2048x2048x908): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x908): 140.888

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4162.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x909x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x909x2048): 87.904
Elapsed time for attention_prob_times_values (128x2048x2048x909): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x909): 88.063

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2587.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x910x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x910x2048): 139.787
Elapsed time for attention_prob_times_values (128x2048x2048x910): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x910): 141.344

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 4137.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x911x2048): 0.0210
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x911x2048): 46.630
Elapsed time for attention_prob_times_values (128x2048x2048x911): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x911): 86.852

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 1788.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x912x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x912x2048): 201.614
Elapsed time for attention_prob_times_values (128x2048x2048x912): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x912): 204.475

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 5989.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x913x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x913x2048): 89.369
Elapsed time for attention_prob_times_values (128x2048x2048x913): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x913): 85.789

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2585.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x914x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x914x2048): 140.560
Elapsed time for attention_prob_times_values (128x2048x2048x914): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x914): 138.876

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 4130.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x915x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x915x2048): 87.481
Elapsed time for attention_prob_times_values (128x2048x2048x915): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x915): 86.900

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2580.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x916x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x916x2048): 138.465
Elapsed time for attention_prob_times_values (128x2048x2048x916): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x916): 135.278

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4054.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x917x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x917x2048): 87.027
Elapsed time for attention_prob_times_values (128x2048x2048x917): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x917): 86.318

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2570.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x918x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x918x2048): 140.408
Elapsed time for attention_prob_times_values (128x2048x2048x918): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x918): 140.037

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 4162.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x919x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x919x2048): 88.342
Elapsed time for attention_prob_times_values (128x2048x2048x919): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x919): 82.618

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2537.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x920x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x920x2048): 201.543
Elapsed time for attention_prob_times_values (128x2048x2048x920): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x920): 200.489

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 5980.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x921x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x921x2048): 89.319
Elapsed time for attention_prob_times_values (128x2048x2048x921): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x921): 84.888

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2592.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x922x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x922x2048): 139.832
Elapsed time for attention_prob_times_values (128x2048x2048x922): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x922): 139.344

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4161.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x923x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x923x2048): 89.038
Elapsed time for attention_prob_times_values (128x2048x2048x923): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x923): 88.139

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2643.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x924x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x924x2048): 142.878
Elapsed time for attention_prob_times_values (128x2048x2048x924): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x924): 140.490

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 4232.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x925x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x925x2048): 89.651
Elapsed time for attention_prob_times_values (128x2048x2048x925): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x925): 86.821

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2638.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x926x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x926x2048): 140.132
Elapsed time for attention_prob_times_values (128x2048x2048x926): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x926): 136.822

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4145.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x927x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x927x2048): 90.128
Elapsed time for attention_prob_times_values (128x2048x2048x927): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x927): 85.377

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2627.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x928x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x928x2048): 211.302
Elapsed time for attention_prob_times_values (128x2048x2048x928): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x928): 209.302

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 6308.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x929x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x929x2048): 88.377
Elapsed time for attention_prob_times_values (128x2048x2048x929): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x929): 86.040

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2618.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x930x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x930x2048): 140.370
Elapsed time for attention_prob_times_values (128x2048x2048x930): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x930): 136.651

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4163.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x931x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x931x2048): 83.800
Elapsed time for attention_prob_times_values (128x2048x2048x931): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x931): 85.912

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2553.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x932x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x932x2048): 141.559
Elapsed time for attention_prob_times_values (128x2048x2048x932): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x932): 138.623

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4219.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x933x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x933x2048): 85.168
Elapsed time for attention_prob_times_values (128x2048x2048x933): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x933): 84.439

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2557.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x934x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x934x2048): 122.445
Elapsed time for attention_prob_times_values (128x2048x2048x934): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x934): 137.575

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 3911.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x935x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x935x2048): 87.880
Elapsed time for attention_prob_times_values (128x2048x2048x935): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x935): 86.267

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2631.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x936x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x936x2048): 202.662
Elapsed time for attention_prob_times_values (128x2048x2048x936): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x936): 202.916

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 6134.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x937x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x937x2048): 86.115
Elapsed time for attention_prob_times_values (128x2048x2048x937): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x937): 85.126

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2592.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x938x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x938x2048): 143.711
Elapsed time for attention_prob_times_values (128x2048x2048x938): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x938): 137.696

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4263.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x939x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x939x2048): 87.370
Elapsed time for attention_prob_times_values (128x2048x2048x939): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x939): 88.271

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2664.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x940x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x940x2048): 142.313
Elapsed time for attention_prob_times_values (128x2048x2048x940): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x940): 141.085

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4304.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x941x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x941x2048): 88.487
Elapsed time for attention_prob_times_values (128x2048x2048x941): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x941): 87.138

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2669.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x942x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x942x2048): 145.004
Elapsed time for attention_prob_times_values (128x2048x2048x942): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x942): 139.191

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4323.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x943x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x943x2048): 88.409
Elapsed time for attention_prob_times_values (128x2048x2048x943): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x943): 84.374

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2630.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x944x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x944x2048): 207.102
Elapsed time for attention_prob_times_values (128x2048x2048x944): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x944): 216.510

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 6456.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x945x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x945x2048): 86.974
Elapsed time for attention_prob_times_values (128x2048x2048x945): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x945): 85.087

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2626.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x946x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x946x2048): 144.833
Elapsed time for attention_prob_times_values (128x2048x2048x946): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x946): 139.297

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4340.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x947x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x947x2048): 87.893
Elapsed time for attention_prob_times_values (128x2048x2048x947): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x947): 88.116

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2692.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x948x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x948x2048): 145.075
Elapsed time for attention_prob_times_values (128x2048x2048x948): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x948): 140.724

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4375.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x949x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x949x2048): 88.586
Elapsed time for attention_prob_times_values (128x2048x2048x949): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x949): 86.762

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2687.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x950x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x950x2048): 145.016
Elapsed time for attention_prob_times_values (128x2048x2048x950): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x950): 139.644

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4366.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x951x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x951x2048): 87.598
Elapsed time for attention_prob_times_values (128x2048x2048x951): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x951): 85.729

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2661.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x952x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x952x2048): 205.248
Elapsed time for attention_prob_times_values (128x2048x2048x952): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x952): 203.669

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 6286.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x953x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x953x2048): 87.336
Elapsed time for attention_prob_times_values (128x2048x2048x953): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x953): 85.293

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2656.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x954x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x954x2048): 145.538
Elapsed time for attention_prob_times_values (128x2048x2048x954): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x954): 141.381

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4419.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x955x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x955x2048): 87.283
Elapsed time for attention_prob_times_values (128x2048x2048x955): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x955): 88.001

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2703.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x956x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x956x2048): 145.631
Elapsed time for attention_prob_times_values (128x2048x2048x956): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x956): 142.583

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4448.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x957x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x957x2048): 87.512
Elapsed time for attention_prob_times_values (128x2048x2048x957): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x957): 88.918

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2726.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x958x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x958x2048): 142.171
Elapsed time for attention_prob_times_values (128x2048x2048x958): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x958): 141.018

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 4380.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x959x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x959x2048): 87.894
Elapsed time for attention_prob_times_values (128x2048x2048x959): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x959): 88.057

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2724.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x960x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x960x2048): 213.800
Elapsed time for attention_prob_times_values (128x2048x2048x960): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x960): 215.526

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 6654.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x961x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x961x2048): 87.602
Elapsed time for attention_prob_times_values (128x2048x2048x961): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x961): 87.760

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2720.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x962x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x962x2048): 141.709
Elapsed time for attention_prob_times_values (128x2048x2048x962): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x962): 139.829

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4372.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x963x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x963x2048): 86.745
Elapsed time for attention_prob_times_values (128x2048x2048x963): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x963): 87.388

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2707.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x964x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x964x2048): 141.527
Elapsed time for attention_prob_times_values (128x2048x2048x964): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x964): 142.511

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4420.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x965x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x965x2048): 85.923
Elapsed time for attention_prob_times_values (128x2048x2048x965): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x965): 86.949

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2692.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x966x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x966x2048): 142.238
Elapsed time for attention_prob_times_values (128x2048x2048x966): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x966): 141.737

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4428.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x967x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x967x2048): 85.836
Elapsed time for attention_prob_times_values (128x2048x2048x967): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x967): 86.777

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2694.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x968x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x968x2048): 205.007
Elapsed time for attention_prob_times_values (128x2048x2048x968): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x968): 208.438

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 6459.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x969x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x969x2048): 87.374
Elapsed time for attention_prob_times_values (128x2048x2048x969): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x969): 86.885

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2725.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x970x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x970x2048): 140.692
Elapsed time for attention_prob_times_values (128x2048x2048x970): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x970): 140.389

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 4400.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x971x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x971x2048): 88.452
Elapsed time for attention_prob_times_values (128x2048x2048x971): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x971): 89.065

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2781.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x972x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x972x2048): 142.967
Elapsed time for attention_prob_times_values (128x2048x2048x972): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x972): 138.057

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4407.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x973x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x973x2048): 88.636
Elapsed time for attention_prob_times_values (128x2048x2048x973): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x973): 89.123

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2791.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x974x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x974x2048): 142.595
Elapsed time for attention_prob_times_values (128x2048x2048x974): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x974): 141.642

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4467.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x975x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x975x2048): 89.483
Elapsed time for attention_prob_times_values (128x2048x2048x975): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x975): 88.636

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2802.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x976x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x976x2048): 205.523
Elapsed time for attention_prob_times_values (128x2048x2048x976): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x976): 220.625

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 6703.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x977x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x977x2048): 90.085
Elapsed time for attention_prob_times_values (128x2048x2048x977): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x977): 88.826

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2820.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x978x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x978x2048): 143.013
Elapsed time for attention_prob_times_values (128x2048x2048x978): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x978): 142.463

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4505.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x979x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x979x2048): 90.401
Elapsed time for attention_prob_times_values (128x2048x2048x979): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x979): 88.519

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2826.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x980x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x980x2048): 143.403
Elapsed time for attention_prob_times_values (128x2048x2048x980): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x980): 142.367

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4518.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x981x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x981x2048): 90.893
Elapsed time for attention_prob_times_values (128x2048x2048x981): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x981): 88.479

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2838.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x982x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x982x2048): 144.038
Elapsed time for attention_prob_times_values (128x2048x2048x982): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x982): 144.878

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4577.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x983x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x983x2048): 89.662
Elapsed time for attention_prob_times_values (128x2048x2048x983): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x983): 89.341

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2838.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x984x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x984x2048): 203.937
Elapsed time for attention_prob_times_values (128x2048x2048x984): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x984): 212.350

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 6605.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x985x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x985x2048): 89.174
Elapsed time for attention_prob_times_values (128x2048x2048x985): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x985): 89.568

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2840.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x986x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x986x2048): 143.609
Elapsed time for attention_prob_times_values (128x2048x2048x986): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x986): 144.314

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4579.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x987x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x987x2048): 90.714
Elapsed time for attention_prob_times_values (128x2048x2048x987): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x987): 90.764

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2889.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x988x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x988x2048): 144.461
Elapsed time for attention_prob_times_values (128x2048x2048x988): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x988): 146.918

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4643.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x989x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x989x2048): 90.832
Elapsed time for attention_prob_times_values (128x2048x2048x989): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x989): 92.264

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2920.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x990x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x990x2048): 143.132
Elapsed time for attention_prob_times_values (128x2048x2048x990): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x990): 145.027

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 4601.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x991x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x991x2048): 89.950
Elapsed time for attention_prob_times_values (128x2048x2048x991): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x991): 91.663

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2902.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x992x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x992x2048): 212.590
Elapsed time for attention_prob_times_values (128x2048x2048x992): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x992): 221.705

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 6945.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x993x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x993x2048): 91.588
Elapsed time for attention_prob_times_values (128x2048x2048x993): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x993): 91.083

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2925.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x994x2048): 0.0785
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x994x2048): 13.597
Elapsed time for attention_prob_times_values (128x2048x2048x994): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x994): 147.962

Attention duration (in seconds): 0.0857
Attention throughput (in TFLOP/s): 798.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0857
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x995x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x995x2048): 91.404
Elapsed time for attention_prob_times_values (128x2048x2048x995): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x995): 90.850

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2924.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x996x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x996x2048): 146.239
Elapsed time for attention_prob_times_values (128x2048x2048x996): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x996): 149.057

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 4742.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x997x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x997x2048): 90.845
Elapsed time for attention_prob_times_values (128x2048x2048x997): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x997): 92.110

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2941.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x998x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x998x2048): 143.542
Elapsed time for attention_prob_times_values (128x2048x2048x998): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x998): 147.980

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4690.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x999x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x999x2048): 91.813
Elapsed time for attention_prob_times_values (128x2048x2048x999): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x999): 91.250

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2949.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1000x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1000x2048): 206.048
Elapsed time for attention_prob_times_values (128x2048x2048x1000): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1000): 213.370

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 6761.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1001x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1001x2048): 91.159
Elapsed time for attention_prob_times_values (128x2048x2048x1001): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1001): 90.182

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2926.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1002x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1002x2048): 145.081
Elapsed time for attention_prob_times_values (128x2048x2048x1002): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1002): 142.709

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 4649.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1003x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1003x2048): 91.619
Elapsed time for attention_prob_times_values (128x2048x2048x1003): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1003): 90.378

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2943.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1004x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1004x2048): 146.264
Elapsed time for attention_prob_times_values (128x2048x2048x1004): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1004): 149.757

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4791.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1005x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1005x2048): 82.150
Elapsed time for attention_prob_times_values (128x2048x2048x1005): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1005): 93.627

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2835.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1006x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1006x2048): 145.263
Elapsed time for attention_prob_times_values (128x2048x2048x1006): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1006): 148.215

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4759.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1007x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1007x2048): 90.549
Elapsed time for attention_prob_times_values (128x2048x2048x1007): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1007): 92.339

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2968.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1008x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1008x2048): 206.920
Elapsed time for attention_prob_times_values (128x2048x2048x1008): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1008): 214.457

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 6845.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1009x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1009x2048): 92.556
Elapsed time for attention_prob_times_values (128x2048x2048x1009): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1009): 93.634

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 3028.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1010x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1010x2048): 145.106
Elapsed time for attention_prob_times_values (128x2048x2048x1010): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1010): 146.929

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4754.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1011x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1011x2048): 94.120
Elapsed time for attention_prob_times_values (128x2048x2048x1011): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1011): 95.640

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 3092.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1012x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1012x2048): 151.311
Elapsed time for attention_prob_times_values (128x2048x2048x1012): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1012): 154.355

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4985.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1013x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1013x2048): 93.004
Elapsed time for attention_prob_times_values (128x2048x2048x1013): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1013): 93.879

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 3051.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1014x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1014x2048): 151.179
Elapsed time for attention_prob_times_values (128x2048x2048x1014): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1014): 154.003

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4987.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1015x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1015x2048): 93.051
Elapsed time for attention_prob_times_values (128x2048x2048x1015): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1015): 94.991

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 3075.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1016x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1016x2048): 205.712
Elapsed time for attention_prob_times_values (128x2048x2048x1016): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1016): 227.713

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 7079.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1017x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1017x2048): 93.522
Elapsed time for attention_prob_times_values (128x2048x2048x1017): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1017): 95.516

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 3098.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1018x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1018x2048): 151.524
Elapsed time for attention_prob_times_values (128x2048x2048x1018): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1018): 157.036

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 5060.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1019x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1019x2048): 95.789
Elapsed time for attention_prob_times_values (128x2048x2048x1019): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1019): 98.040

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 3182.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1020x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1020x2048): 153.005
Elapsed time for attention_prob_times_values (128x2048x2048x1020): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1020): 157.523

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 5103.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1021x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1021x2048): 87.433
Elapsed time for attention_prob_times_values (128x2048x2048x1021): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1021): 98.392

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 3046.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1022x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1022x2048): 152.336
Elapsed time for attention_prob_times_values (128x2048x2048x1022): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1022): 158.402

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 5115.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1023x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1023x2048): 95.612
Elapsed time for attention_prob_times_values (128x2048x2048x1023): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1023): 98.324

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 3196.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1024x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1024x2048): 214.547
Elapsed time for attention_prob_times_values (128x2048x2048x1024): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1024): 223.857

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 7230.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
