[2023-11-28 20:37:36,867] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+cu121 

[2023-11-28 20:37:42,037] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-11-28 20:37:42,037] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-11-28 20:37:42,497] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.0.0.92, master_port=6000
[2023-11-28 20:37:42,498] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-11-28 20:37:42,505] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 2.284
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 10.486
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 2.911
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 4.475
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 41.845
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 6.787
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 7.554
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 91.599
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 8.661
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 7.772
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 117.072
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 10.919
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 9.062
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 143.793
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 12.381
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 12.580
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 152.306
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 16.088
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 14.659
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 164.011
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 22.077
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 21.929
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 173.685
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 28.519
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 17.581
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 174.876
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 25.265
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 18.698
MLP duration (in seconds): 0.0012
MLP throughput (in TFLOP/s): 180.108
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 28.162
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 21.111
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 182.314
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 41.629
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 24.935
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 184.789
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 39.029
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 29.131
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 192.296
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 47.706
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 32.456
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 192.815
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 51.322
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 29.996
MLP duration (in seconds): 0.0025
MLP throughput (in TFLOP/s): 197.084
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 53.776
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 48.764
MLP duration (in seconds): 0.0027
MLP throughput (in TFLOP/s): 204.636
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 84.025
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 35.981
MLP duration (in seconds): 0.0031
MLP throughput (in TFLOP/s): 198.799
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 62.876
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 43.233
MLP duration (in seconds): 0.0034
MLP throughput (in TFLOP/s): 205.386
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 70.144
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 43.421
MLP duration (in seconds): 0.0038
MLP throughput (in TFLOP/s): 206.346
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 89.658
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 47.375
MLP duration (in seconds): 0.0042
MLP throughput (in TFLOP/s): 206.268
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 78.098
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 51.562
MLP duration (in seconds): 0.0046
MLP throughput (in TFLOP/s): 206.325
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 100.957
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 55.668
MLP duration (in seconds): 0.0050
MLP throughput (in TFLOP/s): 206.915
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 109.621
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 65.485
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 212.572
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 99.847
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 86.536
MLP duration (in seconds): 0.0060
MLP throughput (in TFLOP/s): 207.742
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 123.297
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 55.906
MLP duration (in seconds): 0.0063
MLP throughput (in TFLOP/s): 212.908
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 101.052
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 57.401
MLP duration (in seconds): 0.0067
MLP throughput (in TFLOP/s): 215.246
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 114.383
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 59.915
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 217.267
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 127.672
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 60.671
MLP duration (in seconds): 0.0079
MLP throughput (in TFLOP/s): 214.399
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 130.618
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 57.644
MLP duration (in seconds): 0.0083
MLP throughput (in TFLOP/s): 216.703
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 118.715
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 57.867
MLP duration (in seconds): 0.0088
MLP throughput (in TFLOP/s): 220.159
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 128.237
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 58.891
MLP duration (in seconds): 0.0095
MLP throughput (in TFLOP/s): 216.756
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 132.414
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 87.027
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 219.239
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 177.869
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 53.132
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 227.636
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 105.750
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 55.946
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 223.767
Transformer duration (in seconds): 0.0349
Transformer throughput (in TFLOP/s): 115.117
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 58.880
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 227.619
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 148.321
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 62.003
MLP duration (in seconds): 0.0120
MLP throughput (in TFLOP/s): 231.715
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 169.967
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 65.391
MLP duration (in seconds): 0.0126
MLP throughput (in TFLOP/s): 232.526
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 161.507
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 68.679
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 228.262
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 162.646
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 74.183
MLP duration (in seconds): 0.0141
MLP throughput (in TFLOP/s): 231.716
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 170.119
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 108.431
MLP duration (in seconds): 0.0148
MLP throughput (in TFLOP/s): 232.553
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 187.280
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 76.258
MLP duration (in seconds): 0.0157
MLP throughput (in TFLOP/s): 230.258
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 173.944
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 73.790
MLP duration (in seconds): 0.0164
MLP throughput (in TFLOP/s): 231.048
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 182.447
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 82.802
MLP duration (in seconds): 0.0170
MLP throughput (in TFLOP/s): 234.109
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 179.646
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 77.509
MLP duration (in seconds): 0.0180
MLP throughput (in TFLOP/s): 230.658
Transformer duration (in seconds): 0.0470
Transformer throughput (in TFLOP/s): 140.859
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 80.713
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 234.584
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 187.347
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 82.901
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 235.413
Transformer duration (in seconds): 0.0402
Transformer throughput (in TFLOP/s): 179.393
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 87.873
MLP duration (in seconds): 0.0204
MLP throughput (in TFLOP/s): 233.053
Transformer duration (in seconds): 0.0419
Transformer throughput (in TFLOP/s): 179.560
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 102.800
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 236.548
Transformer duration (in seconds): 0.0401
Transformer throughput (in TFLOP/s): 195.464
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 86.496
MLP duration (in seconds): 0.0224
MLP throughput (in TFLOP/s): 230.663
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 182.785
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 84.454
MLP duration (in seconds): 0.0227
MLP throughput (in TFLOP/s): 236.246
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 192.241
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0380
Attention throughput (in TFLOP/s): 85.036
MLP duration (in seconds): 0.0240
MLP throughput (in TFLOP/s): 232.942
Transformer duration (in seconds): 0.0571
Transformer throughput (in TFLOP/s): 154.415
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 89.886
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 237.221
Transformer duration (in seconds): 0.0535
Transformer throughput (in TFLOP/s): 171.170
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 86.959
MLP duration (in seconds): 0.0255
MLP throughput (in TFLOP/s): 236.699
Transformer duration (in seconds): 0.0502
Transformer throughput (in TFLOP/s): 189.150
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 90.081
MLP duration (in seconds): 0.0262
MLP throughput (in TFLOP/s): 239.051
Transformer duration (in seconds): 0.0570
Transformer throughput (in TFLOP/s): 172.962
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0400
Attention throughput (in TFLOP/s): 93.014
MLP duration (in seconds): 0.0279
MLP throughput (in TFLOP/s): 232.598
Transformer duration (in seconds): 0.0544
Transformer throughput (in TFLOP/s): 187.762
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 129.173
MLP duration (in seconds): 0.0285
MLP throughput (in TFLOP/s): 236.484
Transformer duration (in seconds): 0.0522
Transformer throughput (in TFLOP/s): 202.755
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 135.925
MLP duration (in seconds): 0.0302
MLP throughput (in TFLOP/s): 231.118
Transformer duration (in seconds): 0.0567
Transformer throughput (in TFLOP/s): 193.048
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 93.060
MLP duration (in seconds): 0.0307
MLP throughput (in TFLOP/s): 234.949
Transformer duration (in seconds): 0.0572
Transformer throughput (in TFLOP/s): 198.126
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 94.230
MLP duration (in seconds): 0.0323
MLP throughput (in TFLOP/s): 231.332
Transformer duration (in seconds): 0.0604
Transformer throughput (in TFLOP/s): 194.152
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 101.104
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 233.293
Transformer duration (in seconds): 0.0611
Transformer throughput (in TFLOP/s): 198.242
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 97.724
MLP duration (in seconds): 0.0344
MLP throughput (in TFLOP/s): 232.278
Transformer duration (in seconds): 0.0660
Transformer throughput (in TFLOP/s): 189.582
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 99.262
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 232.570
Transformer duration (in seconds): 0.0656
Transformer throughput (in TFLOP/s): 196.869
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 103.814
MLP duration (in seconds): 0.0365
MLP throughput (in TFLOP/s): 233.694
Transformer duration (in seconds): 0.0668
Transformer throughput (in TFLOP/s): 199.557
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 136.071
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 232.479
Transformer duration (in seconds): 0.0665
Transformer throughput (in TFLOP/s): 206.700
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 175.766
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 230.922
Transformer duration (in seconds): 0.0719
Transformer throughput (in TFLOP/s): 197.165
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0486
Attention throughput (in TFLOP/s): 107.996
MLP duration (in seconds): 0.0404
MLP throughput (in TFLOP/s): 231.514
Transformer duration (in seconds): 0.0722
Transformer throughput (in TFLOP/s): 202.265
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 111.385
MLP duration (in seconds): 0.0416
MLP throughput (in TFLOP/s): 231.493
Transformer duration (in seconds): 0.0742
Transformer throughput (in TFLOP/s): 202.657
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 135.258
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 233.299
Transformer duration (in seconds): 0.0764
Transformer throughput (in TFLOP/s): 202.694
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0515
Attention throughput (in TFLOP/s): 110.745
MLP duration (in seconds): 0.0439
MLP throughput (in TFLOP/s): 232.809
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 200.042
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 113.044
MLP duration (in seconds): 0.0452
MLP throughput (in TFLOP/s): 232.916
Transformer duration (in seconds): 0.0813
Transformer throughput (in TFLOP/s): 201.607
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 116.095
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 238.108
Transformer duration (in seconds): 0.0816
Transformer throughput (in TFLOP/s): 206.415
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 154.864
MLP duration (in seconds): 0.0477
MLP throughput (in TFLOP/s): 233.301
Transformer duration (in seconds): 0.0818
Transformer throughput (in TFLOP/s): 211.794
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 182.538
MLP duration (in seconds): 0.0490
MLP throughput (in TFLOP/s): 233.628
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 206.846
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0558
Attention throughput (in TFLOP/s): 116.813
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 230.062
Transformer duration (in seconds): 0.0902
Transformer throughput (in TFLOP/s): 202.605
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0556
Attention throughput (in TFLOP/s): 120.274
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 229.383
Transformer duration (in seconds): 0.0923
Transformer throughput (in TFLOP/s): 203.385
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 126.350
MLP duration (in seconds): 0.0539
MLP throughput (in TFLOP/s): 230.255
Transformer duration (in seconds): 0.0941
Transformer throughput (in TFLOP/s): 204.709
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 124.578
MLP duration (in seconds): 0.0555
MLP throughput (in TFLOP/s): 229.574
Transformer duration (in seconds): 0.0969
Transformer throughput (in TFLOP/s): 203.988
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 124.605
MLP duration (in seconds): 0.0569
MLP throughput (in TFLOP/s): 229.656
Transformer duration (in seconds): 0.0994
Transformer throughput (in TFLOP/s): 203.983
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 125.715
MLP duration (in seconds): 0.0585
MLP throughput (in TFLOP/s): 228.941
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 205.133
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 207.697
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 229.028
Transformer duration (in seconds): 0.1011
Transformer throughput (in TFLOP/s): 210.662
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 138.583
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 230.005
Transformer duration (in seconds): 0.1062
Transformer throughput (in TFLOP/s): 205.560
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0603
Attention throughput (in TFLOP/s): 131.393
MLP duration (in seconds): 0.0628
MLP throughput (in TFLOP/s): 229.922
Transformer duration (in seconds): 0.1085
Transformer throughput (in TFLOP/s): 206.036
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0603
Attention throughput (in TFLOP/s): 134.546
MLP duration (in seconds): 0.0643
MLP throughput (in TFLOP/s): 229.983
Transformer duration (in seconds): 0.1110
Transformer throughput (in TFLOP/s): 206.384
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 204.912
MLP duration (in seconds): 0.0660
MLP throughput (in TFLOP/s): 229.417
Transformer duration (in seconds): 0.1115
Transformer throughput (in TFLOP/s): 210.316
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 207.377
MLP duration (in seconds): 0.0676
MLP throughput (in TFLOP/s): 229.409
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 210.851
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0421
Attention throughput (in TFLOP/s): 206.038
MLP duration (in seconds): 0.0692
MLP throughput (in TFLOP/s): 229.567
Transformer duration (in seconds): 0.1166
Transformer throughput (in TFLOP/s): 210.595
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 200.530
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 229.581
Transformer duration (in seconds): 0.1201
Transformer throughput (in TFLOP/s): 209.218
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0424
Attention throughput (in TFLOP/s): 213.969
MLP duration (in seconds): 0.0727
MLP throughput (in TFLOP/s): 228.784
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 213.400
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0471
Attention throughput (in TFLOP/s): 196.649
MLP duration (in seconds): 0.0721
MLP throughput (in TFLOP/s): 235.855
Transformer duration (in seconds): 0.1231
Transformer throughput (in TFLOP/s): 213.458
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 199.276
MLP duration (in seconds): 0.0755
MLP throughput (in TFLOP/s): 230.330
Transformer duration (in seconds): 0.1274
Transformer throughput (in TFLOP/s): 210.860
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 198.256
MLP duration (in seconds): 0.0773
MLP throughput (in TFLOP/s): 230.190
Transformer duration (in seconds): 0.1301
Transformer throughput (in TFLOP/s): 211.044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 203.657
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 229.784
Transformer duration (in seconds): 0.1327
Transformer throughput (in TFLOP/s): 211.368
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 203.877
MLP duration (in seconds): 0.0812
MLP throughput (in TFLOP/s): 228.747
Transformer duration (in seconds): 0.1355
Transformer throughput (in TFLOP/s): 211.482
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 205.758
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 232.683
Transformer duration (in seconds): 0.1368
Transformer throughput (in TFLOP/s): 213.957
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 202.770
MLP duration (in seconds): 0.0831
MLP throughput (in TFLOP/s): 233.142
Transformer duration (in seconds): 0.1400
Transformer throughput (in TFLOP/s): 213.509
========================================================================================================================
