num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 2.286
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 10.461
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 2.875
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 4.489
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 40.988
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 6.813
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 7.585
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 91.702
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 8.673
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 8.131
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 116.787
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 10.916
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 9.082
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 143.977
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 12.454
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 12.629
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 151.311
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 16.095
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 14.723
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 163.403
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 22.120
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 21.982
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 174.897
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 30.268
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 17.620
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 174.333
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 25.526
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 18.727
MLP duration (in seconds): 0.0012
MLP throughput (in TFLOP/s): 179.248
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 28.248
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 21.139
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 182.619
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 41.633
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 24.932
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 186.409
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 39.000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 29.149
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 190.778
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 47.749
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 32.462
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 192.794
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 51.476
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 38.950
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 197.296
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 62.135
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 48.753
MLP duration (in seconds): 0.0027
MLP throughput (in TFLOP/s): 203.391
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 89.084
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 35.977
MLP duration (in seconds): 0.0031
MLP throughput (in TFLOP/s): 200.068
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 62.911
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 43.238
MLP duration (in seconds): 0.0034
MLP throughput (in TFLOP/s): 205.545
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 71.588
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 43.438
MLP duration (in seconds): 0.0038
MLP throughput (in TFLOP/s): 206.333
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 89.650
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 47.397
MLP duration (in seconds): 0.0042
MLP throughput (in TFLOP/s): 206.729
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 77.164
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 51.480
MLP duration (in seconds): 0.0046
MLP throughput (in TFLOP/s): 207.100
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 83.119
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 55.802
MLP duration (in seconds): 0.0050
MLP throughput (in TFLOP/s): 207.111
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 109.987
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 59.973
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 212.383
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 94.401
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 86.575
MLP duration (in seconds): 0.0059
MLP throughput (in TFLOP/s): 209.707
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 123.075
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 49.384
MLP duration (in seconds): 0.0063
MLP throughput (in TFLOP/s): 212.643
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 91.436
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 50.376
MLP duration (in seconds): 0.0067
MLP throughput (in TFLOP/s): 215.147
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 97.569
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 53.489
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 217.699
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 109.967
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 56.637
MLP duration (in seconds): 0.0079
MLP throughput (in TFLOP/s): 214.171
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 124.159
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 57.718
MLP duration (in seconds): 0.0083
MLP throughput (in TFLOP/s): 218.679
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 119.162
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 58.093
MLP duration (in seconds): 0.0088
MLP throughput (in TFLOP/s): 219.640
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 129.596
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 61.790
MLP duration (in seconds): 0.0095
MLP throughput (in TFLOP/s): 217.409
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 126.271
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 87.022
MLP duration (in seconds): 0.0100
MLP throughput (in TFLOP/s): 219.207
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 175.620
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 53.117
MLP duration (in seconds): 0.0103
MLP throughput (in TFLOP/s): 227.557
Transformer duration (in seconds): 0.0398
Transformer throughput (in TFLOP/s): 95.245
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 56.060
MLP duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 226.379
Transformer duration (in seconds): 0.0353
Transformer throughput (in TFLOP/s): 113.651
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 58.991
MLP duration (in seconds): 0.0115
MLP throughput (in TFLOP/s): 228.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 166.168
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 64.348
MLP duration (in seconds): 0.0120
MLP throughput (in TFLOP/s): 231.614
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 158.074
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 71.005
MLP duration (in seconds): 0.0126
MLP throughput (in TFLOP/s): 233.080
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 165.088
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 68.716
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 228.387
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 163.686
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 71.951
MLP duration (in seconds): 0.0141
MLP throughput (in TFLOP/s): 231.767
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 170.143
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 129.898
MLP duration (in seconds): 0.0148
MLP throughput (in TFLOP/s): 231.801
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 188.908
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 145.173
MLP duration (in seconds): 0.0157
MLP throughput (in TFLOP/s): 230.000
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 174.490
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 141.827
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 232.199
Transformer duration (in seconds): 0.0341
Transformer throughput (in TFLOP/s): 177.315
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 148.544
MLP duration (in seconds): 0.0168
MLP throughput (in TFLOP/s): 235.857
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 180.572
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 154.797
MLP duration (in seconds): 0.0180
MLP throughput (in TFLOP/s): 230.951
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 186.879
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 159.262
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 234.889
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 183.576
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 83.044
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 235.244
Transformer duration (in seconds): 0.0401
Transformer throughput (in TFLOP/s): 179.798
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 86.066
MLP duration (in seconds): 0.0205
MLP throughput (in TFLOP/s): 231.572
Transformer duration (in seconds): 0.0410
Transformer throughput (in TFLOP/s): 183.565
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 102.216
MLP duration (in seconds): 0.0209
MLP throughput (in TFLOP/s): 236.464
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 193.744
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 86.445
MLP duration (in seconds): 0.0224
MLP throughput (in TFLOP/s): 229.905
Transformer duration (in seconds): 0.0433
Transformer throughput (in TFLOP/s): 188.318
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 84.485
MLP duration (in seconds): 0.0227
MLP throughput (in TFLOP/s): 236.315
Transformer duration (in seconds): 0.0562
Transformer throughput (in TFLOP/s): 150.812
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 80.906
MLP duration (in seconds): 0.0242
MLP throughput (in TFLOP/s): 230.815
Transformer duration (in seconds): 0.0459
Transformer throughput (in TFLOP/s): 192.004
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 84.045
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 236.875
Transformer duration (in seconds): 0.0483
Transformer throughput (in TFLOP/s): 189.666
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 88.539
MLP duration (in seconds): 0.0257
MLP throughput (in TFLOP/s): 235.026
Transformer duration (in seconds): 0.0500
Transformer throughput (in TFLOP/s): 190.131
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0389
Attention throughput (in TFLOP/s): 92.343
MLP duration (in seconds): 0.0265
MLP throughput (in TFLOP/s): 236.003
Transformer duration (in seconds): 0.0623
Transformer throughput (in TFLOP/s): 158.202
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 93.401
MLP duration (in seconds): 0.0280
MLP throughput (in TFLOP/s): 232.107
Transformer duration (in seconds): 0.0539
Transformer throughput (in TFLOP/s): 189.390
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 117.144
MLP duration (in seconds): 0.0286
MLP throughput (in TFLOP/s): 235.785
Transformer duration (in seconds): 0.0523
Transformer throughput (in TFLOP/s): 202.364
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0357
Attention throughput (in TFLOP/s): 111.559
MLP duration (in seconds): 0.0303
MLP throughput (in TFLOP/s): 230.494
Transformer duration (in seconds): 0.0566
Transformer throughput (in TFLOP/s): 193.706
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 92.990
MLP duration (in seconds): 0.0308
MLP throughput (in TFLOP/s): 234.234
Transformer duration (in seconds): 0.0569
Transformer throughput (in TFLOP/s): 199.063
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 95.980
MLP duration (in seconds): 0.0325
MLP throughput (in TFLOP/s): 230.252
Transformer duration (in seconds): 0.0606
Transformer throughput (in TFLOP/s): 193.335
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 101.765
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 231.156
Transformer duration (in seconds): 0.0612
Transformer throughput (in TFLOP/s): 197.762
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0459
Attention throughput (in TFLOP/s): 98.355
MLP duration (in seconds): 0.0346
MLP throughput (in TFLOP/s): 230.860
Transformer duration (in seconds): 0.0627
Transformer throughput (in TFLOP/s): 199.516
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 99.517
MLP duration (in seconds): 0.0357
MLP throughput (in TFLOP/s): 231.295
Transformer duration (in seconds): 0.0672
Transformer throughput (in TFLOP/s): 192.122
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 105.926
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 232.069
Transformer duration (in seconds): 0.0680
Transformer throughput (in TFLOP/s): 196.039
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0363
Attention throughput (in TFLOP/s): 136.231
MLP duration (in seconds): 0.0381
MLP throughput (in TFLOP/s): 230.830
Transformer duration (in seconds): 0.0668
Transformer throughput (in TFLOP/s): 205.862
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 175.799
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 230.580
Transformer duration (in seconds): 0.0716
Transformer throughput (in TFLOP/s): 197.907
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 116.208
MLP duration (in seconds): 0.0404
MLP throughput (in TFLOP/s): 231.326
Transformer duration (in seconds): 0.0726
Transformer throughput (in TFLOP/s): 201.181
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0489
Attention throughput (in TFLOP/s): 110.367
MLP duration (in seconds): 0.0417
MLP throughput (in TFLOP/s): 230.966
Transformer duration (in seconds): 0.0742
Transformer throughput (in TFLOP/s): 202.739
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0404
Attention throughput (in TFLOP/s): 137.442
MLP duration (in seconds): 0.0428
MLP throughput (in TFLOP/s): 232.108
Transformer duration (in seconds): 0.0766
Transformer throughput (in TFLOP/s): 201.965
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0514
Attention throughput (in TFLOP/s): 111.060
MLP duration (in seconds): 0.0441
MLP throughput (in TFLOP/s): 232.088
Transformer duration (in seconds): 0.0797
Transformer throughput (in TFLOP/s): 199.830
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 114.435
MLP duration (in seconds): 0.0453
MLP throughput (in TFLOP/s): 232.089
Transformer duration (in seconds): 0.0815
Transformer throughput (in TFLOP/s): 201.007
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0525
Attention throughput (in TFLOP/s): 114.673
MLP duration (in seconds): 0.0459
MLP throughput (in TFLOP/s): 235.847
Transformer duration (in seconds): 0.0813
Transformer throughput (in TFLOP/s): 207.216
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 154.899
MLP duration (in seconds): 0.0478
MLP throughput (in TFLOP/s): 232.794
Transformer duration (in seconds): 0.0819
Transformer throughput (in TFLOP/s): 211.355
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0402
Attention throughput (in TFLOP/s): 157.918
MLP duration (in seconds): 0.0492
MLP throughput (in TFLOP/s): 232.593
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 206.791
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0555
Attention throughput (in TFLOP/s): 117.468
MLP duration (in seconds): 0.0513
MLP throughput (in TFLOP/s): 229.376
Transformer duration (in seconds): 0.0898
Transformer throughput (in TFLOP/s): 203.486
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0562
Attention throughput (in TFLOP/s): 118.865
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 229.175
Transformer duration (in seconds): 0.0920
Transformer throughput (in TFLOP/s): 203.866
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 124.520
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 229.249
Transformer duration (in seconds): 0.0937
Transformer throughput (in TFLOP/s): 205.567
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0535
Attention throughput (in TFLOP/s): 131.371
MLP duration (in seconds): 0.0557
MLP throughput (in TFLOP/s): 228.675
Transformer duration (in seconds): 0.0967
Transformer throughput (in TFLOP/s): 204.411
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0571
Attention throughput (in TFLOP/s): 126.061
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 228.582
Transformer duration (in seconds): 0.0992
Transformer throughput (in TFLOP/s): 204.383
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 124.535
MLP duration (in seconds): 0.0586
MLP throughput (in TFLOP/s): 228.783
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 204.900
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 206.603
MLP duration (in seconds): 0.0601
MLP throughput (in TFLOP/s): 228.791
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 210.363
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 129.878
MLP duration (in seconds): 0.0614
MLP throughput (in TFLOP/s): 229.321
Transformer duration (in seconds): 0.1059
Transformer throughput (in TFLOP/s): 206.044
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0604
Attention throughput (in TFLOP/s): 131.271
MLP duration (in seconds): 0.0631
MLP throughput (in TFLOP/s): 228.756
Transformer duration (in seconds): 0.1076
Transformer throughput (in TFLOP/s): 207.901
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0576
Attention throughput (in TFLOP/s): 140.705
MLP duration (in seconds): 0.0647
MLP throughput (in TFLOP/s): 228.793
Transformer duration (in seconds): 0.1102
Transformer throughput (in TFLOP/s): 207.934
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0411
Attention throughput (in TFLOP/s): 201.736
MLP duration (in seconds): 0.0662
MLP throughput (in TFLOP/s): 229.057
Transformer duration (in seconds): 0.1127
Transformer throughput (in TFLOP/s): 208.138
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0419
Attention throughput (in TFLOP/s): 202.387
MLP duration (in seconds): 0.0677
MLP throughput (in TFLOP/s): 229.337
Transformer duration (in seconds): 0.1146
Transformer throughput (in TFLOP/s): 209.459
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 201.641
MLP duration (in seconds): 0.0693
MLP throughput (in TFLOP/s): 229.224
Transformer duration (in seconds): 0.1175
Transformer throughput (in TFLOP/s): 209.035
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0440
Attention throughput (in TFLOP/s): 201.867
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 229.481
Transformer duration (in seconds): 0.1203
Transformer throughput (in TFLOP/s): 208.835
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 212.268
MLP duration (in seconds): 0.0726
MLP throughput (in TFLOP/s): 228.922
Transformer duration (in seconds): 0.1197
Transformer throughput (in TFLOP/s): 214.692
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 199.357
MLP duration (in seconds): 0.0726
MLP throughput (in TFLOP/s): 234.179
Transformer duration (in seconds): 0.1228
Transformer throughput (in TFLOP/s): 213.955
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 188.702
MLP duration (in seconds): 0.0755
MLP throughput (in TFLOP/s): 230.269
Transformer duration (in seconds): 0.1271
Transformer throughput (in TFLOP/s): 211.418
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 198.084
MLP duration (in seconds): 0.0773
MLP throughput (in TFLOP/s): 230.141
Transformer duration (in seconds): 0.1299
Transformer throughput (in TFLOP/s): 211.288
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 199.677
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 229.702
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 211.036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 200.833
MLP duration (in seconds): 0.0808
MLP throughput (in TFLOP/s): 229.802
Transformer duration (in seconds): 0.1354
Transformer throughput (in TFLOP/s): 211.725
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0514
Attention throughput (in TFLOP/s): 200.133
MLP duration (in seconds): 0.0817
MLP throughput (in TFLOP/s): 232.225
Transformer duration (in seconds): 0.1369
Transformer throughput (in TFLOP/s): 213.857
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0519
Attention throughput (in TFLOP/s): 202.270
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 232.571
Transformer duration (in seconds): 0.1403
Transformer throughput (in TFLOP/s): 212.965