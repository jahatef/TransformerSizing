1.13.1 

num_attention_heads: 40, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x1x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x1x2048): 0.933
Elapsed time for attention_prob_times_values (160x2048x2048x1): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x1): 1.408

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 1.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x2x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x2x2048): 1.216
Elapsed time for attention_prob_times_values (160x2048x2048x2): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x2): 2.753

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x3x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x3x2048): 2.813
Elapsed time for attention_prob_times_values (160x2048x2048x3): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x3): 3.256

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 3.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x4x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x4x2048): 2.427
Elapsed time for attention_prob_times_values (160x2048x2048x4): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x4): 5.138

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 3.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x5x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x5x2048): 4.391
Elapsed time for attention_prob_times_values (160x2048x2048x5): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x5): 4.311

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 5.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x6x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x6x2048): 3.654
Elapsed time for attention_prob_times_values (160x2048x2048x6): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x6): 8.123

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 6.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x7x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x7x2048): 6.337
Elapsed time for attention_prob_times_values (160x2048x2048x7): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x7): 6.935

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 8.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x8x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x8x2048): 9.771
Elapsed time for attention_prob_times_values (160x2048x2048x8): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x8): 10.415

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 13.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x9x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x9x2048): 5.410
Elapsed time for attention_prob_times_values (160x2048x2048x9): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x9): 6.597

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 8.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x10x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x10x2048): 5.936
Elapsed time for attention_prob_times_values (160x2048x2048x10): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x10): 12.460

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 11.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x11x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x11x2048): 7.220
Elapsed time for attention_prob_times_values (160x2048x2048x11): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x11): 10.297

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 12.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x12x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x12x2048): 7.260
Elapsed time for attention_prob_times_values (160x2048x2048x12): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x12): 15.598

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 14.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x13x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x13x2048): 8.475
Elapsed time for attention_prob_times_values (160x2048x2048x13): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x13): 11.433

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 14.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x14x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x14x2048): 7.152
Elapsed time for attention_prob_times_values (160x2048x2048x14): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x14): 16.719

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 15.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x15x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x15x2048): 10.089
Elapsed time for attention_prob_times_values (160x2048x2048x15): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x15): 14.269

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 18.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x16x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x16x2048): 18.541
Elapsed time for attention_prob_times_values (160x2048x2048x16): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x16): 21.324

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 32.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x17x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x17x2048): 11.555
Elapsed time for attention_prob_times_values (160x2048x2048x17): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x17): 16.234

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 22.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x18x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x18x2048): 10.578
Elapsed time for attention_prob_times_values (160x2048x2048x18): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x18): 1.538

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 4.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x19x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x19x2048): 7.895
Elapsed time for attention_prob_times_values (160x2048x2048x19): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x19): 16.959

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 18.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x20x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x20x2048): 11.628
Elapsed time for attention_prob_times_values (160x2048x2048x20): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x20): 23.383

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 27.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x21x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x21x2048): 13.562
Elapsed time for attention_prob_times_values (160x2048x2048x21): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x21): 16.762

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 27.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x22x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x22x2048): 13.279
Elapsed time for attention_prob_times_values (160x2048x2048x22): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x22): 27.244

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 33.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x23x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x23x2048): 9.171
Elapsed time for attention_prob_times_values (160x2048x2048x23): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x23): 19.403

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 23.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x24x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x24x2048): 26.419
Elapsed time for attention_prob_times_values (160x2048x2048x24): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x24): 30.840

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 55.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x25x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x25x2048): 16.395
Elapsed time for attention_prob_times_values (160x2048x2048x25): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x25): 22.659

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 37.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x26x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x26x2048): 15.250
Elapsed time for attention_prob_times_values (160x2048x2048x26): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x26): 28.767

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 40.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x27x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x27x2048): 18.112
Elapsed time for attention_prob_times_values (160x2048x2048x27): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x27): 23.037

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 41.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x28x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x28x2048): 16.269
Elapsed time for attention_prob_times_values (160x2048x2048x28): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x28): 30.076

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 44.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x29x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x29x2048): 17.590
Elapsed time for attention_prob_times_values (160x2048x2048x29): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x29): 25.338

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 44.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x30x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x30x2048): 16.992
Elapsed time for attention_prob_times_values (160x2048x2048x30): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x30): 32.095

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 48.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x31x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x31x2048): 19.597
Elapsed time for attention_prob_times_values (160x2048x2048x31): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x31): 26.894

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 50.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x32x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x32x2048): 32.653
Elapsed time for attention_prob_times_values (160x2048x2048x32): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x32): 36.496

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 77.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x33x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x33x2048): 16.938
Elapsed time for attention_prob_times_values (160x2048x2048x33): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x33): 23.043

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 44.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x34x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x34x2048): 14.144
Elapsed time for attention_prob_times_values (160x2048x2048x34): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x34): 37.029

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 47.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x35x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x35x2048): 19.450
Elapsed time for attention_prob_times_values (160x2048x2048x35): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x35): 24.660

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 51.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x36x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x36x2048): 14.874
Elapsed time for attention_prob_times_values (160x2048x2048x36): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x36): 39.359

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 51.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x37x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x37x2048): 20.443
Elapsed time for attention_prob_times_values (160x2048x2048x37): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x37): 25.501

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 55.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x38x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x38x2048): 16.168
Elapsed time for attention_prob_times_values (160x2048x2048x38): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x38): 39.505

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 57.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x39x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x39x2048): 20.342
Elapsed time for attention_prob_times_values (160x2048x2048x39): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x39): 26.303

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 57.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x40x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x40x2048): 36.081
Elapsed time for attention_prob_times_values (160x2048x2048x40): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x40): 47.839

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 105.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x41x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x41x2048): 21.541
Elapsed time for attention_prob_times_values (160x2048x2048x41): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x41): 25.890

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 61.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x42x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x42x2048): 17.168
Elapsed time for attention_prob_times_values (160x2048x2048x42): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x42): 44.857

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 65.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x43x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x43x2048): 22.076
Elapsed time for attention_prob_times_values (160x2048x2048x43): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x43): 29.056

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 67.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x44x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x44x2048): 17.343
Elapsed time for attention_prob_times_values (160x2048x2048x44): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x44): 47.534

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 69.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x45x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x45x2048): 23.329
Elapsed time for attention_prob_times_values (160x2048x2048x45): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x45): 30.924

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 73.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x46x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x46x2048): 18.396
Elapsed time for attention_prob_times_values (160x2048x2048x46): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x46): 49.637

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 75.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x47x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x47x2048): 24.576
Elapsed time for attention_prob_times_values (160x2048x2048x47): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x47): 25.103

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 70.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x48x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x48x2048): 45.437
Elapsed time for attention_prob_times_values (160x2048x2048x48): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x48): 60.410

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 149.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x49x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x49x2048): 24.644
Elapsed time for attention_prob_times_values (160x2048x2048x49): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x49): 32.135

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 81.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x50x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x50x2048): 19.320
Elapsed time for attention_prob_times_values (160x2048x2048x50): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x50): 53.119

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 83.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x51x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x51x2048): 25.518
Elapsed time for attention_prob_times_values (160x2048x2048x51): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x51): 35.047

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 88.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x52x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x52x2048): 19.379
Elapsed time for attention_prob_times_values (160x2048x2048x52): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x52): 51.339

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 85.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x53x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x53x2048): 27.094
Elapsed time for attention_prob_times_values (160x2048x2048x53): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x53): 36.222

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 95.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x54x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x54x2048): 20.360
Elapsed time for attention_prob_times_values (160x2048x2048x54): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x54): 57.684

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 93.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x55x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x55x2048): 27.067
Elapsed time for attention_prob_times_values (160x2048x2048x55): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x55): 35.176

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 96.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x56x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x56x2048): 46.739
Elapsed time for attention_prob_times_values (160x2048x2048x56): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x56): 65.243

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 173.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x57x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x57x2048): 27.398
Elapsed time for attention_prob_times_values (160x2048x2048x57): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x57): 37.221

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 101.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x58x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x58x2048): 21.817
Elapsed time for attention_prob_times_values (160x2048x2048x58): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x58): 60.667

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 104.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x59x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x59x2048): 28.920
Elapsed time for attention_prob_times_values (160x2048x2048x59): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x59): 39.787

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 110.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x60x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x60x2048): 21.128
Elapsed time for attention_prob_times_values (160x2048x2048x60): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x60): 62.713

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 105.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x61x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x61x2048): 28.157
Elapsed time for attention_prob_times_values (160x2048x2048x61): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x61): 39.521

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 111.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x62x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x62x2048): 22.100
Elapsed time for attention_prob_times_values (160x2048x2048x62): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x62): 47.904

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 103.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x63x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x63x2048): 29.335
Elapsed time for attention_prob_times_values (160x2048x2048x63): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x63): 39.966

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 117.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x64x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x64x2048): 58.602
Elapsed time for attention_prob_times_values (160x2048x2048x64): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x64): 79.394

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 236.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x65x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x65x2048): 25.950
Elapsed time for attention_prob_times_values (160x2048x2048x65): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x65): 38.903

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 110.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x66x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x66x2048): 19.737
Elapsed time for attention_prob_times_values (160x2048x2048x66): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x66): 68.287

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 109.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x67x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x67x2048): 27.443
Elapsed time for attention_prob_times_values (160x2048x2048x67): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x67): 41.343

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 119.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x68x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x68x2048): 18.547
Elapsed time for attention_prob_times_values (160x2048x2048x68): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x68): 70.641

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 107.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x69x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x69x2048): 27.732
Elapsed time for attention_prob_times_values (160x2048x2048x69): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x69): 42.732

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 124.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x70x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x70x2048): 19.369
Elapsed time for attention_prob_times_values (160x2048x2048x70): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x70): 71.492

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 113.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x71x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x71x2048): 27.710
Elapsed time for attention_prob_times_values (160x2048x2048x71): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x71): 43.649

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 127.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x72x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x72x2048): 67.296
Elapsed time for attention_prob_times_values (160x2048x2048x72): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x72): 82.166

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 282.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x73x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x73x2048): 12.398
Elapsed time for attention_prob_times_values (160x2048x2048x73): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x73): 44.170

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 74.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x74x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x74x2048): 49.605
Elapsed time for attention_prob_times_values (160x2048x2048x74): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x74): 73.059

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 229.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x75x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x75x2048): 28.650
Elapsed time for attention_prob_times_values (160x2048x2048x75): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x75): 46.003

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 138.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x76x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x76x2048): 51.641
Elapsed time for attention_prob_times_values (160x2048x2048x76): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x76): 77.804

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 246.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x77x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x77x2048): 29.851
Elapsed time for attention_prob_times_values (160x2048x2048x77): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x77): 47.271

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 146.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x78x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x78x2048): 6.005
Elapsed time for attention_prob_times_values (160x2048x2048x78): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x78): 79.217

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 45.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x79x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x79x2048): 29.228
Elapsed time for attention_prob_times_values (160x2048x2048x79): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x79): 46.578

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 146.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x80x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x80x2048): 73.146
Elapsed time for attention_prob_times_values (160x2048x2048x80): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x80): 90.963

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 334.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x81x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x81x2048): 29.862
Elapsed time for attention_prob_times_values (160x2048x2048x81): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x81): 49.323

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 154.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x82x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x82x2048): 55.066
Elapsed time for attention_prob_times_values (160x2048x2048x82): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x82): 79.535

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 273.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x83x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x83x2048): 30.657
Elapsed time for attention_prob_times_values (160x2048x2048x83): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x83): 50.312

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 161.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x84x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x84x2048): 56.322
Elapsed time for attention_prob_times_values (160x2048x2048x84): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x84): 84.307

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 289.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x85x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x85x2048): 30.186
Elapsed time for attention_prob_times_values (160x2048x2048x85): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x85): 51.652

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 164.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x86x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x86x2048): 55.578
Elapsed time for attention_prob_times_values (160x2048x2048x86): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x86): 87.138

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 295.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x87x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x87x2048): 31.438
Elapsed time for attention_prob_times_values (160x2048x2048x87): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x87): 52.270

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 172.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x88x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x88x2048): 68.529
Elapsed time for attention_prob_times_values (160x2048x2048x88): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x88): 97.346

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 356.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x89x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x89x2048): 30.642
Elapsed time for attention_prob_times_values (160x2048x2048x89): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x89): 53.114

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 173.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x90x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x90x2048): 27.425
Elapsed time for attention_prob_times_values (160x2048x2048x90): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x90): 79.713

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 184.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x91x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x91x2048): 31.617
Elapsed time for attention_prob_times_values (160x2048x2048x91): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x91): 51.362

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 178.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x92x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x92x2048): 60.667
Elapsed time for attention_prob_times_values (160x2048x2048x92): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x92): 88.699

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 330.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x93x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x93x2048): 32.615
Elapsed time for attention_prob_times_values (160x2048x2048x93): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x93): 55.916

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 190.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x94x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x94x2048): 61.884
Elapsed time for attention_prob_times_values (160x2048x2048x94): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x94): 94.411

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 349.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x95x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x95x2048): 34.561
Elapsed time for attention_prob_times_values (160x2048x2048x95): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x95): 54.466

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 199.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x96x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x96x2048): 92.826
Elapsed time for attention_prob_times_values (160x2048x2048x96): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x96): 96.661

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 449.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x97x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x97x2048): 30.891
Elapsed time for attention_prob_times_values (160x2048x2048x97): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x97): 57.589

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 192.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x98x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x98x2048): 54.639
Elapsed time for attention_prob_times_values (160x2048x2048x98): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x98): 97.524

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 338.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x99x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x99x2048): 30.314
Elapsed time for attention_prob_times_values (160x2048x2048x99): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x99): 59.786

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 195.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x100x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x100x2048): 58.066
Elapsed time for attention_prob_times_values (160x2048x2048x100): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x100): 95.254

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 353.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x101x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x101x2048): 31.046
Elapsed time for attention_prob_times_values (160x2048x2048x101): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x101): 58.884

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 201.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x102x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x102x2048): 61.380
Elapsed time for attention_prob_times_values (160x2048x2048x102): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x102): 101.702

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 381.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x103x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x103x2048): 29.859
Elapsed time for attention_prob_times_values (160x2048x2048x103): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x103): 59.777

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 200.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x104x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x104x2048): 84.617
Elapsed time for attention_prob_times_values (160x2048x2048x104): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x104): 110.695

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 485.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x105x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x105x2048): 30.300
Elapsed time for attention_prob_times_values (160x2048x2048x105): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x105): 60.557

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 206.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x106x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x106x2048): 61.266
Elapsed time for attention_prob_times_values (160x2048x2048x106): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x106): 99.009

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 389.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x107x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x107x2048): 31.389
Elapsed time for attention_prob_times_values (160x2048x2048x107): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x107): 61.729

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 215.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x108x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x108x2048): 64.113
Elapsed time for attention_prob_times_values (160x2048x2048x108): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x108): 100.344

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 408.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x109x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x109x2048): 31.761
Elapsed time for attention_prob_times_values (160x2048x2048x109): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x109): 65.480

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 224.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x110x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x110x2048): 42.872
Elapsed time for attention_prob_times_values (160x2048x2048x110): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x110): 97.673

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 315.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x111x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x111x2048): 30.216
Elapsed time for attention_prob_times_values (160x2048x2048x111): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x111): 60.744

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 215.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x112x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x112x2048): 94.656
Elapsed time for attention_prob_times_values (160x2048x2048x112): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x112): 113.933

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 555.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x113x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x113x2048): 30.124
Elapsed time for attention_prob_times_values (160x2048x2048x113): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x113): 63.518

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 221.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x114x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x114x2048): 64.318
Elapsed time for attention_prob_times_values (160x2048x2048x114): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x114): 109.180

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 441.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x115x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x115x2048): 28.783
Elapsed time for attention_prob_times_values (160x2048x2048x115): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x115): 68.061

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 222.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x116x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x116x2048): 68.408
Elapsed time for attention_prob_times_values (160x2048x2048x116): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x116): 114.045

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 473.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x117x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x117x2048): 34.483
Elapsed time for attention_prob_times_values (160x2048x2048x117): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x117): 70.204

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 257.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x118x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x118x2048): 67.201
Elapsed time for attention_prob_times_values (160x2048x2048x118): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x118): 112.800

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 472.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x119x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x119x2048): 32.903
Elapsed time for attention_prob_times_values (160x2048x2048x119): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x119): 70.140

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 253.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x120x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x120x2048): 90.337
Elapsed time for attention_prob_times_values (160x2048x2048x120): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x120): 124.363

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 595.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x121x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x121x2048): 33.803
Elapsed time for attention_prob_times_values (160x2048x2048x121): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x121): 70.282

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 261.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x122x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x122x2048): 72.547
Elapsed time for attention_prob_times_values (160x2048x2048x122): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x122): 119.610

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 520.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x123x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x123x2048): 35.287
Elapsed time for attention_prob_times_values (160x2048x2048x123): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x123): 68.544

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 270.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x124x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x124x2048): 72.541
Elapsed time for attention_prob_times_values (160x2048x2048x124): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x124): 119.428

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 527.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x125x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x125x2048): 36.521
Elapsed time for attention_prob_times_values (160x2048x2048x125): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x125): 68.894

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 280.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x126x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x126x2048): 75.020
Elapsed time for attention_prob_times_values (160x2048x2048x126): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x126): 115.599

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 538.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x127x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x127x2048): 35.508
Elapsed time for attention_prob_times_values (160x2048x2048x127): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x127): 68.818

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 279.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x128x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x128x2048): 109.844
Elapsed time for attention_prob_times_values (160x2048x2048x128): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x128): 139.107

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 736.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x129x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x129x2048): 33.549
Elapsed time for attention_prob_times_values (160x2048x2048x129): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x129): 54.328

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 250.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x130x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x130x2048): 70.166
Elapsed time for attention_prob_times_values (160x2048x2048x130): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x130): 91.628

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 483.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x131x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x131x2048): 33.959
Elapsed time for attention_prob_times_values (160x2048x2048x131): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x131): 56.763

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 259.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x132x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x132x2048): 10.717
Elapsed time for attention_prob_times_values (160x2048x2048x132): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x132): 93.471

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 118.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x133x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x133x2048): 33.883
Elapsed time for attention_prob_times_values (160x2048x2048x133): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x133): 56.465

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 262.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x134x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x134x2048): 71.672
Elapsed time for attention_prob_times_values (160x2048x2048x134): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x134): 87.715

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 491.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x135x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x135x2048): 32.492
Elapsed time for attention_prob_times_values (160x2048x2048x135): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x135): 54.288

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 255.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x136x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x136x2048): 95.749
Elapsed time for attention_prob_times_values (160x2048x2048x136): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x136): 122.166

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 677.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x137x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x137x2048): 32.584
Elapsed time for attention_prob_times_values (160x2048x2048x137): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x137): 57.095

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 263.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x138x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x138x2048): 67.525
Elapsed time for attention_prob_times_values (160x2048x2048x138): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x138): 92.894

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 499.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x139x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x139x2048): 35.079
Elapsed time for attention_prob_times_values (160x2048x2048x139): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x139): 56.769

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 278.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x140x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x140x2048): 74.697
Elapsed time for attention_prob_times_values (160x2048x2048x140): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x140): 91.964

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 533.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x141x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x141x2048): 34.112
Elapsed time for attention_prob_times_values (160x2048x2048x141): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x141): 48.664

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 261.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x142x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x142x2048): 70.937
Elapsed time for attention_prob_times_values (160x2048x2048x142): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x142): 87.816

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 513.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x143x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x143x2048): 34.056
Elapsed time for attention_prob_times_values (160x2048x2048x143): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x143): 58.007

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 282.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x144x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x144x2048): 110.729
Elapsed time for attention_prob_times_values (160x2048x2048x144): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x144): 134.191

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 803.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x145x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x145x2048): 53.937
Elapsed time for attention_prob_times_values (160x2048x2048x145): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x145): 56.248

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 366.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x146x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x146x2048): 72.761
Elapsed time for attention_prob_times_values (160x2048x2048x146): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x146): 85.893

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 528.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x147x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x147x2048): 55.413
Elapsed time for attention_prob_times_values (160x2048x2048x147): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x147): 57.817

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 381.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x148x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x148x2048): 75.087
Elapsed time for attention_prob_times_values (160x2048x2048x148): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x148): 97.435

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 575.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x149x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x149x2048): 55.553
Elapsed time for attention_prob_times_values (160x2048x2048x149): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x149): 53.886

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 373.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x150x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x150x2048): 74.583
Elapsed time for attention_prob_times_values (160x2048x2048x150): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x150): 98.983

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 583.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x151x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x151x2048): 55.483
Elapsed time for attention_prob_times_values (160x2048x2048x151): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x151): 57.005

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 387.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x152x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x152x2048): 104.902
Elapsed time for attention_prob_times_values (160x2048x2048x152): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x152): 93.070

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 684.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x153x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x153x2048): 55.397
Elapsed time for attention_prob_times_values (160x2048x2048x153): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x153): 58.837

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 398.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x154x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x154x2048): 76.289
Elapsed time for attention_prob_times_values (160x2048x2048x154): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x154): 100.399

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 608.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x155x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x155x2048): 58.830
Elapsed time for attention_prob_times_values (160x2048x2048x155): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x155): 61.950

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 425.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x156x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x156x2048): 81.428
Elapsed time for attention_prob_times_values (160x2048x2048x156): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x156): 101.597

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 641.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x157x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x157x2048): 57.899
Elapsed time for attention_prob_times_values (160x2048x2048x157): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x157): 60.711

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 422.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x158x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x158x2048): 81.624
Elapsed time for attention_prob_times_values (160x2048x2048x158): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x158): 101.956

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 650.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x159x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x159x2048): 58.818
Elapsed time for attention_prob_times_values (160x2048x2048x159): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x159): 62.324

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 436.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x160x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x160x2048): 109.165
Elapsed time for attention_prob_times_values (160x2048x2048x160): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x160): 147.683

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 910.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x161x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x161x2048): 55.837
Elapsed time for attention_prob_times_values (160x2048x2048x161): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x161): 60.039

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 421.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x162x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x162x2048): 74.894
Elapsed time for attention_prob_times_values (160x2048x2048x162): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x162): 93.278

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 608.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x163x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x163x2048): 56.123
Elapsed time for attention_prob_times_values (160x2048x2048x163): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x163): 64.164

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 441.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x164x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x164x2048): 79.713
Elapsed time for attention_prob_times_values (160x2048x2048x164): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x164): 104.498

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 669.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x165x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x165x2048): 57.794
Elapsed time for attention_prob_times_values (160x2048x2048x165): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x165): 63.218

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 449.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x166x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x166x2048): 79.390
Elapsed time for attention_prob_times_values (160x2048x2048x166): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x166): 102.839

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 670.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x167x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x167x2048): 57.493
Elapsed time for attention_prob_times_values (160x2048x2048x167): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x167): 61.478

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 447.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x168x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x168x2048): 110.434
Elapsed time for attention_prob_times_values (160x2048x2048x168): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x168): 146.221

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 951.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x169x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x169x2048): 55.468
Elapsed time for attention_prob_times_values (160x2048x2048x169): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x169): 63.312

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 449.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x170x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x170x2048): 80.645
Elapsed time for attention_prob_times_values (160x2048x2048x170): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x170): 106.845

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 702.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x171x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x171x2048): 59.463
Elapsed time for attention_prob_times_values (160x2048x2048x171): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x171): 67.252

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 484.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x172x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x172x2048): 74.615
Elapsed time for attention_prob_times_values (160x2048x2048x172): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x172): 110.131

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 686.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x173x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x173x2048): 57.740
Elapsed time for attention_prob_times_values (160x2048x2048x173): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x173): 66.202

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 478.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x174x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x174x2048): 81.505
Elapsed time for attention_prob_times_values (160x2048x2048x174): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x174): 107.890

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 724.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x175x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x175x2048): 58.949
Elapsed time for attention_prob_times_values (160x2048x2048x175): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x175): 66.336

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 489.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x176x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x176x2048): 120.388
Elapsed time for attention_prob_times_values (160x2048x2048x176): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x176): 153.373

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 1062.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x177x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x177x2048): 58.206
Elapsed time for attention_prob_times_values (160x2048x2048x177): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x177): 66.668

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 491.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x178x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x178x2048): 82.372
Elapsed time for attention_prob_times_values (160x2048x2048x178): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x178): 109.958

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 749.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x179x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x179x2048): 59.296
Elapsed time for attention_prob_times_values (160x2048x2048x179): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x179): 53.122

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 447.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x180x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x180x2048): 53.456
Elapsed time for attention_prob_times_values (160x2048x2048x180): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x180): 114.706

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 585.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x181x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x181x2048): 56.007
Elapsed time for attention_prob_times_values (160x2048x2048x181): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x181): 70.180

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 502.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x182x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x182x2048): 85.990
Elapsed time for attention_prob_times_values (160x2048x2048x182): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x182): 106.140

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 770.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x183x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x183x2048): 59.885
Elapsed time for attention_prob_times_values (160x2048x2048x183): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x183): 67.329

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 516.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x184x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x184x2048): 111.559
Elapsed time for attention_prob_times_values (160x2048x2048x184): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x184): 155.670

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1064.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x185x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x185x2048): 60.801
Elapsed time for attention_prob_times_values (160x2048x2048x185): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x185): 65.940

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 520.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x186x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x186x2048): 78.758
Elapsed time for attention_prob_times_values (160x2048x2048x186): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x186): 117.242

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 778.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x187x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x187x2048): 57.222
Elapsed time for attention_prob_times_values (160x2048x2048x187): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x187): 69.500

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 521.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x188x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x188x2048): 90.149
Elapsed time for attention_prob_times_values (160x2048x2048x188): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x188): 101.530

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 796.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x189x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x189x2048): 54.841
Elapsed time for attention_prob_times_values (160x2048x2048x189): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x189): 72.790

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 524.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x190x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x190x2048): 87.730
Elapsed time for attention_prob_times_values (160x2048x2048x190): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x190): 114.372

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 836.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x191x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x191x2048): 61.970
Elapsed time for attention_prob_times_values (160x2048x2048x191): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x191): 71.956

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 563.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x192x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x192x2048): 115.539
Elapsed time for attention_prob_times_values (160x2048x2048x192): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x192): 165.219

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1155.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x193x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x193x2048): 59.119
Elapsed time for attention_prob_times_values (160x2048x2048x193): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x193): 71.725

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 553.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x194x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x194x2048): 85.731
Elapsed time for attention_prob_times_values (160x2048x2048x194): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x194): 118.426

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 853.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x195x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x195x2048): 61.330
Elapsed time for attention_prob_times_values (160x2048x2048x195): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x195): 74.409

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 579.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x196x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x196x2048): 86.074
Elapsed time for attention_prob_times_values (160x2048x2048x196): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x196): 119.725

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 866.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x197x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x197x2048): 61.092
Elapsed time for attention_prob_times_values (160x2048x2048x197): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x197): 74.176

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 582.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x198x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x198x2048): 86.053
Elapsed time for attention_prob_times_values (160x2048x2048x198): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x198): 117.133

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 866.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x199x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x199x2048): 61.883
Elapsed time for attention_prob_times_values (160x2048x2048x199): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x199): 71.523

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 582.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x200x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x200x2048): 118.106
Elapsed time for attention_prob_times_values (160x2048x2048x200): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x200): 174.396

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1241.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x201x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x201x2048): 62.309
Elapsed time for attention_prob_times_values (160x2048x2048x201): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x201): 67.899

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 575.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x202x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x202x2048): 89.399
Elapsed time for attention_prob_times_values (160x2048x2048x202): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x202): 116.704

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 900.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x203x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x203x2048): 63.485
Elapsed time for attention_prob_times_values (160x2048x2048x203): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x203): 75.169

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 614.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x204x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x204x2048): 89.218
Elapsed time for attention_prob_times_values (160x2048x2048x204): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x204): 116.864

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 907.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x205x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x205x2048): 62.079
Elapsed time for attention_prob_times_values (160x2048x2048x205): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x205): 74.662

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 610.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x206x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x206x2048): 91.342
Elapsed time for attention_prob_times_values (160x2048x2048x206): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x206): 121.204

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 942.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x207x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x207x2048): 63.123
Elapsed time for attention_prob_times_values (160x2048x2048x207): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x207): 72.873

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 614.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x208x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x208x2048): 131.359
Elapsed time for attention_prob_times_values (160x2048x2048x208): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x208): 183.705

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 1397.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x209x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x209x2048): 63.434
Elapsed time for attention_prob_times_values (160x2048x2048x209): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x209): 73.444

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 623.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x210x2048): 0.0192
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x210x2048): 14.671
Elapsed time for attention_prob_times_values (160x2048x2048x210): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x210): 116.783

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 239.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x211x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x211x2048): 62.864
Elapsed time for attention_prob_times_values (160x2048x2048x211): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x211): 74.091

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 628.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x212x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x212x2048): 90.647
Elapsed time for attention_prob_times_values (160x2048x2048x212): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x212): 117.017

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 948.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x213x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x213x2048): 63.487
Elapsed time for attention_prob_times_values (160x2048x2048x213): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x213): 75.022

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 641.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x214x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x214x2048): 93.621
Elapsed time for attention_prob_times_values (160x2048x2048x214): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x214): 120.423

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 985.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x215x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x215x2048): 63.675
Elapsed time for attention_prob_times_values (160x2048x2048x215): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x215): 75.251

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 648.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x216x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x216x2048): 115.796
Elapsed time for attention_prob_times_values (160x2048x2048x216): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x216): 186.071

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 1347.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x217x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x217x2048): 63.354
Elapsed time for attention_prob_times_values (160x2048x2048x217): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x217): 73.229

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 643.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x218x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x218x2048): 95.795
Elapsed time for attention_prob_times_values (160x2048x2048x218): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x218): 125.471

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 1033.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x219x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x219x2048): 64.319
Elapsed time for attention_prob_times_values (160x2048x2048x219): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x219): 76.206

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 666.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x220x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x220x2048): 86.936
Elapsed time for attention_prob_times_values (160x2048x2048x220): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x220): 122.332

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 975.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x221x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x221x2048): 66.777
Elapsed time for attention_prob_times_values (160x2048x2048x221): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x221): 79.314

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 698.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x222x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x222x2048): 97.075
Elapsed time for attention_prob_times_values (160x2048x2048x222): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x222): 121.630

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1044.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x223x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x223x2048): 63.851
Elapsed time for attention_prob_times_values (160x2048x2048x223): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x223): 79.269

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 686.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x224x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x224x2048): 145.094
Elapsed time for attention_prob_times_values (160x2048x2048x224): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x224): 183.180

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 1578.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x225x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x225x2048): 64.707
Elapsed time for attention_prob_times_values (160x2048x2048x225): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x225): 77.286

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 689.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x226x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x226x2048): 90.883
Elapsed time for attention_prob_times_values (160x2048x2048x226): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x226): 129.916

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1051.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x227x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x227x2048): 65.369
Elapsed time for attention_prob_times_values (160x2048x2048x227): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x227): 80.951

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 713.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x228x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x228x2048): 92.300
Elapsed time for attention_prob_times_values (160x2048x2048x228): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x228): 127.044

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1059.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x229x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x229x2048): 64.619
Elapsed time for attention_prob_times_values (160x2048x2048x229): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x229): 81.125

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 715.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x230x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x230x2048): 91.221
Elapsed time for attention_prob_times_values (160x2048x2048x230): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x230): 130.972

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1073.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x231x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x231x2048): 64.550
Elapsed time for attention_prob_times_values (160x2048x2048x231): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x231): 79.298

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 713.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x232x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x232x2048): 125.040
Elapsed time for attention_prob_times_values (160x2048x2048x232): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x232): 199.609

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 1547.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x233x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x233x2048): 65.580
Elapsed time for attention_prob_times_values (160x2048x2048x233): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x233): 80.689

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 730.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x234x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x234x2048): 94.498
Elapsed time for attention_prob_times_values (160x2048x2048x234): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x234): 134.611

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1126.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x235x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x235x2048): 66.104
Elapsed time for attention_prob_times_values (160x2048x2048x235): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x235): 84.538

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 755.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x236x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x236x2048): 94.775
Elapsed time for attention_prob_times_values (160x2048x2048x236): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x236): 130.674

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1122.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x237x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x237x2048): 67.001
Elapsed time for attention_prob_times_values (160x2048x2048x237): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x237): 75.957

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 730.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x238x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x238x2048): 80.282
Elapsed time for attention_prob_times_values (160x2048x2048x238): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x238): 136.730

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1041.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x239x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x239x2048): 68.224
Elapsed time for attention_prob_times_values (160x2048x2048x239): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x239): 83.032

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 774.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x240x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x240x2048): 143.366
Elapsed time for attention_prob_times_values (160x2048x2048x240): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x240): 203.998

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1747.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x241x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x241x2048): 66.919
Elapsed time for attention_prob_times_values (160x2048x2048x241): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x241): 84.194

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 776.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x242x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x242x2048): 99.202
Elapsed time for attention_prob_times_values (160x2048x2048x242): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x242): 136.343

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1200.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x243x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x243x2048): 67.795
Elapsed time for attention_prob_times_values (160x2048x2048x243): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x243): 88.679

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 806.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x244x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x244x2048): 101.462
Elapsed time for attention_prob_times_values (160x2048x2048x244): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x244): 141.871

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1245.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x245x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x245x2048): 70.668
Elapsed time for attention_prob_times_values (160x2048x2048x245): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x245): 86.815

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 823.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x246x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x246x2048): 96.830
Elapsed time for attention_prob_times_values (160x2048x2048x246): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x246): 134.845

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1195.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x247x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x247x2048): 67.588
Elapsed time for attention_prob_times_values (160x2048x2048x247): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x247): 85.568

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 804.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x248x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x248x2048): 121.953
Elapsed time for attention_prob_times_values (160x2048x2048x248): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x248): 205.432

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 1635.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x249x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x249x2048): 70.482
Elapsed time for attention_prob_times_values (160x2048x2048x249): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x249): 89.500

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 845.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x250x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x250x2048): 98.937
Elapsed time for attention_prob_times_values (160x2048x2048x250): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x250): 140.695

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1250.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x251x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x251x2048): 70.516
Elapsed time for attention_prob_times_values (160x2048x2048x251): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x251): 88.324

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 847.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x252x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x252x2048): 102.517
Elapsed time for attention_prob_times_values (160x2048x2048x252): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x252): 144.523

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1300.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x253x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x253x2048): 71.553
Elapsed time for attention_prob_times_values (160x2048x2048x253): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x253): 90.550

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 869.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x254x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x254x2048): 102.479
Elapsed time for attention_prob_times_values (160x2048x2048x254): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x254): 144.332

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1309.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x255x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x255x2048): 71.259
Elapsed time for attention_prob_times_values (160x2048x2048x255): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x255): 90.730

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 874.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x256x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x256x2048): 152.810
Elapsed time for attention_prob_times_values (160x2048x2048x256): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x256): 218.588

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1978.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x257x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x257x2048): 69.423
Elapsed time for attention_prob_times_values (160x2048x2048x257): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x257): 49.932

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 641.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x258x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x258x2048): 98.449
Elapsed time for attention_prob_times_values (160x2048x2048x258): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x258): 95.843

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1076.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x259x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x259x2048): 69.464
Elapsed time for attention_prob_times_values (160x2048x2048x259): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x259): 56.542

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 693.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x260x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x260x2048): 87.087
Elapsed time for attention_prob_times_values (160x2048x2048x260): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x260): 94.376

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1010.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x261x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x261x2048): 68.340
Elapsed time for attention_prob_times_values (160x2048x2048x261): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x261): 55.483

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 685.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x262x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x262x2048): 97.150
Elapsed time for attention_prob_times_values (160x2048x2048x262): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x262): 95.496

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1082.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x263x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x263x2048): 69.117
Elapsed time for attention_prob_times_values (160x2048x2048x263): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x263): 56.037

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 697.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x264x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x264x2048): 123.952
Elapsed time for attention_prob_times_values (160x2048x2048x264): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x264): 127.877

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1424.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x265x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x265x2048): 62.605
Elapsed time for attention_prob_times_values (160x2048x2048x265): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x265): 54.655

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 662.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x266x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x266x2048): 99.571
Elapsed time for attention_prob_times_values (160x2048x2048x266): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x266): 93.078

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1095.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x267x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x267x2048): 70.402
Elapsed time for attention_prob_times_values (160x2048x2048x267): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x267): 56.198

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 714.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x268x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x268x2048): 103.188
Elapsed time for attention_prob_times_values (160x2048x2048x268): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x268): 93.837

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1127.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x269x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x269x2048): 69.798
Elapsed time for attention_prob_times_values (160x2048x2048x269): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x269): 56.672

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 719.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x270x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x270x2048): 102.700
Elapsed time for attention_prob_times_values (160x2048x2048x270): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x270): 83.846

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1066.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x271x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x271x2048): 64.679
Elapsed time for attention_prob_times_values (160x2048x2048x271): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x271): 54.462

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 685.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x272x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x272x2048): 147.318
Elapsed time for attention_prob_times_values (160x2048x2048x272): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x272): 131.188

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 1613.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x273x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x273x2048): 69.579
Elapsed time for attention_prob_times_values (160x2048x2048x273): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x273): 55.794

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 722.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x274x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x274x2048): 101.962
Elapsed time for attention_prob_times_values (160x2048x2048x274): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x274): 94.666

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1148.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x275x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x275x2048): 71.639
Elapsed time for attention_prob_times_values (160x2048x2048x275): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x275): 58.463

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 756.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x276x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x276x2048): 102.334
Elapsed time for attention_prob_times_values (160x2048x2048x276): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x276): 97.941

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1179.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x277x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x277x2048): 70.512
Elapsed time for attention_prob_times_values (160x2048x2048x277): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x277): 58.010

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 752.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x278x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x278x2048): 104.802
Elapsed time for attention_prob_times_values (160x2048x2048x278): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x278): 95.398

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1184.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x279x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x279x2048): 70.353
Elapsed time for attention_prob_times_values (160x2048x2048x279): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x279): 57.126

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 750.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x280x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x280x2048): 134.643
Elapsed time for attention_prob_times_values (160x2048x2048x280): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x280): 140.324

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1640.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x281x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x281x2048): 72.065
Elapsed time for attention_prob_times_values (160x2048x2048x281): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x281): 56.737

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 760.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x282x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x282x2048): 104.147
Elapsed time for attention_prob_times_values (160x2048x2048x282): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x282): 98.134

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1214.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x283x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x283x2048): 72.248
Elapsed time for attention_prob_times_values (160x2048x2048x283): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x283): 56.341

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 763.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x284x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x284x2048): 104.570
Elapsed time for attention_prob_times_values (160x2048x2048x284): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x284): 97.892

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1222.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x285x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x285x2048): 71.574
Elapsed time for attention_prob_times_values (160x2048x2048x285): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x285): 58.908

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 784.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x286x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x286x2048): 106.442
Elapsed time for attention_prob_times_values (160x2048x2048x286): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x286): 96.317

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1230.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x287x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x287x2048): 73.270
Elapsed time for attention_prob_times_values (160x2048x2048x287): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x287): 58.224

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 792.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x288x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x288x2048): 157.316
Elapsed time for attention_prob_times_values (160x2048x2048x288): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x288): 141.970

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1828.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x289x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x289x2048): 71.076
Elapsed time for attention_prob_times_values (160x2048x2048x289): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x289): 59.028

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 792.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x290x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x290x2048): 101.895
Elapsed time for attention_prob_times_values (160x2048x2048x290): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x290): 100.040

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1244.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x291x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x291x2048): 71.157
Elapsed time for attention_prob_times_values (160x2048x2048x291): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x291): 59.322

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 800.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x292x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x292x2048): 101.923
Elapsed time for attention_prob_times_values (160x2048x2048x292): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x292): 94.110

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1214.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x293x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x293x2048): 70.495
Elapsed time for attention_prob_times_values (160x2048x2048x293): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x293): 59.444

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 802.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x294x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x294x2048): 91.148
Elapsed time for attention_prob_times_values (160x2048x2048x294): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x294): 98.329

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 1181.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x295x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x295x2048): 66.465
Elapsed time for attention_prob_times_values (160x2048x2048x295): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x295): 58.878

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 781.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x296x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x296x2048): 134.273
Elapsed time for attention_prob_times_values (160x2048x2048x296): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x296): 145.113

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1752.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x297x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x297x2048): 69.215
Elapsed time for attention_prob_times_values (160x2048x2048x297): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x297): 58.499

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 799.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x298x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x298x2048): 106.143
Elapsed time for attention_prob_times_values (160x2048x2048x298): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x298): 99.744

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1300.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x299x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x299x2048): 67.421
Elapsed time for attention_prob_times_values (160x2048x2048x299): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x299): 60.309

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 807.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x300x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x300x2048): 106.277
Elapsed time for attention_prob_times_values (160x2048x2048x300): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x300): 95.702

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1280.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x301x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x301x2048): 69.103
Elapsed time for attention_prob_times_values (160x2048x2048x301): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x301): 55.408

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 784.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x302x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x302x2048): 106.536
Elapsed time for attention_prob_times_values (160x2048x2048x302): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x302): 103.400

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1342.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x303x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x303x2048): 71.049
Elapsed time for attention_prob_times_values (160x2048x2048x303): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x303): 59.819

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 833.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x304x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x304x2048): 143.319
Elapsed time for attention_prob_times_values (160x2048x2048x304): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x304): 141.564

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1833.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x305x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x305x2048): 69.270
Elapsed time for attention_prob_times_values (160x2048x2048x305): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x305): 18.241

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 372.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x306x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x306x2048): 102.874
Elapsed time for attention_prob_times_values (160x2048x2048x306): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x306): 103.449

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1336.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x307x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x307x2048): 69.626
Elapsed time for attention_prob_times_values (160x2048x2048x307): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x307): 61.840

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 851.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x308x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x308x2048): 106.439
Elapsed time for attention_prob_times_values (160x2048x2048x308): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x308): 104.357

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1373.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x309x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x309x2048): 71.091
Elapsed time for attention_prob_times_values (160x2048x2048x309): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x309): 62.035

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 865.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x310x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x310x2048): 108.139
Elapsed time for attention_prob_times_values (160x2048x2048x310): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x310): 102.547

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1380.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x311x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x311x2048): 70.696
Elapsed time for attention_prob_times_values (160x2048x2048x311): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x311): 60.901

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 860.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x312x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x312x2048): 132.659
Elapsed time for attention_prob_times_values (160x2048x2048x312): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x312): 143.897

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1820.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x313x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x313x2048): 69.541
Elapsed time for attention_prob_times_values (160x2048x2048x313): 0.0220
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x313): 19.119

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 396.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x314x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x314x2048): 108.074
Elapsed time for attention_prob_times_values (160x2048x2048x314): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x314): 106.172

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1420.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x315x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x315x2048): 69.467
Elapsed time for attention_prob_times_values (160x2048x2048x315): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x315): 62.907

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 878.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x316x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x316x2048): 110.533
Elapsed time for attention_prob_times_values (160x2048x2048x316): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x316): 105.863

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1443.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x317x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x317x2048): 69.281
Elapsed time for attention_prob_times_values (160x2048x2048x317): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x317): 63.595

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 887.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x318x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x318x2048): 110.710
Elapsed time for attention_prob_times_values (160x2048x2048x318): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x318): 108.051

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1467.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x319x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x319x2048): 70.835
Elapsed time for attention_prob_times_values (160x2048x2048x319): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x319): 63.280

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 899.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x320x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x320x2048): 167.436
Elapsed time for attention_prob_times_values (160x2048x2048x320): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x320): 157.855

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 2193.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x321x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x321x2048): 69.452
Elapsed time for attention_prob_times_values (160x2048x2048x321): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x321): 63.999

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 901.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x322x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x322x2048): 108.124
Elapsed time for attention_prob_times_values (160x2048x2048x322): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x322): 107.362

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1462.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x323x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x323x2048): 68.016
Elapsed time for attention_prob_times_values (160x2048x2048x323): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x323): 63.150

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 891.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x324x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x324x2048): 104.202
Elapsed time for attention_prob_times_values (160x2048x2048x324): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x324): 108.744

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1453.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x325x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x325x2048): 69.355
Elapsed time for attention_prob_times_values (160x2048x2048x325): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x325): 63.631

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 908.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x326x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x326x2048): 108.915
Elapsed time for attention_prob_times_values (160x2048x2048x326): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x326): 106.916

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1482.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x327x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x327x2048): 69.684
Elapsed time for attention_prob_times_values (160x2048x2048x327): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x327): 62.881

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 910.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x328x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x328x2048): 142.398
Elapsed time for attention_prob_times_values (160x2048x2048x328): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x328): 160.368

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 2083.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x329x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x329x2048): 71.317
Elapsed time for attention_prob_times_values (160x2048x2048x329): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x329): 63.270

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 928.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x330x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x330x2048): 106.272
Elapsed time for attention_prob_times_values (160x2048x2048x330): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x330): 109.234

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1496.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x331x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x331x2048): 72.598
Elapsed time for attention_prob_times_values (160x2048x2048x331): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x331): 66.046

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 963.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x332x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x332x2048): 110.291
Elapsed time for attention_prob_times_values (160x2048x2048x332): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x332): 109.928

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1538.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x333x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x333x2048): 71.027
Elapsed time for attention_prob_times_values (160x2048x2048x333): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x333): 64.904

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 950.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x334x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x334x2048): 98.737
Elapsed time for attention_prob_times_values (160x2048x2048x334): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x334): 111.574

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1471.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x335x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x335x2048): 72.201
Elapsed time for attention_prob_times_values (160x2048x2048x335): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x335): 64.769

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 961.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x336x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x336x2048): 153.445
Elapsed time for attention_prob_times_values (160x2048x2048x336): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x336): 165.371

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 2248.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x337x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x337x2048): 73.212
Elapsed time for attention_prob_times_values (160x2048x2048x337): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x337): 63.877

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 966.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x338x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x338x2048): 110.274
Elapsed time for attention_prob_times_values (160x2048x2048x338): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x338): 110.626

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1568.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x339x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x339x2048): 72.078
Elapsed time for attention_prob_times_values (160x2048x2048x339): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x339): 66.437

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 984.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x340x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x340x2048): 113.049
Elapsed time for attention_prob_times_values (160x2048x2048x340): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x340): 112.023

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1607.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x341x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x341x2048): 73.138
Elapsed time for attention_prob_times_values (160x2048x2048x341): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x341): 66.629

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 998.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x342x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x342x2048): 111.896
Elapsed time for attention_prob_times_values (160x2048x2048x342): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x342): 109.379

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 1588.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x343x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x343x2048): 74.579
Elapsed time for attention_prob_times_values (160x2048x2048x343): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x343): 66.453

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1011.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x344x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x344x2048): 171.513
Elapsed time for attention_prob_times_values (160x2048x2048x344): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x344): 162.982

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 2413.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x345x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x345x2048): 74.305
Elapsed time for attention_prob_times_values (160x2048x2048x345): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x345): 66.543

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1016.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x346x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x346x2048): 115.659
Elapsed time for attention_prob_times_values (160x2048x2048x346): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x346): 112.492

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1655.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x347x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x347x2048): 73.941
Elapsed time for attention_prob_times_values (160x2048x2048x347): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x347): 67.993

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1031.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x348x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x348x2048): 114.498
Elapsed time for attention_prob_times_values (160x2048x2048x348): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x348): 114.385

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1670.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x349x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x349x2048): 73.980
Elapsed time for attention_prob_times_values (160x2048x2048x349): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x349): 68.207

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1038.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x350x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x350x2048): 112.758
Elapsed time for attention_prob_times_values (160x2048x2048x350): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x350): 111.387

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1644.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x351x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x351x2048): 75.154
Elapsed time for attention_prob_times_values (160x2048x2048x351): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x351): 67.804

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1048.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x352x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x352x2048): 196.957
Elapsed time for attention_prob_times_values (160x2048x2048x352): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x352): 163.619

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 2636.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x353x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x353x2048): 73.833
Elapsed time for attention_prob_times_values (160x2048x2048x353): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x353): 68.313

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1049.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x354x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x354x2048): 109.569
Elapsed time for attention_prob_times_values (160x2048x2048x354): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x354): 111.438

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1638.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x355x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x355x2048): 72.640
Elapsed time for attention_prob_times_values (160x2048x2048x355): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x355): 69.466

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1055.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x356x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x356x2048): 108.453
Elapsed time for attention_prob_times_values (160x2048x2048x356): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x356): 115.750

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 1669.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x357x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x357x2048): 72.020
Elapsed time for attention_prob_times_values (160x2048x2048x357): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x357): 68.954

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1052.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x358x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x358x2048): 112.208
Elapsed time for attention_prob_times_values (160x2048x2048x358): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x358): 115.341

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1704.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x359x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x359x2048): 71.906
Elapsed time for attention_prob_times_values (160x2048x2048x359): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x359): 68.795

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1056.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x360x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x360x2048): 181.499
Elapsed time for attention_prob_times_values (160x2048x2048x360): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x360): 167.600

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 2624.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x361x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x361x2048): 71.240
Elapsed time for attention_prob_times_values (160x2048x2048x361): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x361): 69.719

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1064.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x362x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x362x2048): 110.227
Elapsed time for attention_prob_times_values (160x2048x2048x362): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x362): 116.364

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1714.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x363x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x363x2048): 71.773
Elapsed time for attention_prob_times_values (160x2048x2048x363): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x363): 70.427

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1079.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x364x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x364x2048): 111.028
Elapsed time for attention_prob_times_values (160x2048x2048x364): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x364): 115.503

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1723.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x365x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x365x2048): 72.110
Elapsed time for attention_prob_times_values (160x2048x2048x365): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x365): 70.603

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1088.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x366x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x366x2048): 111.996
Elapsed time for attention_prob_times_values (160x2048x2048x366): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x366): 118.318

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 1760.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x367x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x367x2048): 72.198
Elapsed time for attention_prob_times_values (160x2048x2048x367): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x367): 69.949

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1089.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x368x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x368x2048): 190.200
Elapsed time for attention_prob_times_values (160x2048x2048x368): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x368): 177.216

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 2820.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x369x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x369x2048): 35.549
Elapsed time for attention_prob_times_values (160x2048x2048x369): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x369): 70.996

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 730.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x370x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x370x2048): 109.087
Elapsed time for attention_prob_times_values (160x2048x2048x370): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x370): 121.361

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1775.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x371x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x371x2048): 72.066
Elapsed time for attention_prob_times_values (160x2048x2048x371): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x371): 65.690

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1064.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x372x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x372x2048): 112.948
Elapsed time for attention_prob_times_values (160x2048x2048x372): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x372): 117.525

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1789.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x373x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x373x2048): 72.920
Elapsed time for attention_prob_times_values (160x2048x2048x373): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x373): 73.445

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1139.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x374x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x374x2048): 111.181
Elapsed time for attention_prob_times_values (160x2048x2048x374): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x374): 122.260

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1817.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x375x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x375x2048): 73.413
Elapsed time for attention_prob_times_values (160x2048x2048x375): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x375): 71.749

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1135.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x376x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x376x2048): 176.288
Elapsed time for attention_prob_times_values (160x2048x2048x376): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x376): 179.609

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 2791.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x377x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x377x2048): 74.016
Elapsed time for attention_prob_times_values (160x2048x2048x377): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x377): 73.551

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1160.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x378x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x378x2048): 110.342
Elapsed time for attention_prob_times_values (160x2048x2048x378): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x378): 122.960

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1833.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x379x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x379x2048): 75.472
Elapsed time for attention_prob_times_values (160x2048x2048x379): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x379): 74.983

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1188.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x380x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x380x2048): 111.580
Elapsed time for attention_prob_times_values (160x2048x2048x380): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x380): 125.122

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1868.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x381x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x381x2048): 74.595
Elapsed time for attention_prob_times_values (160x2048x2048x381): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x381): 76.277

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1197.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x382x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x382x2048): 104.285
Elapsed time for attention_prob_times_values (160x2048x2048x382): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x382): 123.569

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 1800.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x383x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x383x2048): 76.946
Elapsed time for attention_prob_times_values (160x2048x2048x383): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x383): 74.883

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1211.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x384x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x384x2048): 190.209
Elapsed time for attention_prob_times_values (160x2048x2048x384): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x384): 186.276

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 3011.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x385x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x385x2048): 74.370
Elapsed time for attention_prob_times_values (160x2048x2048x385): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x385): 76.117

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1206.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x386x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x386x2048): 105.592
Elapsed time for attention_prob_times_values (160x2048x2048x386): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x386): 124.235

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 1835.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x387x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x387x2048): 73.184
Elapsed time for attention_prob_times_values (160x2048x2048x387): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x387): 74.805

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1192.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x388x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x388x2048): 107.229
Elapsed time for attention_prob_times_values (160x2048x2048x388): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x388): 124.551

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1861.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x389x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x389x2048): 74.503
Elapsed time for attention_prob_times_values (160x2048x2048x389): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x389): 73.867

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1201.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x390x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x390x2048): 106.258
Elapsed time for attention_prob_times_values (160x2048x2048x390): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x390): 123.795

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1856.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x391x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x391x2048): 74.531
Elapsed time for attention_prob_times_values (160x2048x2048x391): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x391): 73.427

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1203.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x392x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x392x2048): 182.967
Elapsed time for attention_prob_times_values (160x2048x2048x392): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x392): 185.489

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 3005.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x393x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x393x2048): 73.872
Elapsed time for attention_prob_times_values (160x2048x2048x393): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x393): 72.800

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1199.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x394x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x394x2048): 110.718
Elapsed time for attention_prob_times_values (160x2048x2048x394): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x394): 124.727

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1922.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x395x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x395x2048): 75.422
Elapsed time for attention_prob_times_values (160x2048x2048x395): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x395): 73.660

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1224.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x396x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x396x2048): 111.086
Elapsed time for attention_prob_times_values (160x2048x2048x396): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x396): 118.220

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1886.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x397x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x397x2048): 75.351
Elapsed time for attention_prob_times_values (160x2048x2048x397): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x397): 74.480

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1236.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x398x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x398x2048): 111.110
Elapsed time for attention_prob_times_values (160x2048x2048x398): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x398): 121.557

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1921.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x399x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x399x2048): 76.149
Elapsed time for attention_prob_times_values (160x2048x2048x399): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x399): 75.223

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1255.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x400x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x400x2048): 182.672
Elapsed time for attention_prob_times_values (160x2048x2048x400): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x400): 186.871

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 3071.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x401x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x401x2048): 76.990
Elapsed time for attention_prob_times_values (160x2048x2048x401): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x401): 75.778

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1272.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x402x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x402x2048): 108.104
Elapsed time for attention_prob_times_values (160x2048x2048x402): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x402): 121.162

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1908.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x403x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x403x2048): 76.868
Elapsed time for attention_prob_times_values (160x2048x2048x403): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x403): 76.003

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1279.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x404x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x404x2048): 112.679
Elapsed time for attention_prob_times_values (160x2048x2048x404): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x404): 124.124

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1982.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x405x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x405x2048): 75.608
Elapsed time for attention_prob_times_values (160x2048x2048x405): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x405): 76.255

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1277.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x406x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x406x2048): 112.790
Elapsed time for attention_prob_times_values (160x2048x2048x406): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x406): 125.664

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2004.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x407x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x407x2048): 77.671
Elapsed time for attention_prob_times_values (160x2048x2048x407): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x407): 76.021

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1298.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x408x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x408x2048): 180.754
Elapsed time for attention_prob_times_values (160x2048x2048x408): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x408): 187.267

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 3115.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x409x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x409x2048): 76.223
Elapsed time for attention_prob_times_values (160x2048x2048x409): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x409): 74.918

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1282.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x410x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x410x2048): 114.256
Elapsed time for attention_prob_times_values (160x2048x2048x410): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x410): 119.399

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1986.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x411x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x411x2048): 75.501
Elapsed time for attention_prob_times_values (160x2048x2048x411): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x411): 77.070

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1300.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x412x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x412x2048): 111.032
Elapsed time for attention_prob_times_values (160x2048x2048x412): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x412): 125.676

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2015.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x413x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x413x2048): 78.714
Elapsed time for attention_prob_times_values (160x2048x2048x413): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x413): 77.414

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1337.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x414x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x414x2048): 115.047
Elapsed time for attention_prob_times_values (160x2048x2048x414): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x414): 125.857

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2064.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x415x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x415x2048): 79.050
Elapsed time for attention_prob_times_values (160x2048x2048x415): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x415): 79.440

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1363.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x416x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x416x2048): 192.493
Elapsed time for attention_prob_times_values (160x2048x2048x416): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x416): 197.660

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 3364.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x417x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x417x2048): 78.341
Elapsed time for attention_prob_times_values (160x2048x2048x417): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x417): 76.768

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1340.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x418x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x418x2048): 112.289
Elapsed time for attention_prob_times_values (160x2048x2048x418): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x418): 126.465

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2061.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x419x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x419x2048): 76.350
Elapsed time for attention_prob_times_values (160x2048x2048x419): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x419): 78.481

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1344.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x420x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x420x2048): 114.944
Elapsed time for attention_prob_times_values (160x2048x2048x420): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x420): 128.068

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 2108.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x421x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x421x2048): 74.785
Elapsed time for attention_prob_times_values (160x2048x2048x421): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x421): 78.728

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1338.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x422x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x422x2048): 112.345
Elapsed time for attention_prob_times_values (160x2048x2048x422): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x422): 127.244

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2086.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x423x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x423x2048): 75.963
Elapsed time for attention_prob_times_values (160x2048x2048x423): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x423): 77.181

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1341.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x424x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x424x2048): 173.127
Elapsed time for attention_prob_times_values (160x2048x2048x424): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x424): 194.389

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 3216.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x425x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x425x2048): 73.894
Elapsed time for attention_prob_times_values (160x2048x2048x425): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x425): 77.516

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1331.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x426x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x426x2048): 114.888
Elapsed time for attention_prob_times_values (160x2048x2048x426): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x426): 127.814

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2134.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x427x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x427x2048): 77.487
Elapsed time for attention_prob_times_values (160x2048x2048x427): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x427): 79.733

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1389.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x428x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x428x2048): 119.007
Elapsed time for attention_prob_times_values (160x2048x2048x428): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x428): 129.164

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 2194.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x429x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x429x2048): 37.229
Elapsed time for attention_prob_times_values (160x2048x2048x429): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x429): 81.980

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 909.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x430x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x430x2048): 119.081
Elapsed time for attention_prob_times_values (160x2048x2048x430): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x430): 131.487

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2224.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x431x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x431x2048): 76.780
Elapsed time for attention_prob_times_values (160x2048x2048x431): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x431): 78.902

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1388.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x432x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x432x2048): 194.384
Elapsed time for attention_prob_times_values (160x2048x2048x432): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x432): 198.672

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 3512.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x433x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x433x2048): 73.494
Elapsed time for attention_prob_times_values (160x2048x2048x433): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x433): 77.716

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1353.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x434x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x434x2048): 120.183
Elapsed time for attention_prob_times_values (160x2048x2048x434): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x434): 131.003

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 2250.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x435x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x435x2048): 78.033
Elapsed time for attention_prob_times_values (160x2048x2048x435): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x435): 82.129

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1439.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x436x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x436x2048): 120.470
Elapsed time for attention_prob_times_values (160x2048x2048x436): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x436): 133.860

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2286.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x437x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x437x2048): 78.155
Elapsed time for attention_prob_times_values (160x2048x2048x437): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x437): 81.108

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1438.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x438x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x438x2048): 117.718
Elapsed time for attention_prob_times_values (160x2048x2048x438): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x438): 132.054

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2254.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x439x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x439x2048): 77.750
Elapsed time for attention_prob_times_values (160x2048x2048x439): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x439): 79.827

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1429.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x440x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x440x2048): 179.974
Elapsed time for attention_prob_times_values (160x2048x2048x440): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x440): 179.608

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 3269.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x441x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x441x2048): 78.464
Elapsed time for attention_prob_times_values (160x2048x2048x441): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x441): 80.861

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1451.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x442x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x442x2048): 120.467
Elapsed time for attention_prob_times_values (160x2048x2048x442): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x442): 114.213

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2141.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x443x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x443x2048): 79.651
Elapsed time for attention_prob_times_values (160x2048x2048x443): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x443): 83.337

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1490.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x444x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x444x2048): 121.600
Elapsed time for attention_prob_times_values (160x2048x2048x444): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x444): 137.139

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2364.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x445x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x445x2048): 80.144
Elapsed time for attention_prob_times_values (160x2048x2048x445): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x445): 83.554

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1503.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x446x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x446x2048): 121.580
Elapsed time for attention_prob_times_values (160x2048x2048x446): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x446): 134.438

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 2352.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x447x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x447x2048): 80.160
Elapsed time for attention_prob_times_values (160x2048x2048x447): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x447): 83.327

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1508.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x448x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x448x2048): 204.063
Elapsed time for attention_prob_times_values (160x2048x2048x448): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x448): 211.934

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 3846.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x449x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x449x2048): 79.448
Elapsed time for attention_prob_times_values (160x2048x2048x449): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x449): 82.530

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1500.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x450x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x450x2048): 113.778
Elapsed time for attention_prob_times_values (160x2048x2048x450): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x450): 136.779

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2307.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x451x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x451x2048): 79.577
Elapsed time for attention_prob_times_values (160x2048x2048x451): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x451): 84.064

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1522.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x452x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x452x2048): 114.826
Elapsed time for attention_prob_times_values (160x2048x2048x452): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x452): 131.351

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2286.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x453x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x453x2048): 78.101
Elapsed time for attention_prob_times_values (160x2048x2048x453): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x453): 81.231

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1488.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x454x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x454x2048): 115.699
Elapsed time for attention_prob_times_values (160x2048x2048x454): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x454): 135.156

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2335.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x455x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x455x2048): 78.932
Elapsed time for attention_prob_times_values (160x2048x2048x455): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x455): 40.003

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 996.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x456x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x456x2048): 178.951
Elapsed time for attention_prob_times_values (160x2048x2048x456): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x456): 203.315

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3581.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x457x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x457x2048): 78.822
Elapsed time for attention_prob_times_values (160x2048x2048x457): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x457): 81.709

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1512.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x458x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x458x2048): 116.544
Elapsed time for attention_prob_times_values (160x2048x2048x458): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x458): 135.181

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2364.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x459x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x459x2048): 79.808
Elapsed time for attention_prob_times_values (160x2048x2048x459): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x459): 85.158

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1559.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x460x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x460x2048): 115.787
Elapsed time for attention_prob_times_values (160x2048x2048x460): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x460): 135.836

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2371.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x461x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x461x2048): 79.052
Elapsed time for attention_prob_times_values (160x2048x2048x461): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x461): 85.374

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1560.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x462x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x462x2048): 50.688
Elapsed time for attention_prob_times_values (160x2048x2048x462): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x462): 134.737

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1403.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x463x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x463x2048): 78.835
Elapsed time for attention_prob_times_values (160x2048x2048x463): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x463): 84.494

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1556.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x464x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x464x2048): 188.503
Elapsed time for attention_prob_times_values (160x2048x2048x464): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x464): 39.582

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1251.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x465x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x465x2048): 79.692
Elapsed time for attention_prob_times_values (160x2048x2048x465): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x465): 83.147

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1559.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x466x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x466x2048): 115.714
Elapsed time for attention_prob_times_values (160x2048x2048x466): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x466): 133.762

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2382.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x467x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x467x2048): 79.329
Elapsed time for attention_prob_times_values (160x2048x2048x467): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x467): 85.345

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1582.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x468x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x468x2048): 117.057
Elapsed time for attention_prob_times_values (160x2048x2048x468): 0.0194
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x468): 32.324

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 976.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x469x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x469x2048): 79.971
Elapsed time for attention_prob_times_values (160x2048x2048x469): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x469): 85.708

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1598.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x470x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x470x2048): 116.307
Elapsed time for attention_prob_times_values (160x2048x2048x470): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x470): 138.346

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2446.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x471x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x471x2048): 78.749
Elapsed time for attention_prob_times_values (160x2048x2048x471): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x471): 82.434

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1562.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x472x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x472x2048): 184.318
Elapsed time for attention_prob_times_values (160x2048x2048x472): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x472): 210.449

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3819.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x473x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x473x2048): 79.642
Elapsed time for attention_prob_times_values (160x2048x2048x473): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x473): 84.833

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1600.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x474x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x474x2048): 106.323
Elapsed time for attention_prob_times_values (160x2048x2048x474): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x474): 138.610

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 2348.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x475x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x475x2048): 77.079
Elapsed time for attention_prob_times_values (160x2048x2048x475): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x475): 76.051

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1497.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x476x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x476x2048): 118.852
Elapsed time for attention_prob_times_values (160x2048x2048x476): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x476): 138.741

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2508.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x477x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x477x2048): 79.843
Elapsed time for attention_prob_times_values (160x2048x2048x477): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x477): 87.903

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1642.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x478x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x478x2048): 119.330
Elapsed time for attention_prob_times_values (160x2048x2048x478): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x478): 139.780

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2532.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x479x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x479x2048): 81.760
Elapsed time for attention_prob_times_values (160x2048x2048x479): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x479): 87.089

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1662.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x480x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x480x2048): 199.039
Elapsed time for attention_prob_times_values (160x2048x2048x480): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x480): 214.918

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 4081.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x481x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x481x2048): 81.230
Elapsed time for attention_prob_times_values (160x2048x2048x481): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x481): 87.739

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1669.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x482x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x482x2048): 123.669
Elapsed time for attention_prob_times_values (160x2048x2048x482): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x482): 142.616

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2626.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x483x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x483x2048): 79.972
Elapsed time for attention_prob_times_values (160x2048x2048x483): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x483): 89.972

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1682.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x484x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x484x2048): 122.805
Elapsed time for attention_prob_times_values (160x2048x2048x484): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x484): 141.461

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2617.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x485x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x485x2048): 79.506
Elapsed time for attention_prob_times_values (160x2048x2048x485): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x485): 88.955

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1674.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x486x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x486x2048): 120.627
Elapsed time for attention_prob_times_values (160x2048x2048x486): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x486): 138.753

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2579.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x487x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x487x2048): 79.576
Elapsed time for attention_prob_times_values (160x2048x2048x487): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x487): 84.907

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1645.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x488x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x488x2048): 175.472
Elapsed time for attention_prob_times_values (160x2048x2048x488): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x488): 52.647

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1624.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x489x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x489x2048): 77.536
Elapsed time for attention_prob_times_values (160x2048x2048x489): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x489): 85.918

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1638.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x490x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x490x2048): 122.822
Elapsed time for attention_prob_times_values (160x2048x2048x490): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x490): 140.208

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2637.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x491x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x491x2048): 80.814
Elapsed time for attention_prob_times_values (160x2048x2048x491): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x491): 89.953

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1718.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x492x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x492x2048): 123.246
Elapsed time for attention_prob_times_values (160x2048x2048x492): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x492): 142.850

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2675.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x493x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x493x2048): 81.770
Elapsed time for attention_prob_times_values (160x2048x2048x493): 0.0272
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x493): 24.321

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 759.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0353
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x494x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x494x2048): 124.651
Elapsed time for attention_prob_times_values (160x2048x2048x494): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x494): 144.993

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2720.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x495x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x495x2048): 81.143
Elapsed time for attention_prob_times_values (160x2048x2048x495): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x495): 88.363

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1720.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x496x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x496x2048): 202.718
Elapsed time for attention_prob_times_values (160x2048x2048x496): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x496): 227.436

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 4367.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x497x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x497x2048): 81.616
Elapsed time for attention_prob_times_values (160x2048x2048x497): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x497): 89.257

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1740.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x498x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x498x2048): 120.043
Elapsed time for attention_prob_times_values (160x2048x2048x498): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x498): 141.633

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2657.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x499x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x499x2048): 83.461
Elapsed time for attention_prob_times_values (160x2048x2048x499): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x499): 91.279

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1786.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x500x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x500x2048): 128.940
Elapsed time for attention_prob_times_values (160x2048x2048x500): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x500): 63.746

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1751.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x501x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x501x2048): 77.110
Elapsed time for attention_prob_times_values (160x2048x2048x501): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x501): 87.581

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1687.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x502x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x502x2048): 127.620
Elapsed time for attention_prob_times_values (160x2048x2048x502): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x502): 149.540

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2838.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x503x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x503x2048): 82.408
Elapsed time for attention_prob_times_values (160x2048x2048x503): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x503): 90.662

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1782.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x504x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x504x2048): 192.802
Elapsed time for attention_prob_times_values (160x2048x2048x504): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x504): 217.482

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 4228.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x505x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x505x2048): 83.026
Elapsed time for attention_prob_times_values (160x2048x2048x505): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x505): 92.455

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1813.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x506x2048): 0.0365
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x506x2048): 18.611
Elapsed time for attention_prob_times_values (160x2048x2048x506): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x506): 151.172

Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 688.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x507x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x507x2048): 84.518
Elapsed time for attention_prob_times_values (160x2048x2048x507): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x507): 94.312

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1854.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x508x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x508x2048): 129.256
Elapsed time for attention_prob_times_values (160x2048x2048x508): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x508): 151.096

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2904.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x509x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x509x2048): 85.065
Elapsed time for attention_prob_times_values (160x2048x2048x509): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x509): 95.048

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1874.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x510x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x510x2048): 129.888
Elapsed time for attention_prob_times_values (160x2048x2048x510): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x510): 152.943

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2939.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x511x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x511x2048): 84.993
Elapsed time for attention_prob_times_values (160x2048x2048x511): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x511): 94.652

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1877.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x512x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x512x2048): 201.729
Elapsed time for attention_prob_times_values (160x2048x2048x512): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x512): 229.959

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 4513.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x513x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x513x2048): 84.581
Elapsed time for attention_prob_times_values (160x2048x2048x513): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x513): 71.395

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1629.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x514x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x514x2048): 121.491
Elapsed time for attention_prob_times_values (160x2048x2048x514): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x514): 119.930

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 2544.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x515x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x515x2048): 84.497
Elapsed time for attention_prob_times_values (160x2048x2048x515): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x515): 73.449

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1659.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x516x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x516x2048): 122.997
Elapsed time for attention_prob_times_values (160x2048x2048x516): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x516): 42.117

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1327.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x517x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x517x2048): 77.928
Elapsed time for attention_prob_times_values (160x2048x2048x517): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x517): 71.748

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1583.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x518x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x518x2048): 113.290
Elapsed time for attention_prob_times_values (160x2048x2048x518): 0.0272
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x518): 25.519

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 884.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x519x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x519x2048): 82.029
Elapsed time for attention_prob_times_values (160x2048x2048x519): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x519): 71.065

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1620.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x520x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x520x2048): 194.288
Elapsed time for attention_prob_times_values (160x2048x2048x520): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x520): 158.303

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 3718.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x521x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x521x2048): 81.410
Elapsed time for attention_prob_times_values (160x2048x2048x521): 0.0229
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x521): 30.510

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 947.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x522x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x522x2048): 121.154
Elapsed time for attention_prob_times_values (160x2048x2048x522): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x522): 117.879

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 2556.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x523x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x523x2048): 83.867
Elapsed time for attention_prob_times_values (160x2048x2048x523): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x523): 72.767

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1669.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x524x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x524x2048): 121.593
Elapsed time for attention_prob_times_values (160x2048x2048x524): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x524): 115.590

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 2544.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x525x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x525x2048): 84.740
Elapsed time for attention_prob_times_values (160x2048x2048x525): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x525): 72.733

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1683.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x526x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x526x2048): 124.621
Elapsed time for attention_prob_times_values (160x2048x2048x526): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x526): 118.246

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 2614.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x527x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x527x2048): 82.612
Elapsed time for attention_prob_times_values (160x2048x2048x527): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x527): 70.946

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1647.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x528x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x528x2048): 204.371
Elapsed time for attention_prob_times_values (160x2048x2048x528): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x528): 166.875

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 3973.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x529x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x529x2048): 83.854
Elapsed time for attention_prob_times_values (160x2048x2048x529): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x529): 68.692

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1636.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x530x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x530x2048): 125.089
Elapsed time for attention_prob_times_values (160x2048x2048x530): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x530): 34.488

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1173.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x531x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x531x2048): 82.846
Elapsed time for attention_prob_times_values (160x2048x2048x531): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x531): 73.060

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1688.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x532x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x532x2048): 125.551
Elapsed time for attention_prob_times_values (160x2048x2048x532): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x532): 118.845

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 2659.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x533x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x533x2048): 83.802
Elapsed time for attention_prob_times_values (160x2048x2048x533): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x533): 72.805

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1700.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x534x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x534x2048): 124.432
Elapsed time for attention_prob_times_values (160x2048x2048x534): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x534): 117.976

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 2647.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x535x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x535x2048): 82.056
Elapsed time for attention_prob_times_values (160x2048x2048x535): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x535): 71.189

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1669.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x536x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x536x2048): 172.178
Elapsed time for attention_prob_times_values (160x2048x2048x536): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x536): 167.225

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 3722.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x537x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x537x2048): 81.591
Elapsed time for attention_prob_times_values (160x2048x2048x537): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x537): 70.834

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1666.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x538x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x538x2048): 123.918
Elapsed time for attention_prob_times_values (160x2048x2048x538): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x538): 117.573

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2656.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x539x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x539x2048): 82.150
Elapsed time for attention_prob_times_values (160x2048x2048x539): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x539): 74.128

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1718.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x540x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x540x2048): 125.337
Elapsed time for attention_prob_times_values (160x2048x2048x540): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x540): 121.539

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 2726.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x541x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x541x2048): 43.964
Elapsed time for attention_prob_times_values (160x2048x2048x541): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x541): 73.921

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1220.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x542x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x542x2048): 123.270
Elapsed time for attention_prob_times_values (160x2048x2048x542): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x542): 119.579

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2691.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x543x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x543x2048): 36.233
Elapsed time for attention_prob_times_values (160x2048x2048x543): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x543): 74.214

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1081.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x544x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x544x2048): 212.818
Elapsed time for attention_prob_times_values (160x2048x2048x544): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x544): 171.402

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 4224.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x545x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x545x2048): 83.349
Elapsed time for attention_prob_times_values (160x2048x2048x545): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x545): 73.860

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1745.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x546x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x546x2048): 125.853
Elapsed time for attention_prob_times_values (160x2048x2048x546): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x546): 121.500

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 2760.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x547x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x547x2048): 81.063
Elapsed time for attention_prob_times_values (160x2048x2048x547): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x547): 74.627

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1738.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x548x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x548x2048): 128.055
Elapsed time for attention_prob_times_values (160x2048x2048x548): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x548): 122.114

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 2801.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x549x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x549x2048): 80.382
Elapsed time for attention_prob_times_values (160x2048x2048x549): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x549): 75.613

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1749.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x550x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x550x2048): 124.777
Elapsed time for attention_prob_times_values (160x2048x2048x550): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x550): 121.311

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2766.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x551x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x551x2048): 81.092
Elapsed time for attention_prob_times_values (160x2048x2048x551): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x551): 73.857

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1741.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x552x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x552x2048): 182.804
Elapsed time for attention_prob_times_values (160x2048x2048x552): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x552): 165.830

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 3923.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x553x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x553x2048): 81.263
Elapsed time for attention_prob_times_values (160x2048x2048x553): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x553): 72.176

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1727.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x554x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x554x2048): 125.442
Elapsed time for attention_prob_times_values (160x2048x2048x554): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x554): 121.655

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2796.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x555x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x555x2048): 81.464
Elapsed time for attention_prob_times_values (160x2048x2048x555): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x555): 74.938

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1770.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x556x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x556x2048): 129.876
Elapsed time for attention_prob_times_values (160x2048x2048x556): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x556): 119.571

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2828.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x557x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x557x2048): 82.027
Elapsed time for attention_prob_times_values (160x2048x2048x557): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x557): 75.842

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1793.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x558x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x558x2048): 127.419
Elapsed time for attention_prob_times_values (160x2048x2048x558): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x558): 121.967

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2841.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x559x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x559x2048): 82.420
Elapsed time for attention_prob_times_values (160x2048x2048x559): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x559): 72.799

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1765.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x560x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x560x2048): 189.091
Elapsed time for attention_prob_times_values (160x2048x2048x560): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x560): 177.917

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 4193.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x561x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x561x2048): 83.556
Elapsed time for attention_prob_times_values (160x2048x2048x561): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x561): 73.833

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1796.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x562x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x562x2048): 127.700
Elapsed time for attention_prob_times_values (160x2048x2048x562): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x562): 118.047

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 2815.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x563x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x563x2048): 83.758
Elapsed time for attention_prob_times_values (160x2048x2048x563): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x563): 72.665

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1789.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x564x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x564x2048): 128.062
Elapsed time for attention_prob_times_values (160x2048x2048x564): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x564): 121.088

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 2866.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x565x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x565x2048): 83.777
Elapsed time for attention_prob_times_values (160x2048x2048x565): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x565): 76.242

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1841.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x566x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x566x2048): 128.963
Elapsed time for attention_prob_times_values (160x2048x2048x566): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x566): 122.080

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 2898.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x567x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x567x2048): 82.323
Elapsed time for attention_prob_times_values (160x2048x2048x567): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x567): 74.660

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1812.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x568x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x568x2048): 183.126
Elapsed time for attention_prob_times_values (160x2048x2048x568): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x568): 176.700

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 4170.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x569x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x569x2048): 82.817
Elapsed time for attention_prob_times_values (160x2048x2048x569): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x569): 73.025

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1802.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x570x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x570x2048): 129.476
Elapsed time for attention_prob_times_values (160x2048x2048x570): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x570): 121.749

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 2919.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x571x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x571x2048): 83.803
Elapsed time for attention_prob_times_values (160x2048x2048x571): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x571): 73.673

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1827.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x572x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x572x2048): 121.503
Elapsed time for attention_prob_times_values (160x2048x2048x572): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x572): 124.303

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 2868.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x573x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x573x2048): 83.583
Elapsed time for attention_prob_times_values (160x2048x2048x573): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x573): 77.558

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1881.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x574x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x574x2048): 130.876
Elapsed time for attention_prob_times_values (160x2048x2048x574): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x574): 125.027

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 2995.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x575x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x575x2048): 85.078
Elapsed time for attention_prob_times_values (160x2048x2048x575): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x575): 75.651

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1878.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x576x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x576x2048): 197.070
Elapsed time for attention_prob_times_values (160x2048x2048x576): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x576): 178.793

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 4405.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x577x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x577x2048): 82.507
Elapsed time for attention_prob_times_values (160x2048x2048x577): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x577): 75.898

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1861.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x578x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x578x2048): 126.221
Elapsed time for attention_prob_times_values (160x2048x2048x578): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x578): 124.927

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 2960.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x579x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x579x2048): 82.406
Elapsed time for attention_prob_times_values (160x2048x2048x579): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x579): 77.731

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1889.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x580x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x580x2048): 122.859
Elapsed time for attention_prob_times_values (160x2048x2048x580): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x580): 122.624

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 2903.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x581x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x581x2048): 82.047
Elapsed time for attention_prob_times_values (160x2048x2048x581): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x581): 77.368

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1887.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x582x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x582x2048): 122.784
Elapsed time for attention_prob_times_values (160x2048x2048x582): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x582): 125.430

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 2945.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x583x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x583x2048): 81.792
Elapsed time for attention_prob_times_values (160x2048x2048x583): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x583): 75.455

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1866.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x584x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x584x2048): 183.461
Elapsed time for attention_prob_times_values (160x2048x2048x584): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x584): 175.743

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 4274.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x585x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x585x2048): 82.432
Elapsed time for attention_prob_times_values (160x2048x2048x585): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x585): 75.395

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1878.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x586x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x586x2048): 127.017
Elapsed time for attention_prob_times_values (160x2048x2048x586): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x586): 122.549

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 2980.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x587x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x587x2048): 83.214
Elapsed time for attention_prob_times_values (160x2048x2048x587): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x587): 78.518

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1933.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x588x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x588x2048): 124.563
Elapsed time for attention_prob_times_values (160x2048x2048x588): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x588): 124.101

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 2980.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x589x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x589x2048): 82.896
Elapsed time for attention_prob_times_values (160x2048x2048x589): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x589): 78.064

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1930.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x590x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x590x2048): 124.476
Elapsed time for attention_prob_times_values (160x2048x2048x590): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x590): 124.448

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 2992.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x591x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x591x2048): 81.935
Elapsed time for attention_prob_times_values (160x2048x2048x591): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x591): 76.520

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1906.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x592x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x592x2048): 194.529
Elapsed time for attention_prob_times_values (160x2048x2048x592): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x592): 178.935

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 4497.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x593x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x593x2048): 81.060
Elapsed time for attention_prob_times_values (160x2048x2048x593): 0.0317
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x593): 25.084

Attention duration (in seconds): 0.0415
Attention throughput (in TFLOP/s): 925.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0415
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x594x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x594x2048): 125.560
Elapsed time for attention_prob_times_values (160x2048x2048x594): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x594): 126.649

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3052.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x595x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x595x2048): 83.607
Elapsed time for attention_prob_times_values (160x2048x2048x595): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x595): 79.272

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1972.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x596x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x596x2048): 121.433
Elapsed time for attention_prob_times_values (160x2048x2048x596): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x596): 125.676

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 2999.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x597x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x597x2048): 83.650
Elapsed time for attention_prob_times_values (160x2048x2048x597): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x597): 78.590

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1970.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x598x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x598x2048): 121.139
Elapsed time for attention_prob_times_values (160x2048x2048x598): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x598): 126.245

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3011.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x599x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x599x2048): 82.657
Elapsed time for attention_prob_times_values (160x2048x2048x599): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x599): 76.499

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1938.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x600x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x600x2048): 188.341
Elapsed time for attention_prob_times_values (160x2048x2048x600): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x600): 176.824

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 4457.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x601x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x601x2048): 83.839
Elapsed time for attention_prob_times_values (160x2048x2048x601): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x601): 77.241

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1968.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x602x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x602x2048): 128.980
Elapsed time for attention_prob_times_values (160x2048x2048x602): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x602): 124.589

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3107.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x603x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x603x2048): 83.792
Elapsed time for attention_prob_times_values (160x2048x2048x603): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x603): 66.393

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1819.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x604x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x604x2048): 127.660
Elapsed time for attention_prob_times_values (160x2048x2048x604): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x604): 126.318

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3123.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x605x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x605x2048): 83.733
Elapsed time for attention_prob_times_values (160x2048x2048x605): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x605): 79.366

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2007.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x606x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x606x2048): 127.938
Elapsed time for attention_prob_times_values (160x2048x2048x606): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x606): 126.576

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3139.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x607x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x607x2048): 84.998
Elapsed time for attention_prob_times_values (160x2048x2048x607): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x607): 78.617

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2018.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x608x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x608x2048): 195.161
Elapsed time for attention_prob_times_values (160x2048x2048x608): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x608): 190.068

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 4766.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x609x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x609x2048): 32.601
Elapsed time for attention_prob_times_values (160x2048x2048x609): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x609): 78.855

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1143.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x610x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x610x2048): 128.455
Elapsed time for attention_prob_times_values (160x2048x2048x610): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x610): 129.516

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3202.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x611x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x611x2048): 80.756
Elapsed time for attention_prob_times_values (160x2048x2048x611): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x611): 80.224

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2001.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x612x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x612x2048): 131.604
Elapsed time for attention_prob_times_values (160x2048x2048x612): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x612): 129.307

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3248.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x613x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x613x2048): 81.881
Elapsed time for attention_prob_times_values (160x2048x2048x613): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x613): 78.264

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1996.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x614x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x614x2048): 128.057
Elapsed time for attention_prob_times_values (160x2048x2048x614): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x614): 128.304

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3202.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x615x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x615x2048): 81.768
Elapsed time for attention_prob_times_values (160x2048x2048x615): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x615): 78.017

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1998.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x616x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x616x2048): 188.568
Elapsed time for attention_prob_times_values (160x2048x2048x616): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x616): 184.928

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 4679.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x617x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x617x2048): 81.572
Elapsed time for attention_prob_times_values (160x2048x2048x617): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x617): 78.360

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2006.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x618x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x618x2048): 128.686
Elapsed time for attention_prob_times_values (160x2048x2048x618): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x618): 129.698

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3247.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x619x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x619x2048): 80.995
Elapsed time for attention_prob_times_values (160x2048x2048x619): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x619): 80.451

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2032.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x620x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x620x2048): 129.862
Elapsed time for attention_prob_times_values (160x2048x2048x620): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x620): 129.371

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3268.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x621x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x621x2048): 81.574
Elapsed time for attention_prob_times_values (160x2048x2048x621): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x621): 80.512

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2046.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x622x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x622x2048): 130.005
Elapsed time for attention_prob_times_values (160x2048x2048x622): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x622): 130.694

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3297.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x623x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x623x2048): 81.545
Elapsed time for attention_prob_times_values (160x2048x2048x623): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x623): 78.434

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2025.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x624x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x624x2048): 199.150
Elapsed time for attention_prob_times_values (160x2048x2048x624): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x624): 197.138

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 5027.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x625x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x625x2048): 81.668
Elapsed time for attention_prob_times_values (160x2048x2048x625): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x625): 79.321

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2045.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x626x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x626x2048): 131.231
Elapsed time for attention_prob_times_values (160x2048x2048x626): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x626): 130.695

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3333.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x627x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x627x2048): 81.470
Elapsed time for attention_prob_times_values (160x2048x2048x627): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x627): 82.489

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2089.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x628x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x628x2048): 132.429
Elapsed time for attention_prob_times_values (160x2048x2048x628): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x628): 132.370

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3380.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x629x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x629x2048): 81.551
Elapsed time for attention_prob_times_values (160x2048x2048x629): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x629): 79.895

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2063.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x630x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x630x2048): 133.541
Elapsed time for attention_prob_times_values (160x2048x2048x630): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x630): 133.150

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3414.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x631x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x631x2048): 81.231
Elapsed time for attention_prob_times_values (160x2048x2048x631): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x631): 80.193

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2070.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x632x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x632x2048): 187.858
Elapsed time for attention_prob_times_values (160x2048x2048x632): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x632): 185.973

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 4801.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x633x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x633x2048): 80.959
Elapsed time for attention_prob_times_values (160x2048x2048x633): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x633): 80.673

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2079.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x634x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x634x2048): 133.704
Elapsed time for attention_prob_times_values (160x2048x2048x634): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x634): 134.948

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3460.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x635x2048): 0.0215
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x635x2048): 39.703
Elapsed time for attention_prob_times_values (160x2048x2048x635): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x635): 82.977

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1385.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x636x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x636x2048): 44.877
Elapsed time for attention_prob_times_values (160x2048x2048x636): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x636): 136.270

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1744.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x637x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x637x2048): 81.304
Elapsed time for attention_prob_times_values (160x2048x2048x637): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x637): 81.280

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2104.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x638x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x638x2048): 126.234
Elapsed time for attention_prob_times_values (160x2048x2048x638): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x638): 135.763

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3391.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x639x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x639x2048): 81.906
Elapsed time for attention_prob_times_values (160x2048x2048x639): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x639): 82.453

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2133.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x640x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x640x2048): 190.518
Elapsed time for attention_prob_times_values (160x2048x2048x640): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x640): 197.743

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 5045.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x641x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x641x2048): 80.677
Elapsed time for attention_prob_times_values (160x2048x2048x641): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x641): 81.892

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2116.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x642x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x642x2048): 126.986
Elapsed time for attention_prob_times_values (160x2048x2048x642): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x642): 136.228

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 3427.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x643x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x643x2048): 80.087
Elapsed time for attention_prob_times_values (160x2048x2048x643): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x643): 82.562

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2123.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x644x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x644x2048): 127.377
Elapsed time for attention_prob_times_values (160x2048x2048x644): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x644): 131.465

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3384.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x645x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x645x2048): 81.609
Elapsed time for attention_prob_times_values (160x2048x2048x645): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x645): 81.477

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2136.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x646x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x646x2048): 126.115
Elapsed time for attention_prob_times_values (160x2048x2048x646): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x646): 134.159

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3410.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x647x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x647x2048): 81.627
Elapsed time for attention_prob_times_values (160x2048x2048x647): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x647): 81.062

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2137.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x648x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x648x2048): 194.338
Elapsed time for attention_prob_times_values (160x2048x2048x648): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x648): 194.224

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5112.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x649x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x649x2048): 81.060
Elapsed time for attention_prob_times_values (160x2048x2048x649): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x649): 80.537

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2129.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x650x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x650x2048): 129.245
Elapsed time for attention_prob_times_values (160x2048x2048x650): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x650): 133.163

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 3461.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x651x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x651x2048): 81.671
Elapsed time for attention_prob_times_values (160x2048x2048x651): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x651): 82.170

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2165.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x652x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x652x2048): 127.672
Elapsed time for attention_prob_times_values (160x2048x2048x652): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x652): 132.939

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3447.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x653x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x653x2048): 82.117
Elapsed time for attention_prob_times_values (160x2048x2048x653): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x653): 82.430

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2180.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x654x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x654x2048): 129.164
Elapsed time for attention_prob_times_values (160x2048x2048x654): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x654): 132.875

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3477.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x655x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x655x2048): 82.796
Elapsed time for attention_prob_times_values (160x2048x2048x655): 0.0186
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x655): 47.335

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1601.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x656x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x656x2048): 198.685
Elapsed time for attention_prob_times_values (160x2048x2048x656): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x656): 197.801

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 5278.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x657x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x657x2048): 83.071
Elapsed time for attention_prob_times_values (160x2048x2048x657): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x657): 81.807

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2198.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x658x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x658x2048): 129.031
Elapsed time for attention_prob_times_values (160x2048x2048x658): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x658): 133.394

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3502.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x659x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x659x2048): 83.708
Elapsed time for attention_prob_times_values (160x2048x2048x659): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x659): 82.437

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2221.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x660x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x660x2048): 129.716
Elapsed time for attention_prob_times_values (160x2048x2048x660): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x660): 133.612

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3525.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x661x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x661x2048): 84.476
Elapsed time for attention_prob_times_values (160x2048x2048x661): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x661): 83.408

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2251.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x662x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x662x2048): 130.169
Elapsed time for attention_prob_times_values (160x2048x2048x662): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x662): 135.888

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 3571.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x663x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x663x2048): 85.091
Elapsed time for attention_prob_times_values (160x2048x2048x663): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x663): 81.979

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2246.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x664x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x664x2048): 191.800
Elapsed time for attention_prob_times_values (160x2048x2048x664): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x664): 196.302

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 5226.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x665x2048): 0.0212
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x665x2048): 42.069
Elapsed time for attention_prob_times_values (160x2048x2048x665): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x665): 81.944

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 1499.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x666x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x666x2048): 130.700
Elapsed time for attention_prob_times_values (160x2048x2048x666): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x666): 133.193

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3564.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x667x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x667x2048): 85.983
Elapsed time for attention_prob_times_values (160x2048x2048x667): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x667): 84.484

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2305.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x668x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x668x2048): 130.442
Elapsed time for attention_prob_times_values (160x2048x2048x668): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x668): 133.380

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3573.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x669x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x669x2048): 86.393
Elapsed time for attention_prob_times_values (160x2048x2048x669): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x669): 84.775

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2321.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x670x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x670x2048): 131.397
Elapsed time for attention_prob_times_values (160x2048x2048x670): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x670): 136.042

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3632.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x671x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x671x2048): 87.533
Elapsed time for attention_prob_times_values (160x2048x2048x671): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x671): 84.334

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2337.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x672x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x672x2048): 202.745
Elapsed time for attention_prob_times_values (160x2048x2048x672): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x672): 200.012

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5487.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x673x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x673x2048): 86.261
Elapsed time for attention_prob_times_values (160x2048x2048x673): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x673): 84.062

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2323.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x674x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x674x2048): 132.403
Elapsed time for attention_prob_times_values (160x2048x2048x674): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x674): 134.173

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3642.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x675x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x675x2048): 85.341
Elapsed time for attention_prob_times_values (160x2048x2048x675): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x675): 84.360

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2322.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x676x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x676x2048): 110.379
Elapsed time for attention_prob_times_values (160x2048x2048x676): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x676): 138.853

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3370.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x677x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x677x2048): 85.812
Elapsed time for attention_prob_times_values (160x2048x2048x677): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x677): 84.851

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2341.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x678x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x678x2048): 132.583
Elapsed time for attention_prob_times_values (160x2048x2048x678): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x678): 136.886

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3702.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x679x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x679x2048): 86.042
Elapsed time for attention_prob_times_values (160x2048x2048x679): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x679): 82.039

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2311.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x680x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x680x2048): 192.655
Elapsed time for attention_prob_times_values (160x2048x2048x680): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x680): 188.379

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 5250.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x681x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x681x2048): 86.420
Elapsed time for attention_prob_times_values (160x2048x2048x681): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x681): 83.439

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2343.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x682x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x682x2048): 134.139
Elapsed time for attention_prob_times_values (160x2048x2048x682): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x682): 136.262

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3736.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x683x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x683x2048): 86.880
Elapsed time for attention_prob_times_values (160x2048x2048x683): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x683): 85.552

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2386.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x684x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x684x2048): 134.232
Elapsed time for attention_prob_times_values (160x2048x2048x684): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x684): 81.682

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2815.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x685x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x685x2048): 86.612
Elapsed time for attention_prob_times_values (160x2048x2048x685): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x685): 85.424

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2387.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x686x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x686x2048): 136.379
Elapsed time for attention_prob_times_values (160x2048x2048x686): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x686): 136.896

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3798.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x687x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x687x2048): 86.878
Elapsed time for attention_prob_times_values (160x2048x2048x687): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x687): 84.624

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2386.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x688x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x688x2048): 198.630
Elapsed time for attention_prob_times_values (160x2048x2048x688): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x688): 199.974

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 5555.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x689x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x689x2048): 87.266
Elapsed time for attention_prob_times_values (160x2048x2048x689): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x689): 84.348

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2394.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x690x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x690x2048): 135.000
Elapsed time for attention_prob_times_values (160x2048x2048x690): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x690): 138.327

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3819.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x691x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x691x2048): 86.743
Elapsed time for attention_prob_times_values (160x2048x2048x691): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x691): 86.337

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2422.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x692x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x692x2048): 135.717
Elapsed time for attention_prob_times_values (160x2048x2048x692): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x692): 138.995

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3849.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x693x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x693x2048): 87.284
Elapsed time for attention_prob_times_values (160x2048x2048x693): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x693): 86.598

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2440.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x694x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x694x2048): 135.773
Elapsed time for attention_prob_times_values (160x2048x2048x694): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x694): 137.799

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3844.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x695x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x695x2048): 51.864
Elapsed time for attention_prob_times_values (160x2048x2048x695): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x695): 85.008

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 1813.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x696x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x696x2048): 188.200
Elapsed time for attention_prob_times_values (160x2048x2048x696): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x696): 204.784

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 5528.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x697x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x697x2048): 87.379
Elapsed time for attention_prob_times_values (160x2048x2048x697): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x697): 85.000

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2432.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x698x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x698x2048): 138.223
Elapsed time for attention_prob_times_values (160x2048x2048x698): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x698): 138.784

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3914.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x699x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x699x2048): 87.447
Elapsed time for attention_prob_times_values (160x2048x2048x699): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x699): 88.061

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2483.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x700x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x700x2048): 139.025
Elapsed time for attention_prob_times_values (160x2048x2048x700): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x700): 138.191

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 3928.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x701x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x701x2048): 87.759
Elapsed time for attention_prob_times_values (160x2048x2048x701): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x701): 87.689

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2489.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x702x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x702x2048): 136.825
Elapsed time for attention_prob_times_values (160x2048x2048x702): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x702): 141.717

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3957.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x703x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x703x2048): 88.876
Elapsed time for attention_prob_times_values (160x2048x2048x703): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x703): 86.390

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2493.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x704x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x704x2048): 198.785
Elapsed time for attention_prob_times_values (160x2048x2048x704): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x704): 217.386

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 5918.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x705x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x705x2048): 86.714
Elapsed time for attention_prob_times_values (160x2048x2048x705): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x705): 87.003

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2478.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x706x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x706x2048): 133.053
Elapsed time for attention_prob_times_values (160x2048x2048x706): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x706): 139.248

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 3888.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x707x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x707x2048): 86.772
Elapsed time for attention_prob_times_values (160x2048x2048x707): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x707): 87.179

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2488.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x708x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x708x2048): 132.371
Elapsed time for attention_prob_times_values (160x2048x2048x708): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x708): 136.993

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 3858.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x709x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x709x2048): 85.870
Elapsed time for attention_prob_times_values (160x2048x2048x709): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x709): 85.994

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2465.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x710x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x710x2048): 130.713
Elapsed time for attention_prob_times_values (160x2048x2048x710): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x710): 135.398

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 3822.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x711x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x711x2048): 85.210
Elapsed time for attention_prob_times_values (160x2048x2048x711): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x711): 84.148

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2436.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x712x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x712x2048): 193.952
Elapsed time for attention_prob_times_values (160x2048x2048x712): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x712): 204.521

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 5736.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x713x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x713x2048): 84.809
Elapsed time for attention_prob_times_values (160x2048x2048x713): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x713): 84.058

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2435.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x714x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x714x2048): 131.085
Elapsed time for attention_prob_times_values (160x2048x2048x714): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x714): 138.540

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 3891.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x715x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x715x2048): 86.541
Elapsed time for attention_prob_times_values (160x2048x2048x715): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x715): 87.102

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2511.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x716x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x716x2048): 134.474
Elapsed time for attention_prob_times_values (160x2048x2048x716): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x716): 139.582

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 3968.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x717x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x717x2048): 83.639
Elapsed time for attention_prob_times_values (160x2048x2048x717): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x717): 86.522

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2467.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x718x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x718x2048): 134.107
Elapsed time for attention_prob_times_values (160x2048x2048x718): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x718): 136.447

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 3929.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x719x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x719x2048): 84.552
Elapsed time for attention_prob_times_values (160x2048x2048x719): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x719): 85.533

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2473.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x720x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x720x2048): 193.260
Elapsed time for attention_prob_times_values (160x2048x2048x720): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x720): 207.263

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 5825.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x721x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x721x2048): 85.102
Elapsed time for attention_prob_times_values (160x2048x2048x721): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x721): 85.637

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2489.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x722x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x722x2048): 131.337
Elapsed time for attention_prob_times_values (160x2048x2048x722): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x722): 140.591

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 3965.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x723x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x723x2048): 84.676
Elapsed time for attention_prob_times_values (160x2048x2048x723): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x723): 86.705

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2505.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x724x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x724x2048): 132.407
Elapsed time for attention_prob_times_values (160x2048x2048x724): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x724): 141.215

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4001.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x725x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x725x2048): 84.608
Elapsed time for attention_prob_times_values (160x2048x2048x725): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x725): 86.510

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2508.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x726x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x726x2048): 131.861
Elapsed time for attention_prob_times_values (160x2048x2048x726): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x726): 138.430

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 3965.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x727x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x727x2048): 84.559
Elapsed time for attention_prob_times_values (160x2048x2048x727): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x727): 85.335

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2497.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x728x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x728x2048): 189.367
Elapsed time for attention_prob_times_values (160x2048x2048x728): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x728): 212.148

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 5890.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x729x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x729x2048): 84.215
Elapsed time for attention_prob_times_values (160x2048x2048x729): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x729): 85.718

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2504.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x730x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x730x2048): 133.565
Elapsed time for attention_prob_times_values (160x2048x2048x730): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x730): 138.948

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4020.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x731x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x731x2048): 85.324
Elapsed time for attention_prob_times_values (160x2048x2048x731): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x731): 87.854

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2558.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x732x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x732x2048): 133.277
Elapsed time for attention_prob_times_values (160x2048x2048x732): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x732): 142.544

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4076.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x733x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x733x2048): 85.246
Elapsed time for attention_prob_times_values (160x2048x2048x733): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x733): 87.422

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2557.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x734x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x734x2048): 133.801
Elapsed time for attention_prob_times_values (160x2048x2048x734): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x734): 142.691

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4097.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x735x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x735x2048): 86.553
Elapsed time for attention_prob_times_values (160x2048x2048x735): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x735): 87.144

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2580.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x736x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x736x2048): 205.236
Elapsed time for attention_prob_times_values (160x2048x2048x736): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x736): 211.869

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 6202.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x737x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x737x2048): 85.807
Elapsed time for attention_prob_times_values (160x2048x2048x737): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x737): 86.552

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2567.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x738x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x738x2048): 135.460
Elapsed time for attention_prob_times_values (160x2048x2048x738): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x738): 144.231

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4167.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x739x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x739x2048): 85.086
Elapsed time for attention_prob_times_values (160x2048x2048x739): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x739): 88.834

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2596.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x740x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x740x2048): 137.782
Elapsed time for attention_prob_times_values (160x2048x2048x740): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x740): 142.704

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4192.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x741x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x741x2048): 85.165
Elapsed time for attention_prob_times_values (160x2048x2048x741): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x741): 87.865

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2590.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x742x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x742x2048): 137.830
Elapsed time for attention_prob_times_values (160x2048x2048x742): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x742): 143.686

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4218.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x743x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x743x2048): 84.949
Elapsed time for attention_prob_times_values (160x2048x2048x743): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x743): 86.388

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2571.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x744x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x744x2048): 192.214
Elapsed time for attention_prob_times_values (160x2048x2048x744): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x744): 202.806

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 5933.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x745x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x745x2048): 85.358
Elapsed time for attention_prob_times_values (160x2048x2048x745): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x745): 86.543

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2587.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x746x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x746x2048): 136.875
Elapsed time for attention_prob_times_values (160x2048x2048x746): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x746): 142.829

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4213.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x747x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x747x2048): 84.944
Elapsed time for attention_prob_times_values (160x2048x2048x747): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x747): 88.784

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2620.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x748x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x748x2048): 138.066
Elapsed time for attention_prob_times_values (160x2048x2048x748): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x748): 145.328

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4279.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x749x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x749x2048): 85.541
Elapsed time for attention_prob_times_values (160x2048x2048x749): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x749): 88.182

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2627.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x750x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x750x2048): 139.880
Elapsed time for attention_prob_times_values (160x2048x2048x750): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x750): 145.195

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 4316.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x751x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x751x2048): 85.520
Elapsed time for attention_prob_times_values (160x2048x2048x751): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x751): 88.155

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2633.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x752x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x752x2048): 196.582
Elapsed time for attention_prob_times_values (160x2048x2048x752): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x752): 199.115

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 6009.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x753x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x753x2048): 57.086
Elapsed time for attention_prob_times_values (160x2048x2048x753): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x753): 87.764

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2103.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x754x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x754x2048): 139.866
Elapsed time for attention_prob_times_values (160x2048x2048x754): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x754): 145.964

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 4350.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x755x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x755x2048): 83.083
Elapsed time for attention_prob_times_values (160x2048x2048x755): 0.0258
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x755): 39.347

Attention duration (in seconds): 0.0380
Attention throughput (in TFLOP/s): 1628.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0380
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x756x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x756x2048): 136.119
Elapsed time for attention_prob_times_values (160x2048x2048x756): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x756): 148.662

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4338.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x757x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x757x2048): 85.444
Elapsed time for attention_prob_times_values (160x2048x2048x757): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x757): 88.397

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2656.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x758x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x758x2048): 138.819
Elapsed time for attention_prob_times_values (160x2048x2048x758): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x758): 149.856

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 4411.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x759x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x759x2048): 86.279
Elapsed time for attention_prob_times_values (160x2048x2048x759): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x759): 88.776

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2682.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x760x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x760x2048): 195.371
Elapsed time for attention_prob_times_values (160x2048x2048x760): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x760): 215.223

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 6285.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x761x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x761x2048): 82.849
Elapsed time for attention_prob_times_values (160x2048x2048x761): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x761): 89.179

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2639.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x762x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x762x2048): 140.309
Elapsed time for attention_prob_times_values (160x2048x2048x762): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x762): 149.367

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 4451.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x763x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x763x2048): 83.242
Elapsed time for attention_prob_times_values (160x2048x2048x763): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x763): 91.128

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2680.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x764x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x764x2048): 141.446
Elapsed time for attention_prob_times_values (160x2048x2048x764): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x764): 152.095

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 4520.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x765x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x765x2048): 85.920
Elapsed time for attention_prob_times_values (160x2048x2048x765): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x765): 91.177

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2732.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x766x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x766x2048): 137.087
Elapsed time for attention_prob_times_values (160x2048x2048x766): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x766): 151.162

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 4445.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x767x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x767x2048): 86.353
Elapsed time for attention_prob_times_values (160x2048x2048x767): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x767): 91.404

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2749.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x768x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x768x2048): 206.311
Elapsed time for attention_prob_times_values (160x2048x2048x768): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x768): 225.568

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 6680.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x769x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x769x2048): 86.021
Elapsed time for attention_prob_times_values (160x2048x2048x769): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x769): 77.144

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2524.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x770x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x770x2048): 135.156
Elapsed time for attention_prob_times_values (160x2048x2048x770): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x770): 128.844

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4099.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x771x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x771x2048): 84.815
Elapsed time for attention_prob_times_values (160x2048x2048x771): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x771): 77.971

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2528.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x772x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x772x2048): 136.028
Elapsed time for attention_prob_times_values (160x2048x2048x772): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x772): 127.736

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4104.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x773x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x773x2048): 85.084
Elapsed time for attention_prob_times_values (160x2048x2048x773): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x773): 78.289

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2543.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x774x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x774x2048): 137.478
Elapsed time for attention_prob_times_values (160x2048x2048x774): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x774): 126.933

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4122.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x775x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x775x2048): 84.914
Elapsed time for attention_prob_times_values (160x2048x2048x775): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x775): 76.478

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2516.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x776x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x776x2048): 194.978
Elapsed time for attention_prob_times_values (160x2048x2048x776): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x776): 184.138

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 5930.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x777x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x777x2048): 84.699
Elapsed time for attention_prob_times_values (160x2048x2048x777): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x777): 76.279

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2516.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x778x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x778x2048): 138.110
Elapsed time for attention_prob_times_values (160x2048x2048x778): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x778): 123.953

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4101.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x779x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x779x2048): 85.841
Elapsed time for attention_prob_times_values (160x2048x2048x779): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x779): 78.036

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2569.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x780x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x780x2048): 138.408
Elapsed time for attention_prob_times_values (160x2048x2048x780): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x780): 127.992

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4185.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x781x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x781x2048): 86.371
Elapsed time for attention_prob_times_values (160x2048x2048x781): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x781): 77.876

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2580.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x782x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x782x2048): 139.246
Elapsed time for attention_prob_times_values (160x2048x2048x782): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x782): 126.527

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 4182.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x783x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x783x2048): 86.235
Elapsed time for attention_prob_times_values (160x2048x2048x783): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x783): 77.307

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2575.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x784x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x784x2048): 205.175
Elapsed time for attention_prob_times_values (160x2048x2048x784): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x784): 187.125

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 6190.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x785x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x785x2048): 86.807
Elapsed time for attention_prob_times_values (160x2048x2048x785): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x785): 76.250

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2570.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x786x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x786x2048): 135.859
Elapsed time for attention_prob_times_values (160x2048x2048x786): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x786): 126.560

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 4154.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x787x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x787x2048): 86.701
Elapsed time for attention_prob_times_values (160x2048x2048x787): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x787): 77.885

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2604.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x788x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x788x2048): 134.548
Elapsed time for attention_prob_times_values (160x2048x2048x788): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x788): 126.297

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 4140.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x789x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x789x2048): 86.758
Elapsed time for attention_prob_times_values (160x2048x2048x789): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x789): 76.020

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2578.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x790x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x790x2048): 138.787
Elapsed time for attention_prob_times_values (160x2048x2048x790): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x790): 127.408

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4232.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x791x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x791x2048): 86.941
Elapsed time for attention_prob_times_values (160x2048x2048x791): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x791): 76.596

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2597.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x792x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x792x2048): 197.116
Elapsed time for attention_prob_times_values (160x2048x2048x792): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x792): 186.029

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 6113.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x793x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x793x2048): 86.901
Elapsed time for attention_prob_times_values (160x2048x2048x793): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x793): 76.912

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2609.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x794x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x794x2048): 138.793
Elapsed time for attention_prob_times_values (160x2048x2048x794): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x794): 127.436

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4253.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x795x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x795x2048): 87.030
Elapsed time for attention_prob_times_values (160x2048x2048x795): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x795): 79.986

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2672.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x796x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x796x2048): 140.257
Elapsed time for attention_prob_times_values (160x2048x2048x796): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x796): 127.852

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4293.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x797x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x797x2048): 88.063
Elapsed time for attention_prob_times_values (160x2048x2048x797): 0.0447
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x797): 23.948

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 1209.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0568
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x798x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x798x2048): 136.995
Elapsed time for attention_prob_times_values (160x2048x2048x798): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x798): 128.038

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 4258.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x799x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x799x2048): 88.800
Elapsed time for attention_prob_times_values (160x2048x2048x799): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x799): 78.634

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2686.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x800x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x800x2048): 207.444
Elapsed time for attention_prob_times_values (160x2048x2048x800): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x800): 188.530

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 6370.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x801x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x801x2048): 86.888
Elapsed time for attention_prob_times_values (160x2048x2048x801): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x801): 78.511

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2663.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x802x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x802x2048): 138.099
Elapsed time for attention_prob_times_values (160x2048x2048x802): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x802): 128.871

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 4310.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x803x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x803x2048): 86.129
Elapsed time for attention_prob_times_values (160x2048x2048x803): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x803): 80.407

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2691.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x804x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x804x2048): 140.598
Elapsed time for attention_prob_times_values (160x2048x2048x804): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x804): 128.887

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4358.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x805x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x805x2048): 70.874
Elapsed time for attention_prob_times_values (160x2048x2048x805): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x805): 79.939

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 2437.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x806x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x806x2048): 138.626
Elapsed time for attention_prob_times_values (160x2048x2048x806): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x806): 128.819

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 4338.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x807x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x807x2048): 86.405
Elapsed time for attention_prob_times_values (160x2048x2048x807): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x807): 78.198

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2670.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x808x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x808x2048): 198.025
Elapsed time for attention_prob_times_values (160x2048x2048x808): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x808): 182.977

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 6193.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x809x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x809x2048): 86.571
Elapsed time for attention_prob_times_values (160x2048x2048x809): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x809): 78.638

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2686.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x810x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x810x2048): 140.016
Elapsed time for attention_prob_times_values (160x2048x2048x810): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x810): 128.760

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 4378.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x811x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x811x2048): 86.961
Elapsed time for attention_prob_times_values (160x2048x2048x811): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x811): 81.168

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2743.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x812x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x812x2048): 143.121
Elapsed time for attention_prob_times_values (160x2048x2048x812): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x812): 130.507

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 4466.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x813x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x813x2048): 87.769
Elapsed time for attention_prob_times_values (160x2048x2048x813): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x813): 81.340

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2765.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x814x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x814x2048): 140.388
Elapsed time for attention_prob_times_values (160x2048x2048x814): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x814): 129.817

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 4424.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x815x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x815x2048): 88.055
Elapsed time for attention_prob_times_values (160x2048x2048x815): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x815): 79.912

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2751.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x816x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x816x2048): 199.942
Elapsed time for attention_prob_times_values (160x2048x2048x816): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x816): 182.891

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 6280.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x817x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x817x2048): 88.627
Elapsed time for attention_prob_times_values (160x2048x2048x817): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x817): 79.357

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2756.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x818x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x818x2048): 139.865
Elapsed time for attention_prob_times_values (160x2048x2048x818): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x818): 130.337

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 4446.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x819x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x819x2048): 89.057
Elapsed time for attention_prob_times_values (160x2048x2048x819): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x819): 82.144

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2819.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x820x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x820x2048): 140.609
Elapsed time for attention_prob_times_values (160x2048x2048x820): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x820): 130.019

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 4462.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
