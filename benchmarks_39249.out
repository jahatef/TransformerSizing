bash: /fsx/home-jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
/fsx/home-jacob/write_hostfile.sh: line 7: /fsx/home-quentin/jacob/hostfiles/hosts_39249: Permission denied
1.13.1 

[2023-10-22 21:42:59,120] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-22 21:42:59,971] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.149.177, master_port=6000
[2023-10-22 21:42:59,971] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-22 21:43:03,218] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.209
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 8.711
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.254
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.412
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 32.169
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.573
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.742
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 51.242
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.133
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.959
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 61.221
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.575
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1.403
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 73.878
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 2.433
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1.622
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 76.657
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 2.940
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2.201
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 83.463
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 4.120
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3.324
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 89.015
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 6.346
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2.971
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 89.147
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 5.863
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 3.159
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 98.075
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 6.382
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 4.009
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 92.253
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 8.165
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 4.291
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 93.393
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 8.872
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 5.178
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 98.347
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 10.770
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 5.383
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 98.726
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 11.328
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 6.444
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 100.837
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 13.548
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 9.439
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 104.945
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 19.359
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 7.406
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 102.897
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 15.714
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 8.749
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 105.135
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 18.454
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 8.868
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 105.971
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 18.839
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 10.205
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 106.777
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 21.473
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 10.238
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 106.521
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 21.697
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 11.986
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 106.558
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 24.967
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 12.016
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 108.820
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 25.261
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 17.556
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 106.454
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 34.508
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 13.506
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 110.097
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 28.199
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 16.183
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 110.953
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 32.904
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 15.439
MLP duration (in seconds): 0.0070
MLP throughput (in TFLOP/s): 112.251
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 31.908
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 18.147
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 110.156
Transformer duration (in seconds): 0.0382
Transformer throughput (in TFLOP/s): 36.162
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 17.218
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 111.819
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 35.080
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 20.114
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 113.798
Transformer duration (in seconds): 0.0398
Transformer throughput (in TFLOP/s): 39.640
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 18.888
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 111.761
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 37.751
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 27.118
MLP duration (in seconds): 0.0097
MLP throughput (in TFLOP/s): 113.345
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 49.616
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 20.628
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 118.062
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 40.889
Transformer - MLP - Attention (in seconds): 0.0012
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 22.049
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 116.618
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 42.982
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 22.012
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 117.984
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 43.202
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 23.520
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 120.059
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 45.561
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 23.566
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 121.036
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 45.766
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 24.884
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 118.133
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 47.472
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 24.961
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 120.044
Transformer duration (in seconds): 0.0547
Transformer throughput (in TFLOP/s): 47.806
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 35.698
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 120.797
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 61.506
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 26.616
MLP duration (in seconds): 0.0152
MLP throughput (in TFLOP/s): 119.133
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 49.768
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 28.227
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 119.971
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 52.118
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 28.609
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 121.557
Transformer duration (in seconds): 0.0600
Transformer throughput (in TFLOP/s): 52.689
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 30.204
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 119.149
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 54.374
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 30.666
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 122.139
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 55.914
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 32.504
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 122.567
Transformer duration (in seconds): 0.0620
Transformer throughput (in TFLOP/s): 58.111
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 32.669
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 122.854
Transformer duration (in seconds): 0.0646
Transformer throughput (in TFLOP/s): 58.172
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 45.533
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 123.079
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 72.518
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0433
Attention throughput (in TFLOP/s): 34.670
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 119.866
Transformer duration (in seconds): 0.0681
Transformer throughput (in TFLOP/s): 59.856
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 36.430
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 122.790
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 62.711
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 36.535
MLP duration (in seconds): 0.0231
MLP throughput (in TFLOP/s): 120.673
Transformer duration (in seconds): 0.0711
Transformer throughput (in TFLOP/s): 62.027
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 38.245
MLP duration (in seconds): 0.0236
MLP throughput (in TFLOP/s): 123.279
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 65.018
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 38.339
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 123.177
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 65.166
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 40.298
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 124.535
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 67.581
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0464
Attention throughput (in TFLOP/s): 40.134
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 120.161
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 66.391
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 54.475
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 123.483
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 81.262
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 42.317
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 119.702
Transformer duration (in seconds): 0.0801
Transformer throughput (in TFLOP/s): 68.410
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 43.886
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 123.194
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 71.215
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0483
Attention throughput (in TFLOP/s): 43.899
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 119.931
Transformer duration (in seconds): 0.0837
Transformer throughput (in TFLOP/s): 70.032
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 45.813
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 124.286
Transformer duration (in seconds): 0.0825
Transformer throughput (in TFLOP/s): 73.435
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 45.736
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 120.683
Transformer duration (in seconds): 0.0867
Transformer throughput (in TFLOP/s): 72.171
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 47.564
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 123.276
Transformer duration (in seconds): 0.0858
Transformer throughput (in TFLOP/s): 75.263
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 47.526
MLP duration (in seconds): 0.0356
MLP throughput (in TFLOP/s): 119.828
Transformer duration (in seconds): 0.0900
Transformer throughput (in TFLOP/s): 74.005
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.104
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 4.549
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.127
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.206
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 15.970
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.286
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.371
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 25.817
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 0.567
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.480
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 30.353
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 0.788
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 0.702
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 37.061
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.217
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 0.811
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 38.166
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.470
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1.101
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 41.732
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 2.061
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1.662
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 44.507
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 3.172
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1.486
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 44.487
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 2.932
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1.580
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 49.102
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 3.191
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2.005
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 46.291
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 4.083
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2.146
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 46.777
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 4.437
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2.590
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 49.205
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 5.386
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2.692
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 49.401
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 5.665
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 3.221
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 50.393
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 6.775
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 4.722
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 52.453
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 9.676
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 3.703
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 51.314
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 7.858
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 4.375
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 52.613
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 9.228
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 4.435
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 53.041
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 9.421
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 5.104
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 53.376
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 10.738
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 5.120
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 53.281
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 10.850
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 5.994
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 53.294
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 12.485
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 6.010
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 54.378
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 12.631
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 8.774
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 53.234
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 17.248
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 6.754
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 55.038
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 14.101
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 8.091
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 55.472
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 16.454
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 7.720
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 56.405
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 15.956
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 9.074
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 55.099
Transformer duration (in seconds): 0.0382
Transformer throughput (in TFLOP/s): 18.081
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 8.611
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 55.893
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 17.545
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 10.059
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 56.896
Transformer duration (in seconds): 0.0399
Transformer throughput (in TFLOP/s): 19.776
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 9.445
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 55.889
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 18.877
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 13.558
MLP duration (in seconds): 0.0097
MLP throughput (in TFLOP/s): 56.703
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 24.820
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 10.316
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 59.061
Transformer duration (in seconds): 0.0464
Transformer throughput (in TFLOP/s): 20.444
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 11.026
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 58.336
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 21.495
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 11.008
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 58.996
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 21.605
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 11.760
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 60.042
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 22.789
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 11.783
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 60.513
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 22.864
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 12.443
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 59.076
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 23.739
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 12.481
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 60.002
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 23.899
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 17.852
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 60.437
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 30.848
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 13.311
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 59.603
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 24.886
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 14.116
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 59.986
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 26.070
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0411
Attention throughput (in TFLOP/s): 14.308
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 60.805
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 26.254
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 15.104
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 59.591
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 27.193
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 15.334
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 61.062
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 27.941
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 16.255
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 61.279
Transformer duration (in seconds): 0.0620
Transformer throughput (in TFLOP/s): 29.060
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 16.338
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 61.430
Transformer duration (in seconds): 0.0646
Transformer throughput (in TFLOP/s): 29.086
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 22.760
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 61.549
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 36.240
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 17.338
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 59.957
Transformer duration (in seconds): 0.0682
Transformer throughput (in TFLOP/s): 29.908
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 18.217
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 61.400
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 31.355
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 18.270
MLP duration (in seconds): 0.0232
MLP throughput (in TFLOP/s): 60.297
Transformer duration (in seconds): 0.0710
Transformer throughput (in TFLOP/s): 31.064
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 19.135
MLP duration (in seconds): 0.0235
MLP throughput (in TFLOP/s): 61.649
Transformer duration (in seconds): 0.0703
Transformer throughput (in TFLOP/s): 32.544
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 19.177
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 61.610
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 32.603
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 20.154
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 62.288
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 33.792
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 20.076
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 60.056
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 33.215
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 27.233
MLP duration (in seconds): 0.0271
MLP throughput (in TFLOP/s): 62.110
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 40.654
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 21.170
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 59.871
Transformer duration (in seconds): 0.0800
Transformer throughput (in TFLOP/s): 34.226
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 21.990
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 61.665
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 35.614
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 22.029
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 59.992
Transformer duration (in seconds): 0.0836
Transformer throughput (in TFLOP/s): 35.058
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 22.917
MLP duration (in seconds): 0.0312
MLP throughput (in TFLOP/s): 61.904
Transformer duration (in seconds): 0.0824
Transformer throughput (in TFLOP/s): 36.725
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 22.867
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 60.311
Transformer duration (in seconds): 0.0866
Transformer throughput (in TFLOP/s): 36.097
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 23.741
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 61.704
Transformer duration (in seconds): 0.0857
Transformer throughput (in TFLOP/s): 37.668
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 4, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 23.758
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 59.992
Transformer duration (in seconds): 0.0900
Transformer throughput (in TFLOP/s): 37.026
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.052
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 2.270
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.063
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.103
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 8.085
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 0.143
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.186
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 12.876
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 0.283
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 0.240
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 15.151
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 0.394
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 0.351
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 18.445
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 0.608
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 0.406
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 19.164
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 0.735
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 0.551
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 20.945
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.031
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 0.831
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 22.240
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.585
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 0.743
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 22.303
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.465
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 0.790
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 24.588
Transformer duration (in seconds): 0.0320
Transformer throughput (in TFLOP/s): 1.596
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1.002
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 23.110
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 2.042
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1.073
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 23.382
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 2.219
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1.295
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 24.612
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 2.693
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1.346
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 24.706
Transformer duration (in seconds): 0.0332
Transformer throughput (in TFLOP/s): 2.833
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1.611
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 25.199
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 3.388
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2.360
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 26.258
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 4.839
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1.850
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 25.742
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 3.929
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 2.188
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 26.325
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 4.615
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2.218
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 26.520
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 4.711
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2.552
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 26.702
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 5.369
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 2.561
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 26.647
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 5.425
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2.997
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 26.654
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 6.243
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 3.005
MLP duration (in seconds): 0.0052
MLP throughput (in TFLOP/s): 27.194
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 6.316
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 4.387
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 26.770
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 8.624
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 3.377
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 27.517
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 7.051
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 4.046
MLP duration (in seconds): 0.0065
MLP throughput (in TFLOP/s): 27.751
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 8.228
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 3.861
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 28.231
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 7.979
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 4.537
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 27.556
Transformer duration (in seconds): 0.0381
Transformer throughput (in TFLOP/s): 9.071
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 4.305
MLP duration (in seconds): 0.0080
MLP throughput (in TFLOP/s): 28.143
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 8.772
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 5.029
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 28.459
Transformer duration (in seconds): 0.0398
Transformer throughput (in TFLOP/s): 9.909
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 4.723
MLP duration (in seconds): 0.0092
MLP throughput (in TFLOP/s): 27.943
Transformer duration (in seconds): 0.0445
Transformer throughput (in TFLOP/s): 9.440
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 6.778
MLP duration (in seconds): 0.0096
MLP throughput (in TFLOP/s): 28.530
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 12.408
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 5.158
MLP duration (in seconds): 0.0099
MLP throughput (in TFLOP/s): 29.534
Transformer duration (in seconds): 0.0463
Transformer throughput (in TFLOP/s): 10.230
Transformer - MLP - Attention (in seconds): 0.0012
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 5.513
MLP duration (in seconds): 0.0106
MLP throughput (in TFLOP/s): 29.168
Transformer duration (in seconds): 0.0467
Transformer throughput (in TFLOP/s): 10.750
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 5.504
MLP duration (in seconds): 0.0111
MLP throughput (in TFLOP/s): 29.510
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 10.803
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 5.881
MLP duration (in seconds): 0.0116
MLP throughput (in TFLOP/s): 30.020
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 11.392
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 5.892
MLP duration (in seconds): 0.0121
MLP throughput (in TFLOP/s): 30.246
Transformer duration (in seconds): 0.0517
Transformer throughput (in TFLOP/s): 11.436
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 6.222
MLP duration (in seconds): 0.0131
MLP throughput (in TFLOP/s): 29.541
Transformer duration (in seconds): 0.0524
Transformer throughput (in TFLOP/s): 11.868
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 6.242
MLP duration (in seconds): 0.0136
MLP throughput (in TFLOP/s): 30.011
Transformer duration (in seconds): 0.0548
Transformer throughput (in TFLOP/s): 11.950
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 8.925
MLP duration (in seconds): 0.0142
MLP throughput (in TFLOP/s): 30.221
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 15.369
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 6.655
MLP duration (in seconds): 0.0151
MLP throughput (in TFLOP/s): 29.785
Transformer duration (in seconds): 0.0579
Transformer throughput (in TFLOP/s): 12.442
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 7.058
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 30.002
Transformer duration (in seconds): 0.0580
Transformer throughput (in TFLOP/s): 13.032
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0411
Attention throughput (in TFLOP/s): 7.154
MLP duration (in seconds): 0.0163
MLP throughput (in TFLOP/s): 30.391
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 13.139
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 7.553
MLP duration (in seconds): 0.0174
MLP throughput (in TFLOP/s): 29.802
Transformer duration (in seconds): 0.0608
Transformer throughput (in TFLOP/s): 13.591
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0417
Attention throughput (in TFLOP/s): 7.667
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 30.521
Transformer duration (in seconds): 0.0618
Transformer throughput (in TFLOP/s): 13.975
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 8.127
MLP duration (in seconds): 0.0185
MLP throughput (in TFLOP/s): 30.646
Transformer duration (in seconds): 0.0621
Transformer throughput (in TFLOP/s): 14.526
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 8.168
MLP duration (in seconds): 0.0193
MLP throughput (in TFLOP/s): 30.710
Transformer duration (in seconds): 0.0647
Transformer throughput (in TFLOP/s): 14.538
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 11.379
MLP duration (in seconds): 0.0201
MLP throughput (in TFLOP/s): 30.768
Transformer duration (in seconds): 0.0540
Transformer throughput (in TFLOP/s): 18.120
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 8.669
MLP duration (in seconds): 0.0215
MLP throughput (in TFLOP/s): 29.982
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 15.033
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 9.107
MLP duration (in seconds): 0.0219
MLP throughput (in TFLOP/s): 30.693
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 15.691
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 9.137
MLP duration (in seconds): 0.0231
MLP throughput (in TFLOP/s): 30.162
Transformer duration (in seconds): 0.0709
Transformer throughput (in TFLOP/s): 15.533
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 9.564
MLP duration (in seconds): 0.0236
MLP throughput (in TFLOP/s): 30.818
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 16.268
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 9.591
MLP duration (in seconds): 0.0245
MLP throughput (in TFLOP/s): 30.799
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 16.297
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 10.079
MLP duration (in seconds): 0.0251
MLP throughput (in TFLOP/s): 31.143
Transformer duration (in seconds): 0.0729
Transformer throughput (in TFLOP/s): 16.905
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 10.038
MLP duration (in seconds): 0.0270
MLP throughput (in TFLOP/s): 30.030
Transformer duration (in seconds): 0.0769
Transformer throughput (in TFLOP/s): 16.610
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 13.612
MLP duration (in seconds): 0.0274
MLP throughput (in TFLOP/s): 30.748
Transformer duration (in seconds): 0.0651
Transformer throughput (in TFLOP/s): 20.324
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 10.583
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 29.944
Transformer duration (in seconds): 0.0801
Transformer throughput (in TFLOP/s): 17.104
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 10.972
MLP duration (in seconds): 0.0294
MLP throughput (in TFLOP/s): 30.720
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 17.801
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 11.013
MLP duration (in seconds): 0.0311
MLP throughput (in TFLOP/s): 30.000
Transformer duration (in seconds): 0.0836
Transformer throughput (in TFLOP/s): 17.515
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 11.457
MLP duration (in seconds): 0.0314
MLP throughput (in TFLOP/s): 30.764
Transformer duration (in seconds): 0.0825
Transformer throughput (in TFLOP/s): 18.360
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 11.431
MLP duration (in seconds): 0.0331
MLP throughput (in TFLOP/s): 30.160
Transformer duration (in seconds): 0.0867
Transformer throughput (in TFLOP/s): 18.039
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 11.900
MLP duration (in seconds): 0.0335
MLP throughput (in TFLOP/s): 30.832
Transformer duration (in seconds): 0.0857
Transformer throughput (in TFLOP/s): 18.828
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0506
Attention throughput (in TFLOP/s): 11.876
MLP duration (in seconds): 0.0355
MLP throughput (in TFLOP/s): 30.026
Transformer duration (in seconds): 0.0901
Transformer throughput (in TFLOP/s): 18.490
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
