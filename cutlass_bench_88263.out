/bin/bash: /fsx/quentin/jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)
bash: /fsx/quentin/jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
[ip-26-0-159-127:2055952] mtl_ofi_component.c:362: mtl:ofi:provider: rdmap16s27-rdm
Traceback (most recent call last):
  File "transformer_flops.py", line 475, in <module>
    benchmark_transformer_from_mm_and_bmm(configuration, seq_length, train_batch_size)
  File "transformer_flops.py", line 323, in benchmark_transformer_from_mm_and_bmm
    elapsed_mlp_time += benchmark_mm_cutlass(
  File "transformer_flops.py", line 70, in benchmark_mm_cutlass
    As, Bs, Cs, Ds, = initialize_mm_b(dtype, m, n, k, b)
  File "transformer_flops.py", line 24, in initialize_mm_b
    return [torch.randint(-3, 3, size, device='cuda').to(dtype) for size in sizes]
  File "transformer_flops.py", line 24, in <listcomp>
    return [torch.randint(-3, 3, size, device='cuda').to(dtype) for size in sizes]
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.18 GiB (GPU 0; 39.56 GiB total capacity; 30.61 GiB already allocated; 7.12 GiB free; 30.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
