bash: /fsx/home-jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
/fsx/home-jacob/write_hostfile.sh: line 7: /fsx/home-quentin/jacob/hostfiles/hosts_39232: Permission denied
1.13.1 

[2023-10-22 20:12:51,462] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-22 20:12:52,480] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.157.218, master_port=6000
[2023-10-22 20:12:52,480] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-22 20:12:55,177] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 0.417
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 14.343
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 0.507
Transformer - MLP - Attention (in seconds): -0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 0.827
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 64.222
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.148
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1.487
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 103.005
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 2.270
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1.922
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 123.070
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 3.157
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2.811
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 148.536
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 4.875
Transformer - MLP - Attention (in seconds): 0.0001
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 3.250
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 153.823
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 5.892
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 4.410
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 167.433
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 8.255
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 6.660
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 178.140
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 12.704
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 5.952
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 177.991
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 11.734
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 6.333
MLP duration (in seconds): 0.0011
MLP throughput (in TFLOP/s): 196.107
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 12.783
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 8.032
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 184.912
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 16.339
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 8.601
MLP duration (in seconds): 0.0017
MLP throughput (in TFLOP/s): 186.329
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 17.775
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 10.374
MLP duration (in seconds): 0.0019
MLP throughput (in TFLOP/s): 195.181
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 21.563
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 10.787
MLP duration (in seconds): 0.0022
MLP throughput (in TFLOP/s): 195.657
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 22.689
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 12.903
MLP duration (in seconds): 0.0024
MLP throughput (in TFLOP/s): 198.903
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 27.115
Transformer - MLP - Attention (in seconds): 0.0003
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 18.905
MLP duration (in seconds): 0.0027
MLP throughput (in TFLOP/s): 206.672
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 38.748
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 14.818
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 203.684
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 31.448
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 17.510
MLP duration (in seconds): 0.0033
MLP throughput (in TFLOP/s): 208.780
Transformer duration (in seconds): 0.0324
Transformer throughput (in TFLOP/s): 36.955
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 17.746
MLP duration (in seconds): 0.0037
MLP throughput (in TFLOP/s): 209.808
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 37.753
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 20.432
MLP duration (in seconds): 0.0041
MLP throughput (in TFLOP/s): 210.375
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 43.044
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 20.491
MLP duration (in seconds): 0.0045
MLP throughput (in TFLOP/s): 211.263
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 43.470
Transformer - MLP - Attention (in seconds): 0.0004
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 24.009
MLP duration (in seconds): 0.0049
MLP throughput (in TFLOP/s): 211.062
Transformer duration (in seconds): 0.0350
Transformer throughput (in TFLOP/s): 49.996
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 24.062
MLP duration (in seconds): 0.0053
MLP throughput (in TFLOP/s): 215.602
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 50.606
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 35.163
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 211.623
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 69.080
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 27.045
MLP duration (in seconds): 0.0062
MLP throughput (in TFLOP/s): 216.536
Transformer duration (in seconds): 0.0394
Transformer throughput (in TFLOP/s): 56.503
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 32.409
MLP duration (in seconds): 0.0066
MLP throughput (in TFLOP/s): 218.646
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 65.520
Transformer - MLP - Attention (in seconds): 0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 30.919
MLP duration (in seconds): 0.0070
MLP throughput (in TFLOP/s): 223.304
Transformer duration (in seconds): 0.0404
Transformer throughput (in TFLOP/s): 63.908
Transformer - MLP - Attention (in seconds): 0.0005
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 36.344
MLP duration (in seconds): 0.0077
MLP throughput (in TFLOP/s): 218.789
Transformer duration (in seconds): 0.0383
Transformer throughput (in TFLOP/s): 72.290
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 34.478
MLP duration (in seconds): 0.0081
MLP throughput (in TFLOP/s): 221.726
Transformer duration (in seconds): 0.0423
Transformer throughput (in TFLOP/s): 69.907
Transformer - MLP - Attention (in seconds): 0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 40.299
MLP duration (in seconds): 0.0086
MLP throughput (in TFLOP/s): 225.663
Transformer duration (in seconds): 0.0400
Transformer throughput (in TFLOP/s): 78.870
Transformer - MLP - Attention (in seconds): 0.0011
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 37.818
MLP duration (in seconds): 0.0093
MLP throughput (in TFLOP/s): 222.163
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 75.437
Transformer - MLP - Attention (in seconds): 0.0009
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 54.340
MLP duration (in seconds): 0.0098
MLP throughput (in TFLOP/s): 224.675
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 99.175
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 41.316
MLP duration (in seconds): 0.0101
MLP throughput (in TFLOP/s): 232.674
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 81.456
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 44.159
MLP duration (in seconds): 0.0108
MLP throughput (in TFLOP/s): 229.316
Transformer duration (in seconds): 0.0468
Transformer throughput (in TFLOP/s): 85.720
Transformer - MLP - Attention (in seconds): 0.0013
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 44.090
MLP duration (in seconds): 0.0113
MLP throughput (in TFLOP/s): 233.505
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 86.131
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 47.120
MLP duration (in seconds): 0.0118
MLP throughput (in TFLOP/s): 234.961
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 90.864
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 47.190
MLP duration (in seconds): 0.0123
MLP throughput (in TFLOP/s): 238.766
Transformer duration (in seconds): 0.0518
Transformer throughput (in TFLOP/s): 91.279
Transformer - MLP - Attention (in seconds): 0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 49.856
MLP duration (in seconds): 0.0133
MLP throughput (in TFLOP/s): 232.485
Transformer duration (in seconds): 0.0527
Transformer throughput (in TFLOP/s): 94.496
Transformer - MLP - Attention (in seconds): 0.0017
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0394
Attention throughput (in TFLOP/s): 50.005
MLP duration (in seconds): 0.0139
MLP throughput (in TFLOP/s): 234.999
Transformer duration (in seconds): 0.0550
Transformer throughput (in TFLOP/s): 95.120
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 71.535
MLP duration (in seconds): 0.0144
MLP throughput (in TFLOP/s): 238.491
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 122.913
Transformer - MLP - Attention (in seconds): 0.0015
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0404
Attention throughput (in TFLOP/s): 53.362
MLP duration (in seconds): 0.0153
MLP throughput (in TFLOP/s): 236.085
Transformer duration (in seconds): 0.0576
Transformer throughput (in TFLOP/s): 100.084
Transformer - MLP - Attention (in seconds): 0.0019
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 56.640
MLP duration (in seconds): 0.0158
MLP throughput (in TFLOP/s): 239.392
Transformer duration (in seconds): 0.0576
Transformer throughput (in TFLOP/s): 104.904
Transformer - MLP - Attention (in seconds): 0.0020
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0410
Attention throughput (in TFLOP/s): 57.411
MLP duration (in seconds): 0.0164
MLP throughput (in TFLOP/s): 242.767
Transformer duration (in seconds): 0.0596
Transformer throughput (in TFLOP/s): 106.146
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 60.634
MLP duration (in seconds): 0.0176
MLP throughput (in TFLOP/s): 236.578
Transformer duration (in seconds): 0.0603
Transformer throughput (in TFLOP/s): 109.605
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0416
Attention throughput (in TFLOP/s): 61.555
MLP duration (in seconds): 0.0179
MLP throughput (in TFLOP/s): 243.402
Transformer duration (in seconds): 0.0613
Transformer throughput (in TFLOP/s): 112.782
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0409
Attention throughput (in TFLOP/s): 65.260
MLP duration (in seconds): 0.0186
MLP throughput (in TFLOP/s): 243.905
Transformer duration (in seconds): 0.0616
Transformer throughput (in TFLOP/s): 117.073
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 65.579
MLP duration (in seconds): 0.0195
MLP throughput (in TFLOP/s): 242.918
Transformer duration (in seconds): 0.0643
Transformer throughput (in TFLOP/s): 116.962
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 91.495
MLP duration (in seconds): 0.0202
MLP throughput (in TFLOP/s): 245.062
Transformer duration (in seconds): 0.0537
Transformer throughput (in TFLOP/s): 145.831
Transformer - MLP - Attention (in seconds): 0.0020
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 69.582
MLP duration (in seconds): 0.0217
MLP throughput (in TFLOP/s): 237.610
Transformer duration (in seconds): 0.0678
Transformer throughput (in TFLOP/s): 120.337
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0426
Attention throughput (in TFLOP/s): 73.089
MLP duration (in seconds): 0.0222
MLP throughput (in TFLOP/s): 241.853
Transformer duration (in seconds): 0.0673
Transformer throughput (in TFLOP/s): 126.035
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 73.333
MLP duration (in seconds): 0.0234
MLP throughput (in TFLOP/s): 238.917
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 125.181
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 76.857
MLP duration (in seconds): 0.0238
MLP throughput (in TFLOP/s): 243.925
Transformer duration (in seconds): 0.0700
Transformer throughput (in TFLOP/s): 130.818
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 77.058
MLP duration (in seconds): 0.0247
MLP throughput (in TFLOP/s): 243.830
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 131.188
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0444
Attention throughput (in TFLOP/s): 81.035
MLP duration (in seconds): 0.0255
MLP throughput (in TFLOP/s): 245.229
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 136.095
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0461
Attention throughput (in TFLOP/s): 80.726
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 237.897
Transformer duration (in seconds): 0.0763
Transformer throughput (in TFLOP/s): 133.821
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 109.553
MLP duration (in seconds): 0.0275
MLP throughput (in TFLOP/s): 244.731
Transformer duration (in seconds): 0.0647
Transformer throughput (in TFLOP/s): 163.536
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 84.979
MLP duration (in seconds): 0.0293
MLP throughput (in TFLOP/s): 238.399
Transformer duration (in seconds): 0.0796
Transformer throughput (in TFLOP/s): 137.688
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0466
Attention throughput (in TFLOP/s): 88.175
MLP duration (in seconds): 0.0298
MLP throughput (in TFLOP/s): 242.740
Transformer duration (in seconds): 0.0791
Transformer throughput (in TFLOP/s): 143.313
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0481
Attention throughput (in TFLOP/s): 88.162
MLP duration (in seconds): 0.0315
MLP throughput (in TFLOP/s): 237.633
Transformer duration (in seconds): 0.0833
Transformer throughput (in TFLOP/s): 140.739
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0477
Attention throughput (in TFLOP/s): 91.852
MLP duration (in seconds): 0.0317
MLP throughput (in TFLOP/s): 243.822
Transformer duration (in seconds): 0.0820
Transformer throughput (in TFLOP/s): 147.786
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 91.517
MLP duration (in seconds): 0.0334
MLP throughput (in TFLOP/s): 239.214
Transformer duration (in seconds): 0.0866
Transformer throughput (in TFLOP/s): 144.533
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 94.995
MLP duration (in seconds): 0.0338
MLP throughput (in TFLOP/s): 243.906
Transformer duration (in seconds): 0.0856
Transformer throughput (in TFLOP/s): 150.826
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 95.380
MLP duration (in seconds): 0.0359
MLP throughput (in TFLOP/s): 237.438
Transformer duration (in seconds): 0.0904
Transformer throughput (in TFLOP/s): 147.453
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
