ERROR: Unable to locate a modulefile for 'cuda/11.7'
1.13.1 

[2023-11-24 22:06:24,266] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-24 22:06:24,826] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.152.78, master_port=6000
[2023-11-24 22:06:24,826] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-24 22:06:27,922] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Traceback (most recent call last):
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 490, in <module>
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
    benchmark_transformer(configuration, seq_length, train_batch_size)
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 433, in benchmark_transformer
    out = layer(inp, attention_mask)
  File "/fsx/home-jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 727, in forward
    context_layer = self.flash_attention(query_layer, key_layer, value_layer)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 594, in flash_attention
    output = self.flash_qkv_fn(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 642, in flash_attn_varlen_qkvpacked_func
    return FlashAttnVarlenQKVPackedFunc.apply(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 557, in forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_varlen_forward(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 483, in _flash_attn_varlen_forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.varlen_fwd(
TypeError: varlen_fwd(): incompatible function arguments. The following argument types are supported:
    1. (arg0: at::Tensor, arg1: at::Tensor, arg2: at::Tensor, arg3: Optional[at::Tensor], arg4: at::Tensor, arg5: at::Tensor, arg6: int, arg7: int, arg8: float, arg9: float, arg10: bool, arg11: bool, arg12: int, arg13: int, arg14: bool, arg15: Optional[at::Generator]) -> List[at::Tensor]

Invoked with: tensor([[[ 0.0150],
         [ 0.4111],
         [ 0.3071],
         ...,
         [-0.1887],
         [ 0.3218],
         [-0.5488]],

        [[-0.3000],
         [ 0.3013],
         [ 0.0145],
         ...,
         [ 0.0990],
         [ 0.1791],
         [-0.4976]],

        [[ 0.0765],
         [-0.3777],
         [-0.3748],
         ...,
         [-0.3621],
         [-0.2998],
         [ 0.2808]],

        ...,

        [[ 0.3071],
         [-0.1109],
         [ 0.3352],
         ...,
         [-0.4382],
         [-0.1979],
         [ 0.1671]],

        [[-0.1235],
         [-0.0514],
         [-0.2050],
         ...,
         [ 0.3110],
         [-0.2098],
         [-0.2837]],

        [[ 0.2810],
         [-0.1703],
         [ 0.4729],
         ...,
         [-0.0143],
         [ 0.0283],
         [-0.1639]]], device='cuda:0', dtype=torch.float16), tensor([[[-0.0859],
         [-0.0963],
         [-0.4590],
         ...,
         [ 0.2433],
         [-0.1929],
         [ 0.3264]],

        [[-0.1748],
         [ 0.1522],
         [-0.0457],
         ...,
         [-0.0303],
         [ 0.0672],
         [-0.2333]],

        [[ 0.3572],
         [ 0.0146],
         [-0.0729],
         ...,
         [ 0.0015],
         [-0.0289],
         [-0.2712]],

        ...,

        [[-0.2590],
         [-0.1812],
         [-0.2264],
         ...,
         [-0.0291],
         [-0.0074],
         [-0.1633]],

        [[-0.1337],
         [ 0.0605],
         [ 0.1410],
         ...,
         [-0.2727],
         [-0.0035],
         [ 0.1569]],

        [[ 0.0866],
         [-0.0611],
         [-0.1118],
         ...,
         [ 0.3430],
         [ 0.0292],
         [ 0.2330]]], device='cuda:0', dtype=torch.float16), tensor([[[-0.3101],
         [ 0.2510],
         [-0.1254],
         ...,
         [-0.2512],
         [ 0.3501],
         [ 0.0446]],

        [[ 0.0159],
         [ 0.0149],
         [-0.2339],
         ...,
         [-0.3289],
         [ 0.0854],
         [-0.0320]],

        [[ 0.3687],
         [-0.6279],
         [-0.0462],
         ...,
         [-0.1307],
         [-0.2395],
         [ 0.2256]],

        ...,

        [[-0.0574],
         [ 0.0692],
         [ 0.2388],
         ...,
         [-0.0791],
         [-0.1980],
         [-0.2377]],

        [[-0.1946],
         [ 0.0479],
         [ 0.0297],
         ...,
         [-0.2234],
         [ 0.0570],
         [-0.4299]],

        [[-0.1603],
         [-0.2335],
         [ 0.1478],
         ...,
         [ 0.2446],
         [-0.3730],
         [ 0.4180]]], device='cuda:0', dtype=torch.float16), None, tensor([   0, 2048, 4096, 6144, 8192], device='cuda:0', dtype=torch.int32), tensor([   0, 2048, 4096, 6144, 8192], device='cuda:0', dtype=torch.int32), 2048, 2048, 0.0, 1.0, False, True, False, None
