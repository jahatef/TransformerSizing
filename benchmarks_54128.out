1.13.1 

[2023-11-24 20:06:22,288] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-24 20:06:23,057] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.158.231, master_port=6000
[2023-11-24 20:06:23,058] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-24 20:06:26,009] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
Traceback (most recent call last):
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 490, in <module>
num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
    benchmark_transformer(configuration, seq_length, train_batch_size)
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 433, in benchmark_transformer
    out = layer(inp, attention_mask)
  File "/fsx/home-jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 724, in forward
    context_layer = self.flash_attention(query_layer, key_layer, value_layer)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 591, in flash_attention
    output = self.flash_qkv_fn(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 190, in flash_attn_unpadded_qkvpacked_func_cuda
    return FlashAttnQKVPackedFunc.apply(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 130, in forward
    out, softmax_lse, S_dmask = _flash_attn_forward_cuda(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 38, in _flash_attn_forward_cuda
    softmax_lse, *rest = flash_attn_cuda.fwd(
RuntimeError: Expected (head_size % 8 == 0) && (head_size <= 128) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)
