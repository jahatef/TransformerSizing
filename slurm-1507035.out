
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
2
4
6
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-22 13:02:12,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 13:02:12,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 13:02:12,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 13:02:12,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 8, hidden_size: 8, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1x2048): 0.918
Elapsed time for attention_prob_times_values (32x2048x2048x1): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1): 0.186

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 0.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 16, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2x2048): 1.714
Elapsed time for attention_prob_times_values (32x2048x2048x2): 0.0003
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2): 1.692

Attention duration (in seconds): 0.0006
Attention throughput (in TFLOP/s): 1.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0006
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3x2048): 2.394
Elapsed time for attention_prob_times_values (32x2048x2048x3): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3): 2.293

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 2.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4x2048): 3.130
Elapsed time for attention_prob_times_values (32x2048x2048x4): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4): 2.858

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 3.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x5x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x5x2048): 3.880
Elapsed time for attention_prob_times_values (32x2048x2048x5): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x5): 2.869

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 3.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x6x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x6x2048): 4.522
Elapsed time for attention_prob_times_values (32x2048x2048x6): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x6): 3.398

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 4.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 56, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x7x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x7x2048): 5.387
Elapsed time for attention_prob_times_values (32x2048x2048x7): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x7): 3.869

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 4.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x8x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x8x2048): 6.161
Elapsed time for attention_prob_times_values (32x2048x2048x8): 0.0003
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x8): 6.425

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 6.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 72, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x9x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x9x2048): 6.667
Elapsed time for attention_prob_times_values (32x2048x2048x9): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x9): 3.512

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 4.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x10x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x10x2048): 7.368
Elapsed time for attention_prob_times_values (32x2048x2048x10): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x10): 3.858

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 5.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 88, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x11x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x11x2048): 8.063
Elapsed time for attention_prob_times_values (32x2048x2048x11): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x11): 4.193

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 5.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x12x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x12x2048): 8.802
Elapsed time for attention_prob_times_values (32x2048x2048x12): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x12): 4.580

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 6.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x13x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x13x2048): 9.240
Elapsed time for attention_prob_times_values (32x2048x2048x13): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x13): 3.757

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 5.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x14x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x14x2048): 10.002
Elapsed time for attention_prob_times_values (32x2048x2048x14): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x14): 4.065

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 6.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x15x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x15x2048): 10.981
Elapsed time for attention_prob_times_values (32x2048x2048x15): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x15): 4.340

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 6.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x16x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x16x2048): 11.720
Elapsed time for attention_prob_times_values (32x2048x2048x16): 0.0003
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x16): 12.536

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 13.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x17x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x17x2048): 11.735
Elapsed time for attention_prob_times_values (32x2048x2048x17): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x17): 12.676

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 13.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x18x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x18x2048): 12.730
Elapsed time for attention_prob_times_values (32x2048x2048x18): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x18): 13.083

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 14.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x19x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x19x2048): 13.312
Elapsed time for attention_prob_times_values (32x2048x2048x19): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x19): 14.083

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 15.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
2.1.1+rocm5.6 

num_attention_heads: 20, hidden_size: 20, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1x2048): 0.991
Elapsed time for attention_prob_times_values (80x2048x2048x1): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1): 0.206

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 0.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x2x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x2x2048): 1.825
Elapsed time for attention_prob_times_values (80x2048x2048x2): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x2): 1.726

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 1.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 60, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x3x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x3x2048): 2.537
Elapsed time for attention_prob_times_values (80x2048x2048x3): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x3): 2.376

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 2.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x4x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x4x2048): 3.309
Elapsed time for attention_prob_times_values (80x2048x2048x4): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x4): 3.004

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 3.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x5x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x5x2048): 4.040
Elapsed time for attention_prob_times_values (80x2048x2048x5): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x5): 3.158

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 3.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x6x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x6x2048): 4.870
Elapsed time for attention_prob_times_values (80x2048x2048x6): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x6): 3.662

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 4.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x7x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x7x2048): 5.617
Elapsed time for attention_prob_times_values (80x2048x2048x7): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x7): 5.120

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 6.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x8x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x8x2048): 6.518
Elapsed time for attention_prob_times_values (80x2048x2048x8): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x8): 6.926

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 7.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x9x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x9x2048): 7.006
Elapsed time for attention_prob_times_values (80x2048x2048x9): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x9): 7.399

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 8.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x10x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x10x2048): 7.591
Elapsed time for attention_prob_times_values (80x2048x2048x10): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x10): 8.496

Attention duration (in seconds): 0.0017
Elapsed time for attention_key_query_prob (32x2048x20x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x20x2048): 14.065
Elapsed time for attention_prob_times_values (32x2048x2048x20): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x20): 15.143

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 16.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x21x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x21x2048): 14.759
Elapsed time for attention_prob_times_values (32x2048x2048x21): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x21): 14.964

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 17.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x22x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x22x2048): 15.433
Elapsed time for attention_prob_times_values (32x2048x2048x22): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x22): 16.580

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 18.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x23x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x23x2048): 16.094
Elapsed time for attention_prob_times_values (32x2048x2048x23): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x23): 16.859

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 19.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x24x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x24x2048): 16.357
Elapsed time for attention_prob_times_values (32x2048x2048x24): 0.0003
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x24): 18.457

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 20.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x25x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x25x2048): 17.121
Elapsed time for attention_prob_times_values (32x2048x2048x25): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x25): 18.266

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 21.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x26x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x26x2048): 17.795
Elapsed time for attention_prob_times_values (32x2048x2048x26): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x26): 19.425

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 22.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x27x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x27x2048): 18.424
Elapsed time for attention_prob_times_values (32x2048x2048x27): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x27): 19.587

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 22.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x28x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x28x2048): 19.048
Elapsed time for attention_prob_times_values (32x2048x2048x28): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x28): 20.260

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 23.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x29x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x29x2048): 19.669
Elapsed time for attention_prob_times_values (32x2048x2048x29): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x29): 21.011

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 24.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x30x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x30x2048): 20.384
Elapsed time for attention_prob_times_values (32x2048x2048x30): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x30): 22.178

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 26.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x31x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x31x2048): 21.064
Elapsed time for attention_prob_times_values (32x2048x2048x31): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x31): 22.274

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 26.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x32x2048): 0.0002
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x32x2048): 35.814
Elapsed time for attention_prob_times_values (32x2048x2048x32): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x32): 24.213

Attention duration (in seconds): 0.0006
Attention throughput (in TFLOP/s): 36.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0006
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x33x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x33x2048): 23.726
Elapsed time for attention_prob_times_values (32x2048x2048x33): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x33): 23.756

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 29.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x34x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x34x2048): 24.106
Elapsed time for attention_prob_times_values (32x2048x2048x34): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x34): 24.445

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 30.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x35x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x35x2048): 23.428
Elapsed time for attention_prob_times_values (32x2048x2048x35): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x35): 24.476

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 30.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x36x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x36x2048): 24.069
Elapsed time for attention_prob_times_values (32x2048x2048x36): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x36): 26.184

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 32.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x37x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x37x2048): 24.333
Elapsed time for attention_prob_times_values (32x2048x2048x37): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x37): 18.144

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 26.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x38x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x38x2048): 24.962
Elapsed time for attention_prob_times_values (32x2048x2048x38): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x38): 27.216

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 33.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x39x2048): 0.0004
Attention throughput (in TFLOP/s): 9.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x11x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x11x2048): 8.355
Elapsed time for attention_prob_times_values (80x2048x2048x11): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x11): 9.043

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 10.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x12x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x12x2048): 8.523
Elapsed time for attention_prob_times_values (80x2048x2048x12): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x12): 10.014

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 11.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x13x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x13x2048): 9.973
Elapsed time for attention_prob_times_values (80x2048x2048x13): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x13): 10.548

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 12.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x14x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x14x2048): 10.737
Elapsed time for attention_prob_times_values (80x2048x2048x14): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x14): 11.617

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 14.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x15x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x15x2048): 11.371
Elapsed time for attention_prob_times_values (80x2048x2048x15): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x15): 12.087

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 15.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x16x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x16x2048): 12.295
Elapsed time for attention_prob_times_values (80x2048x2048x16): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x16): 13.492

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 16.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x17x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x17x2048): 12.530
Elapsed time for attention_prob_times_values (80x2048x2048x17): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x17): 13.422

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 17.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x18x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x18x2048): 13.428
Elapsed time for attention_prob_times_values (80x2048x2048x18): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x18): 14.784

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 19.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x19x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x19x2048): 14.103
Elapsed time for attention_prob_times_values (80x2048x2048x19): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x19): 15.202

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 20.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
2.1.1+rocm5.6 

num_attention_heads: 40, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x1x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x1x2048): 1.023
Elapsed time for attention_prob_times_values (160x2048x2048x1): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x1): 0.212

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 0.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x2x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x2x2048): 1.892
Elapsed time for attention_prob_times_values (160x2048x2048x2): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x2): 1.529

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x3x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x3x2048): 2.636
Elapsed time for attention_prob_times_values (160x2048x2048x3): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x3): 2.259

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 2.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x4x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x4x2048): 3.381
Elapsed time for attention_prob_times_values (160x2048x2048x4): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x4): 3.018

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 3.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x5x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x5x2048): 3.972
Elapsed time for attention_prob_times_values (160x2048x2048x5): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x5): 3.188

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 4.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x6x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x6x2048): 4.963
Elapsed time for attention_prob_times_values (160x2048x2048x6): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x6): 3.940

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 5.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x7x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x7x2048): 5.836
Elapsed time for attention_prob_times_values (160x2048x2048x7): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x7): 4.609

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 6.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x8x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x8x2048): 6.571
Elapsed time for attention_prob_times_values (160x2048x2048x8): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x8): 7.041

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 8.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x9x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x9x2048): 7.131
Elapsed time for attention_prob_times_values (160x2048x2048x9): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x9): 7.604

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 9.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x10x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x10x2048): 7.893
Elapsed time for attention_prob_times_values (160x2048x2048x10): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x10): 8.720

Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x39x2048): 25.396
Elapsed time for attention_prob_times_values (32x2048x2048x39): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x39): 27.791

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 34.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x40x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x40x2048): 26.214
Elapsed time for attention_prob_times_values (32x2048x2048x40): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x40): 26.414

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 34.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x41x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x41x2048): 26.036
Elapsed time for attention_prob_times_values (32x2048x2048x41): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x41): 28.833

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 36.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x42x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x42x2048): 26.822
Elapsed time for attention_prob_times_values (32x2048x2048x42): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x42): 29.986

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 37.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x43x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x43x2048): 27.229
Elapsed time for attention_prob_times_values (32x2048x2048x43): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x43): 30.108

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 38.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x44x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x44x2048): 28.100
Elapsed time for attention_prob_times_values (32x2048x2048x44): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x44): 31.354

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 39.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x45x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x45x2048): 28.528
Elapsed time for attention_prob_times_values (32x2048x2048x45): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x45): 31.256

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 40.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x46x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x46x2048): 29.277
Elapsed time for attention_prob_times_values (32x2048x2048x46): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x46): 32.491

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 41.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x47x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x47x2048): 29.188
Elapsed time for attention_prob_times_values (32x2048x2048x47): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x47): 32.267

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 41.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x48x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x48x2048): 30.672
Elapsed time for attention_prob_times_values (32x2048x2048x48): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x48): 35.139

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 45.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x49x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x49x2048): 30.890
Elapsed time for attention_prob_times_values (32x2048x2048x49): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x49): 33.805

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 44.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x50x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x50x2048): 31.698
Elapsed time for attention_prob_times_values (32x2048x2048x50): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x50): 34.814

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 46.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x51x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x51x2048): 32.169
Elapsed time for attention_prob_times_values (32x2048x2048x51): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x51): 34.885

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 46.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x52x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x52x2048): 32.873
Elapsed time for attention_prob_times_values (32x2048x2048x52): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x52): 36.410

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 48.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x53x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x53x2048): 33.337
Elapsed time for attention_prob_times_values (32x2048x2048x53): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x53): 36.209

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 49.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x54x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x54x2048): 34.176
Elapsed time for attention_prob_times_values (32x2048x2048x54): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x54): 37.623

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 50.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x55x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x55x2048): 34.653
Elapsed time for attention_prob_times_values (32x2048x2048x55): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x55): 37.417

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 51.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x56x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x56x2048): 35.481
Elapsed time for attention_prob_times_values (32x2048x2048x56): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x56): 40.391

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 54.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x57x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x57x2048): 34.993
Elapsed time for attention_prob_times_values (32x2048x2048x57): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x57): 37.618

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 52.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x58x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x58x2048): 34.940
Elapsed time for attention_prob_times_values (32x2048x2048x58): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x58): 39.649

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 53.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x59x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x59x2048): 36.122
Elapsed time for attention_prob_times_values (32x2048x2048x59): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x59): 39.753

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 55.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x60x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x60x2048): 36.814
Elapsed time for attention_prob_times_values (32x2048x2048x60): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x60): 41.444

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 57.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x61x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x61x2048): 37.205
Elapsed time for attention_prob_times_values (32x2048x2048x61): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x61): 40.495

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 57.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x62x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x62x2048): 32.423
Elapsed time for attention_prob_times_values (32x2048x2048x62): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x62): 42.565

Attention duration (in seconds): 0.0009
Attention throughput (in TFLOP/s): 54.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x63x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x63x2048): 38.197
Elapsed time for attention_prob_times_values (32x2048x2048x63): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x63): 42.246

Attention duration (in seconds): 0.0008
Attention throughput (in TFLOP/s): 59.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x64x2048): 0.0003
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x64x2048): 51.728
Elapsed time for attention_prob_times_values (32x2048x2048x64): 0.0004
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x64): 46.369

Attention duration (in seconds): 0.0007
Attention throughput (in TFLOP/s): 73.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x65x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x65x2048): 39.709
Elapsed time for attention_prob_times_values (32x2048x2048x65): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x65): 31.342

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 52.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x66x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x66x2048): 40.762
Elapsed time for attention_prob_times_values (32x2048x2048x66): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x66): 32.635

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 54.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x67x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x67x2048): 40.361
Elapsed time for attention_prob_times_values (32x2048x2048x67): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x67): 32.127

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 54.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 20, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x20x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x20x2048): 14.637
Elapsed time for attention_prob_times_values (80x2048x2048x20): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x20): 16.181

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 21.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x21x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x21x2048): 15.543
Elapsed time for attention_prob_times_values (80x2048x2048x21): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x21): 16.609

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 22.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x22x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x22x2048): 16.287
Elapsed time for attention_prob_times_values (80x2048x2048x22): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x22): 14.141

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 21.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x23x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x23x2048): 17.005
Elapsed time for attention_prob_times_values (80x2048x2048x23): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x23): 18.165

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 25.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x24x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x24x2048): 17.735
Elapsed time for attention_prob_times_values (80x2048x2048x24): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x24): 19.758

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 27.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x25x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x25x2048): 16.315
Elapsed time for attention_prob_times_values (80x2048x2048x25): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x25): 18.387

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 25.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x26x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x26x2048): 18.746
Elapsed time for attention_prob_times_values (80x2048x2048x26): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x26): 20.738

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 29.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x27x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x27x2048): 19.407
Elapsed time for attention_prob_times_values (80x2048x2048x27): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x27): 21.017

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 30.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x28x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x28x2048): 20.039
Elapsed time for attention_prob_times_values (80x2048x2048x28): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x28): 19.877

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 30.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x29x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x29x2048): 20.733
Elapsed time for attention_prob_times_values (80x2048x2048x29): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x29): 22.438

========================================================================================================================
num_attention_heads: 8, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x68x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x68x2048): 41.362
Elapsed time for attention_prob_times_values (32x2048x2048x68): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x68): 33.594

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 56.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x69x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x69x2048): 41.126
Elapsed time for attention_prob_times_values (32x2048x2048x69): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x69): 33.285

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 56.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x70x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x70x2048): 41.989
Elapsed time for attention_prob_times_values (32x2048x2048x70): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x70): 34.522

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 58.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x71x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x71x2048): 41.897
Elapsed time for attention_prob_times_values (32x2048x2048x71): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x71): 33.872

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 58.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x72x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x72x2048): 42.937
Elapsed time for attention_prob_times_values (32x2048x2048x72): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x72): 34.118

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 59.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x73x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x73x2048): 41.849
Elapsed time for attention_prob_times_values (32x2048x2048x73): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x73): 34.346

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 59.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x74x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x74x2048): 42.814
Elapsed time for attention_prob_times_values (32x2048x2048x74): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x74): 36.209

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 61.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x75x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x75x2048): 42.777
Elapsed time for attention_prob_times_values (32x2048x2048x75): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x75): 35.630

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 61.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x76x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x76x2048): 42.784
Elapsed time for attention_prob_times_values (32x2048x2048x76): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x76): 37.252

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 63.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x77x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x77x2048): 43.478
Elapsed time for attention_prob_times_values (32x2048x2048x77): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x77): 36.153

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 63.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x78x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x78x2048): 27.047
Elapsed time for attention_prob_times_values (32x2048x2048x78): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x78): 37.244

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 50.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x79x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x79x2048): 43.794
Elapsed time for attention_prob_times_values (32x2048x2048x79): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x79): 37.076

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 64.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x80x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x80x2048): 45.815
Elapsed time for attention_prob_times_values (32x2048x2048x80): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x80): 37.593

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 67.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x81x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x81x2048): 43.552
Elapsed time for attention_prob_times_values (32x2048x2048x81): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x81): 38.031

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 66.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x82x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x82x2048): 44.365
Elapsed time for attention_prob_times_values (32x2048x2048x82): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x82): 39.488

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 68.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x83x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x83x2048): 44.373
Elapsed time for attention_prob_times_values (32x2048x2048x83): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x83): 38.600

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 68.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x84x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x84x2048): 45.251
Elapsed time for attention_prob_times_values (32x2048x2048x84): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x84): 39.771

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 70.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x85x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x85x2048): 45.142
Elapsed time for attention_prob_times_values (32x2048x2048x85): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x85): 39.465

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 70.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x86x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x86x2048): 46.021
Elapsed time for attention_prob_times_values (32x2048x2048x86): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x86): 19.008

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 44.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 11.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x11x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x11x2048): 7.885
Elapsed time for attention_prob_times_values (160x2048x2048x11): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x11): 9.247

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 12.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x12x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x12x2048): 8.819
Elapsed time for attention_prob_times_values (160x2048x2048x12): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x12): 10.457

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 14.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x13x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x13x2048): 10.159
Elapsed time for attention_prob_times_values (160x2048x2048x13): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x13): 10.920

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 15.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x14x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x14x2048): 10.952
Elapsed time for attention_prob_times_values (160x2048x2048x14): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x14): 12.125

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 17.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x15x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x15x2048): 11.709
Elapsed time for attention_prob_times_values (160x2048x2048x15): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x15): 12.488

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 19.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x16x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x16x2048): 12.555
Elapsed time for attention_prob_times_values (160x2048x2048x16): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x16): 13.911

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 21.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x17x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x17x2048): 12.832
Elapsed time for attention_prob_times_values (160x2048x2048x17): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x17): 13.058

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 21.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x18x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x18x2048): 13.636
Elapsed time for attention_prob_times_values (160x2048x2048x18): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x18): 15.261

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 24.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x19x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x19x2048): 11.568
Elapsed time for attention_prob_times_values (160x2048x2048x19): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x19): 15.665

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 23.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 33.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x30x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x30x2048): 21.476
Elapsed time for attention_prob_times_values (80x2048x2048x30): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x30): 23.780

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 35.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x31x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x31x2048): 22.090
Elapsed time for attention_prob_times_values (80x2048x2048x31): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x31): 23.906

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 36.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x32x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x32x2048): 25.640
Elapsed time for attention_prob_times_values (80x2048x2048x32): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x32): 25.647

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 41.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x33x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x33x2048): 25.924
Elapsed time for attention_prob_times_values (80x2048x2048x33): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x33): 25.490

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 42.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x34x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x34x2048): 26.020
Elapsed time for attention_prob_times_values (80x2048x2048x34): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x34): 26.451

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 43.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x35x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x35x2048): 25.339
Elapsed time for attention_prob_times_values (80x2048x2048x35): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x35): 26.932

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 43.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x36x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x36x2048): 23.343
Elapsed time for attention_prob_times_values (80x2048x2048x36): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x36): 21.186

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 37.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x37x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x37x2048): 25.614
Elapsed time for attention_prob_times_values (80x2048x2048x37): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x37): 21.986

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 40.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x38x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x38x2048): 26.462
Elapsed time for attention_prob_times_values (80x2048x2048x38): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x38): 29.137

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 48.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 8, hidden_size: 696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x87x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x87x2048): 24.624
Elapsed time for attention_prob_times_values (32x2048x2048x87): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x87): 33.386

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 47.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x88x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x88x2048): 47.226
Elapsed time for attention_prob_times_values (32x2048x2048x88): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x88): 40.342

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 73.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x89x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x89x2048): 36.705
Elapsed time for attention_prob_times_values (32x2048x2048x89): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x89): 41.000

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 65.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x90x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x90x2048): 46.696
Elapsed time for attention_prob_times_values (32x2048x2048x90): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x90): 42.702

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 75.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x91x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x91x2048): 46.656
Elapsed time for attention_prob_times_values (32x2048x2048x91): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x91): 41.147

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 74.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x92x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x92x2048): 47.385
Elapsed time for attention_prob_times_values (32x2048x2048x92): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x92): 43.743

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 78.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x93x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x93x2048): 47.358
Elapsed time for attention_prob_times_values (32x2048x2048x93): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x93): 42.582

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 77.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x94x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x94x2048): 48.041
Elapsed time for attention_prob_times_values (32x2048x2048x94): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x94): 44.282

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 79.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x95x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x95x2048): 48.029
Elapsed time for attention_prob_times_values (32x2048x2048x95): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x95): 41.995

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 78.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x96x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x96x2048): 60.928
Elapsed time for attention_prob_times_values (32x2048x2048x96): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x96): 45.130

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 90.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x97x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x97x2048): 48.303
Elapsed time for attention_prob_times_values (32x2048x2048x97): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x97): 43.459

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 80.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x98x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x98x2048): 49.061
Elapsed time for attention_prob_times_values (32x2048x2048x98): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x98): 45.764

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 83.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x99x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x99x2048): 48.086
Elapsed time for attention_prob_times_values (32x2048x2048x99): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x99): 41.483

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 78.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x100x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x100x2048): 49.797
Elapsed time for attention_prob_times_values (32x2048x2048x100): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x100): 46.932

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 86.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x101x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x101x2048): 49.442
Elapsed time for attention_prob_times_values (32x2048x2048x101): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x101): 45.835

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 85.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x102x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x102x2048): 40.710
Elapsed time for attention_prob_times_values (32x2048x2048x102): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x102): 45.374

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 77.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x103x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x103x2048): 49.943
Elapsed time for attention_prob_times_values (32x2048x2048x103): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x103): 46.536

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 86.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x104x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x104x2048): 51.312
Elapsed time for attention_prob_times_values (32x2048x2048x104): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x104): 47.234

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 89.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x105x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x105x2048): 49.756
Elapsed time for attention_prob_times_values (32x2048x2048x105): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x105): 47.156

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 88.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
2.1.1+rocm5.6 

num_attention_heads: 96, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x1x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x1x2048): 1.042
Elapsed time for attention_prob_times_values (384x2048x2048x1): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x1): 0.211

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 0.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x2x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x2x2048): 1.937
Elapsed time for attention_prob_times_values (384x2048x2048x2): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x2): 1.445

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x3x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x3x2048): 2.668
Elapsed time for attention_prob_times_values (384x2048x2048x3): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x3): 2.056

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 2.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x4x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x4x2048): 3.428
Elapsed time for attention_prob_times_values (384x2048x2048x4): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x4): 2.856

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 4.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x5x2048): 0.0166
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x5x2048): 0.971
Elapsed time for attention_prob_times_values (384x2048x2048x5): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x5): 3.768

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x6x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x6x2048): 4.920
Elapsed time for attention_prob_times_values (384x2048x2048x6): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x6): 4.557

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 7.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x7x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x7x2048): 5.480
Elapsed time for attention_prob_times_values (384x2048x2048x7): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x7): 4.803

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 8.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x8x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x8x2048): 6.381
Elapsed time for attention_prob_times_values (384x2048x2048x8): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x8): 7.190

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 11.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x9x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x9x2048): 6.834
Elapsed time for attention_prob_times_values (384x2048x2048x9): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x9): 7.670

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 13.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x10x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x10x2048): 7.502
Elapsed time for attention_prob_times_values (384x2048x2048x10): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x10): 8.388

num_attention_heads: 20, hidden_size: 780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x39x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x39x2048): 26.651
Elapsed time for attention_prob_times_values (80x2048x2048x39): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x39): 29.653

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 49.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x40x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x40x2048): 27.562
Elapsed time for attention_prob_times_values (80x2048x2048x40): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x40): 31.538

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 52.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x41x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x41x2048): 26.764
Elapsed time for attention_prob_times_values (80x2048x2048x41): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x41): 27.464

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 48.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x42x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x42x2048): 27.712
Elapsed time for attention_prob_times_values (80x2048x2048x42): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x42): 30.243

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 52.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x43x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x43x2048): 28.499
Elapsed time for attention_prob_times_values (80x2048x2048x43): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x43): 32.011

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 55.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x44x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x44x2048): 29.432
Elapsed time for attention_prob_times_values (80x2048x2048x44): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x44): 33.509

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 58.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x45x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x45x2048): 29.838
Elapsed time for attention_prob_times_values (80x2048x2048x45): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x45): 33.228

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 59.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x46x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x46x2048): 30.777
Elapsed time for attention_prob_times_values (80x2048x2048x46): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x46): 34.797

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 62.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x47x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x47x2048): 31.157
Elapsed time for attention_prob_times_values (80x2048x2048x47): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x47): 29.438

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 58.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x48x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x48x2048): 31.993
Elapsed time for attention_prob_times_values (80x2048x2048x48): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x48): 37.364

========================================================================================================================
num_attention_heads: 8, hidden_size: 848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x106x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x106x2048): 50.937
Elapsed time for attention_prob_times_values (32x2048x2048x106): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x106): 49.033

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 91.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x107x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x107x2048): 50.491
Elapsed time for attention_prob_times_values (32x2048x2048x107): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x107): 47.787

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 90.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x108x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x108x2048): 51.678
Elapsed time for attention_prob_times_values (32x2048x2048x108): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x108): 4.024

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 13.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x109x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x109x2048): 51.050
Elapsed time for attention_prob_times_values (32x2048x2048x109): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x109): 48.411

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 92.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x110x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x110x2048): 52.016
Elapsed time for attention_prob_times_values (32x2048x2048x110): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x110): 50.448

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 95.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x111x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x111x2048): 50.969
Elapsed time for attention_prob_times_values (32x2048x2048x111): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x111): 49.145

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 93.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x112x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x112x2048): 53.660
Elapsed time for attention_prob_times_values (32x2048x2048x112): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x112): 26.796

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 67.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x113x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x113x2048): 24.123
Elapsed time for attention_prob_times_values (32x2048x2048x113): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x113): 49.912

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 61.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x114x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x114x2048): 52.798
Elapsed time for attention_prob_times_values (32x2048x2048x114): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x114): 51.986

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 99.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x115x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x115x2048): 42.494
Elapsed time for attention_prob_times_values (32x2048x2048x115): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x115): 50.518

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 87.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x116x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x116x2048): 53.439
Elapsed time for attention_prob_times_values (32x2048x2048x116): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x116): 52.325

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 100.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x117x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x117x2048): 53.053
Elapsed time for attention_prob_times_values (32x2048x2048x117): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x117): 51.038

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 99.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x118x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x118x2048): 54.051
Elapsed time for attention_prob_times_values (32x2048x2048x118): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x118): 52.868

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 102.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x119x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x119x2048): 53.486
Elapsed time for attention_prob_times_values (32x2048x2048x119): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x119): 51.571

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 101.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x120x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x120x2048): 55.191
Elapsed time for attention_prob_times_values (32x2048x2048x120): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x120): 53.445

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 105.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x121x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x121x2048): 53.237
Elapsed time for attention_prob_times_values (32x2048x2048x121): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x121): 42.600

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 92.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x122x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x122x2048): 54.057
Elapsed time for attention_prob_times_values (32x2048x2048x122): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x122): 54.465

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 105.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x123x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x123x2048): 53.469
Elapsed time for attention_prob_times_values (32x2048x2048x123): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x123): 46.054

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 97.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x124x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x124x2048): 54.814
Elapsed time for attention_prob_times_values (32x2048x2048x124): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x124): 55.117

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 108.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
num_attention_heads: 40, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x20x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x20x2048): 14.917
Elapsed time for attention_prob_times_values (160x2048x2048x20): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x20): 15.481

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 27.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x21x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x21x2048): 15.792
Elapsed time for attention_prob_times_values (160x2048x2048x21): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x21): 17.133

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 29.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x22x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x22x2048): 16.564
Elapsed time for attention_prob_times_values (160x2048x2048x22): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x22): 18.405

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 32.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x23x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x23x2048): 14.821
Elapsed time for attention_prob_times_values (160x2048x2048x23): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x23): 16.672

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 29.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x24x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x24x2048): 18.010
Elapsed time for attention_prob_times_values (160x2048x2048x24): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x24): 18.812

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 35.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x25x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x25x2048): 18.278
Elapsed time for attention_prob_times_values (160x2048x2048x25): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x25): 19.303

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 37.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x26x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x26x2048): 18.959
Elapsed time for attention_prob_times_values (160x2048x2048x26): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x26): 21.315

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 40.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x27x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x27x2048): 19.454
Elapsed time for attention_prob_times_values (160x2048x2048x27): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x27): 21.667

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 42.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x28x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x28x2048): 19.379
Elapsed time for attention_prob_times_values (160x2048x2048x28): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x28): 22.951

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 43.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x29x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x29x2048): 20.901
Elapsed time for attention_prob_times_values (160x2048x2048x29): 0.0017
Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 66.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x49x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x49x2048): 32.652
Elapsed time for attention_prob_times_values (80x2048x2048x49): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x49): 35.964

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 66.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x50x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x50x2048): 33.421
Elapsed time for attention_prob_times_values (80x2048x2048x50): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x50): 37.183

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 69.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x51x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x51x2048): 34.009
Elapsed time for attention_prob_times_values (80x2048x2048x51): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x51): 28.353

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 61.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x52x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x52x2048): 34.198
Elapsed time for attention_prob_times_values (80x2048x2048x52): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x52): 38.958

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 73.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x53x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x53x2048): 35.385
Elapsed time for attention_prob_times_values (80x2048x2048x53): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x53): 32.403

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 68.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x54x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x54x2048): 36.172
Elapsed time for attention_prob_times_values (80x2048x2048x54): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x54): 39.894

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 77.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x55x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x55x2048): 36.790
Elapsed time for attention_prob_times_values (80x2048x2048x55): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x55): 39.941

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 79.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x56x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x56x2048): 37.450
Elapsed time for attention_prob_times_values (80x2048x2048x56): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x56): 43.138

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 83.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x57x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x57x2048): 36.900
Elapsed time for attention_prob_times_values (80x2048x2048x57): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x57): 41.619

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 82.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x125x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x125x2048): 54.360
Elapsed time for attention_prob_times_values (32x2048x2048x125): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x125): 44.425

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 96.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x126x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x126x2048): 54.986
Elapsed time for attention_prob_times_values (32x2048x2048x126): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x126): 55.852

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 109.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x127x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x127x2048): 54.639
Elapsed time for attention_prob_times_values (32x2048x2048x127): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x127): 44.407

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 97.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x128x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x128x2048): 61.352
Elapsed time for attention_prob_times_values (32x2048x2048x128): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x128): 59.282

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 120.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x129x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x129x2048): 53.674
Elapsed time for attention_prob_times_values (32x2048x2048x129): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x129): 42.406

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 95.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x130x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x130x2048): 54.922
Elapsed time for attention_prob_times_values (32x2048x2048x130): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x130): 44.843

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 99.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x131x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x131x2048): 54.526
Elapsed time for attention_prob_times_values (32x2048x2048x131): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x131): 43.355

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 97.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x132x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x132x2048): 55.956
Elapsed time for attention_prob_times_values (32x2048x2048x132): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x132): 45.631

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 102.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x133x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x133x2048): 54.952
Elapsed time for attention_prob_times_values (32x2048x2048x133): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x133): 44.186

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 99.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x134x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x134x2048): 56.232
Elapsed time for attention_prob_times_values (32x2048x2048x134): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x134): 46.053

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 103.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x135x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x135x2048): 55.697
Elapsed time for attention_prob_times_values (32x2048x2048x135): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x135): 44.863

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 102.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x136x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x136x2048): 57.457
Elapsed time for attention_prob_times_values (32x2048x2048x136): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x136): 44.026

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 102.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x137x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x137x2048): 55.685
Elapsed time for attention_prob_times_values (32x2048x2048x137): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x137): 44.944

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 102.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x138x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x138x2048): 56.603
Elapsed time for attention_prob_times_values (32x2048x2048x138): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x138): 46.785

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 106.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x139x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x139x2048): 56.174
Elapsed time for attention_prob_times_values (32x2048x2048x139): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x139): 45.667

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 105.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x140x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x140x2048): 57.277
Elapsed time for attention_prob_times_values (32x2048x2048x140): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x140): 46.996

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 108.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x141x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x141x2048): 56.717
Elapsed time for attention_prob_times_values (32x2048x2048x141): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x141): 46.351

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 107.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x142x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x142x2048): 57.676
Elapsed time for attention_prob_times_values (32x2048x2048x142): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x142): 47.526

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 109.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x143x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x143x2048): 57.256
Elapsed time for attention_prob_times_values (32x2048x2048x143): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x143): 46.940

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 109.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
num_attention_heads: 20, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x58x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x58x2048): 34.471
Elapsed time for attention_prob_times_values (80x2048x2048x58): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x58): 43.098

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 81.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x59x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x59x2048): 38.107
Elapsed time for attention_prob_times_values (80x2048x2048x59): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x59): 42.462

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 86.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x60x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x60x2048): 38.532
Elapsed time for attention_prob_times_values (80x2048x2048x60): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x60): 43.775

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 89.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x61x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x61x2048): 37.399
Elapsed time for attention_prob_times_values (80x2048x2048x61): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x61): 43.756

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 88.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x62x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x62x2048): 38.448
Elapsed time for attention_prob_times_values (80x2048x2048x62): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x62): 42.513

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 89.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x63x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x63x2048): 37.923
Elapsed time for attention_prob_times_values (80x2048x2048x63): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x63): 44.893

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 91.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x64x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x64x2048): 55.600
Elapsed time for attention_prob_times_values (80x2048x2048x64): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x64): 48.886

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 117.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x65x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x65x2048): 39.448
Elapsed time for attention_prob_times_values (80x2048x2048x65): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x65): 29.557

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 76.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x66x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x66x2048): 43.324
Elapsed time for attention_prob_times_values (80x2048x2048x66): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x66): 34.212

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 87.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x67x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x67x2048): 42.998
Elapsed time for attention_prob_times_values (80x2048x2048x67): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x67): 33.955

Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x144x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x144x2048): 59.497
Elapsed time for attention_prob_times_values (32x2048x2048x144): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x144): 46.283

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 110.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x145x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x145x2048): 56.943
Elapsed time for attention_prob_times_values (32x2048x2048x145): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x145): 46.912

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 109.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x146x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x146x2048): 58.229
Elapsed time for attention_prob_times_values (32x2048x2048x146): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x146): 47.813

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 112.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x147x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x147x2048): 56.584
Elapsed time for attention_prob_times_values (32x2048x2048x147): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x147): 48.071

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 111.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x148x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x148x2048): 58.715
Elapsed time for attention_prob_times_values (32x2048x2048x148): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x148): 50.434

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 116.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x149x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x149x2048): 58.108
Elapsed time for attention_prob_times_values (32x2048x2048x149): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x149): 48.668

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 114.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x150x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x150x2048): 59.362
Elapsed time for attention_prob_times_values (32x2048x2048x150): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x150): 50.716

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 118.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x151x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x151x2048): 58.746
Elapsed time for attention_prob_times_values (32x2048x2048x151): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x151): 49.307

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 116.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x152x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x152x2048): 60.515
Elapsed time for attention_prob_times_values (32x2048x2048x152): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x152): 49.248

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 118.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x153x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x153x2048): 58.315
Elapsed time for attention_prob_times_values (32x2048x2048x153): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x153): 49.643

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 117.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x154x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x154x2048): 59.278
Elapsed time for attention_prob_times_values (32x2048x2048x154): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x154): 52.037

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 122.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x155x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x155x2048): 58.838
Elapsed time for attention_prob_times_values (32x2048x2048x155): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x155): 50.292

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 119.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x156x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x156x2048): 59.823
Elapsed time for attention_prob_times_values (32x2048x2048x156): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x156): 52.777

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 124.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x157x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x157x2048): 59.638
Elapsed time for attention_prob_times_values (32x2048x2048x157): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x157): 50.985

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 122.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x158x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x158x2048): 60.569
Elapsed time for attention_prob_times_values (32x2048x2048x158): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x158): 53.150

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 126.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x159x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x159x2048): 60.093
Elapsed time for attention_prob_times_values (32x2048x2048x159): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x159): 50.930

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 123.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x160x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x160x2048): 71.771
Elapsed time for attention_prob_times_values (32x2048x2048x160): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x160): 53.124

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 137.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x161x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x161x2048): 59.026
Elapsed time for attention_prob_times_values (32x2048x2048x161): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x161): 52.254

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 125.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x162x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x162x2048): 57.285
Elapsed time for attention_prob_times_values (32x2048x2048x162): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x162): 54.091

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 126.065
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x29): 23.196

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 46.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x30x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x30x2048): 21.621
Elapsed time for attention_prob_times_values (160x2048x2048x30): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x30): 24.448

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 49.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x31x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x31x2048): 22.279
Elapsed time for attention_prob_times_values (160x2048x2048x31): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x31): 24.663

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 51.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x32x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x32x2048): 39.445
Elapsed time for attention_prob_times_values (160x2048x2048x32): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x32): 26.341

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 71.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x33x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x33x2048): 26.388
Elapsed time for attention_prob_times_values (160x2048x2048x33): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x33): 26.059

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 60.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x34x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x34x2048): 26.665
Elapsed time for attention_prob_times_values (160x2048x2048x34): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x34): 27.038

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 62.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x35x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x35x2048): 23.417
Elapsed time for attention_prob_times_values (160x2048x2048x35): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x35): 27.453

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 59.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x36x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x36x2048): 23.767
Elapsed time for attention_prob_times_values (160x2048x2048x36): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x36): 28.791

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 62.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x37x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x37x2048): 24.936
Elapsed time for attention_prob_times_values (160x2048x2048x37): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x37): 29.030

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 65.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x38x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x38x2048): 25.858
Elapsed time for attention_prob_times_values (160x2048x2048x38): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x38): 30.142

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 69.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 87.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x68x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x68x2048): 43.910
Elapsed time for attention_prob_times_values (80x2048x2048x68): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x68): 35.295

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 91.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x69x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x69x2048): 41.966
Elapsed time for attention_prob_times_values (80x2048x2048x69): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x69): 34.819

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 89.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x70x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x70x2048): 44.658
Elapsed time for attention_prob_times_values (80x2048x2048x70): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x70): 35.463

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 93.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x71x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x71x2048): 44.440
Elapsed time for attention_prob_times_values (80x2048x2048x71): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x71): 35.547

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 94.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x72x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x72x2048): 45.645
Elapsed time for attention_prob_times_values (80x2048x2048x72): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x72): 35.674

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 96.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x73x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x73x2048): 44.351
Elapsed time for attention_prob_times_values (80x2048x2048x73): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x73): 36.458

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 97.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x74x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x74x2048): 45.163
Elapsed time for attention_prob_times_values (80x2048x2048x74): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x74): 38.198

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 101.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x75x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x75x2048): 37.285
Elapsed time for attention_prob_times_values (80x2048x2048x75): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x75): 37.437

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 92.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x76x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x76x2048): 40.654
Elapsed time for attention_prob_times_values (80x2048x2048x76): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x76): 39.144

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 99.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x163x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x163x2048): 59.643
Elapsed time for attention_prob_times_values (32x2048x2048x163): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x163): 52.873

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 127.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x164x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x164x2048): 61.304
Elapsed time for attention_prob_times_values (32x2048x2048x164): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x164): 54.792

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 132.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x165x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x165x2048): 56.881
Elapsed time for attention_prob_times_values (32x2048x2048x165): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x165): 52.807

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 125.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x166x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x166x2048): 51.261
Elapsed time for attention_prob_times_values (32x2048x2048x166): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x166): 55.214

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 122.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x167x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x167x2048): 60.712
Elapsed time for attention_prob_times_values (32x2048x2048x167): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x167): 54.046

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 131.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x168x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x168x2048): 62.200
Elapsed time for attention_prob_times_values (32x2048x2048x168): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x168): 52.732

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 131.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x169x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x169x2048): 60.405
Elapsed time for attention_prob_times_values (32x2048x2048x169): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x169): 53.979

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 132.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x170x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x170x2048): 61.406
Elapsed time for attention_prob_times_values (32x2048x2048x170): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x170): 56.395

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 136.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x171x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x171x2048): 60.811
Elapsed time for attention_prob_times_values (32x2048x2048x171): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x171): 54.665

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 134.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x172x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x172x2048): 62.228
Elapsed time for attention_prob_times_values (32x2048x2048x172): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x172): 56.707

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 139.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x173x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x173x2048): 61.426
Elapsed time for attention_prob_times_values (32x2048x2048x173): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x173): 55.320

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 136.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x174x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x174x2048): 62.590
Elapsed time for attention_prob_times_values (32x2048x2048x174): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x174): 49.572

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 130.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x175x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x175x2048): 62.018
Elapsed time for attention_prob_times_values (32x2048x2048x175): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x175): 56.087

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 139.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x176x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x176x2048): 64.779
Elapsed time for attention_prob_times_values (32x2048x2048x176): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x176): 57.189

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 144.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x177x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x177x2048): 61.641
Elapsed time for attention_prob_times_values (32x2048x2048x177): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x177): 56.519

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 140.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x178x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x178x2048): 62.667
Elapsed time for attention_prob_times_values (32x2048x2048x178): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x178): 58.720

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 144.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x179x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x179x2048): 61.575
Elapsed time for attention_prob_times_values (32x2048x2048x179): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x179): 57.125

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 142.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x180x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x180x2048): 63.233
Elapsed time for attention_prob_times_values (32x2048x2048x180): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x180): 59.362

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 147.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x181x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x181x2048): 62.588
Elapsed time for attention_prob_times_values (32x2048x2048x181): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x181): 57.730

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 144.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x182x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x182x2048): 63.856
Elapsed time for attention_prob_times_values (32x2048x2048x182): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x182): 59.620

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 149.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x183x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x183x2048): 63.086
Elapsed time for attention_prob_times_values (32x2048x2048x183): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x183): 57.138

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 145.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x184x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x184x2048): 64.922
Elapsed time for attention_prob_times_values (32x2048x2048x184): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x184): 58.488

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 149.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x185x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x185x2048): 62.720
Elapsed time for attention_prob_times_values (32x2048x2048x185): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x185): 58.198

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 147.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x186x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x186x2048): 63.925
Elapsed time for attention_prob_times_values (32x2048x2048x186): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x186): 60.771

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 152.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x187x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x187x2048): 62.868
Elapsed time for attention_prob_times_values (32x2048x2048x187): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x187): 58.976

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 149.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x188x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x188x2048): 64.514
Elapsed time for attention_prob_times_values (32x2048x2048x188): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x188): 61.123

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 154.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x189x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x189x2048): 63.464
Elapsed time for attention_prob_times_values (32x2048x2048x189): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x189): 59.357

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 151.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x190x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x190x2048): 64.668
Elapsed time for attention_prob_times_values (32x2048x2048x190): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x190): 61.684

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 156.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x77x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x77x2048): 46.045
Elapsed time for attention_prob_times_values (80x2048x2048x77): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x77): 38.051

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 104.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x78x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x78x2048): 43.227
Elapsed time for attention_prob_times_values (80x2048x2048x78): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x78): 39.759

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 104.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x79x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x79x2048): 46.765
Elapsed time for attention_prob_times_values (80x2048x2048x79): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x79): 39.011

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 108.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x80x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x80x2048): 48.825
Elapsed time for attention_prob_times_values (80x2048x2048x80): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x80): 39.340

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 111.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x81x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x81x2048): 45.819
Elapsed time for attention_prob_times_values (80x2048x2048x81): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x81): 39.908

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 110.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x82x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x82x2048): 46.544
Elapsed time for attention_prob_times_values (80x2048x2048x82): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x82): 41.587

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 114.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x83x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x83x2048): 46.446
Elapsed time for attention_prob_times_values (80x2048x2048x83): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x83): 40.616

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 113.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x84x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x84x2048): 47.373
Elapsed time for attention_prob_times_values (80x2048x2048x84): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x84): 42.494

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 118.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x85x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x85x2048): 47.014
Elapsed time for attention_prob_times_values (80x2048x2048x85): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x85): 41.336

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 117.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x86x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x86x2048): 48.068
Elapsed time for attention_prob_times_values (80x2048x2048x86): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x86): 36.850

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 15.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x11x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x11x2048): 8.739
Elapsed time for attention_prob_times_values (384x2048x2048x11): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x11): 9.362

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 18.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x12x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x12x2048): 9.502
Elapsed time for attention_prob_times_values (384x2048x2048x12): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x12): 10.583

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 21.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x13x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x13x2048): 9.895
Elapsed time for attention_prob_times_values (384x2048x2048x13): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x13): 11.020

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 23.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x14x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x14x2048): 10.709
Elapsed time for attention_prob_times_values (384x2048x2048x14): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x14): 11.749

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 25.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x15x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x15x2048): 11.690
Elapsed time for attention_prob_times_values (384x2048x2048x15): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x15): 12.679

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 29.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x16x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x16x2048): 12.657
Elapsed time for attention_prob_times_values (384x2048x2048x16): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x16): 14.084

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 33.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x17x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x17x2048): 12.951
Elapsed time for attention_prob_times_values (384x2048x2048x17): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x17): 14.148

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 35.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x18x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x18x2048): 13.762
Elapsed time for attention_prob_times_values (384x2048x2048x18): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x18): 15.517

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 39.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x19x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x19x2048): 13.896
Elapsed time for attention_prob_times_values (384x2048x2048x19): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x19): 15.904

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 41.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x191x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x191x2048): 64.002
Elapsed time for attention_prob_times_values (32x2048x2048x191): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x191): 59.520

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 153.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x192x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x192x2048): 73.729
Elapsed time for attention_prob_times_values (32x2048x2048x192): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x192): 63.711

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 170.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x193x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x193x2048): 63.187
Elapsed time for attention_prob_times_values (32x2048x2048x193): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x193): 47.414

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 135.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x194x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x194x2048): 64.356
Elapsed time for attention_prob_times_values (32x2048x2048x194): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x194): 50.561

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 142.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x195x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x195x2048): 63.253
Elapsed time for attention_prob_times_values (32x2048x2048x195): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x195): 45.626

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 133.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x196x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x196x2048): 64.962
Elapsed time for attention_prob_times_values (32x2048x2048x196): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x196): 50.730

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 144.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x197x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x197x2048): 63.938
Elapsed time for attention_prob_times_values (32x2048x2048x197): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x197): 49.093

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 141.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x198x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x198x2048): 65.165
Elapsed time for attention_prob_times_values (32x2048x2048x198): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x198): 51.366

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 146.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x199x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x199x2048): 64.089
Elapsed time for attention_prob_times_values (32x2048x2048x199): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x199): 47.120

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 138.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x200x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x200x2048): 66.190
Elapsed time for attention_prob_times_values (32x2048x2048x200): 0.0011
========================================================================================================================
num_attention_heads: 40, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x39x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x39x2048): 26.246
Elapsed time for attention_prob_times_values (160x2048x2048x39): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x39): 30.304

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 70.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x40x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x40x2048): 28.007
Elapsed time for attention_prob_times_values (160x2048x2048x40): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x40): 29.435

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 73.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x41x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x41x2048): 26.995
Elapsed time for attention_prob_times_values (160x2048x2048x41): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x41): 31.536

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 75.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x42x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x42x2048): 28.487
Elapsed time for attention_prob_times_values (160x2048x2048x42): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x42): 30.839

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 78.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x43x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x43x2048): 28.684
Elapsed time for attention_prob_times_values (160x2048x2048x43): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x43): 33.002

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 82.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x44x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x44x2048): 29.707
Elapsed time for attention_prob_times_values (160x2048x2048x44): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x44): 34.455

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 86.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x45x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x45x2048): 29.300
Elapsed time for attention_prob_times_values (160x2048x2048x45): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x45): 34.100

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 86.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x46x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x46x2048): 31.031
Elapsed time for attention_prob_times_values (160x2048x2048x46): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x46): 32.528

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 88.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x47x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x47x2048): 31.581
Elapsed time for attention_prob_times_values (160x2048x2048x47): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x47): 35.784

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 95.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x48x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x48x2048): 31.056
Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 111.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x87x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x87x2048): 48.339
Elapsed time for attention_prob_times_values (80x2048x2048x87): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x87): 42.250

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 121.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x88x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x88x2048): 49.264
Elapsed time for attention_prob_times_values (80x2048x2048x88): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x88): 42.378

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 123.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x89x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x89x2048): 47.945
Elapsed time for attention_prob_times_values (80x2048x2048x89): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x89): 43.147

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 124.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x90x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x90x2048): 48.679
Elapsed time for attention_prob_times_values (80x2048x2048x90): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x90): 41.475

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 123.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x91x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x91x2048): 48.826
Elapsed time for attention_prob_times_values (80x2048x2048x91): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x91): 43.837

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 128.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x92x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x92x2048): 49.675
Elapsed time for attention_prob_times_values (80x2048x2048x92): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x92): 42.355

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 127.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x93x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x93x2048): 49.298
Elapsed time for attention_prob_times_values (80x2048x2048x93): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x93): 41.703

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 127.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x94x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x94x2048): 50.121
Elapsed time for attention_prob_times_values (80x2048x2048x94): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x94): 41.264

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 128.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x95x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x95x2048): 48.751
Elapsed time for attention_prob_times_values (80x2048x2048x95): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x95): 41.477

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 127.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x200): 48.931

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 144.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x201x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x201x2048): 63.910
Elapsed time for attention_prob_times_values (32x2048x2048x201): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x201): 47.704

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 140.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x202x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x202x2048): 65.055
Elapsed time for attention_prob_times_values (32x2048x2048x202): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x202): 52.199

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 149.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x203x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x203x2048): 63.950
Elapsed time for attention_prob_times_values (32x2048x2048x203): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x203): 49.354

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 144.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x204x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x204x2048): 65.831
Elapsed time for attention_prob_times_values (32x2048x2048x204): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x204): 52.764

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 151.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x205x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x205x2048): 64.653
Elapsed time for attention_prob_times_values (32x2048x2048x205): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x205): 49.636

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 146.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x206x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x206x2048): 65.947
Elapsed time for attention_prob_times_values (32x2048x2048x206): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x206): 51.633

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 151.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x207x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x207x2048): 65.046
Elapsed time for attention_prob_times_values (32x2048x2048x207): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x207): 48.403

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 145.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x208x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x208x2048): 67.959
Elapsed time for attention_prob_times_values (32x2048x2048x208): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x208): 52.018

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 154.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x209x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x209x2048): 64.593
Elapsed time for attention_prob_times_values (32x2048x2048x209): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x209): 48.538

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 145.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x210x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x210x2048): 65.751
Elapsed time for attention_prob_times_values (32x2048x2048x210): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x210): 53.700

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 156.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x211x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x211x2048): 65.301
Elapsed time for attention_prob_times_values (32x2048x2048x211): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x211): 51.166

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 151.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x212x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x212x2048): 66.692
Elapsed time for attention_prob_times_values (32x2048x2048x212): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x212): 54.334

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 159.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x213x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x213x2048): 65.685
Elapsed time for attention_prob_times_values (32x2048x2048x213): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x213): 51.718

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 154.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x214x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x214x2048): 65.957
Elapsed time for attention_prob_times_values (32x2048x2048x214): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x214): 54.377

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 159.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x215x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x215x2048): 66.193
Elapsed time for attention_prob_times_values (32x2048x2048x215): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x215): 52.407

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 156.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x216x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x216x2048): 68.045
Elapsed time for attention_prob_times_values (32x2048x2048x216): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x216): 53.438

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 160.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x217x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x217x2048): 57.339
Elapsed time for attention_prob_times_values (32x2048x2048x217): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x217): 52.183

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 147.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x218x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x218x2048): 66.625
Elapsed time for attention_prob_times_values (32x2048x2048x218): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x218): 55.418

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 163.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x219x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x219x2048): 66.176
Elapsed time for attention_prob_times_values (32x2048x2048x219): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x219): 47.776

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 150.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x220x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x220x2048): 67.456
Elapsed time for attention_prob_times_values (32x2048x2048x220): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x220): 56.040

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 166.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x221x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x221x2048): 66.477
Elapsed time for attention_prob_times_values (32x2048x2048x221): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x221): 50.045

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 155.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x222x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x222x2048): 67.737
Elapsed time for attention_prob_times_values (32x2048x2048x222): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x222): 56.333

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 168.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x223x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x223x2048): 67.043
Elapsed time for attention_prob_times_values (32x2048x2048x223): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x223): 53.318

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 162.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x224x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x224x2048): 76.448
Elapsed time for attention_prob_times_values (32x2048x2048x224): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x224): 56.982

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 179.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x225x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x225x2048): 65.765
Elapsed time for attention_prob_times_values (32x2048x2048x225): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x225): 54.940

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 165.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x226x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x226x2048): 67.138
Elapsed time for attention_prob_times_values (32x2048x2048x226): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x226): 57.284

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 170.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x227x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x227x2048): 66.007
Elapsed time for attention_prob_times_values (32x2048x2048x227): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x227): 55.225

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 166.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x228x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x228x2048): 67.554
Elapsed time for attention_prob_times_values (32x2048x2048x228): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x228): 57.751

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 173.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
num_attention_heads: 20, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x96x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x96x2048): 59.769
Elapsed time for attention_prob_times_values (80x2048x2048x96): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x96): 42.688

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 143.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x97x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x97x2048): 51.015
Elapsed time for attention_prob_times_values (80x2048x2048x97): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x97): 41.268

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 132.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x98x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x98x2048): 49.302
Elapsed time for attention_prob_times_values (80x2048x2048x98): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x98): 43.709

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 135.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x99x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x99x2048): 51.480
Elapsed time for attention_prob_times_values (80x2048x2048x99): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x99): 6.720

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 34.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x100x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x100x2048): 53.058
Elapsed time for attention_prob_times_values (80x2048x2048x100): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x100): 44.299

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 142.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x101x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x101x2048): 51.944
Elapsed time for attention_prob_times_values (80x2048x2048x101): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x101): 44.399

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 142.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x102x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x102x2048): 52.372
Elapsed time for attention_prob_times_values (80x2048x2048x102): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x102): 49.775

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 152.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x103x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x103x2048): 50.168
Elapsed time for attention_prob_times_values (80x2048x2048x103): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x103): 48.907

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 149.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x104x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x104x2048): 53.051
Elapsed time for attention_prob_times_values (80x2048x2048x104): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x104): 49.323

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 154.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x105x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x105x2048): 52.116
Elapsed time for attention_prob_times_values (80x2048x2048x105): 0.0014
Elapsed time for attention_prob_times_values (160x2048x2048x48): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x48): 38.421

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 98.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x49x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x49x2048): 30.714
Elapsed time for attention_prob_times_values (160x2048x2048x49): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x49): 37.181

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 98.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x50x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x50x2048): 33.774
Elapsed time for attention_prob_times_values (160x2048x2048x50): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x50): 38.073

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 105.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x51x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x51x2048): 33.080
Elapsed time for attention_prob_times_values (160x2048x2048x51): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x51): 37.717

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 105.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x52x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x52x2048): 35.189
Elapsed time for attention_prob_times_values (160x2048x2048x52): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x52): 39.441

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 112.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x53x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x53x2048): 35.456
Elapsed time for attention_prob_times_values (160x2048x2048x53): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x53): 39.503

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 114.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x54x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x54x2048): 36.039
Elapsed time for attention_prob_times_values (160x2048x2048x54): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x54): 41.136

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 119.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x55x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x55x2048): 36.961
Elapsed time for attention_prob_times_values (160x2048x2048x55): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x55): 40.837

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 122.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x56x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x56x2048): 37.936
Elapsed time for attention_prob_times_values (160x2048x2048x56): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x56): 42.763

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 128.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x57x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x57x2048): 37.247
Elapsed time for attention_prob_times_values (160x2048x2048x57): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x57): 42.149

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 127.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x229x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x229x2048): 66.366
Elapsed time for attention_prob_times_values (32x2048x2048x229): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x229): 55.651

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 168.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x230x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x230x2048): 67.719
Elapsed time for attention_prob_times_values (32x2048x2048x230): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x230): 57.971

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 174.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x231x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x231x2048): 66.688
Elapsed time for attention_prob_times_values (32x2048x2048x231): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x231): 55.396

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 169.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x232x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x232x2048): 69.012
Elapsed time for attention_prob_times_values (32x2048x2048x232): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x232): 56.600

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 174.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x233x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x233x2048): 66.246
Elapsed time for attention_prob_times_values (32x2048x2048x233): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x233): 56.114

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 171.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x234x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x234x2048): 67.606
Elapsed time for attention_prob_times_values (32x2048x2048x234): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x234): 58.599

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 177.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x235x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x235x2048): 66.147
Elapsed time for attention_prob_times_values (32x2048x2048x235): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x235): 56.487

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 172.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x236x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x236x2048): 68.289
Elapsed time for attention_prob_times_values (32x2048x2048x236): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x236): 59.218

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 180.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x237x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x237x2048): 57.176
Elapsed time for attention_prob_times_values (32x2048x2048x237): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x237): 56.774

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 162.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x238x2048): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x105): 49.456

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 154.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x106x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x106x2048): 53.470
Elapsed time for attention_prob_times_values (80x2048x2048x106): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x106): 51.407

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 160.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x107x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x107x2048): 53.278
Elapsed time for attention_prob_times_values (80x2048x2048x107): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x107): 50.088

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 159.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x108x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x108x2048): 54.596
Elapsed time for attention_prob_times_values (80x2048x2048x108): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x108): 52.152

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 165.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x109x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x109x2048): 53.892
Elapsed time for attention_prob_times_values (80x2048x2048x109): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x109): 50.872

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 163.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x110x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x110x2048): 54.937
Elapsed time for attention_prob_times_values (80x2048x2048x110): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x110): 52.846

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 169.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x111x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x111x2048): 52.396
Elapsed time for attention_prob_times_values (80x2048x2048x111): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x111): 51.354

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 164.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x112x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x112x2048): 54.093
Elapsed time for attention_prob_times_values (80x2048x2048x112): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x112): 53.396

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 171.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x113x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x113x2048): 54.548
Elapsed time for attention_prob_times_values (80x2048x2048x113): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x113): 51.972

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 170.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x114x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x114x2048): 55.757
Elapsed time for attention_prob_times_values (80x2048x2048x114): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x114): 53.984

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 176.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x238x2048): 68.411
Elapsed time for attention_prob_times_values (32x2048x2048x238): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x238): 59.363

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 181.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x239x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x239x2048): 54.715
Elapsed time for attention_prob_times_values (32x2048x2048x239): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x239): 57.278

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 160.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x240x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x240x2048): 54.402
Elapsed time for attention_prob_times_values (32x2048x2048x240): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x240): 59.440

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 163.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x241x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x241x2048): 52.241
Elapsed time for attention_prob_times_values (32x2048x2048x241): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x241): 57.683

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 158.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x242x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x242x2048): 52.896
Elapsed time for attention_prob_times_values (32x2048x2048x242): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x242): 60.134

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 162.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x243x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x243x2048): 52.685
Elapsed time for attention_prob_times_values (32x2048x2048x243): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x243): 58.113

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 160.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x244x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x244x2048): 53.572
Elapsed time for attention_prob_times_values (32x2048x2048x244): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x244): 54.584

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 157.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x245x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x245x2048): 52.824
Elapsed time for attention_prob_times_values (32x2048x2048x245): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x245): 58.183

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 161.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x246x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x246x2048): 53.583
Elapsed time for attention_prob_times_values (32x2048x2048x246): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x246): 53.315

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 156.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x247x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x247x2048): 49.326
Elapsed time for attention_prob_times_values (32x2048x2048x247): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x247): 58.844

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 157.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x248x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x248x2048): 54.610
Elapsed time for attention_prob_times_values (32x2048x2048x248): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x248): 55.423

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 161.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 1992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x249x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x249x2048): 49.497
Elapsed time for attention_prob_times_values (32x2048x2048x249): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x249): 59.968

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 159.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x250x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x250x2048): 53.028
Elapsed time for attention_prob_times_values (32x2048x2048x250): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x250): 54.986

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 159.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x251x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x251x2048): 49.605
Elapsed time for attention_prob_times_values (32x2048x2048x251): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x251): 60.269

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 161.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x252x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x252x2048): 53.746
Elapsed time for attention_prob_times_values (32x2048x2048x252): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x252): 61.868

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 170.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x253x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x253x2048): 52.363
Elapsed time for attention_prob_times_values (32x2048x2048x253): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x253): 60.918

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 167.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x254x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x254x2048): 53.534
Elapsed time for attention_prob_times_values (32x2048x2048x254): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x254): 58.795

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 167.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x255x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x255x2048): 52.458
Elapsed time for attention_prob_times_values (32x2048x2048x255): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x255): 60.647

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 168.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x256x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x256x2048): 71.753
Elapsed time for attention_prob_times_values (32x2048x2048x256): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x256): 66.002

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 206.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 20, hidden_size: 2300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x115x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x115x2048): 55.219
Elapsed time for attention_prob_times_values (80x2048x2048x115): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x115): 48.429

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 167.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x116x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x116x2048): 56.402
Elapsed time for attention_prob_times_values (80x2048x2048x116): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x116): 54.848

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 181.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x117x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x117x2048): 55.970
Elapsed time for attention_prob_times_values (80x2048x2048x117): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x117): 53.384

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 179.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x118x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x118x2048): 56.873
Elapsed time for attention_prob_times_values (80x2048x2048x118): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x118): 55.682

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 185.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x119x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x119x2048): 56.504
Elapsed time for attention_prob_times_values (80x2048x2048x119): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x119): 52.263

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 180.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x120x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x120x2048): 58.176
Elapsed time for attention_prob_times_values (80x2048x2048x120): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x120): 50.196

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 180.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x121x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x121x2048): 53.509
Elapsed time for attention_prob_times_values (80x2048x2048x121): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x121): 42.916

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 160.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x122x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x122x2048): 55.147
Elapsed time for attention_prob_times_values (80x2048x2048x122): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x122): 57.005

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 189.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x123x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x123x2048): 55.940
Elapsed time for attention_prob_times_values (80x2048x2048x123): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x123): 45.405

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 170.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x124x2048): 0.0014
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x58x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x58x2048): 37.082
Elapsed time for attention_prob_times_values (160x2048x2048x58): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x58): 43.839

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 131.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x59x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x59x2048): 35.496
Elapsed time for attention_prob_times_values (160x2048x2048x59): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x59): 43.276

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 128.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x60x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x60x2048): 36.862
Elapsed time for attention_prob_times_values (160x2048x2048x60): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x60): 45.601

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 136.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x61x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x61x2048): 39.439
Elapsed time for attention_prob_times_values (160x2048x2048x61): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x61): 44.650

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 141.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x62x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x62x2048): 38.601
Elapsed time for attention_prob_times_values (160x2048x2048x62): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x62): 46.894

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 144.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x63x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x63x2048): 40.569
Elapsed time for attention_prob_times_values (160x2048x2048x63): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x63): 46.222

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 149.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x64x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x64x2048): 49.436
Elapsed time for attention_prob_times_values (160x2048x2048x64): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x64): 50.341

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 174.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x65x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x65x2048): 42.648
Elapsed time for attention_prob_times_values (160x2048x2048x65): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x65): 32.971

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 131.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x66x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x66x2048): 43.461
Elapsed time for attention_prob_times_values (160x2048x2048x66): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x66): 34.231

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 137.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x67x2048): 0.0021
Elapsed time for attention_key_query_prob (32x2048x257x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x257x2048): 55.011
Elapsed time for attention_prob_times_values (32x2048x2048x257): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x257): 52.052

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 160.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x258x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x258x2048): 55.840
Elapsed time for attention_prob_times_values (32x2048x2048x258): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x258): 49.444

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 158.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x259x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x259x2048): 55.292
Elapsed time for attention_prob_times_values (32x2048x2048x259): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x259): 52.684

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 163.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x260x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x260x2048): 56.263
Elapsed time for attention_prob_times_values (32x2048x2048x260): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x260): 54.809

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 168.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x261x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x261x2048): 55.278
Elapsed time for attention_prob_times_values (32x2048x2048x261): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x261): 53.226

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 164.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x262x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x262x2048): 55.096
Elapsed time for attention_prob_times_values (32x2048x2048x262): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x262): 55.511

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 168.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x263x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x263x2048): 55.111
Elapsed time for attention_prob_times_values (32x2048x2048x263): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x263): 53.634

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 166.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x264x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x264x2048): 50.032
Elapsed time for attention_prob_times_values (32x2048x2048x264): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x264): 52.965

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 157.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x265x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x265x2048): 54.199
Elapsed time for attention_prob_times_values (32x2048x2048x265): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x265): 53.634

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 165.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x266x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x266x2048): 52.634
Elapsed time for attention_prob_times_values (32x2048x2048x266): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x266): 56.284

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 167.445
MLP duration (in seconds): 0.0000
num_attention_heads: 96, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x20x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x20x2048): 15.014
Elapsed time for attention_prob_times_values (384x2048x2048x20): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x20): 17.200

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 46.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x21x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x21x2048): 15.542
Elapsed time for attention_prob_times_values (384x2048x2048x21): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x21): 17.444

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 48.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x22x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x22x2048): 16.138
Elapsed time for attention_prob_times_values (384x2048x2048x22): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x22): 18.672

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 53.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x23x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x23x2048): 17.427
Elapsed time for attention_prob_times_values (384x2048x2048x23): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x23): 18.985

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 57.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x24x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x24x2048): 18.174
Elapsed time for attention_prob_times_values (384x2048x2048x24): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x24): 19.772

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 61.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x25x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x25x2048): 18.416
Elapsed time for attention_prob_times_values (384x2048x2048x25): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x25): 20.483

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 64.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x26x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x26x2048): 19.051
Elapsed time for attention_prob_times_values (384x2048x2048x26): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x26): 21.560

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 69.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x27x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x27x2048): 19.771
Elapsed time for attention_prob_times_values (384x2048x2048x27): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x27): 21.985

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 73.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x28x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x28x2048): 20.325
Elapsed time for attention_prob_times_values (384x2048x2048x28): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x28): 23.032

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 78.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x29x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x29x2048): 20.818
Elapsed time for attention_prob_times_values (384x2048x2048x29): 0.0043
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x267x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x267x2048): 54.598
Elapsed time for attention_prob_times_values (32x2048x2048x267): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x267): 48.145

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 157.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x268x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x268x2048): 55.498
Elapsed time for attention_prob_times_values (32x2048x2048x268): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x268): 56.782

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 173.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x269x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x269x2048): 54.738
Elapsed time for attention_prob_times_values (32x2048x2048x269): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x269): 54.258

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 169.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x270x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x270x2048): 55.402
Elapsed time for attention_prob_times_values (32x2048x2048x270): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x270): 57.034

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 174.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x271x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x271x2048): 55.215
Elapsed time for attention_prob_times_values (32x2048x2048x271): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x271): 54.759

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 171.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x272x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x272x2048): 56.786
Elapsed time for attention_prob_times_values (32x2048x2048x272): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x272): 69.506

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 195.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x273x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x273x2048): 54.605
Elapsed time for attention_prob_times_values (32x2048x2048x273): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x273): 55.025

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 171.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x274x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x274x2048): 55.316
Elapsed time for attention_prob_times_values (32x2048x2048x274): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x274): 57.684

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 177.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x275x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x275x2048): 54.830
Elapsed time for attention_prob_times_values (32x2048x2048x275): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x275): 51.254

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 166.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x124x2048): 57.406
Elapsed time for attention_prob_times_values (80x2048x2048x124): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x124): 57.302

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 196.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x125x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x125x2048): 56.512
Elapsed time for attention_prob_times_values (80x2048x2048x125): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x125): 43.909

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 170.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x126x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x126x2048): 57.425
Elapsed time for attention_prob_times_values (80x2048x2048x126): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x126): 56.718

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 197.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x127x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x127x2048): 56.995
Elapsed time for attention_prob_times_values (80x2048x2048x127): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x127): 43.121

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 170.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x128x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x128x2048): 68.198
Elapsed time for attention_prob_times_values (80x2048x2048x128): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x128): 61.346

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 226.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x129x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x129x2048): 56.744
Elapsed time for attention_prob_times_values (80x2048x2048x129): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x129): 42.613

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 171.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x130x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x130x2048): 57.853
Elapsed time for attention_prob_times_values (80x2048x2048x130): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x130): 46.900

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 183.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x131x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x131x2048): 57.462
Elapsed time for attention_prob_times_values (80x2048x2048x131): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x131): 45.277

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 180.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x132x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x132x2048): 48.147
Elapsed time for attention_prob_times_values (80x2048x2048x132): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x132): 47.769

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 171.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x133x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x133x2048): 57.523
Elapsed time for attention_prob_times_values (80x2048x2048x133): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x133): 45.940

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 183.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x276x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x276x2048): 55.860
Elapsed time for attention_prob_times_values (32x2048x2048x276): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x276): 58.182

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 179.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x277x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x277x2048): 54.220
Elapsed time for attention_prob_times_values (32x2048x2048x277): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x277): 55.801

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 174.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x278x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x278x2048): 55.734
Elapsed time for attention_prob_times_values (32x2048x2048x278): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x278): 57.664

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 179.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x279x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x279x2048): 54.754
Elapsed time for attention_prob_times_values (32x2048x2048x279): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x279): 55.746

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 175.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x280x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x280x2048): 56.802
Elapsed time for attention_prob_times_values (32x2048x2048x280): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x280): 71.067

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 201.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x281x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x281x2048): 54.832
Elapsed time for attention_prob_times_values (32x2048x2048x281): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x281): 56.506

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 177.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x282x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x282x2048): 55.334
Elapsed time for attention_prob_times_values (32x2048x2048x282): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x282): 59.280

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 183.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x283x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x283x2048): 55.145
Elapsed time for attention_prob_times_values (32x2048x2048x283): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x283): 57.051

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 180.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x284x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x284x2048): 56.275
Elapsed time for attention_prob_times_values (32x2048x2048x284): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x284): 59.192

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 185.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x285x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x285x2048): 52.855
Elapsed time for attention_prob_times_values (32x2048x2048x285): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x285): 57.290

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 177.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x286x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x286x2048): 56.070
Elapsed time for attention_prob_times_values (32x2048x2048x286): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x286): 59.488

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 186.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x287x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x287x2048): 55.303
Elapsed time for attention_prob_times_values (32x2048x2048x287): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x287): 57.754

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 183.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x288x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x288x2048): 75.287
Elapsed time for attention_prob_times_values (32x2048x2048x288): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x288): 70.783

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 237.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x289x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x289x2048): 45.674
Elapsed time for attention_prob_times_values (32x2048x2048x289): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x289): 56.766

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 164.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x290x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x290x2048): 59.866
Elapsed time for attention_prob_times_values (32x2048x2048x290): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x290): 60.633

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 196.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x291x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x291x2048): 58.970
Elapsed time for attention_prob_times_values (32x2048x2048x291): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x291): 58.569

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 192.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x292x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x292x2048): 60.781
Elapsed time for attention_prob_times_values (32x2048x2048x292): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x292): 61.222

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 200.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x293x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x293x2048): 56.750
Elapsed time for attention_prob_times_values (32x2048x2048x293): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x293): 59.088

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 190.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x294x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x294x2048): 59.675
Elapsed time for attention_prob_times_values (32x2048x2048x294): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x294): 59.632

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 196.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x67x2048): 43.071
Elapsed time for attention_prob_times_values (160x2048x2048x67): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x67): 33.821

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 137.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x68x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x68x2048): 44.097
Elapsed time for attention_prob_times_values (160x2048x2048x68): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x68): 31.752

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 134.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x69x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x69x2048): 43.792
Elapsed time for attention_prob_times_values (160x2048x2048x69): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x69): 33.936

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 141.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x70x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x70x2048): 44.846
Elapsed time for attention_prob_times_values (160x2048x2048x70): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x70): 36.149

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 149.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x71x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x71x2048): 43.754
Elapsed time for attention_prob_times_values (160x2048x2048x71): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x71): 35.113

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 147.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x72x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x72x2048): 42.992
Elapsed time for attention_prob_times_values (160x2048x2048x72): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x72): 35.567

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 148.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x73x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x73x2048): 40.290
Elapsed time for attention_prob_times_values (160x2048x2048x73): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x73): 35.970

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 146.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x74x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x74x2048): 45.444
Elapsed time for attention_prob_times_values (160x2048x2048x74): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x74): 36.784

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 158.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x75x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x75x2048): 45.375
Elapsed time for attention_prob_times_values (160x2048x2048x75): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x75): 36.903

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 159.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x76x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x76x2048): 46.153
Elapsed time for attention_prob_times_values (160x2048x2048x76): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x76): 34.977

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 157.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x134x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x134x2048): 59.258
Elapsed time for attention_prob_times_values (80x2048x2048x134): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x134): 48.060

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 191.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x135x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x135x2048): 58.559
Elapsed time for attention_prob_times_values (80x2048x2048x135): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x135): 45.242

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 185.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x136x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x136x2048): 60.322
Elapsed time for attention_prob_times_values (80x2048x2048x136): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x136): 45.654

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 190.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x137x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x137x2048): 58.489
Elapsed time for attention_prob_times_values (80x2048x2048x137): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x137): 46.838

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 191.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x138x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x138x2048): 59.658
Elapsed time for attention_prob_times_values (80x2048x2048x138): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x138): 47.860

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 196.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x139x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x139x2048): 57.050
Elapsed time for attention_prob_times_values (80x2048x2048x139): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x139): 45.404

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 187.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x140x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x140x2048): 60.071
Elapsed time for attention_prob_times_values (80x2048x2048x140): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x140): 49.388

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 202.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x141x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x141x2048): 59.645
Elapsed time for attention_prob_times_values (80x2048x2048x141): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x141): 48.212

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 200.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x142x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x142x2048): 60.799
Elapsed time for attention_prob_times_values (80x2048x2048x142): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x142): 50.710

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 208.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 8, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x295x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x295x2048): 58.848
Elapsed time for attention_prob_times_values (32x2048x2048x295): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x295): 48.665

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 176.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x296x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x296x2048): 55.786
Elapsed time for attention_prob_times_values (32x2048x2048x296): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x296): 74.208

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 210.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x297x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x297x2048): 57.994
Elapsed time for attention_prob_times_values (32x2048x2048x297): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x297): 59.575

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 195.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x298x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x298x2048): 58.402
Elapsed time for attention_prob_times_values (32x2048x2048x298): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x298): 62.248

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 200.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x299x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x299x2048): 50.814
Elapsed time for attention_prob_times_values (32x2048x2048x299): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x299): 57.982

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 180.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x300x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x300x2048): 59.708
Elapsed time for attention_prob_times_values (32x2048x2048x300): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x300): 62.527

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 204.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x301x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x301x2048): 58.653
Elapsed time for attention_prob_times_values (32x2048x2048x301): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x301): 60.302

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 199.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x302x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x302x2048): 59.310
Elapsed time for attention_prob_times_values (32x2048x2048x302): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x302): 61.210

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 202.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x303x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x303x2048): 52.867
Elapsed time for attention_prob_times_values (32x2048x2048x303): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x303): 60.919

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 190.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x304x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x304x2048): 60.101
Elapsed time for attention_prob_times_values (32x2048x2048x304): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x304): 75.044

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 225.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x305x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x305x2048): 58.115
Elapsed time for attention_prob_times_values (32x2048x2048x305): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x305): 61.299

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 201.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x306x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x306x2048): 57.164
Elapsed time for attention_prob_times_values (32x2048x2048x306): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x306): 63.991

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 204.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x307x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x307x2048): 58.476
Elapsed time for attention_prob_times_values (32x2048x2048x307): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x307): 61.613

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 203.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x308x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x308x2048): 59.471
Elapsed time for attention_prob_times_values (32x2048x2048x308): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x308): 64.541

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 210.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x309x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x309x2048): 58.442
Elapsed time for attention_prob_times_values (32x2048x2048x309): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x309): 62.214

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 205.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x310x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x310x2048): 58.868
Elapsed time for attention_prob_times_values (32x2048x2048x310): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x310): 64.875

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 211.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x311x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x311x2048): 59.068
Elapsed time for attention_prob_times_values (32x2048x2048x311): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x311): 59.753

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 203.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x312x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x312x2048): 60.691
Elapsed time for attention_prob_times_values (32x2048x2048x312): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x312): 77.803

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 234.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x313x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x313x2048): 58.345
Elapsed time for attention_prob_times_values (32x2048x2048x313): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x313): 62.885

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 208.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (80x2048x143x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x143x2048): 60.283
Elapsed time for attention_prob_times_values (80x2048x2048x143): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x143): 48.801

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 204.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x144x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x144x2048): 62.656
Elapsed time for attention_prob_times_values (80x2048x2048x144): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x144): 49.023

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 209.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x145x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x145x2048): 60.003
Elapsed time for attention_prob_times_values (80x2048x2048x145): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x145): 46.527

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 200.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x146x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x146x2048): 61.254
Elapsed time for attention_prob_times_values (80x2048x2048x146): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x146): 51.967

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 216.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x147x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x147x2048): 60.599
Elapsed time for attention_prob_times_values (80x2048x2048x147): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x147): 49.996

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 212.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x148x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x148x2048): 61.992
Elapsed time for attention_prob_times_values (80x2048x2048x148): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x148): 52.566

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 221.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x149x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x149x2048): 61.039
Elapsed time for attention_prob_times_values (80x2048x2048x149): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x149): 48.379

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 211.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x150x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x150x2048): 61.655
Elapsed time for attention_prob_times_values (80x2048x2048x150): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x150): 52.949

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 223.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x151x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x151x2048): 57.819
Elapsed time for attention_prob_times_values (80x2048x2048x151): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x151): 51.295

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 214.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x152x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x152x2048): 63.290
Elapsed time for attention_prob_times_values (80x2048x2048x152): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x152): 51.013

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 224.203
========================================================================================================================
num_attention_heads: 8, hidden_size: 2512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x314x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x314x2048): 58.746
Elapsed time for attention_prob_times_values (32x2048x2048x314): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x314): 65.348

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 213.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x315x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x315x2048): 58.074
Elapsed time for attention_prob_times_values (32x2048x2048x315): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x315): 63.185

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 209.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x316x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x316x2048): 56.572
Elapsed time for attention_prob_times_values (32x2048x2048x316): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x316): 65.631

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 210.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x317x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x317x2048): 58.760
Elapsed time for attention_prob_times_values (32x2048x2048x317): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x317): 63.609

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 212.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x318x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x318x2048): 59.474
Elapsed time for attention_prob_times_values (32x2048x2048x318): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x318): 66.315

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 218.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x319x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x319x2048): 58.362
Elapsed time for attention_prob_times_values (32x2048x2048x319): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x319): 60.844

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 208.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x320x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x320x2048): 76.348
Elapsed time for attention_prob_times_values (32x2048x2048x320): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x320): 77.398

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 269.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x321x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x321x2048): 61.454
Elapsed time for attention_prob_times_values (32x2048x2048x321): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x321): 56.059

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 205.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x322x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x322x2048): 62.389
Elapsed time for attention_prob_times_values (32x2048x2048x322): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x322): 58.314

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 211.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x323x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x323x2048): 61.151
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x77x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x77x2048): 46.124
Elapsed time for attention_prob_times_values (160x2048x2048x77): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x77): 36.857

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 164.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x78x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x78x2048): 47.230
Elapsed time for attention_prob_times_values (160x2048x2048x78): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x78): 39.705

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 174.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x79x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x79x2048): 41.712
Elapsed time for attention_prob_times_values (160x2048x2048x79): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x79): 36.492

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 159.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x80x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x80x2048): 48.867
Elapsed time for attention_prob_times_values (160x2048x2048x80): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x80): 39.165

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 179.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x81x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x81x2048): 46.293
Elapsed time for attention_prob_times_values (160x2048x2048x81): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x81): 39.745

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 178.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x82x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x82x2048): 47.268
Elapsed time for attention_prob_times_values (160x2048x2048x82): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x82): 38.369

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 178.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x83x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x83x2048): 47.543
Elapsed time for attention_prob_times_values (160x2048x2048x83): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x83): 39.806

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 183.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x84x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x84x2048): 46.274
Elapsed time for attention_prob_times_values (160x2048x2048x84): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x84): 42.403

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 189.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x85x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x85x2048): 48.111
Elapsed time for attention_prob_times_values (160x2048x2048x85): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x85): 39.737

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 188.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (32x2048x2048x323): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x323): 56.147

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 206.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x324x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x324x2048): 63.299
Elapsed time for attention_prob_times_values (32x2048x2048x324): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x324): 58.432

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 214.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x325x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x325x2048): 61.396
Elapsed time for attention_prob_times_values (32x2048x2048x325): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x325): 56.617

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 208.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x326x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x326x2048): 61.980
Elapsed time for attention_prob_times_values (32x2048x2048x326): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x326): 58.699

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 213.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x327x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x327x2048): 61.117
Elapsed time for attention_prob_times_values (32x2048x2048x327): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x327): 57.063

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 209.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x328x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x328x2048): 61.041
Elapsed time for attention_prob_times_values (32x2048x2048x328): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x328): 68.873

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 230.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x329x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x329x2048): 60.114
Elapsed time for attention_prob_times_values (32x2048x2048x329): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x329): 56.432

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 207.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x330x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x330x2048): 60.033
Elapsed time for attention_prob_times_values (32x2048x2048x330): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x330): 59.705

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 214.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x331x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x331x2048): 60.597
Elapsed time for attention_prob_times_values (32x2048x2048x331): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x331): 56.517

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 209.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x332x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x332x2048): 61.918
Elapsed time for attention_prob_times_values (32x2048x2048x332): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x332): 60.009

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 219.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x153x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x153x2048): 58.849
Elapsed time for attention_prob_times_values (80x2048x2048x153): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x153): 51.712

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 219.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x154x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x154x2048): 62.541
Elapsed time for attention_prob_times_values (80x2048x2048x154): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x154): 54.029

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 232.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x155x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x155x2048): 59.238
Elapsed time for attention_prob_times_values (80x2048x2048x155): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x155): 52.331

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 223.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x156x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x156x2048): 63.362
Elapsed time for attention_prob_times_values (80x2048x2048x156): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x156): 52.367

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 232.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x157x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x157x2048): 62.250
Elapsed time for attention_prob_times_values (80x2048x2048x157): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x157): 53.000

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 232.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x158x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x158x2048): 63.935
Elapsed time for attention_prob_times_values (80x2048x2048x158): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x158): 55.020

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 241.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x159x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x159x2048): 63.123
Elapsed time for attention_prob_times_values (80x2048x2048x159): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x159): 53.419

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 237.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x160x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x160x2048): 68.820
Elapsed time for attention_prob_times_values (80x2048x2048x160): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x160): 55.259

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 252.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x161x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x161x2048): 62.232
Elapsed time for attention_prob_times_values (80x2048x2048x161): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x161): 51.620

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 233.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x29): 21.943

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 79.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x30x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x30x2048): 21.406
Elapsed time for attention_prob_times_values (384x2048x2048x30): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x30): 23.508

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 85.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x31x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x31x2048): 20.743
Elapsed time for attention_prob_times_values (384x2048x2048x31): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x31): 23.709

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 86.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x32x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x32x2048): 40.880
Elapsed time for attention_prob_times_values (384x2048x2048x32): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x32): 25.157

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 124.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x33x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x33x2048): 26.743
Elapsed time for attention_prob_times_values (384x2048x2048x33): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x33): 23.605

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 102.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x34x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x34x2048): 27.137
Elapsed time for attention_prob_times_values (384x2048x2048x34): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x34): 26.882

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 113.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x35x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x35x2048): 19.237
Elapsed time for attention_prob_times_values (384x2048x2048x35): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x35): 26.626

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 95.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x36x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x36x2048): 26.014
Elapsed time for attention_prob_times_values (384x2048x2048x36): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x36): 28.916

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 119.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x37x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x37x2048): 26.468
Elapsed time for attention_prob_times_values (384x2048x2048x37): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x37): 29.190

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 124.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x38x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x38x2048): 27.167
Elapsed time for attention_prob_times_values (384x2048x2048x38): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x38): 30.629

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 131.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x333x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x333x2048): 60.717
Elapsed time for attention_prob_times_values (32x2048x2048x333): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x333): 56.678

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 211.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x334x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x334x2048): 61.366
Elapsed time for attention_prob_times_values (32x2048x2048x334): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x334): 60.178

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 219.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x335x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x335x2048): 60.904
Elapsed time for attention_prob_times_values (32x2048x2048x335): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x335): 57.183

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 213.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x336x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x336x2048): 59.641
Elapsed time for attention_prob_times_values (32x2048x2048x336): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x336): 70.043

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 233.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x337x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x337x2048): 55.971
Elapsed time for attention_prob_times_values (32x2048x2048x337): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x337): 55.569

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 202.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x338x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x338x2048): 60.908
Elapsed time for attention_prob_times_values (32x2048x2048x338): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x338): 60.501

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 221.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x339x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x339x2048): 53.144
Elapsed time for attention_prob_times_values (32x2048x2048x339): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x339): 58.227

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 202.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x340x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x340x2048): 61.594
Elapsed time for attention_prob_times_values (32x2048x2048x340): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x340): 61.239

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 224.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x341x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x341x2048): 58.491
Elapsed time for attention_prob_times_values (32x2048x2048x341): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x341): 58.154

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 213.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x342x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x342x2048): 61.393
Elapsed time for attention_prob_times_values (32x2048x2048x342): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x342): 60.649

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 224.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x343x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x343x2048): 60.511
Elapsed time for attention_prob_times_values (32x2048x2048x343): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x343): 58.601

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 219.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x344x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x344x2048): 62.329
Elapsed time for attention_prob_times_values (32x2048x2048x344): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x344): 72.178

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 246.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x345x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x345x2048): 54.349
Elapsed time for attention_prob_times_values (32x2048x2048x345): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x345): 56.557

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 204.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x346x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x346x2048): 60.765
Elapsed time for attention_prob_times_values (32x2048x2048x346): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x346): 60.746

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 224.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x347x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x347x2048): 60.431
Elapsed time for attention_prob_times_values (32x2048x2048x347): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x347): 59.186

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 221.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x348x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x348x2048): 61.790
Elapsed time for attention_prob_times_values (32x2048x2048x348): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x348): 60.983

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 228.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x349x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x349x2048): 60.714
Elapsed time for attention_prob_times_values (32x2048x2048x349): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x349): 54.880

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 214.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x350x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x350x2048): 61.352
Elapsed time for attention_prob_times_values (32x2048x2048x350): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x350): 61.573

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 229.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x351x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x351x2048): 60.845
Elapsed time for attention_prob_times_values (32x2048x2048x351): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x351): 59.841

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 225.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
num_attention_heads: 20, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x162x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x162x2048): 62.464
Elapsed time for attention_prob_times_values (80x2048x2048x162): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x162): 56.420

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 246.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x163x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x163x2048): 62.704
Elapsed time for attention_prob_times_values (80x2048x2048x163): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x163): 55.052

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 245.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x164x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x164x2048): 64.562
Elapsed time for attention_prob_times_values (80x2048x2048x164): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x164): 57.401

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 255.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x165x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x165x2048): 63.223
Elapsed time for attention_prob_times_values (80x2048x2048x165): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x165): 55.257

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 249.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x166x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x166x2048): 64.466
Elapsed time for attention_prob_times_values (80x2048x2048x166): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x166): 57.685

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 258.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x167x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x167x2048): 63.772
Elapsed time for attention_prob_times_values (80x2048x2048x167): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x167): 56.047

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 254.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x168x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x168x2048): 65.559
Elapsed time for attention_prob_times_values (80x2048x2048x168): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x168): 55.567

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 257.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x169x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x169x2048): 63.324
Elapsed time for attention_prob_times_values (80x2048x2048x169): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x169): 56.435

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 256.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x170x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x170x2048): 64.023
Elapsed time for attention_prob_times_values (80x2048x2048x170): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x170): 58.763

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 264.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x171x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x171x2048): 63.895
Elapsed time for attention_prob_times_values (80x2048x2048x171): 0.0020
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x352x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x352x2048): 75.503
Elapsed time for attention_prob_times_values (32x2048x2048x352): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x352): 73.528

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 279.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x353x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x353x2048): 63.176
Elapsed time for attention_prob_times_values (32x2048x2048x353): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x353): 60.438

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 232.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x354x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x354x2048): 64.109
Elapsed time for attention_prob_times_values (32x2048x2048x354): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x354): 62.364

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 238.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x355x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x355x2048): 63.163
Elapsed time for attention_prob_times_values (32x2048x2048x355): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x355): 60.624

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 233.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x356x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x356x2048): 64.711
Elapsed time for attention_prob_times_values (32x2048x2048x356): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x356): 62.933

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 241.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x357x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x357x2048): 62.843
Elapsed time for attention_prob_times_values (32x2048x2048x357): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x357): 60.846

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 234.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x358x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x358x2048): 63.717
Elapsed time for attention_prob_times_values (32x2048x2048x358): 0.0201
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x358): 4.773

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 33.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x359x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x359x2048): 62.725
Elapsed time for attention_prob_times_values (32x2048x2048x359): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x359): 60.837

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 235.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x360x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x360x2048): 64.501
Elapsed time for attention_prob_times_values (32x2048x2048x360): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x360): 74.045

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 262.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x86x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x86x2048): 46.276
Elapsed time for attention_prob_times_values (160x2048x2048x86): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x86): 41.819

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 191.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x87x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x87x2048): 47.216
Elapsed time for attention_prob_times_values (160x2048x2048x87): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x87): 38.643

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 186.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x88x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x88x2048): 48.270
Elapsed time for attention_prob_times_values (160x2048x2048x88): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x88): 42.323

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 200.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x89x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x89x2048): 48.334
Elapsed time for attention_prob_times_values (160x2048x2048x89): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x89): 42.940

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 203.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x90x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x90x2048): 44.482
Elapsed time for attention_prob_times_values (160x2048x2048x90): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x90): 44.845

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 201.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x91x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x91x2048): 49.013
Elapsed time for attention_prob_times_values (160x2048x2048x91): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x91): 43.830

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 210.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x92x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x92x2048): 49.934
Elapsed time for attention_prob_times_values (160x2048x2048x92): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x92): 45.910

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 219.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x93x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x93x2048): 49.838
Elapsed time for attention_prob_times_values (160x2048x2048x93): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x93): 43.712

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 215.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x94x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x94x2048): 50.537
Elapsed time for attention_prob_times_values (160x2048x2048x94): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x94): 46.607

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 226.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x95x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x95x2048): 6.331
Elapsed time for attention_prob_times_values (160x2048x2048x95): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x95): 45.418

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 52.355
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x171): 56.813

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 261.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x172x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x172x2048): 65.628
Elapsed time for attention_prob_times_values (80x2048x2048x172): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x172): 59.542

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 272.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x173x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x173x2048): 64.574
Elapsed time for attention_prob_times_values (80x2048x2048x173): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x173): 53.074

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 255.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x174x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x174x2048): 65.953
Elapsed time for attention_prob_times_values (80x2048x2048x174): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x174): 60.124

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 276.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x175x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x175x2048): 64.491
Elapsed time for attention_prob_times_values (80x2048x2048x175): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x175): 56.083

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 265.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x176x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x176x2048): 68.133
Elapsed time for attention_prob_times_values (80x2048x2048x176): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x176): 59.244

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 281.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x177x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x177x2048): 57.918
Elapsed time for attention_prob_times_values (80x2048x2048x177): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x177): 58.779

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 260.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x178x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x178x2048): 66.194
Elapsed time for attention_prob_times_values (80x2048x2048x178): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x178): 61.168

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 284.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x179x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x179x2048): 65.324
Elapsed time for attention_prob_times_values (80x2048x2048x179): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x179): 56.484

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 272.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x180x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x180x2048): 59.705
Elapsed time for attention_prob_times_values (80x2048x2048x180): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x180): 55.579

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 259.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (32x2048x361x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x361x2048): 61.752
Elapsed time for attention_prob_times_values (32x2048x2048x361): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x361): 60.233

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 232.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x362x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x362x2048): 62.665
Elapsed time for attention_prob_times_values (32x2048x2048x362): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x362): 63.426

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 241.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x363x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x363x2048): 62.037
Elapsed time for attention_prob_times_values (32x2048x2048x363): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x363): 61.101

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 236.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x364x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x364x2048): 59.104
Elapsed time for attention_prob_times_values (32x2048x2048x364): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x364): 56.310

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 221.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x365x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x365x2048): 62.417
Elapsed time for attention_prob_times_values (32x2048x2048x365): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x365): 61.529

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 238.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x366x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x366x2048): 62.144
Elapsed time for attention_prob_times_values (32x2048x2048x366): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x366): 64.147

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 243.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x367x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x367x2048): 62.541
Elapsed time for attention_prob_times_values (32x2048x2048x367): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x367): 62.024

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 240.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x368x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x368x2048): 61.565
Elapsed time for attention_prob_times_values (32x2048x2048x368): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x368): 75.074

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 262.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x369x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x369x2048): 61.944
Elapsed time for attention_prob_times_values (32x2048x2048x369): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x369): 54.372

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 224.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x370x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x370x2048): 61.964
Elapsed time for attention_prob_times_values (32x2048x2048x370): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x370): 64.617

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 246.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x371x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x371x2048): 61.131
Elapsed time for attention_prob_times_values (32x2048x2048x371): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x371): 62.663

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 241.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x372x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x372x2048): 54.650
Elapsed time for attention_prob_times_values (32x2048x2048x372): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x372): 65.026

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 231.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x373x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x373x2048): 61.969
Elapsed time for attention_prob_times_values (32x2048x2048x373): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x373): 62.878

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 244.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 2992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x374x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x374x2048): 62.755
Elapsed time for attention_prob_times_values (32x2048x2048x374): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x374): 65.214

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 250.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x375x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x375x2048): 62.439
Elapsed time for attention_prob_times_values (32x2048x2048x375): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x375): 62.559

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 245.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x376x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x376x2048): 64.132
Elapsed time for attention_prob_times_values (32x2048x2048x376): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x376): 72.242

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 267.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x377x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x377x2048): 54.537
Elapsed time for attention_prob_times_values (32x2048x2048x377): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x377): 63.183

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 230.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x378x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x378x2048): 62.477
Elapsed time for attention_prob_times_values (32x2048x2048x378): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x378): 65.799

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 253.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x379x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x379x2048): 61.915
Elapsed time for attention_prob_times_values (32x2048x2048x379): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x379): 63.537

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 248.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
========================================================================================================================
num_attention_heads: 20, hidden_size: 3620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x181x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x181x2048): 62.032
Elapsed time for attention_prob_times_values (80x2048x2048x181): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x181): 59.790

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 276.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x182x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x182x2048): 67.273
Elapsed time for attention_prob_times_values (80x2048x2048x182): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x182): 62.005

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 293.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x183x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x183x2048): 66.396
Elapsed time for attention_prob_times_values (80x2048x2048x183): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x183): 56.264

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 278.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x184x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x184x2048): 66.323
Elapsed time for attention_prob_times_values (80x2048x2048x184): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x184): 60.688

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 291.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x185x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x185x2048): 65.566
Elapsed time for attention_prob_times_values (80x2048x2048x185): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x185): 61.075

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 291.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x186x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x186x2048): 57.780
Elapsed time for attention_prob_times_values (80x2048x2048x186): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x186): 56.728

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 265.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x187x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x187x2048): 66.661
Elapsed time for attention_prob_times_values (80x2048x2048x187): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x187): 60.162

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 294.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x188x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x188x2048): 65.274
Elapsed time for attention_prob_times_values (80x2048x2048x188): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x188): 59.800

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 291.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x189x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x189x2048): 66.824
Elapsed time for attention_prob_times_values (80x2048x2048x189): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x189): 62.003

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 301.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x190x2048): 0.0019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x96x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x96x2048): 65.341
Elapsed time for attention_prob_times_values (160x2048x2048x96): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x96): 43.207

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 247.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x97x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x97x2048): 51.168
Elapsed time for attention_prob_times_values (160x2048x2048x97): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x97): 46.426

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 233.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x98x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x98x2048): 52.293
Elapsed time for attention_prob_times_values (160x2048x2048x98): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x98): 46.698

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 238.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x99x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x99x2048): 50.661
Elapsed time for attention_prob_times_values (160x2048x2048x99): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x99): 47.267

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 238.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x100x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x100x2048): 53.113
Elapsed time for attention_prob_times_values (160x2048x2048x100): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x100): 49.037

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 250.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x101x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x101x2048): 52.101
Elapsed time for attention_prob_times_values (160x2048x2048x101): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x101): 48.160

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 247.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x102x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x102x2048): 52.111
Elapsed time for attention_prob_times_values (160x2048x2048x102): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x102): 44.909

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 240.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x103x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x103x2048): 52.973
Elapsed time for attention_prob_times_values (160x2048x2048x103): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x103): 48.328

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 253.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x104x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x104x2048): 54.487
Elapsed time for attention_prob_times_values (160x2048x2048x104): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x104): 48.056

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 258.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x380x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x380x2048): 63.243
Elapsed time for attention_prob_times_values (32x2048x2048x380): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x380): 61.085

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 246.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x381x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x381x2048): 57.165
Elapsed time for attention_prob_times_values (32x2048x2048x381): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x381): 63.835

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 239.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x382x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x382x2048): 62.751
Elapsed time for attention_prob_times_values (32x2048x2048x382): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x382): 66.413

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 257.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x383x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x383x2048): 61.647
Elapsed time for attention_prob_times_values (32x2048x2048x383): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x383): 63.620

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 249.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x384x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x384x2048): 75.850
Elapsed time for attention_prob_times_values (32x2048x2048x384): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x384): 15.456

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 102.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x385x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x385x2048): 64.019
Elapsed time for attention_prob_times_values (32x2048x2048x385): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x385): 56.376

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 240.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x386x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x386x2048): 64.827
Elapsed time for attention_prob_times_values (32x2048x2048x386): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x386): 57.608

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 244.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x387x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x387x2048): 64.294
Elapsed time for attention_prob_times_values (32x2048x2048x387): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x387): 49.786

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 225.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x388x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x388x2048): 65.485
Elapsed time for attention_prob_times_values (32x2048x2048x388): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x388): 60.015

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 252.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x389x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x389x2048): 64.342
Elapsed time for attention_prob_times_values (32x2048x2048x389): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x389): 56.259

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 242.463
========================================================================================================================
num_attention_heads: 96, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x39x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x39x2048): 27.248
Elapsed time for attention_prob_times_values (384x2048x2048x39): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x39): 30.943

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 134.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x40x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x40x2048): 27.617
Elapsed time for attention_prob_times_values (384x2048x2048x40): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x40): 32.601

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 142.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x41x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x41x2048): 27.909
Elapsed time for attention_prob_times_values (384x2048x2048x41): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x41): 32.298

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 145.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x42x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x42x2048): 28.420
Elapsed time for attention_prob_times_values (384x2048x2048x42): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x42): 32.740

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 150.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x43x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x43x2048): 28.657
Elapsed time for attention_prob_times_values (384x2048x2048x43): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x43): 31.380

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 150.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x44x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x44x2048): 16.675
Elapsed time for attention_prob_times_values (384x2048x2048x44): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x44): 33.896

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 114.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x45x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x45x2048): 29.863
Elapsed time for attention_prob_times_values (384x2048x2048x45): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x45): 33.758

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 165.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x46x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x46x2048): 31.500
Elapsed time for attention_prob_times_values (384x2048x2048x46): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x46): 36.207

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 178.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x47x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x47x2048): 31.892
Elapsed time for attention_prob_times_values (384x2048x2048x47): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x47): 35.970

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 182.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x48x2048): 0.0048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x390x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x390x2048): 62.231
Elapsed time for attention_prob_times_values (32x2048x2048x390): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x390): 60.027

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 247.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x391x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x391x2048): 63.315
Elapsed time for attention_prob_times_values (32x2048x2048x391): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x391): 55.118

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 238.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x392x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x392x2048): 65.541
Elapsed time for attention_prob_times_values (32x2048x2048x392): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x392): 70.134

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 275.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x393x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x393x2048): 63.238
Elapsed time for attention_prob_times_values (32x2048x2048x393): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x393): 57.495

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 245.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x394x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x394x2048): 64.021
Elapsed time for attention_prob_times_values (32x2048x2048x394): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x394): 56.445

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 244.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x395x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x395x2048): 63.587
Elapsed time for attention_prob_times_values (32x2048x2048x395): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x395): 57.780

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 247.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x396x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x396x2048): 64.579
Elapsed time for attention_prob_times_values (32x2048x2048x396): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x396): 60.447

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 255.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x397x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x397x2048): 62.046
Elapsed time for attention_prob_times_values (32x2048x2048x397): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x397): 57.817

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 245.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x398x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x398x2048): 64.476
Elapsed time for attention_prob_times_values (32x2048x2048x398): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x398): 60.959

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 257.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x190x2048): 68.293
Elapsed time for attention_prob_times_values (80x2048x2048x190): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x190): 64.310

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 312.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x191x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x191x2048): 67.676
Elapsed time for attention_prob_times_values (80x2048x2048x191): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x191): 22.297

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 158.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x192x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x192x2048): 77.782
Elapsed time for attention_prob_times_values (80x2048x2048x192): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x192): 61.100

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 325.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x193x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x193x2048): 64.123
Elapsed time for attention_prob_times_values (80x2048x2048x193): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x193): 51.052

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 271.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x194x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x194x2048): 67.699
Elapsed time for attention_prob_times_values (80x2048x2048x194): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x194): 53.441

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 286.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x195x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x195x2048): 66.627
Elapsed time for attention_prob_times_values (80x2048x2048x195): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x195): 51.077

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 278.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x196x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x196x2048): 68.279
Elapsed time for attention_prob_times_values (80x2048x2048x196): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x196): 47.605

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 270.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x197x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x197x2048): 66.503
Elapsed time for attention_prob_times_values (80x2048x2048x197): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x197): 52.130

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 283.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x198x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x198x2048): 68.576
Elapsed time for attention_prob_times_values (80x2048x2048x198): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x198): 53.894

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 293.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x199x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x199x2048): 67.657
Elapsed time for attention_prob_times_values (80x2048x2048x199): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x199): 51.874

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 286.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
num_attention_heads: 8, hidden_size: 3192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x399x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x399x2048): 63.848
Elapsed time for attention_prob_times_values (32x2048x2048x399): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x399): 58.312

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 250.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x400x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x400x2048): 64.828
Elapsed time for attention_prob_times_values (32x2048x2048x400): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x400): 71.001

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 279.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x401x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x401x2048): 63.030
Elapsed time for attention_prob_times_values (32x2048x2048x401): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x401): 58.619

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 251.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x402x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x402x2048): 63.928
Elapsed time for attention_prob_times_values (32x2048x2048x402): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x402): 61.454

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 259.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x403x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x403x2048): 63.611
Elapsed time for attention_prob_times_values (32x2048x2048x403): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x403): 58.577

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 253.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x404x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x404x2048): 64.611
Elapsed time for attention_prob_times_values (32x2048x2048x404): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x404): 61.827

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 262.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x405x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x405x2048): 63.873
Elapsed time for attention_prob_times_values (32x2048x2048x405): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x405): 58.603

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 254.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x406x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x406x2048): 63.727
Elapsed time for attention_prob_times_values (32x2048x2048x406): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x406): 61.923

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 262.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x407x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x407x2048): 64.144
Elapsed time for attention_prob_times_values (32x2048x2048x407): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x407): 59.235

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 257.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x408x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x408x2048): 64.937
Elapsed time for attention_prob_times_values (32x2048x2048x408): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x408): 72.950

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 287.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x409x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x409x2048): 63.543
Elapsed time for attention_prob_times_values (32x2048x2048x409): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x409): 59.060

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 256.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x410x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x410x2048): 64.149
Elapsed time for attention_prob_times_values (32x2048x2048x410): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x410): 56.350

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 252.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x411x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x411x2048): 58.687
Elapsed time for attention_prob_times_values (32x2048x2048x411): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x411): 59.402

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 248.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x412x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x412x2048): 64.895
Elapsed time for attention_prob_times_values (32x2048x2048x412): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x412): 62.966

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 269.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x413x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x413x2048): 63.076
Elapsed time for attention_prob_times_values (32x2048x2048x413): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x413): 59.447

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 258.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x414x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x414x2048): 64.453
Elapsed time for attention_prob_times_values (32x2048x2048x414): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x414): 62.735

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 269.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x415x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x415x2048): 63.928
Elapsed time for attention_prob_times_values (32x2048x2048x415): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x415): 60.073

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 262.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x416x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x416x2048): 80.532
Elapsed time for attention_prob_times_values (32x2048x2048x416): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x416): 74.051

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 327.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x417x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x417x2048): 65.010
Elapsed time for attention_prob_times_values (32x2048x2048x417): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x417): 60.714

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 267.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 40, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x105x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x105x2048): 52.631
Elapsed time for attention_prob_times_values (160x2048x2048x105): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x105): 48.795

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 258.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x106x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x106x2048): 53.745
Elapsed time for attention_prob_times_values (160x2048x2048x106): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x106): 51.349

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 269.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x107x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x107x2048): 53.339
Elapsed time for attention_prob_times_values (160x2048x2048x107): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x107): 48.235

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 262.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x108x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x108x2048): 52.896
Elapsed time for attention_prob_times_values (160x2048x2048x108): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x108): 52.165

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 274.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x109x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x109x2048): 51.173
Elapsed time for attention_prob_times_values (160x2048x2048x109): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x109): 50.524

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 267.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x110x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x110x2048): 52.226
Elapsed time for attention_prob_times_values (160x2048x2048x110): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x110): 52.765

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 278.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x111x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x111x2048): 54.886
Elapsed time for attention_prob_times_values (160x2048x2048x111): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x111): 51.144

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 282.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x112x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x112x2048): 56.751
Elapsed time for attention_prob_times_values (160x2048x2048x112): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x112): 53.437

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 295.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x113x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x113x2048): 53.184
Elapsed time for attention_prob_times_values (160x2048x2048x113): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x113): 52.155

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 285.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x114x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x114x2048): 54.880
Elapsed time for attention_prob_times_values (160x2048x2048x114): 0.0028
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x200x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x200x2048): 64.176
Elapsed time for attention_prob_times_values (80x2048x2048x200): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x200): 51.373

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 279.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x201x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x201x2048): 67.337
Elapsed time for attention_prob_times_values (80x2048x2048x201): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x201): 40.891

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 250.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x202x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x202x2048): 68.702
Elapsed time for attention_prob_times_values (80x2048x2048x202): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x202): 48.982

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 282.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x203x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x203x2048): 62.694
Elapsed time for attention_prob_times_values (80x2048x2048x203): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x203): 51.361

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 280.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x204x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x204x2048): 69.240
Elapsed time for attention_prob_times_values (80x2048x2048x204): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x204): 55.250

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 306.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x205x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x205x2048): 67.949
Elapsed time for attention_prob_times_values (80x2048x2048x205): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x205): 52.658

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 296.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x206x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x206x2048): 69.400
Elapsed time for attention_prob_times_values (80x2048x2048x206): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x206): 51.624

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 297.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x207x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x207x2048): 68.709
Elapsed time for attention_prob_times_values (80x2048x2048x207): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x207): 52.978

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 301.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x208x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x208x2048): 71.669
Elapsed time for attention_prob_times_values (80x2048x2048x208): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x208): 52.457

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 306.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 8, hidden_size: 3344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x418x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x418x2048): 67.794
Elapsed time for attention_prob_times_values (32x2048x2048x418): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x418): 63.299

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 279.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x419x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x419x2048): 66.820
Elapsed time for attention_prob_times_values (32x2048x2048x419): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x419): 61.021

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 272.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x420x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x420x2048): 63.457
Elapsed time for attention_prob_times_values (32x2048x2048x420): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x420): 63.653

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 272.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x421x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x421x2048): 66.874
Elapsed time for attention_prob_times_values (32x2048x2048x421): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x421): 61.384

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 274.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x422x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x422x2048): 67.014
Elapsed time for attention_prob_times_values (32x2048x2048x422): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x422): 59.088

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 269.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x423x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x423x2048): 55.430
Elapsed time for attention_prob_times_values (32x2048x2048x423): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x423): 61.564

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 251.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x424x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x424x2048): 65.368
Elapsed time for attention_prob_times_values (32x2048x2048x424): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x424): 75.428

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 302.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x425x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x425x2048): 65.621
Elapsed time for attention_prob_times_values (32x2048x2048x425): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x425): 61.489

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 274.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x426x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x426x2048): 66.275
Elapsed time for attention_prob_times_values (32x2048x2048x426): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x426): 64.372

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 282.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x427x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x427x2048): 65.939
Elapsed time for attention_prob_times_values (32x2048x2048x427): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x427): 61.818

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 276.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x428x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x428x2048): 66.966
Elapsed time for attention_prob_times_values (32x2048x2048x428): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x428): 60.929

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 277.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x429x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x429x2048): 66.193
Elapsed time for attention_prob_times_values (32x2048x2048x429): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x429): 59.845

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 273.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x430x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x430x2048): 66.971
Elapsed time for attention_prob_times_values (32x2048x2048x430): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x430): 65.133

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 287.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x431x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x431x2048): 63.608
Elapsed time for attention_prob_times_values (32x2048x2048x431): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x431): 54.653

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 256.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x432x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x432x2048): 67.780
Elapsed time for attention_prob_times_values (32x2048x2048x432): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x432): 68.265

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 297.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x433x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x433x2048): 65.482
Elapsed time for attention_prob_times_values (32x2048x2048x433): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x433): 62.824

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 281.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x434x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x434x2048): 66.328
Elapsed time for attention_prob_times_values (32x2048x2048x434): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x434): 59.036

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 274.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x435x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x435x2048): 65.337
Elapsed time for attention_prob_times_values (32x2048x2048x435): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x435): 59.969

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 275.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x436x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x436x2048): 67.356
Elapsed time for attention_prob_times_values (32x2048x2048x436): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x436): 66.310

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 294.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Elapsed time for attention_key_query_prob (80x2048x209x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x209x2048): 68.365
Elapsed time for attention_prob_times_values (80x2048x2048x209): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x209): 53.622

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 305.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x210x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x210x2048): 69.582
Elapsed time for attention_prob_times_values (80x2048x2048x210): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x210): 56.140

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 317.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x211x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x211x2048): 68.756
Elapsed time for attention_prob_times_values (80x2048x2048x211): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x211): 53.972

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 309.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x212x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x212x2048): 70.063
Elapsed time for attention_prob_times_values (80x2048x2048x212): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x212): 56.793

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 322.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x213x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x213x2048): 68.692
Elapsed time for attention_prob_times_values (80x2048x2048x213): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x213): 50.675

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 300.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x214x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x214x2048): 70.410
Elapsed time for attention_prob_times_values (80x2048x2048x214): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x214): 57.009

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 326.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x215x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x215x2048): 69.488
Elapsed time for attention_prob_times_values (80x2048x2048x215): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x215): 54.762

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 318.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x216x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x216x2048): 70.198
Elapsed time for attention_prob_times_values (80x2048x2048x216): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x216): 53.071

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 315.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x217x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x217x2048): 67.447
Elapsed time for attention_prob_times_values (80x2048x2048x217): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x217): 55.111

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 317.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x218x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x218x2048): 70.514
Elapsed time for attention_prob_times_values (80x2048x2048x218): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x218): 54.288

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 322.547
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x437x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x437x2048): 63.741
Elapsed time for attention_prob_times_values (32x2048x2048x437): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x437): 63.185

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 280.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x438x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x438x2048): 59.935
Elapsed time for attention_prob_times_values (32x2048x2048x438): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x438): 66.141

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 278.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x439x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x439x2048): 66.256
Elapsed time for attention_prob_times_values (32x2048x2048x439): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x439): 63.604

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 287.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x440x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x440x2048): 67.928
Elapsed time for attention_prob_times_values (32x2048x2048x440): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x440): 77.600

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 321.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x441x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x441x2048): 61.557
Elapsed time for attention_prob_times_values (32x2048x2048x441): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x441): 63.681

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 278.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x442x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x442x2048): 42.541
Elapsed time for attention_prob_times_values (32x2048x2048x442): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x442): 66.745

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 231.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x443x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x443x2048): 65.845
Elapsed time for attention_prob_times_values (32x2048x2048x443): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x443): 64.110

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 289.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x444x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x444x2048): 65.466
Elapsed time for attention_prob_times_values (32x2048x2048x444): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x444): 66.948

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 295.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x445x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x445x2048): 66.029
Elapsed time for attention_prob_times_values (32x2048x2048x445): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x445): 64.424

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 291.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x446x2048): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x114): 54.322

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 297.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x115x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x115x2048): 54.189
Elapsed time for attention_prob_times_values (160x2048x2048x115): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x115): 51.327

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 289.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x116x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x116x2048): 55.804
Elapsed time for attention_prob_times_values (160x2048x2048x116): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x116): 55.010

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 306.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x117x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x117x2048): 55.941
Elapsed time for attention_prob_times_values (160x2048x2048x117): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x117): 52.051

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 300.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x118x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x118x2048): 55.906
Elapsed time for attention_prob_times_values (160x2048x2048x118): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x118): 55.756

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 313.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x119x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x119x2048): 54.983
Elapsed time for attention_prob_times_values (160x2048x2048x119): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x119): 53.882

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 307.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x120x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x120x2048): 56.010
Elapsed time for attention_prob_times_values (160x2048x2048x120): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x120): 55.761

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 317.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x121x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x121x2048): 52.021
Elapsed time for attention_prob_times_values (160x2048x2048x121): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x121): 42.919

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 269.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x122x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x122x2048): 55.173
Elapsed time for attention_prob_times_values (160x2048x2048x122): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x122): 56.746

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 322.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x123x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x123x2048): 55.864
Elapsed time for attention_prob_times_values (160x2048x2048x123): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x123): 43.615

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 284.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x219x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x219x2048): 69.582
Elapsed time for attention_prob_times_values (80x2048x2048x219): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x219): 55.589

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 326.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x220x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x220x2048): 70.844
Elapsed time for attention_prob_times_values (80x2048x2048x220): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x220): 58.265

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 338.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x221x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x221x2048): 69.871
Elapsed time for attention_prob_times_values (80x2048x2048x221): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x221): 54.295

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 324.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x222x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x222x2048): 71.284
Elapsed time for attention_prob_times_values (80x2048x2048x222): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x222): 58.734

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 343.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x223x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x223x2048): 70.519
Elapsed time for attention_prob_times_values (80x2048x2048x223): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x223): 54.088

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 327.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x224x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x224x2048): 80.452
Elapsed time for attention_prob_times_values (80x2048x2048x224): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x224): 55.066

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 351.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x225x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x225x2048): 63.843
Elapsed time for attention_prob_times_values (80x2048x2048x225): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x225): 57.324

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 325.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x226x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x226x2048): 70.674
Elapsed time for attention_prob_times_values (80x2048x2048x226): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x226): 57.088

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 341.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x227x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x227x2048): 63.888
Elapsed time for attention_prob_times_values (80x2048x2048x227): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x227): 57.542

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 329.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x446x2048): 66.466
Elapsed time for attention_prob_times_values (32x2048x2048x446): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x446): 67.394

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 300.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x447x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x447x2048): 65.995
Elapsed time for attention_prob_times_values (32x2048x2048x447): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x447): 60.592

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 283.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x448x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x448x2048): 81.751
Elapsed time for attention_prob_times_values (32x2048x2048x448): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x448): 80.051

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 364.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x449x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x449x2048): 68.094
Elapsed time for attention_prob_times_values (32x2048x2048x449): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x449): 58.981

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 284.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x450x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x450x2048): 68.560
Elapsed time for attention_prob_times_values (32x2048x2048x450): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x450): 61.569

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 292.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x451x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x451x2048): 68.342
Elapsed time for attention_prob_times_values (32x2048x2048x451): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x451): 59.182

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 286.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x452x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x452x2048): 70.155
Elapsed time for attention_prob_times_values (32x2048x2048x452): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x452): 61.514

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 297.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x453x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x453x2048): 62.905
Elapsed time for attention_prob_times_values (32x2048x2048x453): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x453): 48.966

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 249.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x454x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x454x2048): 68.992
Elapsed time for attention_prob_times_values (32x2048x2048x454): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x454): 61.809

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 296.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x455x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x455x2048): 64.887
Elapsed time for attention_prob_times_values (32x2048x2048x455): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x455): 59.499

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 282.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x48x2048): 31.975
Elapsed time for attention_prob_times_values (384x2048x2048x48): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x48): 38.378

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 191.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x49x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x49x2048): 32.738
Elapsed time for attention_prob_times_values (384x2048x2048x49): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x49): 37.716

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 196.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x50x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x50x2048): 29.194
Elapsed time for attention_prob_times_values (384x2048x2048x50): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x50): 38.640

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 189.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x51x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x51x2048): 33.843
Elapsed time for attention_prob_times_values (384x2048x2048x51): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x51): 36.576

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 203.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x52x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x52x2048): 35.331
Elapsed time for attention_prob_times_values (384x2048x2048x52): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x52): 39.548

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 219.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x53x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x53x2048): 35.203
Elapsed time for attention_prob_times_values (384x2048x2048x53): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x53): 40.208

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 224.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x54x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x54x2048): 36.747
Elapsed time for attention_prob_times_values (384x2048x2048x54): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x54): 41.576

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 236.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x55x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x55x2048): 36.964
Elapsed time for attention_prob_times_values (384x2048x2048x55): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x55): 41.405

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 240.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x56x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x56x2048): 38.116
Elapsed time for attention_prob_times_values (384x2048x2048x56): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x56): 44.767

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 257.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x57x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x57x2048): 37.477
Elapsed time for attention_prob_times_values (384x2048x2048x57): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x57): 43.301

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 254.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x456x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x456x2048): 70.061
Elapsed time for attention_prob_times_values (32x2048x2048x456): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x456): 79.549

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 339.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x457x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x457x2048): 66.901
Elapsed time for attention_prob_times_values (32x2048x2048x457): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x457): 59.258

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 287.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x458x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x458x2048): 67.654
Elapsed time for attention_prob_times_values (32x2048x2048x458): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x458): 60.666

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 292.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x459x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x459x2048): 57.351
Elapsed time for attention_prob_times_values (32x2048x2048x459): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x459): 54.896

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 257.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x460x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x460x2048): 63.014
Elapsed time for attention_prob_times_values (32x2048x2048x460): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x460): 62.961

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 289.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x461x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x461x2048): 61.754
Elapsed time for attention_prob_times_values (32x2048x2048x461): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x461): 59.790

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 279.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x462x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x462x2048): 65.290
Elapsed time for attention_prob_times_values (32x2048x2048x462): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x462): 62.845

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 295.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x463x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x463x2048): 67.534
Elapsed time for attention_prob_times_values (32x2048x2048x463): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x463): 60.126

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 293.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x464x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x464x2048): 69.194
Elapsed time for attention_prob_times_values (32x2048x2048x464): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x464): 81.589

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 346.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 20, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x228x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x228x2048): 71.117
Elapsed time for attention_prob_times_values (80x2048x2048x228): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x228): 57.398

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 346.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x229x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x229x2048): 69.797
Elapsed time for attention_prob_times_values (80x2048x2048x229): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x229): 58.081

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 346.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x230x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x230x2048): 69.344
Elapsed time for attention_prob_times_values (80x2048x2048x230): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x230): 60.245

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 354.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x231x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x231x2048): 70.179
Elapsed time for attention_prob_times_values (80x2048x2048x231): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x231): 55.201

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 340.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x232x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x232x2048): 71.942
Elapsed time for attention_prob_times_values (80x2048x2048x232): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x232): 58.688

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 357.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x233x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x233x2048): 69.651
Elapsed time for attention_prob_times_values (80x2048x2048x233): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x233): 54.130

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 338.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x234x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x234x2048): 70.467
Elapsed time for attention_prob_times_values (80x2048x2048x234): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x234): 61.031

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 364.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x235x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x235x2048): 68.228
Elapsed time for attention_prob_times_values (80x2048x2048x235): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x235): 58.766

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 352.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x236x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x236x2048): 69.660
Elapsed time for attention_prob_times_values (80x2048x2048x236): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x236): 60.241

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 362.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x237x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x237x2048): 70.191
Elapsed time for attention_prob_times_values (80x2048x2048x237): 0.0027
Elapsed time for attention_key_query_prob (32x2048x465x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x465x2048): 66.710
Elapsed time for attention_prob_times_values (32x2048x2048x465): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x465): 60.274

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 293.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x466x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x466x2048): 67.595
Elapsed time for attention_prob_times_values (32x2048x2048x466): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x466): 63.412

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 303.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x467x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x467x2048): 66.615
Elapsed time for attention_prob_times_values (32x2048x2048x467): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x467): 60.589

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 294.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x468x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x468x2048): 68.405
Elapsed time for attention_prob_times_values (32x2048x2048x468): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x468): 63.408

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 306.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x469x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x469x2048): 67.096
Elapsed time for attention_prob_times_values (32x2048x2048x469): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x469): 60.556

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 296.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x470x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x470x2048): 67.764
Elapsed time for attention_prob_times_values (32x2048x2048x470): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x470): 63.549

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 306.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x471x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x471x2048): 67.092
Elapsed time for attention_prob_times_values (32x2048x2048x471): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x471): 60.947

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 298.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x472x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x472x2048): 69.295
Elapsed time for attention_prob_times_values (32x2048x2048x472): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x472): 81.783

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 351.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x473x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x473x2048): 66.619
Elapsed time for attention_prob_times_values (32x2048x2048x473): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x473): 61.227

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 299.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x474x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x474x2048): 67.214
Elapsed time for attention_prob_times_values (32x2048x2048x474): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x474): 61.455

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 301.967
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x124x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x124x2048): 54.849
Elapsed time for attention_prob_times_values (160x2048x2048x124): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x124): 57.543

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 328.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x125x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x125x2048): 52.703
Elapsed time for attention_prob_times_values (160x2048x2048x125): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x125): 43.360

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 279.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x126x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x126x2048): 57.351
Elapsed time for attention_prob_times_values (160x2048x2048x126): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x126): 52.753

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 325.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x127x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x127x2048): 54.476
Elapsed time for attention_prob_times_values (160x2048x2048x127): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x127): 43.291

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 287.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x128x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x128x2048): 62.812
Elapsed time for attention_prob_times_values (160x2048x2048x128): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x128): 61.783

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 373.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x129x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x129x2048): 56.615
Elapsed time for attention_prob_times_values (160x2048x2048x129): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x129): 41.182

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 287.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x130x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x130x2048): 55.581
Elapsed time for attention_prob_times_values (160x2048x2048x130): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x130): 46.537

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 307.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x131x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x131x2048): 55.237
Elapsed time for attention_prob_times_values (160x2048x2048x131): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x131): 44.989

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 303.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x132x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x132x2048): 58.567
Elapsed time for attention_prob_times_values (160x2048x2048x132): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x132): 47.316

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 322.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x133x2048): 0.0031
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x475x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x475x2048): 66.867
Elapsed time for attention_prob_times_values (32x2048x2048x475): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x475): 61.309

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 301.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x476x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x476x2048): 67.659
Elapsed time for attention_prob_times_values (32x2048x2048x476): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x476): 64.515

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 311.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x477x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x477x2048): 66.690
Elapsed time for attention_prob_times_values (32x2048x2048x477): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x477): 61.329

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 302.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x478x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x478x2048): 67.611
Elapsed time for attention_prob_times_values (32x2048x2048x478): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x478): 63.909

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 311.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x479x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x479x2048): 67.111
Elapsed time for attention_prob_times_values (32x2048x2048x479): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x479): 61.396

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 304.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x480x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x480x2048): 82.370
Elapsed time for attention_prob_times_values (32x2048x2048x480): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x480): 84.920

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 397.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x481x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x481x2048): 69.209
Elapsed time for attention_prob_times_values (32x2048x2048x481): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x481): 60.754

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 307.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x482x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x482x2048): 64.713
Elapsed time for attention_prob_times_values (32x2048x2048x482): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x482): 64.513

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 307.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x483x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x483x2048): 68.924
Elapsed time for attention_prob_times_values (32x2048x2048x483): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x483): 54.914

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 291.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x237): 59.061

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 361.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x238x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x238x2048): 71.779
Elapsed time for attention_prob_times_values (80x2048x2048x238): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x238): 59.426

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 367.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x239x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x239x2048): 70.627
Elapsed time for attention_prob_times_values (80x2048x2048x239): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x239): 59.538

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 366.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x240x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x240x2048): 55.955
Elapsed time for attention_prob_times_values (80x2048x2048x240): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x240): 61.823

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 334.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x241x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x241x2048): 53.511
Elapsed time for attention_prob_times_values (80x2048x2048x241): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x241): 58.494

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 318.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x242x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x242x2048): 54.268
Elapsed time for attention_prob_times_values (80x2048x2048x242): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x242): 60.737

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 328.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x243x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x243x2048): 53.989
Elapsed time for attention_prob_times_values (80x2048x2048x243): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x243): 59.508

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 325.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x244x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x244x2048): 55.032
Elapsed time for attention_prob_times_values (80x2048x2048x244): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x244): 62.266

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 336.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x245x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x245x2048): 52.743
Elapsed time for attention_prob_times_values (80x2048x2048x245): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x245): 60.834

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 326.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x246x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x246x2048): 54.937
Elapsed time for attention_prob_times_values (80x2048x2048x246): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x246): 63.409

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 341.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x484x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x484x2048): 70.606
Elapsed time for attention_prob_times_values (32x2048x2048x484): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x484): 61.436

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 314.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x485x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x485x2048): 62.208
Elapsed time for attention_prob_times_values (32x2048x2048x485): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x485): 60.445

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 293.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x486x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x486x2048): 59.258
Elapsed time for attention_prob_times_values (32x2048x2048x486): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x486): 56.604

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 277.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x487x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x487x2048): 60.083
Elapsed time for attention_prob_times_values (32x2048x2048x487): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x487): 62.959

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 295.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x488x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x488x2048): 70.378
Elapsed time for attention_prob_times_values (32x2048x2048x488): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x488): 78.886

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 357.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x489x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x489x2048): 62.119
Elapsed time for attention_prob_times_values (32x2048x2048x489): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x489): 55.517

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 282.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x490x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x490x2048): 68.508
Elapsed time for attention_prob_times_values (32x2048x2048x490): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x490): 60.466

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 310.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x491x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x491x2048): 67.930
Elapsed time for attention_prob_times_values (32x2048x2048x491): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x491): 59.220

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 306.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x492x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x492x2048): 69.234
Elapsed time for attention_prob_times_values (32x2048x2048x492): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x492): 64.517

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 323.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x493x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x493x2048): 68.098
Elapsed time for attention_prob_times_values (32x2048x2048x493): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x493): 62.776

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 316.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x494x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x494x2048): 68.734
Elapsed time for attention_prob_times_values (32x2048x2048x494): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x494): 60.839

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 313.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x495x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x495x2048): 68.124
Elapsed time for attention_prob_times_values (32x2048x2048x495): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x495): 63.102

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 318.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x496x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x496x2048): 65.331
Elapsed time for attention_prob_times_values (32x2048x2048x496): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x496): 86.500

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 362.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x497x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x497x2048): 67.647
Elapsed time for attention_prob_times_values (32x2048x2048x497): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x497): 63.107

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 318.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x498x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x498x2048): 68.261
Elapsed time for attention_prob_times_values (32x2048x2048x498): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x498): 65.748

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 327.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 3992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x499x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x499x2048): 67.894
Elapsed time for attention_prob_times_values (32x2048x2048x499): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x499): 63.233

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 320.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x500x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x500x2048): 63.639
Elapsed time for attention_prob_times_values (32x2048x2048x500): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x500): 62.731

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 309.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x501x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x501x2048): 67.896
Elapsed time for attention_prob_times_values (32x2048x2048x501): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x501): 63.809

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 323.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x502x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x502x2048): 64.104
Elapsed time for attention_prob_times_values (32x2048x2048x502): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x502): 66.494

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 321.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x133x2048): 57.741
Elapsed time for attention_prob_times_values (160x2048x2048x133): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x133): 43.142

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 305.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x134x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x134x2048): 59.132
Elapsed time for attention_prob_times_values (160x2048x2048x134): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x134): 46.373

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 324.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x135x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x135x2048): 58.501
Elapsed time for attention_prob_times_values (160x2048x2048x135): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x135): 45.202

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 319.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x136x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x136x2048): 54.737
Elapsed time for attention_prob_times_values (160x2048x2048x136): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x136): 45.388

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 313.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x137x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x137x2048): 56.885
Elapsed time for attention_prob_times_values (160x2048x2048x137): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x137): 46.530

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 325.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x138x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x138x2048): 58.539
Elapsed time for attention_prob_times_values (160x2048x2048x138): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x138): 48.906

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 340.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x139x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x139x2048): 56.963
Elapsed time for attention_prob_times_values (160x2048x2048x139): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x139): 47.144

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 331.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x140x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x140x2048): 58.156
Elapsed time for attention_prob_times_values (160x2048x2048x140): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x140): 49.286

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 345.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x141x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x141x2048): 58.615
Elapsed time for attention_prob_times_values (160x2048x2048x141): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x141): 47.437

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 341.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x142x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x142x2048): 59.009
Elapsed time for attention_prob_times_values (160x2048x2048x142): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x142): 50.140

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 354.934
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x247x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x247x2048): 54.576
Elapsed time for attention_prob_times_values (80x2048x2048x247): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x247): 61.024

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 335.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x248x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x248x2048): 54.345
Elapsed time for attention_prob_times_values (80x2048x2048x248): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x248): 62.121

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 338.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x249x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x249x2048): 53.888
Elapsed time for attention_prob_times_values (80x2048x2048x249): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x249): 63.034

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 340.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x250x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x250x2048): 54.364
Elapsed time for attention_prob_times_values (80x2048x2048x250): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x250): 60.878

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 337.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x251x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x251x2048): 51.800
Elapsed time for attention_prob_times_values (80x2048x2048x251): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x251): 62.478

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 334.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x252x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x252x2048): 55.088
Elapsed time for attention_prob_times_values (80x2048x2048x252): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x252): 64.654

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 352.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x253x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x253x2048): 54.085
Elapsed time for attention_prob_times_values (80x2048x2048x253): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x253): 61.731

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 342.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x254x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x254x2048): 52.908
Elapsed time for attention_prob_times_values (80x2048x2048x254): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x254): 65.042

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 347.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x255x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x255x2048): 53.931
Elapsed time for attention_prob_times_values (80x2048x2048x255): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x255): 65.675

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 354.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x256x2048): 0.0022
num_attention_heads: 8, hidden_size: 4024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x503x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x503x2048): 64.627
Elapsed time for attention_prob_times_values (32x2048x2048x503): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x503): 63.461

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 315.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x504x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x504x2048): 69.643
Elapsed time for attention_prob_times_values (32x2048x2048x504): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x504): 88.169

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 384.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x505x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x505x2048): 65.953
Elapsed time for attention_prob_times_values (32x2048x2048x505): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x505): 59.518

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 309.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x506x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x506x2048): 68.163
Elapsed time for attention_prob_times_values (32x2048x2048x506): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x506): 63.962

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 326.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x507x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x507x2048): 67.626
Elapsed time for attention_prob_times_values (32x2048x2048x507): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x507): 60.469

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 316.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x508x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x508x2048): 68.836
Elapsed time for attention_prob_times_values (32x2048x2048x508): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x508): 65.697

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 334.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x509x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x509x2048): 66.229
Elapsed time for attention_prob_times_values (32x2048x2048x509): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x509): 59.808

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 312.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x510x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x510x2048): 66.513
Elapsed time for attention_prob_times_values (32x2048x2048x510): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x510): 66.107

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 330.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x511x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x511x2048): 67.663
Elapsed time for attention_prob_times_values (32x2048x2048x511): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x511): 61.356

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 321.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x512x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x512x2048): 80.020
Elapsed time for attention_prob_times_values (32x2048x2048x512): 0.0015
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x58x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x58x2048): 37.887
Elapsed time for attention_prob_times_values (384x2048x2048x58): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x58): 44.636

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 263.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x59x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x59x2048): 38.179
Elapsed time for attention_prob_times_values (384x2048x2048x59): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x59): 43.215

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 264.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x60x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x60x2048): 37.708
Elapsed time for attention_prob_times_values (384x2048x2048x60): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x60): 46.164

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 275.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x61x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x61x2048): 39.734
Elapsed time for attention_prob_times_values (384x2048x2048x61): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x61): 45.672

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 285.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x62x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x62x2048): 40.288
Elapsed time for attention_prob_times_values (384x2048x2048x62): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x62): 46.429

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 293.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x63x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x63x2048): 40.828
Elapsed time for attention_prob_times_values (384x2048x2048x63): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x63): 47.063

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 301.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x64x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x64x2048): 54.579
Elapsed time for attention_prob_times_values (384x2048x2048x64): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x64): 51.020

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 369.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x65x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x65x2048): 41.314
Elapsed time for attention_prob_times_values (384x2048x2048x65): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x65): 32.771

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 259.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x66x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x66x2048): 43.944
Elapsed time for attention_prob_times_values (384x2048x2048x66): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x66): 32.089

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 266.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x512): 88.946

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 421.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x513x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x513x2048): 69.547
Elapsed time for attention_prob_times_values (32x2048x2048x513): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x513): 58.704

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 318.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x514x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x514x2048): 72.466
Elapsed time for attention_prob_times_values (32x2048x2048x514): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x514): 62.543

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 336.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x515x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x515x2048): 69.028
Elapsed time for attention_prob_times_values (32x2048x2048x515): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x515): 58.558

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 318.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x516x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x516x2048): 70.849
Elapsed time for attention_prob_times_values (32x2048x2048x516): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x516): 61.811

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 332.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x517x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x517x2048): 69.636
Elapsed time for attention_prob_times_values (32x2048x2048x517): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x517): 59.373

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 322.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x518x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x518x2048): 70.386
Elapsed time for attention_prob_times_values (32x2048x2048x518): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x518): 57.044

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 318.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x519x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x519x2048): 69.399
Elapsed time for attention_prob_times_values (32x2048x2048x519): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x519): 59.828

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 324.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x520x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x520x2048): 69.368
Elapsed time for attention_prob_times_values (32x2048x2048x520): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x520): 73.533

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 361.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x521x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x521x2048): 68.264
Elapsed time for attention_prob_times_values (32x2048x2048x521): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x521): 59.498

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 322.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x256x2048): 77.125
Elapsed time for attention_prob_times_values (80x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x256): 68.646

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 435.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x257x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x257x2048): 55.568
Elapsed time for attention_prob_times_values (80x2048x2048x257): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x257): 52.129

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 323.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x258x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x258x2048): 54.365
Elapsed time for attention_prob_times_values (80x2048x2048x258): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x258): 57.056

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 336.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x259x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x259x2048): 56.990
Elapsed time for attention_prob_times_values (80x2048x2048x259): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x259): 54.478

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 337.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x260x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x260x2048): 58.101
Elapsed time for attention_prob_times_values (80x2048x2048x260): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x260): 58.031

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 352.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x261x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x261x2048): 56.668
Elapsed time for attention_prob_times_values (80x2048x2048x261): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x261): 55.117

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 340.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x262x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x262x2048): 57.556
Elapsed time for attention_prob_times_values (80x2048x2048x262): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x262): 56.218

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 347.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x263x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x263x2048): 56.376
Elapsed time for attention_prob_times_values (80x2048x2048x263): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x263): 51.022

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 328.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x264x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x264x2048): 58.113
Elapsed time for attention_prob_times_values (80x2048x2048x264): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x264): 53.843

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 344.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x265x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x265x2048): 55.769
Elapsed time for attention_prob_times_values (80x2048x2048x265): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x265): 55.339

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 343.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x143x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x143x2048): 60.184
Elapsed time for attention_prob_times_values (160x2048x2048x143): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x143): 46.536

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 345.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x144x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x144x2048): 62.516
Elapsed time for attention_prob_times_values (160x2048x2048x144): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x144): 48.920

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 363.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x145x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x145x2048): 54.994
Elapsed time for attention_prob_times_values (160x2048x2048x145): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x145): 45.635

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 332.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x146x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x146x2048): 61.036
Elapsed time for attention_prob_times_values (160x2048x2048x146): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x146): 51.225

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 373.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x147x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x147x2048): 60.395
Elapsed time for attention_prob_times_values (160x2048x2048x147): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x147): 49.506

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 366.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x148x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x148x2048): 60.126
Elapsed time for attention_prob_times_values (160x2048x2048x148): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x148): 52.220

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 379.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x149x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x149x2048): 60.655
Elapsed time for attention_prob_times_values (160x2048x2048x149): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x149): 50.065

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 374.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x150x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x150x2048): 60.851
Elapsed time for attention_prob_times_values (160x2048x2048x150): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x150): 52.566

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 386.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x151x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x151x2048): 61.589
Elapsed time for attention_prob_times_values (160x2048x2048x151): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x151): 49.592

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 379.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 8, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x522x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x522x2048): 69.405
Elapsed time for attention_prob_times_values (32x2048x2048x522): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x522): 63.169

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 335.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x523x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x523x2048): 67.613
Elapsed time for attention_prob_times_values (32x2048x2048x523): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x523): 59.515

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 321.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x524x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x524x2048): 69.811
Elapsed time for attention_prob_times_values (32x2048x2048x524): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x524): 61.661

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 333.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x525x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x525x2048): 64.278
Elapsed time for attention_prob_times_values (32x2048x2048x525): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x525): 55.507

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 303.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x526x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x526x2048): 63.009
Elapsed time for attention_prob_times_values (32x2048x2048x526): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x526): 56.966

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 305.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x527x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x527x2048): 68.311
Elapsed time for attention_prob_times_values (32x2048x2048x527): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x527): 60.068

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 327.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x528x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x528x2048): 70.897
Elapsed time for attention_prob_times_values (32x2048x2048x528): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x528): 74.880

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 373.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x529x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x529x2048): 68.186
Elapsed time for attention_prob_times_values (32x2048x2048x529): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x529): 60.338

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 328.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x530x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x530x2048): 69.290
Elapsed time for attention_prob_times_values (32x2048x2048x530): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x530): 63.300

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 340.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x531x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x531x2048): 68.774
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x266x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x266x2048): 54.920
Elapsed time for attention_prob_times_values (80x2048x2048x266): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x266): 58.558

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 351.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x267x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x267x2048): 53.007
Elapsed time for attention_prob_times_values (80x2048x2048x267): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x267): 55.649

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 337.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x268x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x268x2048): 56.485
Elapsed time for attention_prob_times_values (80x2048x2048x268): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x268): 59.221

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 360.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x269x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x269x2048): 56.547
Elapsed time for attention_prob_times_values (80x2048x2048x269): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x269): 56.295

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 352.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x270x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x270x2048): 57.167
Elapsed time for attention_prob_times_values (80x2048x2048x270): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x270): 58.968

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 364.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x271x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x271x2048): 54.677
Elapsed time for attention_prob_times_values (80x2048x2048x271): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x271): 56.709

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 350.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x272x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x272x2048): 58.301
Elapsed time for attention_prob_times_values (80x2048x2048x272): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x272): 73.751

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 411.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x273x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x273x2048): 55.659
Elapsed time for attention_prob_times_values (80x2048x2048x273): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x273): 56.681

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 355.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x274x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x274x2048): 56.755
Elapsed time for attention_prob_times_values (80x2048x2048x274): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x274): 59.809

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 369.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (32x2048x2048x531): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x531): 57.365

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 322.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x532x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x532x2048): 66.858
Elapsed time for attention_prob_times_values (32x2048x2048x532): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x532): 61.782

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 331.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x533x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x533x2048): 68.962
Elapsed time for attention_prob_times_values (32x2048x2048x533): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x533): 60.653

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 333.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x534x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x534x2048): 68.799
Elapsed time for attention_prob_times_values (32x2048x2048x534): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x534): 63.602

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 341.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x535x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x535x2048): 69.284
Elapsed time for attention_prob_times_values (32x2048x2048x535): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x535): 60.526

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 334.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x536x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x536x2048): 64.152
Elapsed time for attention_prob_times_values (32x2048x2048x536): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x536): 75.719

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 360.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x537x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x537x2048): 68.426
Elapsed time for attention_prob_times_values (32x2048x2048x537): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x537): 46.259

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 286.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x538x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x538x2048): 61.621
Elapsed time for attention_prob_times_values (32x2048x2048x538): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x538): 63.849

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 326.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x539x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x539x2048): 68.789
Elapsed time for attention_prob_times_values (32x2048x2048x539): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x539): 61.114

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 337.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x540x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x540x2048): 69.651
Elapsed time for attention_prob_times_values (32x2048x2048x540): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x540): 64.330

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 349.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x541x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x541x2048): 66.591
Elapsed time for attention_prob_times_values (32x2048x2048x541): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x541): 59.235

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 327.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x542x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x542x2048): 69.424
Elapsed time for attention_prob_times_values (32x2048x2048x542): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x542): 64.155

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 349.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x543x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x543x2048): 61.296
Elapsed time for attention_prob_times_values (32x2048x2048x543): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x543): 61.431

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 321.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x544x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x544x2048): 84.645
Elapsed time for attention_prob_times_values (32x2048x2048x544): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x544): 70.906

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 405.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x545x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x545x2048): 71.492
Elapsed time for attention_prob_times_values (32x2048x2048x545): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x545): 61.875

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 348.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x546x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x546x2048): 72.425
Elapsed time for attention_prob_times_values (32x2048x2048x546): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x546): 64.223

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 358.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x547x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x547x2048): 70.321
Elapsed time for attention_prob_times_values (32x2048x2048x547): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x547): 62.015

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 347.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x548x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x548x2048): 73.086
Elapsed time for attention_prob_times_values (32x2048x2048x548): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x548): 65.049

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 363.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x549x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x549x2048): 65.841
Elapsed time for attention_prob_times_values (32x2048x2048x549): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x549): 59.423

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 330.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x550x2048): 0.0021
Elapsed time for attention_key_query_prob (80x2048x275x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x275x2048): 56.361
Elapsed time for attention_prob_times_values (80x2048x2048x275): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x275): 57.312

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 362.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x276x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x276x2048): 57.406
Elapsed time for attention_prob_times_values (80x2048x2048x276): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x276): 60.546

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 376.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x277x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x277x2048): 53.127
Elapsed time for attention_prob_times_values (80x2048x2048x277): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x277): 57.900

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 355.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x278x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x278x2048): 56.864
Elapsed time for attention_prob_times_values (80x2048x2048x278): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x278): 59.797

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 374.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x279x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x279x2048): 55.211
Elapsed time for attention_prob_times_values (80x2048x2048x279): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x279): 58.210

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 365.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x280x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x280x2048): 58.497
Elapsed time for attention_prob_times_values (80x2048x2048x280): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x280): 74.853

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 424.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x281x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x281x2048): 56.171
Elapsed time for attention_prob_times_values (80x2048x2048x281): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x281): 58.636

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 372.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x282x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x282x2048): 56.339
Elapsed time for attention_prob_times_values (80x2048x2048x282): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x282): 61.484

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 382.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x283x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x283x2048): 56.615
Elapsed time for attention_prob_times_values (80x2048x2048x283): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x283): 57.752

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 373.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x284x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x284x2048): 55.142
Elapsed time for attention_prob_times_values (80x2048x2048x284): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x284): 62.233

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 382.818
num_attention_heads: 40, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x152x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x152x2048): 61.055
Elapsed time for attention_prob_times_values (160x2048x2048x152): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x152): 49.093

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 377.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x153x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x153x2048): 58.418
Elapsed time for attention_prob_times_values (160x2048x2048x153): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x153): 51.171

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 380.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x154x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x154x2048): 62.182
Elapsed time for attention_prob_times_values (160x2048x2048x154): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x154): 52.019

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 397.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x155x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x155x2048): 61.771
Elapsed time for attention_prob_times_values (160x2048x2048x155): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x155): 47.988

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 381.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x156x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x156x2048): 63.175
Elapsed time for attention_prob_times_values (160x2048x2048x156): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x156): 54.455

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 414.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x157x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x157x2048): 59.549
Elapsed time for attention_prob_times_values (160x2048x2048x157): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x157): 48.517

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 381.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x158x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x158x2048): 63.528
Elapsed time for attention_prob_times_values (160x2048x2048x158): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x158): 54.946

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 422.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x159x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x159x2048): 60.267
Elapsed time for attention_prob_times_values (160x2048x2048x159): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x159): 52.235

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 403.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x160x2048): 76.274
Elapsed time for attention_prob_times_values (160x2048x2048x160): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x160): 55.117

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 463.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x161x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x161x2048): 61.842
Elapsed time for attention_prob_times_values (160x2048x2048x161): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x550x2048): 72.005
Elapsed time for attention_prob_times_values (32x2048x2048x550): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x550): 64.849

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 361.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x551x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x551x2048): 71.095
Elapsed time for attention_prob_times_values (32x2048x2048x551): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x551): 62.575

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 353.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x552x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x552x2048): 70.091
Elapsed time for attention_prob_times_values (32x2048x2048x552): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x552): 77.823

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 391.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x553x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x553x2048): 70.218
Elapsed time for attention_prob_times_values (32x2048x2048x553): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x553): 62.632

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 352.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x554x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x554x2048): 70.881
Elapsed time for attention_prob_times_values (32x2048x2048x554): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x554): 65.472

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 362.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x555x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x555x2048): 70.290
Elapsed time for attention_prob_times_values (32x2048x2048x555): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x555): 62.594

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 353.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x556x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x556x2048): 69.179
Elapsed time for attention_prob_times_values (32x2048x2048x556): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x556): 65.992

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 360.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x557x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x557x2048): 70.686
Elapsed time for attention_prob_times_values (32x2048x2048x557): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x557): 63.225

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 357.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x558x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x558x2048): 71.352
Elapsed time for attention_prob_times_values (32x2048x2048x558): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x558): 66.041

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 367.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x559x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x559x2048): 70.653
Elapsed time for attention_prob_times_values (32x2048x2048x559): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x559): 63.184

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 358.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x560x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x560x2048): 72.555
Elapsed time for attention_prob_times_values (32x2048x2048x560): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x560): 79.379

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 407.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x561x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x561x2048): 69.824
Elapsed time for attention_prob_times_values (32x2048x2048x561): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x561): 60.408

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 348.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x562x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x562x2048): 70.746
Elapsed time for attention_prob_times_values (32x2048x2048x562): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x562): 64.891

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 364.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x563x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x563x2048): 70.205
Elapsed time for attention_prob_times_values (32x2048x2048x563): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x563): 57.542

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 341.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x564x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x564x2048): 71.768
Elapsed time for attention_prob_times_values (32x2048x2048x564): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x564): 67.076

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 374.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x565x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x565x2048): 69.683
Elapsed time for attention_prob_times_values (32x2048x2048x565): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x565): 64.178

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 361.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x566x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x566x2048): 71.170
Elapsed time for attention_prob_times_values (32x2048x2048x566): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x566): 67.328

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 375.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x567x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x567x2048): 70.392
Elapsed time for attention_prob_times_values (32x2048x2048x567): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x567): 64.438

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 365.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x568x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x568x2048): 72.245
Elapsed time for attention_prob_times_values (32x2048x2048x568): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x568): 80.069

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 413.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x67x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x67x2048): 43.044
Elapsed time for attention_prob_times_values (384x2048x2048x67): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x67): 32.797

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 271.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x68x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x68x2048): 43.269
Elapsed time for attention_prob_times_values (384x2048x2048x68): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x68): 35.625

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 288.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x69x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x69x2048): 43.256
Elapsed time for attention_prob_times_values (384x2048x2048x69): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x69): 34.501

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 286.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x70x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x70x2048): 44.605
Elapsed time for attention_prob_times_values (384x2048x2048x70): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x70): 35.634

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 299.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x71x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x71x2048): 43.804
Elapsed time for attention_prob_times_values (384x2048x2048x71): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x71): 36.182

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 303.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x72x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x72x2048): 46.358
Elapsed time for attention_prob_times_values (384x2048x2048x72): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x72): 35.197

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 310.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x73x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x73x2048): 44.911
Elapsed time for attention_prob_times_values (384x2048x2048x73): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x73): 35.792

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 312.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x74x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x74x2048): 45.913
Elapsed time for attention_prob_times_values (384x2048x2048x74): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x74): 38.501

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 332.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x75x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x75x2048): 42.462
Elapsed time for attention_prob_times_values (384x2048x2048x75): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x75): 37.647

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 320.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x76x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x76x2048): 46.844
Elapsed time for attention_prob_times_values (384x2048x2048x76): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x76): 38.656

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 344.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x285x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x285x2048): 56.685
Elapsed time for attention_prob_times_values (80x2048x2048x285): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x285): 58.619

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 378.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x286x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x286x2048): 57.403
Elapsed time for attention_prob_times_values (80x2048x2048x286): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x286): 62.173

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 393.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x287x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x287x2048): 55.701
Elapsed time for attention_prob_times_values (80x2048x2048x287): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x287): 59.795

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 380.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x288x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x288x2048): 77.708
Elapsed time for attention_prob_times_values (80x2048x2048x288): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x288): 76.368

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 510.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x289x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x289x2048): 60.779
Elapsed time for attention_prob_times_values (80x2048x2048x289): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x289): 56.790

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 390.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x290x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x290x2048): 59.474
Elapsed time for attention_prob_times_values (80x2048x2048x290): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x290): 62.627

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 406.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x291x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x291x2048): 60.223
Elapsed time for attention_prob_times_values (80x2048x2048x291): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x291): 60.463

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 403.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x292x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x292x2048): 61.199
Elapsed time for attention_prob_times_values (80x2048x2048x292): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x292): 63.581

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 418.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x293x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x293x2048): 60.655
Elapsed time for attention_prob_times_values (80x2048x2048x293): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x293): 60.517

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 407.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (32x2048x569x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x569x2048): 70.023
Elapsed time for attention_prob_times_values (32x2048x2048x569): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x569): 64.366

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 365.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x570x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x570x2048): 70.585
Elapsed time for attention_prob_times_values (32x2048x2048x570): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x570): 67.306

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 375.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x571x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x571x2048): 70.192
Elapsed time for attention_prob_times_values (32x2048x2048x571): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x571): 64.794

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 367.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x572x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x572x2048): 67.706
Elapsed time for attention_prob_times_values (32x2048x2048x572): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x572): 66.779

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 367.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x573x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x573x2048): 69.942
Elapsed time for attention_prob_times_values (32x2048x2048x573): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x573): 64.995

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 368.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x574x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x574x2048): 70.855
Elapsed time for attention_prob_times_values (32x2048x2048x574): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x574): 67.807

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 380.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x575x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x575x2048): 8.862
Elapsed time for attention_prob_times_values (32x2048x2048x575): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x575): 61.510

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 85.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x576x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x576x2048): 85.197
Elapsed time for attention_prob_times_values (32x2048x2048x576): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x576): 82.761

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 461.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x577x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x577x2048): 72.440
Elapsed time for attention_prob_times_values (32x2048x2048x577): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x577): 60.731

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 363.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x578x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x578x2048): 73.014
Elapsed time for attention_prob_times_values (32x2048x2048x578): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x578): 62.962

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 372.944
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x161): 52.536

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 414.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x162x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x162x2048): 63.143
Elapsed time for attention_prob_times_values (160x2048x2048x162): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x162): 56.080

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 435.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x163x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x163x2048): 62.520
Elapsed time for attention_prob_times_values (160x2048x2048x163): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x163): 54.503

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 429.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x164x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x164x2048): 63.632
Elapsed time for attention_prob_times_values (160x2048x2048x164): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x164): 55.383

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 438.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x165x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x165x2048): 60.402
Elapsed time for attention_prob_times_values (160x2048x2048x165): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x165): 53.751

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 423.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x166x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x166x2048): 58.892
Elapsed time for attention_prob_times_values (160x2048x2048x166): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x166): 56.988

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 433.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x167x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x167x2048): 63.219
Elapsed time for attention_prob_times_values (160x2048x2048x167): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x167): 52.980

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 433.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x168x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x168x2048): 65.536
Elapsed time for attention_prob_times_values (160x2048x2048x168): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x168): 53.321

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 444.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x169x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x169x2048): 62.571
Elapsed time for attention_prob_times_values (160x2048x2048x169): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x169): 55.957

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 449.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x170x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x170x2048): 60.991
Elapsed time for attention_prob_times_values (160x2048x2048x170): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x170): 58.426

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 456.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x579x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x579x2048): 72.224
Elapsed time for attention_prob_times_values (32x2048x2048x579): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x579): 60.698

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 364.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x580x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x580x2048): 73.997
Elapsed time for attention_prob_times_values (32x2048x2048x580): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x580): 61.892

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 372.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x581x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x581x2048): 72.003
Elapsed time for attention_prob_times_values (32x2048x2048x581): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x581): 60.800

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 365.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x582x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x582x2048): 73.092
Elapsed time for attention_prob_times_values (32x2048x2048x582): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x582): 63.305

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 376.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x583x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x583x2048): 73.308
Elapsed time for attention_prob_times_values (32x2048x2048x583): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x583): 61.060

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 370.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x584x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x584x2048): 73.962
Elapsed time for attention_prob_times_values (32x2048x2048x584): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x584): 78.520

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 423.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x585x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x585x2048): 71.244
Elapsed time for attention_prob_times_values (32x2048x2048x585): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x585): 60.239

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 363.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x586x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x586x2048): 71.989
Elapsed time for attention_prob_times_values (32x2048x2048x586): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x586): 63.191

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 375.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x587x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x587x2048): 68.409
Elapsed time for attention_prob_times_values (32x2048x2048x587): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x587): 61.206

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 360.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
num_attention_heads: 20, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x294x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x294x2048): 61.344
Elapsed time for attention_prob_times_values (80x2048x2048x294): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x294): 63.637

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 421.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x295x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x295x2048): 59.066
Elapsed time for attention_prob_times_values (80x2048x2048x295): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x295): 61.576

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 407.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x296x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x296x2048): 62.232
Elapsed time for attention_prob_times_values (80x2048x2048x296): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x296): 79.440

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 473.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x297x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x297x2048): 59.424
Elapsed time for attention_prob_times_values (80x2048x2048x297): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x297): 61.325

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 410.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x298x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x298x2048): 60.030
Elapsed time for attention_prob_times_values (80x2048x2048x298): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x298): 64.657

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 424.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x299x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x299x2048): 59.676
Elapsed time for attention_prob_times_values (80x2048x2048x299): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x299): 59.537

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 407.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x300x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x300x2048): 61.283
Elapsed time for attention_prob_times_values (80x2048x2048x300): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x300): 65.816

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 435.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x301x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x301x2048): 60.126
Elapsed time for attention_prob_times_values (80x2048x2048x301): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x301): 62.712

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 422.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x302x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x302x2048): 60.857
Elapsed time for attention_prob_times_values (80x2048x2048x302): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x302): 65.937

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 436.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x303x2048): 0.0181
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x303x2048): 11.239
Elapsed time for attention_prob_times_values (80x2048x2048x303): 0.0032


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x588x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x588x2048): 73.624
Elapsed time for attention_prob_times_values (32x2048x2048x588): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x588): 63.755

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 382.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x589x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x589x2048): 68.331
Elapsed time for attention_prob_times_values (32x2048x2048x589): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x589): 58.640

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 353.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x590x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x590x2048): 72.314
Elapsed time for attention_prob_times_values (32x2048x2048x590): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x590): 63.568

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 379.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x591x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x591x2048): 71.151
Elapsed time for attention_prob_times_values (32x2048x2048x591): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x591): 61.583

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 370.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x592x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x592x2048): 73.334
Elapsed time for attention_prob_times_values (32x2048x2048x592): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x592): 83.809

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 440.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x593x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x593x2048): 70.914
Elapsed time for attention_prob_times_values (32x2048x2048x593): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x593): 61.826

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 372.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x594x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x594x2048): 71.735
Elapsed time for attention_prob_times_values (32x2048x2048x594): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x594): 64.048

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 381.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x595x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x595x2048): 70.958
Elapsed time for attention_prob_times_values (32x2048x2048x595): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x595): 60.119

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 367.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x596x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x596x2048): 72.404
Elapsed time for attention_prob_times_values (32x2048x2048x596): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x596): 61.439

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 375.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x597x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x597x2048): 71.204
Elapsed time for attention_prob_times_values (32x2048x2048x597): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x597): 60.670

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 371.087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x303): 63.340

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 132.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x304x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x304x2048): 60.545
Elapsed time for attention_prob_times_values (80x2048x2048x304): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x304): 79.799

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 477.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x305x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x305x2048): 59.552
Elapsed time for attention_prob_times_values (80x2048x2048x305): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x305): 63.659

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 428.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x306x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x306x2048): 59.201
Elapsed time for attention_prob_times_values (80x2048x2048x306): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x306): 66.619

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 437.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x307x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x307x2048): 58.202
Elapsed time for attention_prob_times_values (80x2048x2048x307): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x307): 63.986

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 426.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x308x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x308x2048): 61.329
Elapsed time for attention_prob_times_values (80x2048x2048x308): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x308): 67.852

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 451.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x309x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x309x2048): 60.404
Elapsed time for attention_prob_times_values (80x2048x2048x309): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x309): 64.484

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 438.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x310x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x310x2048): 60.866
Elapsed time for attention_prob_times_values (80x2048x2048x310): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x310): 67.883

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 452.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x311x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x311x2048): 60.635
Elapsed time for attention_prob_times_values (80x2048x2048x311): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x311): 64.772

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 443.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x312x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x312x2048): 62.125
Elapsed time for attention_prob_times_values (80x2048x2048x312): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x312): 77.327

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 488.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x598x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x598x2048): 71.940
Elapsed time for attention_prob_times_values (32x2048x2048x598): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x598): 63.644

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 383.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x599x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x599x2048): 71.503
Elapsed time for attention_prob_times_values (32x2048x2048x599): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x599): 62.290

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 378.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x600x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x600x2048): 73.142
Elapsed time for attention_prob_times_values (32x2048x2048x600): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x600): 80.345

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 435.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x601x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x601x2048): 65.303
Elapsed time for attention_prob_times_values (32x2048x2048x601): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x601): 62.458

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 363.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x602x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x602x2048): 71.489
Elapsed time for attention_prob_times_values (32x2048x2048x602): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x602): 64.313

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 386.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x603x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x603x2048): 70.780
Elapsed time for attention_prob_times_values (32x2048x2048x603): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x603): 62.567

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 379.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x604x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x604x2048): 71.848
Elapsed time for attention_prob_times_values (32x2048x2048x604): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x604): 65.076

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 390.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x605x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x605x2048): 70.719
Elapsed time for attention_prob_times_values (32x2048x2048x605): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x605): 61.908

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 378.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x606x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x606x2048): 64.556
Elapsed time for attention_prob_times_values (32x2048x2048x606): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x606): 60.919

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 359.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 40, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x171x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x171x2048): 60.540
Elapsed time for attention_prob_times_values (160x2048x2048x171): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x171): 56.440

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 448.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x172x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x172x2048): 63.207
Elapsed time for attention_prob_times_values (160x2048x2048x172): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x172): 59.005

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 471.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x173x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x173x2048): 64.288
Elapsed time for attention_prob_times_values (160x2048x2048x173): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x173): 55.846

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 463.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x174x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x174x2048): 65.477
Elapsed time for attention_prob_times_values (160x2048x2048x174): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x174): 59.655

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 486.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x175x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x175x2048): 63.151
Elapsed time for attention_prob_times_values (160x2048x2048x175): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x175): 57.896

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 473.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x176x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x176x2048): 67.918
Elapsed time for attention_prob_times_values (160x2048x2048x176): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x176): 58.948

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 497.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x177x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x177x2048): 64.506
Elapsed time for attention_prob_times_values (160x2048x2048x177): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x177): 56.770

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 477.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x178x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x178x2048): 65.644
Elapsed time for attention_prob_times_values (160x2048x2048x178): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x178): 58.709

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 492.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x179x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x179x2048): 65.058
Elapsed time for attention_prob_times_values (160x2048x2048x179): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x179): 58.980

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 494.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x180x2048): 0.0036
num_attention_heads: 8, hidden_size: 4856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x607x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x607x2048): 71.093
Elapsed time for attention_prob_times_values (32x2048x2048x607): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x607): 63.286

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 384.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x608x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x608x2048): 86.389
Elapsed time for attention_prob_times_values (32x2048x2048x608): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x608): 86.684

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 497.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x609x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x609x2048): 72.797
Elapsed time for attention_prob_times_values (32x2048x2048x609): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x609): 61.745

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 384.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x610x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x610x2048): 73.810
Elapsed time for attention_prob_times_values (32x2048x2048x610): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x610): 64.750

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 397.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x611x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x611x2048): 66.306
Elapsed time for attention_prob_times_values (32x2048x2048x611): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x611): 60.233

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 364.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x612x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x612x2048): 74.155
Elapsed time for attention_prob_times_values (32x2048x2048x612): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x612): 64.834

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 399.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x613x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x613x2048): 72.551
Elapsed time for attention_prob_times_values (32x2048x2048x613): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x613): 63.876

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 393.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x614x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x614x2048): 64.194
Elapsed time for attention_prob_times_values (32x2048x2048x614): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x614): 60.951

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 362.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x615x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x615x2048): 72.331
Elapsed time for attention_prob_times_values (32x2048x2048x615): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x615): 55.913

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 366.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x616x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x616x2048): 74.074
Elapsed time for attention_prob_times_values (32x2048x2048x616): 0.0019
========================================================================================================================
num_attention_heads: 20, hidden_size: 6260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x313x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x313x2048): 57.943
Elapsed time for attention_prob_times_values (80x2048x2048x313): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x313): 65.198

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 436.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x314x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x314x2048): 60.449
Elapsed time for attention_prob_times_values (80x2048x2048x314): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x314): 64.645

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 445.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x315x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x315x2048): 57.092
Elapsed time for attention_prob_times_values (80x2048x2048x315): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x315): 65.445

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 436.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x316x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x316x2048): 61.730
Elapsed time for attention_prob_times_values (80x2048x2048x316): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x316): 64.702

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 453.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x317x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x317x2048): 60.469
Elapsed time for attention_prob_times_values (80x2048x2048x317): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x317): 65.904

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 453.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x318x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x318x2048): 58.675
Elapsed time for attention_prob_times_values (80x2048x2048x318): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x318): 68.753

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 456.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x319x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x319x2048): 60.242
Elapsed time for attention_prob_times_values (80x2048x2048x319): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x319): 66.168

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 455.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x320x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x320x2048): 74.249
Elapsed time for attention_prob_times_values (80x2048x2048x320): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x320): 82.152

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 565.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x321x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x321x2048): 63.286
Elapsed time for attention_prob_times_values (80x2048x2048x321): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x321): 58.759

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 442.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x322x2048): 0.0034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x77x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x77x2048): 45.975
Elapsed time for attention_prob_times_values (384x2048x2048x77): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x77): 37.502

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 339.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x78x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x78x2048): 45.581
Elapsed time for attention_prob_times_values (384x2048x2048x78): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x78): 40.218

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 355.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x79x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x79x2048): 46.112
Elapsed time for attention_prob_times_values (384x2048x2048x79): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x79): 38.876

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 354.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x80x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x80x2048): 49.538
Elapsed time for attention_prob_times_values (384x2048x2048x80): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x80): 38.051

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 365.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x81x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x81x2048): 47.200
Elapsed time for attention_prob_times_values (384x2048x2048x81): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x81): 39.923

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 371.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x82x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x82x2048): 45.041
Elapsed time for attention_prob_times_values (384x2048x2048x82): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x82): 40.126

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 368.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x83x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x83x2048): 48.236
Elapsed time for attention_prob_times_values (384x2048x2048x83): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x83): 40.963

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 389.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x84x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x84x2048): 48.648
Elapsed time for attention_prob_times_values (384x2048x2048x84): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x84): 41.564

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 397.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x85x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x85x2048): 48.718
Elapsed time for attention_prob_times_values (384x2048x2048x85): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x85): 39.989

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 393.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x616): 86.846

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 464.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x617x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x617x2048): 71.477
Elapsed time for attention_prob_times_values (32x2048x2048x617): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x617): 61.211

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 383.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x618x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x618x2048): 71.681
Elapsed time for attention_prob_times_values (32x2048x2048x618): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x618): 65.309

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 398.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x619x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x619x2048): 71.708
Elapsed time for attention_prob_times_values (32x2048x2048x619): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x619): 63.868

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 394.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x620x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x620x2048): 72.988
Elapsed time for attention_prob_times_values (32x2048x2048x620): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x620): 66.381

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 406.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x621x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x621x2048): 68.588
Elapsed time for attention_prob_times_values (32x2048x2048x621): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x621): 64.542

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 389.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x622x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x622x2048): 72.752
Elapsed time for attention_prob_times_values (32x2048x2048x622): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x622): 62.679

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 394.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x623x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x623x2048): 71.920
Elapsed time for attention_prob_times_values (32x2048x2048x623): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x623): 62.489

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 392.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x624x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x624x2048): 73.985
Elapsed time for attention_prob_times_values (32x2048x2048x624): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x624): 88.417

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 473.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x625x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x625x2048): 68.472
Elapsed time for attention_prob_times_values (32x2048x2048x625): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x625): 59.893

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 375.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x626x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x626x2048): 72.170
Elapsed time for attention_prob_times_values (32x2048x2048x626): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x626): 66.167

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 406.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x627x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x627x2048): 71.430
Elapsed time for attention_prob_times_values (32x2048x2048x627): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x627): 64.836

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 400.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x628x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x628x2048): 72.549
Elapsed time for attention_prob_times_values (32x2048x2048x628): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x628): 66.591

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 410.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x629x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x629x2048): 70.989
Elapsed time for attention_prob_times_values (32x2048x2048x629): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x629): 64.990

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 401.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x630x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x630x2048): 72.557
Elapsed time for attention_prob_times_values (32x2048x2048x630): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x630): 66.665

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 411.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x631x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x631x2048): 71.172
Elapsed time for attention_prob_times_values (32x2048x2048x631): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x631): 64.486

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 401.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x632x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x632x2048): 73.011
Elapsed time for attention_prob_times_values (32x2048x2048x632): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x632): 89.303

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 477.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x633x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x633x2048): 71.262
Elapsed time for attention_prob_times_values (32x2048x2048x633): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x633): 64.503

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 402.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x634x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x634x2048): 64.916
Elapsed time for attention_prob_times_values (32x2048x2048x634): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x634): 65.047

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 386.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x635x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x635x2048): 69.432
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x180x2048): 66.582
Elapsed time for attention_prob_times_values (160x2048x2048x180): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x180): 55.500

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 486.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x181x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x181x2048): 61.139
Elapsed time for attention_prob_times_values (160x2048x2048x181): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x181): 59.441

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 486.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x182x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x182x2048): 64.293
Elapsed time for attention_prob_times_values (160x2048x2048x182): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x182): 60.733

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 506.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x183x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x183x2048): 65.992
Elapsed time for attention_prob_times_values (160x2048x2048x183): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x183): 59.367

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 509.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x184x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x184x2048): 68.057
Elapsed time for attention_prob_times_values (160x2048x2048x184): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x184): 60.293

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 523.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x185x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x185x2048): 65.463
Elapsed time for attention_prob_times_values (160x2048x2048x185): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x185): 60.536

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x186x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x186x2048): 66.954
Elapsed time for attention_prob_times_values (160x2048x2048x186): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x186): 62.068

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 532.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x187x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x187x2048): 66.084
Elapsed time for attention_prob_times_values (160x2048x2048x187): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x187): 61.041

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 527.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x188x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x188x2048): 67.630
Elapsed time for attention_prob_times_values (160x2048x2048x188): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x188): 60.429

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 532.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x189x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x189x2048): 66.565
Elapsed time for attention_prob_times_values (160x2048x2048x189): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x189): 61.537

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 536.101
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x322x2048): 62.823
Elapsed time for attention_prob_times_values (80x2048x2048x322): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x322): 57.733

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 438.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x323x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x323x2048): 60.309
Elapsed time for attention_prob_times_values (80x2048x2048x323): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x323): 59.148

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 436.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x324x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x324x2048): 65.267
Elapsed time for attention_prob_times_values (80x2048x2048x324): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x324): 59.335

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 455.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x325x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x325x2048): 62.687
Elapsed time for attention_prob_times_values (80x2048x2048x325): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x325): 59.333

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 447.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x326x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x326x2048): 63.153
Elapsed time for attention_prob_times_values (80x2048x2048x326): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x326): 61.296

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 458.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x327x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x327x2048): 62.452
Elapsed time for attention_prob_times_values (80x2048x2048x327): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x327): 59.787

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 451.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x328x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x328x2048): 65.003
Elapsed time for attention_prob_times_values (80x2048x2048x328): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x328): 73.395

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 510.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x329x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x329x2048): 59.071
Elapsed time for attention_prob_times_values (80x2048x2048x329): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x329): 57.994

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 434.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x330x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x330x2048): 61.506
Elapsed time for attention_prob_times_values (80x2048x2048x330): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x330): 62.685

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 462.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x331x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x331x2048): 62.299
Elapsed time for attention_prob_times_values (80x2048x2048x331): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x331): 58.299

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 449.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Elapsed time for attention_prob_times_values (32x2048x2048x635): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x635): 65.310

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 401.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x636x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x636x2048): 72.411
Elapsed time for attention_prob_times_values (32x2048x2048x636): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x636): 64.540

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 407.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x637x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x637x2048): 70.961
Elapsed time for attention_prob_times_values (32x2048x2048x637): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x637): 65.486

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 407.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x638x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x638x2048): 71.948
Elapsed time for attention_prob_times_values (32x2048x2048x638): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x638): 67.102

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 415.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x639x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x639x2048): 65.854
Elapsed time for attention_prob_times_values (32x2048x2048x639): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x639): 64.421

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 390.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x640x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x640x2048): 76.308
Elapsed time for attention_prob_times_values (32x2048x2048x640): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x640): 92.075

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 500.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x641x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x641x2048): 72.988
Elapsed time for attention_prob_times_values (32x2048x2048x641): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x641): 59.882

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 395.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x642x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x642x2048): 71.026
Elapsed time for attention_prob_times_values (32x2048x2048x642): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x642): 61.251

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 395.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x643x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x643x2048): 72.737
Elapsed time for attention_prob_times_values (32x2048x2048x643): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x643): 60.309

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 397.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x644x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x644x2048): 70.458
Elapsed time for attention_prob_times_values (32x2048x2048x644): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x644): 58.697

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 386.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x645x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x645x2048): 69.734
Elapsed time for attention_prob_times_values (32x2048x2048x645): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x645): 61.062

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 393.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x646x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x646x2048): 70.138
Elapsed time for attention_prob_times_values (32x2048x2048x646): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x646): 63.180

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 401.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x647x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x647x2048): 68.593
Elapsed time for attention_prob_times_values (32x2048x2048x647): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x647): 59.617

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 386.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x648x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x648x2048): 74.455
Elapsed time for attention_prob_times_values (32x2048x2048x648): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x648): 76.030

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 456.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x649x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x649x2048): 72.197
Elapsed time for attention_prob_times_values (32x2048x2048x649): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x649): 61.025

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 401.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x650x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x650x2048): 72.827
Elapsed time for attention_prob_times_values (32x2048x2048x650): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x650): 57.099

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 389.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x651x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x651x2048): 72.491
Elapsed time for attention_prob_times_values (32x2048x2048x651): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x651): 61.070

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 403.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x652x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x652x2048): 73.445
Elapsed time for attention_prob_times_values (32x2048x2048x652): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x652): 64.157

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 417.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x653x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x653x2048): 68.006
Elapsed time for attention_prob_times_values (32x2048x2048x653): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x653): 61.462

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 393.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x654x2048): 0.0024
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x332x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x332x2048): 59.477
Elapsed time for attention_prob_times_values (80x2048x2048x332): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x332): 61.748

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 453.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x333x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x333x2048): 62.525
Elapsed time for attention_prob_times_values (80x2048x2048x333): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x333): 58.465

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 453.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x334x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x334x2048): 60.126
Elapsed time for attention_prob_times_values (80x2048x2048x334): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x334): 63.270

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 463.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x335x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x335x2048): 62.306
Elapsed time for attention_prob_times_values (80x2048x2048x335): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x335): 58.871

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 456.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x336x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x336x2048): 64.394
Elapsed time for attention_prob_times_values (80x2048x2048x336): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x336): 75.275

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 524.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x337x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x337x2048): 60.188
Elapsed time for attention_prob_times_values (80x2048x2048x337): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x337): 59.300

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 452.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x338x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x338x2048): 62.464
Elapsed time for attention_prob_times_values (80x2048x2048x338): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x338): 62.040

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 473.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x339x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x339x2048): 62.220
Elapsed time for attention_prob_times_values (80x2048x2048x339): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x339): 58.963

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 461.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x340x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x340x2048): 63.475
Elapsed time for attention_prob_times_values (80x2048x2048x340): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x340): 63.015

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 483.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x190x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x190x2048): 67.998
Elapsed time for attention_prob_times_values (160x2048x2048x190): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x190): 63.414

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 552.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x191x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x191x2048): 64.013
Elapsed time for attention_prob_times_values (160x2048x2048x191): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x191): 58.618

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 517.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x192x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x192x2048): 77.282
Elapsed time for attention_prob_times_values (160x2048x2048x192): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x192): 65.706

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 603.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x193x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x193x2048): 66.100
Elapsed time for attention_prob_times_values (160x2048x2048x193): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x193): 51.549

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 494.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x194x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x194x2048): 67.158
Elapsed time for attention_prob_times_values (160x2048x2048x194): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x194): 52.683

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 506.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x195x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x195x2048): 66.313
Elapsed time for attention_prob_times_values (160x2048x2048x195): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x195): 52.177

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 503.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x196x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x196x2048): 67.038
Elapsed time for attention_prob_times_values (160x2048x2048x196): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x196): 52.550

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 509.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x197x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x197x2048): 66.190
Elapsed time for attention_prob_times_values (160x2048x2048x197): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x197): 52.660

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 510.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x198x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x198x2048): 67.321
Elapsed time for attention_prob_times_values (160x2048x2048x198): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x198): 53.152

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 518.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x654x2048): 73.246
Elapsed time for attention_prob_times_values (32x2048x2048x654): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x654): 60.854

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 406.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x655x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x655x2048): 72.578
Elapsed time for attention_prob_times_values (32x2048x2048x655): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x655): 61.682

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 407.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x656x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x656x2048): 71.121
Elapsed time for attention_prob_times_values (32x2048x2048x656): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x656): 77.356

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 453.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x657x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x657x2048): 72.111
Elapsed time for attention_prob_times_values (32x2048x2048x657): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x657): 56.971

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 390.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x658x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x658x2048): 71.829
Elapsed time for attention_prob_times_values (32x2048x2048x658): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x658): 63.838

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 415.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x659x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x659x2048): 72.190
Elapsed time for attention_prob_times_values (32x2048x2048x659): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x659): 61.939

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 409.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x660x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x660x2048): 73.377
Elapsed time for attention_prob_times_values (32x2048x2048x660): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x660): 64.126

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 421.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x661x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x661x2048): 72.318
Elapsed time for attention_prob_times_values (32x2048x2048x661): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x661): 60.173

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 404.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x662x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x662x2048): 73.130
Elapsed time for attention_prob_times_values (32x2048x2048x662): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x662): 61.502

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 412.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x663x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x663x2048): 72.642
Elapsed time for attention_prob_times_values (32x2048x2048x663): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x663): 62.232

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 414.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Elapsed time for attention_key_query_prob (80x2048x341x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x341x2048): 60.238
Elapsed time for attention_prob_times_values (80x2048x2048x341): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x341): 58.441

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 454.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x342x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x342x2048): 63.099
Elapsed time for attention_prob_times_values (80x2048x2048x342): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x342): 62.701

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 483.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x343x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x343x2048): 62.461
Elapsed time for attention_prob_times_values (80x2048x2048x343): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x343): 60.078

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 471.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x344x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x344x2048): 64.388
Elapsed time for attention_prob_times_values (80x2048x2048x344): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x344): 77.141

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 541.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x345x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x345x2048): 61.888
Elapsed time for attention_prob_times_values (80x2048x2048x345): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x345): 60.309

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 472.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x346x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x346x2048): 58.203
Elapsed time for attention_prob_times_values (80x2048x2048x346): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x346): 63.003

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 469.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x347x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x347x2048): 62.073
Elapsed time for attention_prob_times_values (80x2048x2048x347): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x347): 58.877

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 470.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x348x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x348x2048): 60.758
Elapsed time for attention_prob_times_values (80x2048x2048x348): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x348): 63.755

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 485.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x349x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x349x2048): 60.635
Elapsed time for attention_prob_times_values (80x2048x2048x349): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x349): 60.962

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 475.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x350x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x350x2048): 63.034
Elapsed time for attention_prob_times_values (80x2048x2048x350): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x350): 60.588

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 484.159
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x664x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x664x2048): 74.056
Elapsed time for attention_prob_times_values (32x2048x2048x664): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x664): 77.964

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 470.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x665x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x665x2048): 69.224
Elapsed time for attention_prob_times_values (32x2048x2048x665): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x665): 62.295

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 406.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x666x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x666x2048): 72.737
Elapsed time for attention_prob_times_values (32x2048x2048x666): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x666): 64.299

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 423.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x667x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x667x2048): 72.168
Elapsed time for attention_prob_times_values (32x2048x2048x667): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x667): 62.514

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 416.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x668x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x668x2048): 73.354
Elapsed time for attention_prob_times_values (32x2048x2048x668): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x668): 61.947

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 417.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x669x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x669x2048): 72.148
Elapsed time for attention_prob_times_values (32x2048x2048x669): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x669): 58.430

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 402.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x670x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x670x2048): 72.941
Elapsed time for attention_prob_times_values (32x2048x2048x670): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x670): 64.963

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 428.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x671x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x671x2048): 72.545
Elapsed time for attention_prob_times_values (32x2048x2048x671): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x671): 62.555

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 419.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x672x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x672x2048): 83.107
Elapsed time for attention_prob_times_values (32x2048x2048x672): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x672): 79.151

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 506.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 96, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x86x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x86x2048): 49.515
Elapsed time for attention_prob_times_values (384x2048x2048x86): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x86): 42.298

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 413.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x87x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x87x2048): 49.697
Elapsed time for attention_prob_times_values (384x2048x2048x87): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x87): 16.891

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 230.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x88x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x88x2048): 50.576
Elapsed time for attention_prob_times_values (384x2048x2048x88): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x88): 41.274

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 420.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x89x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x89x2048): 46.547
Elapsed time for attention_prob_times_values (384x2048x2048x89): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x89): 42.582

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 415.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x90x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x90x2048): 49.506
Elapsed time for attention_prob_times_values (384x2048x2048x90): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x90): 45.370

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 446.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x91x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x91x2048): 49.590
Elapsed time for attention_prob_times_values (384x2048x2048x91): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x91): 43.579

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 442.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x92x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x92x2048): 49.439
Elapsed time for attention_prob_times_values (384x2048x2048x92): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x92): 45.304

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 455.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x93x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x93x2048): 49.325
Elapsed time for attention_prob_times_values (384x2048x2048x93): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x93): 45.038

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 457.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x94x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x94x2048): 51.095
Elapsed time for attention_prob_times_values (384x2048x2048x94): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x94): 46.959

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 480.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x95x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x95x2048): 49.900
Elapsed time for attention_prob_times_values (384x2048x2048x95): 0.0067
Elapsed time for attention_key_query_prob (32x2048x673x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x673x2048): 74.433
Elapsed time for attention_prob_times_values (32x2048x2048x673): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x673): 63.081

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 427.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x674x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x674x2048): 75.261
Elapsed time for attention_prob_times_values (32x2048x2048x674): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x674): 65.043

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 437.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x675x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x675x2048): 74.362
Elapsed time for attention_prob_times_values (32x2048x2048x675): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x675): 60.930

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 420.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x676x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x676x2048): 76.050
Elapsed time for attention_prob_times_values (32x2048x2048x676): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x676): 65.444

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 441.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x677x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x677x2048): 74.422
Elapsed time for attention_prob_times_values (32x2048x2048x677): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x677): 63.219

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 429.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x678x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x678x2048): 75.178
Elapsed time for attention_prob_times_values (32x2048x2048x678): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x678): 65.390

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 440.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x679x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x679x2048): 73.735
Elapsed time for attention_prob_times_values (32x2048x2048x679): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x679): 63.264

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 429.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x680x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x680x2048): 79.677
Elapsed time for attention_prob_times_values (32x2048x2048x680): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x680): 79.743

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 503.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x681x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x681x2048): 73.682
Elapsed time for attention_prob_times_values (32x2048x2048x681): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x681): 63.231

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 430.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x682x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x682x2048): 67.850
Elapsed time for attention_prob_times_values (32x2048x2048x682): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x682): 63.691

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 415.791
MLP duration (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x351x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x351x2048): 60.338
Elapsed time for attention_prob_times_values (80x2048x2048x351): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x351): 61.300

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 477.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x352x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x352x2048): 81.406
Elapsed time for attention_prob_times_values (80x2048x2048x352): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x352): 75.969

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 618.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x353x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x353x2048): 62.452
Elapsed time for attention_prob_times_values (80x2048x2048x353): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x353): 62.311

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 492.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x354x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x354x2048): 66.036
Elapsed time for attention_prob_times_values (80x2048x2048x354): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x354): 61.311

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 503.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x355x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x355x2048): 59.468
Elapsed time for attention_prob_times_values (80x2048x2048x355): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x355): 62.169

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 482.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x356x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x356x2048): 66.670
Elapsed time for attention_prob_times_values (80x2048x2048x356): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x356): 64.845

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 522.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x357x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x357x2048): 61.732
Elapsed time for attention_prob_times_values (80x2048x2048x357): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x357): 62.589

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 495.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x358x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x358x2048): 65.553
Elapsed time for attention_prob_times_values (80x2048x2048x358): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x358): 62.302

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 510.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x359x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x359x2048): 62.047
Elapsed time for attention_prob_times_values (80x2048x2048x359): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x359): 62.869

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 500.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x199x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x199x2048): 66.501
Elapsed time for attention_prob_times_values (160x2048x2048x199): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x199): 53.066

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 517.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x200x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x200x2048): 69.487
Elapsed time for attention_prob_times_values (160x2048x2048x200): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x200): 50.151

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 513.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x201x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x201x2048): 66.844
Elapsed time for attention_prob_times_values (160x2048x2048x201): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x201): 51.351

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 514.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x202x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x202x2048): 65.144
Elapsed time for attention_prob_times_values (160x2048x2048x202): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x202): 54.287

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 526.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x203x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x203x2048): 67.322
Elapsed time for attention_prob_times_values (160x2048x2048x203): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x203): 51.505

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 521.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x204x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x204x2048): 68.677
Elapsed time for attention_prob_times_values (160x2048x2048x204): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x204): 54.464

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 544.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x205x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x205x2048): 62.317
Elapsed time for attention_prob_times_values (160x2048x2048x205): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x205): 51.170

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 506.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x206x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x206x2048): 69.115
Elapsed time for attention_prob_times_values (160x2048x2048x206): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x206): 54.529

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 551.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x207x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x207x2048): 68.290
Elapsed time for attention_prob_times_values (160x2048x2048x207): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x207): 51.517

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 533.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x208x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x208x2048): 69.017
Elapsed time for attention_prob_times_values (160x2048x2048x208): 0.0052
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x683x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x683x2048): 73.863
Elapsed time for attention_prob_times_values (32x2048x2048x683): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x683): 58.800

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 414.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x684x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x684x2048): 75.450
Elapsed time for attention_prob_times_values (32x2048x2048x684): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x684): 66.053

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 446.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x685x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x685x2048): 74.058
Elapsed time for attention_prob_times_values (32x2048x2048x685): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x685): 62.237

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 429.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x686x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x686x2048): 74.929
Elapsed time for attention_prob_times_values (32x2048x2048x686): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x686): 66.173

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 446.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x687x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x687x2048): 74.994
Elapsed time for attention_prob_times_values (32x2048x2048x687): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x687): 64.089

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 440.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x688x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x688x2048): 74.741
Elapsed time for attention_prob_times_values (32x2048x2048x688): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x688): 80.976

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 495.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x689x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x689x2048): 73.315
Elapsed time for attention_prob_times_values (32x2048x2048x689): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x689): 61.911

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 428.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x690x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x690x2048): 74.086
Elapsed time for attention_prob_times_values (32x2048x2048x690): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x690): 61.471

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 429.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x691x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x691x2048): 73.493
Elapsed time for attention_prob_times_values (32x2048x2048x691): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x691): 64.096

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 438.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x692x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x692x2048): 64.194
Elapsed time for attention_prob_times_values (32x2048x2048x692): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x692): 66.889

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 419.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x693x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x693x2048): 73.824
Elapsed time for attention_prob_times_values (32x2048x2048x693): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x693): 64.197

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 440.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x694x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x694x2048): 74.212
Elapsed time for attention_prob_times_values (32x2048x2048x694): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x694): 63.958

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 441.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x695x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x695x2048): 73.703
Elapsed time for attention_prob_times_values (32x2048x2048x695): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x695): 64.595

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 442.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x696x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x696x2048): 69.470
Elapsed time for attention_prob_times_values (32x2048x2048x696): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x696): 79.314

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 476.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x697x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x697x2048): 73.129
Elapsed time for attention_prob_times_values (32x2048x2048x697): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x697): 62.341

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 433.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x698x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x698x2048): 73.784
Elapsed time for attention_prob_times_values (32x2048x2048x698): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x698): 67.169

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 453.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x699x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x699x2048): 73.278
Elapsed time for attention_prob_times_values (32x2048x2048x699): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x699): 62.500

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 435.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x700x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x700x2048): 74.443
Elapsed time for attention_prob_times_values (32x2048x2048x700): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x700): 67.138

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 456.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x701x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x701x2048): 73.399
Elapsed time for attention_prob_times_values (32x2048x2048x701): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x701): 65.179

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 447.176
num_attention_heads: 20, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x360x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x360x2048): 62.304
Elapsed time for attention_prob_times_values (80x2048x2048x360): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x360): 80.300

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 563.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x361x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x361x2048): 63.579
Elapsed time for attention_prob_times_values (80x2048x2048x361): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x361): 62.770

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 508.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x362x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x362x2048): 64.209
Elapsed time for attention_prob_times_values (80x2048x2048x362): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x362): 65.071

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 521.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x363x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x363x2048): 60.288
Elapsed time for attention_prob_times_values (80x2048x2048x363): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x363): 62.993

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 498.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x364x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x364x2048): 65.168
Elapsed time for attention_prob_times_values (80x2048x2048x364): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x364): 66.071

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 532.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x365x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x365x2048): 62.315
Elapsed time for attention_prob_times_values (80x2048x2048x365): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x365): 63.080

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 509.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x366x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x366x2048): 61.252
Elapsed time for attention_prob_times_values (80x2048x2048x366): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x366): 65.966

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 517.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x367x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x367x2048): 61.216
Elapsed time for attention_prob_times_values (80x2048x2048x367): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x367): 63.116

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 507.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x368x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x368x2048): 66.006
Elapsed time for attention_prob_times_values (80x2048x2048x368): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x368): 80.855

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 595.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x369x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x369x2048): 62.083
Elapsed time for attention_prob_times_values (80x2048x2048x369): 0.0039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x702x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x702x2048): 74.291
Elapsed time for attention_prob_times_values (32x2048x2048x702): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x702): 67.612

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 459.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x703x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x703x2048): 69.847
Elapsed time for attention_prob_times_values (32x2048x2048x703): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x703): 65.263

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 438.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x704x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x704x2048): 86.665
Elapsed time for attention_prob_times_values (32x2048x2048x704): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x704): 78.791

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 536.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x705x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x705x2048): 75.388
Elapsed time for attention_prob_times_values (32x2048x2048x705): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x705): 61.872

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 442.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x706x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x706x2048): 76.270
Elapsed time for attention_prob_times_values (32x2048x2048x706): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x706): 64.536

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 455.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x707x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x707x2048): 75.280
Elapsed time for attention_prob_times_values (32x2048x2048x707): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x707): 59.524

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 433.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x708x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x708x2048): 76.996
Elapsed time for attention_prob_times_values (32x2048x2048x708): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x708): 64.379

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 457.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x709x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x709x2048): 71.851
Elapsed time for attention_prob_times_values (32x2048x2048x709): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x709): 62.001

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 435.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x710x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x710x2048): 75.293
Elapsed time for attention_prob_times_values (32x2048x2048x710): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x710): 64.801

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 456.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x208): 53.764

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 551.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x209x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x209x2048): 66.744
Elapsed time for attention_prob_times_values (160x2048x2048x209): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x209): 53.262

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 542.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x210x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x210x2048): 66.558
Elapsed time for attention_prob_times_values (160x2048x2048x210): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x210): 54.681

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 552.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x211x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x211x2048): 68.016
Elapsed time for attention_prob_times_values (160x2048x2048x211): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x211): 53.617

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 554.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x212x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x212x2048): 68.034
Elapsed time for attention_prob_times_values (160x2048x2048x212): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x212): 56.399

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 572.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x213x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x213x2048): 68.339
Elapsed time for attention_prob_times_values (160x2048x2048x213): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x213): 53.887

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 561.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x214x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x214x2048): 68.101
Elapsed time for attention_prob_times_values (160x2048x2048x214): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x214): 54.367

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 565.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x215x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x215x2048): 69.044
Elapsed time for attention_prob_times_values (160x2048x2048x215): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x215): 52.865

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 562.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x216x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x216x2048): 71.055
Elapsed time for attention_prob_times_values (160x2048x2048x216): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x216): 54.474

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 582.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x217x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x217x2048): 67.623
Elapsed time for attention_prob_times_values (160x2048x2048x217): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x217): 52.723

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 561.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x369): 64.007

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x370x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x370x2048): 63.111
Elapsed time for attention_prob_times_values (80x2048x2048x370): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x370): 66.250

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 531.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x371x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x371x2048): 63.301
Elapsed time for attention_prob_times_values (80x2048x2048x371): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x371): 63.393

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 522.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x372x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x372x2048): 62.483
Elapsed time for attention_prob_times_values (80x2048x2048x372): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x372): 67.168

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 535.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x373x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x373x2048): 62.989
Elapsed time for attention_prob_times_values (80x2048x2048x373): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x373): 64.829

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 529.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x374x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x374x2048): 64.272
Elapsed time for attention_prob_times_values (80x2048x2048x374): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x374): 65.415

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 538.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x375x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x375x2048): 64.228
Elapsed time for attention_prob_times_values (80x2048x2048x375): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x375): 64.856

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 537.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x376x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x376x2048): 62.769
Elapsed time for attention_prob_times_values (80x2048x2048x376): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x376): 83.949

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 599.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x377x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x377x2048): 63.672
Elapsed time for attention_prob_times_values (80x2048x2048x377): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x377): 64.567

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 536.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x378x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x378x2048): 62.221
Elapsed time for attention_prob_times_values (80x2048x2048x378): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x378): 66.565

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 539.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 5688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x711x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x711x2048): 74.543
Elapsed time for attention_prob_times_values (32x2048x2048x711): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x711): 62.277

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 444.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x712x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x712x2048): 76.859
Elapsed time for attention_prob_times_values (32x2048x2048x712): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x712): 83.469

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 525.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x713x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x713x2048): 73.920
Elapsed time for attention_prob_times_values (32x2048x2048x713): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x713): 62.047

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 443.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x714x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x714x2048): 72.163
Elapsed time for attention_prob_times_values (32x2048x2048x714): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x714): 64.972

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 449.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x715x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x715x2048): 69.261
Elapsed time for attention_prob_times_values (32x2048x2048x715): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x715): 60.441

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 425.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x716x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x716x2048): 75.915
Elapsed time for attention_prob_times_values (32x2048x2048x716): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x716): 64.912

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 461.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x717x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x717x2048): 74.375
Elapsed time for attention_prob_times_values (32x2048x2048x717): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x717): 61.865

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 445.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x718x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x718x2048): 75.214
Elapsed time for attention_prob_times_values (32x2048x2048x718): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x718): 63.018

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 453.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x719x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x719x2048): 69.101
Elapsed time for attention_prob_times_values (32x2048x2048x719): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x719): 57.230

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 414.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x720x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x720x2048): 76.505
Elapsed time for attention_prob_times_values (32x2048x2048x720): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x95): 45.565

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 471.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x96x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x96x2048): 64.927
Elapsed time for attention_prob_times_values (384x2048x2048x96): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x96): 47.090

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 545.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x97x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x97x2048): 50.840
Elapsed time for attention_prob_times_values (384x2048x2048x97): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x97): 46.970

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 492.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x98x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x98x2048): 52.504
Elapsed time for attention_prob_times_values (384x2048x2048x98): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x98): 47.703

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 509.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x99x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x99x2048): 50.843
Elapsed time for attention_prob_times_values (384x2048x2048x99): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x99): 47.760

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 506.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x100x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x100x2048): 30.430
Elapsed time for attention_prob_times_values (384x2048x2048x100): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x100): 48.103

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 386.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x101x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x101x2048): 49.526
Elapsed time for attention_prob_times_values (384x2048x2048x101): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x101): 48.706

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 514.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x102x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x102x2048): 49.976
Elapsed time for attention_prob_times_values (384x2048x2048x102): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x102): 49.352

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 524.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x103x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x103x2048): 52.122
Elapsed time for attention_prob_times_values (384x2048x2048x103): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x103): 47.146

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 527.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x104x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x104x2048): 53.674
Elapsed time for attention_prob_times_values (384x2048x2048x104): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x104): 48.154

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 545.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x720): 84.601

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 532.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x721x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x721x2048): 67.044
Elapsed time for attention_prob_times_values (32x2048x2048x721): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x721): 62.598

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 429.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x722x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x722x2048): 72.354
Elapsed time for attention_prob_times_values (32x2048x2048x722): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x722): 64.281

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 452.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x723x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x723x2048): 74.117
Elapsed time for attention_prob_times_values (32x2048x2048x723): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x723): 60.419

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 442.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x724x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x724x2048): 75.449
Elapsed time for attention_prob_times_values (32x2048x2048x724): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x724): 62.516

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 455.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x725x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x725x2048): 74.119
Elapsed time for attention_prob_times_values (32x2048x2048x725): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x725): 62.742

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 452.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x726x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x726x2048): 74.161
Elapsed time for attention_prob_times_values (32x2048x2048x726): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x726): 64.801

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 461.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x727x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x727x2048): 73.207
Elapsed time for attention_prob_times_values (32x2048x2048x727): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x727): 62.853

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 451.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x728x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x728x2048): 76.091
Elapsed time for attention_prob_times_values (32x2048x2048x728): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x728): 85.230

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 537.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x729x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x729x2048): 73.639
Elapsed time for attention_prob_times_values (32x2048x2048x729): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x729): 63.050

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 454.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x379x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x379x2048): 60.352
Elapsed time for attention_prob_times_values (80x2048x2048x379): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x379): 65.568

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 528.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x380x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x380x2048): 64.586
Elapsed time for attention_prob_times_values (80x2048x2048x380): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x380): 66.250

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 550.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x381x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x381x2048): 63.483
Elapsed time for attention_prob_times_values (80x2048x2048x381): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x381): 65.954

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 546.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x382x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x382x2048): 64.536
Elapsed time for attention_prob_times_values (80x2048x2048x382): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x382): 68.456

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 562.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x383x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x383x2048): 63.594
Elapsed time for attention_prob_times_values (80x2048x2048x383): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x383): 65.147

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 545.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x384x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x384x2048): 78.551
Elapsed time for attention_prob_times_values (80x2048x2048x384): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x384): 83.131

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 686.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x385x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x385x2048): 65.705
Elapsed time for attention_prob_times_values (80x2048x2048x385): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x385): 58.530

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 527.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x386x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x386x2048): 66.935
Elapsed time for attention_prob_times_values (80x2048x2048x386): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x386): 61.610

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 547.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x387x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x387x2048): 65.823
Elapsed time for attention_prob_times_values (80x2048x2048x387): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x387): 59.202

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 533.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x388x2048): 0.0039
========================================================================================================================
num_attention_heads: 40, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x218x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x218x2048): 69.876
Elapsed time for attention_prob_times_values (160x2048x2048x218): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x218): 57.452

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 600.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x219x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x219x2048): 67.230
Elapsed time for attention_prob_times_values (160x2048x2048x219): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x219): 54.457

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 574.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x220x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x220x2048): 68.740
Elapsed time for attention_prob_times_values (160x2048x2048x220): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x220): 58.142

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 604.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x221x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x221x2048): 69.504
Elapsed time for attention_prob_times_values (160x2048x2048x221): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x221): 55.621

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 595.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x222x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x222x2048): 67.781
Elapsed time for attention_prob_times_values (160x2048x2048x222): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x222): 53.545

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 578.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x223x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x223x2048): 67.356
Elapsed time for attention_prob_times_values (160x2048x2048x223): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x223): 55.305

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 589.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x224x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x224x2048): 79.750
Elapsed time for attention_prob_times_values (160x2048x2048x224): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x224): 59.155

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 662.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x225x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x225x2048): 66.133
Elapsed time for attention_prob_times_values (160x2048x2048x225): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x225): 56.001

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 593.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x226x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x226x2048): 68.379
Elapsed time for attention_prob_times_values (160x2048x2048x226): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x226): 56.775

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 609.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x227x2048): 0.0046
========================================================================================================================
num_attention_heads: 8, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x730x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x730x2048): 68.527
Elapsed time for attention_prob_times_values (32x2048x2048x730): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x730): 61.770

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 435.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x731x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x731x2048): 73.941
Elapsed time for attention_prob_times_values (32x2048x2048x731): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x731): 63.155

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 457.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x732x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x732x2048): 67.793
Elapsed time for attention_prob_times_values (32x2048x2048x732): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x732): 16.919

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 181.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x733x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x733x2048): 73.924
Elapsed time for attention_prob_times_values (32x2048x2048x733): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x733): 63.298

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 458.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x734x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x734x2048): 71.034
Elapsed time for attention_prob_times_values (32x2048x2048x734): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x734): 61.320

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 443.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x735x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x735x2048): 70.826
Elapsed time for attention_prob_times_values (32x2048x2048x735): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x735): 61.536

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 444.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x736x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x736x2048): 84.696
Elapsed time for attention_prob_times_values (32x2048x2048x736): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x736): 87.182

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 579.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x737x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x737x2048): 75.642
Elapsed time for attention_prob_times_values (32x2048x2048x737): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x737): 63.712

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 467.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x738x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x738x2048): 76.154
Elapsed time for attention_prob_times_values (32x2048x2048x738): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x738): 65.628

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 476.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x739x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x739x2048): 72.833
Elapsed time for attention_prob_times_values (32x2048x2048x739): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x739): 62.246

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 454.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x740x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x740x2048): 72.632
Elapsed time for attention_prob_times_values (32x2048x2048x740): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x740): 65.853

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 468.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x741x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x741x2048): 75.515
Elapsed time for attention_prob_times_values (32x2048x2048x741): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x741): 63.798

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 469.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x742x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x742x2048): 76.224
Elapsed time for attention_prob_times_values (32x2048x2048x742): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x742): 65.344

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 478.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x743x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x743x2048): 69.509
Elapsed time for attention_prob_times_values (32x2048x2048x743): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x743): 64.157

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 454.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x744x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x744x2048): 70.434
Elapsed time for attention_prob_times_values (32x2048x2048x744): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x744): 72.936

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 488.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x745x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x745x2048): 74.566
Elapsed time for attention_prob_times_values (32x2048x2048x745): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x745): 64.020

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 469.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x746x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x746x2048): 71.258
Elapsed time for attention_prob_times_values (32x2048x2048x746): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x746): 66.198

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 468.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x747x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x747x2048): 74.713
Elapsed time for attention_prob_times_values (32x2048x2048x747): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x747): 64.109

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 471.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x748x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x748x2048): 74.608
Elapsed time for attention_prob_times_values (32x2048x2048x748): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x748): 66.744

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 482.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x388x2048): 67.195
Elapsed time for attention_prob_times_values (80x2048x2048x388): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x388): 62.536

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 555.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x389x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x389x2048): 66.048
Elapsed time for attention_prob_times_values (80x2048x2048x389): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x389): 59.199

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 536.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x390x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x390x2048): 64.631
Elapsed time for attention_prob_times_values (80x2048x2048x390): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x390): 61.915

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 544.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x391x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x391x2048): 64.957
Elapsed time for attention_prob_times_values (80x2048x2048x391): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x391): 58.616

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 532.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x392x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x392x2048): 65.854
Elapsed time for attention_prob_times_values (80x2048x2048x392): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x392): 73.076

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 599.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x393x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x393x2048): 64.785
Elapsed time for attention_prob_times_values (80x2048x2048x393): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x393): 58.868

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 535.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x394x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x394x2048): 65.785
Elapsed time for attention_prob_times_values (80x2048x2048x394): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x394): 60.204

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 546.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x395x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x395x2048): 65.168
Elapsed time for attention_prob_times_values (80x2048x2048x395): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x395): 59.686

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 542.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x396x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x396x2048): 66.265
Elapsed time for attention_prob_times_values (80x2048x2048x396): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x396): 61.599

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 557.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x397x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x397x2048): 63.427
Elapsed time for attention_prob_times_values (80x2048x2048x397): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x397): 59.531

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 537.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 5992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x749x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x749x2048): 72.667
Elapsed time for attention_prob_times_values (32x2048x2048x749): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x749): 60.564

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 452.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x750x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x750x2048): 75.530
Elapsed time for attention_prob_times_values (32x2048x2048x750): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x750): 66.490

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 485.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x751x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x751x2048): 74.669
Elapsed time for attention_prob_times_values (32x2048x2048x751): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x751): 64.541

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 475.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x752x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x752x2048): 77.013
Elapsed time for attention_prob_times_values (32x2048x2048x752): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x752): 88.361

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 565.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x753x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x753x2048): 70.922
Elapsed time for attention_prob_times_values (32x2048x2048x753): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x753): 64.757

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 465.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x754x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x754x2048): 74.409
Elapsed time for attention_prob_times_values (32x2048x2048x754): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x754): 66.604

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 484.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x755x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x755x2048): 71.801
Elapsed time for attention_prob_times_values (32x2048x2048x755): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x755): 64.692

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 469.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x756x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x756x2048): 68.176
Elapsed time for attention_prob_times_values (32x2048x2048x756): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x756): 67.096

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 467.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x757x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x757x2048): 74.424
Elapsed time for attention_prob_times_values (32x2048x2048x757): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x757): 65.052

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 479.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x758x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x227x2048): 66.857
Elapsed time for attention_prob_times_values (160x2048x2048x227): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x227): 56.063

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 601.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x228x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x228x2048): 67.847
Elapsed time for attention_prob_times_values (160x2048x2048x228): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x228): 58.761

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 623.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x229x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x229x2048): 69.035
Elapsed time for attention_prob_times_values (160x2048x2048x229): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x229): 57.459

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 623.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x230x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x230x2048): 66.433
Elapsed time for attention_prob_times_values (160x2048x2048x230): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x230): 59.863

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 628.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x231x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x231x2048): 67.921
Elapsed time for attention_prob_times_values (160x2048x2048x231): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x231): 57.794

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 625.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x232x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x232x2048): 68.347
Elapsed time for attention_prob_times_values (160x2048x2048x232): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x232): 58.564

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 634.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x233x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x233x2048): 66.847
Elapsed time for attention_prob_times_values (160x2048x2048x233): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x233): 57.931

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 627.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x234x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x234x2048): 70.316
Elapsed time for attention_prob_times_values (160x2048x2048x234): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x234): 59.780

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 655.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x235x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x235x2048): 68.567
Elapsed time for attention_prob_times_values (160x2048x2048x235): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x235): 58.315

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 641.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x236x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x236x2048): 70.872
Elapsed time for attention_prob_times_values (160x2048x2048x236): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x236): 61.314

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 671.858
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x758x2048): 72.924
Elapsed time for attention_prob_times_values (32x2048x2048x758): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x758): 63.822

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 471.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x759x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x759x2048): 74.536
Elapsed time for attention_prob_times_values (32x2048x2048x759): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x759): 64.656

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 479.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x760x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x760x2048): 76.047
Elapsed time for attention_prob_times_values (32x2048x2048x760): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x760): 88.644

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 567.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x761x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x761x2048): 73.188
Elapsed time for attention_prob_times_values (32x2048x2048x761): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x761): 63.327

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 471.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x762x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x762x2048): 71.524
Elapsed time for attention_prob_times_values (32x2048x2048x762): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x762): 67.073

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 481.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x763x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x763x2048): 73.936
Elapsed time for attention_prob_times_values (32x2048x2048x763): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x763): 64.952

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 481.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x764x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x764x2048): 75.211
Elapsed time for attention_prob_times_values (32x2048x2048x764): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x764): 67.213

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 494.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x765x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x765x2048): 70.145
Elapsed time for attention_prob_times_values (32x2048x2048x765): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x765): 65.039

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 470.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x766x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x766x2048): 73.293
Elapsed time for attention_prob_times_values (32x2048x2048x766): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x766): 67.163

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 489.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x767x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x767x2048): 73.790
Elapsed time for attention_prob_times_values (32x2048x2048x767): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x767): 64.177

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 480.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x398x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x398x2048): 66.355
Elapsed time for attention_prob_times_values (80x2048x2048x398): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x398): 63.750

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 570.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x399x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x399x2048): 65.482
Elapsed time for attention_prob_times_values (80x2048x2048x399): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x399): 58.576

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 543.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x400x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x400x2048): 67.711
Elapsed time for attention_prob_times_values (80x2048x2048x400): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x400): 75.941

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 630.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x401x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x401x2048): 63.131
Elapsed time for attention_prob_times_values (80x2048x2048x401): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x401): 59.132

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 539.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x402x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x402x2048): 63.763
Elapsed time for attention_prob_times_values (80x2048x2048x402): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x402): 61.540

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 554.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x403x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x403x2048): 65.312
Elapsed time for attention_prob_times_values (80x2048x2048x403): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x403): 61.108

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 560.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x404x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x404x2048): 66.380
Elapsed time for attention_prob_times_values (80x2048x2048x404): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x404): 60.652

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 563.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x405x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x405x2048): 64.079
Elapsed time for attention_prob_times_values (80x2048x2048x405): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x405): 61.388

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 558.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x406x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x406x2048): 66.368
Elapsed time for attention_prob_times_values (80x2048x2048x406): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x406): 62.260

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 573.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 96, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x105x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x105x2048): 53.130
Elapsed time for attention_prob_times_values (384x2048x2048x105): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x105): 49.947

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 558.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x106x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x106x2048): 54.318
Elapsed time for attention_prob_times_values (384x2048x2048x106): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x106): 51.542

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 578.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x107x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x107x2048): 53.872
Elapsed time for attention_prob_times_values (384x2048x2048x107): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x107): 49.092

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 566.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x108x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x108x2048): 54.021
Elapsed time for attention_prob_times_values (384x2048x2048x108): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x108): 51.624

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 587.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x109x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x109x2048): 52.341
Elapsed time for attention_prob_times_values (384x2048x2048x109): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x109): 51.204

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 580.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x110x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x110x2048): 55.702
Elapsed time for attention_prob_times_values (384x2048x2048x110): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x110): 53.383

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 616.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x111x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x111x2048): 55.278
Elapsed time for attention_prob_times_values (384x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x111): 50.549

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 602.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x112x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x112x2048): 55.368
Elapsed time for attention_prob_times_values (384x2048x2048x112): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x112): 52.830

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 621.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x113x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x113x2048): 54.853
Elapsed time for attention_prob_times_values (384x2048x2048x113): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x113): 51.910

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 618.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x114x2048): 0.0068
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x768x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x768x2048): 84.327
Elapsed time for attention_prob_times_values (32x2048x2048x768): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x768): 93.268

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 620.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x769x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x769x2048): 75.406
Elapsed time for attention_prob_times_values (32x2048x2048x769): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x769): 60.118

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 468.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x770x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x770x2048): 72.035
Elapsed time for attention_prob_times_values (32x2048x2048x770): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x770): 63.299

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 472.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x771x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x771x2048): 72.399
Elapsed time for attention_prob_times_values (32x2048x2048x771): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x771): 60.488

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 462.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x772x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x772x2048): 76.934
Elapsed time for attention_prob_times_values (32x2048x2048x772): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x772): 63.510

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 489.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x773x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x773x2048): 75.766
Elapsed time for attention_prob_times_values (32x2048x2048x773): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x773): 60.768

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 474.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x774x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x774x2048): 76.530
Elapsed time for attention_prob_times_values (32x2048x2048x774): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x774): 63.628

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 489.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x775x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x775x2048): 75.508
Elapsed time for attention_prob_times_values (32x2048x2048x775): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x775): 60.857

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 475.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x776x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x776x2048): 71.574
Elapsed time for attention_prob_times_values (32x2048x2048x776): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x776): 79.775

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 532.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x407x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x407x2048): 62.393
Elapsed time for attention_prob_times_values (80x2048x2048x407): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x407): 59.904

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 547.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x408x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x408x2048): 66.948
Elapsed time for attention_prob_times_values (80x2048x2048x408): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x408): 77.774

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 645.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x409x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x409x2048): 63.408
Elapsed time for attention_prob_times_values (80x2048x2048x409): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x409): 61.366

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 560.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x410x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x410x2048): 64.174
Elapsed time for attention_prob_times_values (80x2048x2048x410): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x410): 65.149

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 582.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x411x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x411x2048): 65.444
Elapsed time for attention_prob_times_values (80x2048x2048x411): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x411): 61.960

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 574.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x412x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x412x2048): 66.617
Elapsed time for attention_prob_times_values (80x2048x2048x412): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x412): 65.760

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 598.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x413x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x413x2048): 65.478
Elapsed time for attention_prob_times_values (80x2048x2048x413): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x413): 59.341

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 564.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x414x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x414x2048): 65.937
Elapsed time for attention_prob_times_values (80x2048x2048x414): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x414): 59.276

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 567.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x415x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x415x2048): 65.423
Elapsed time for attention_prob_times_values (80x2048x2048x415): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x415): 62.667

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 582.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x416x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x416x2048): 80.665
Elapsed time for attention_prob_times_values (80x2048x2048x416): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x416): 77.015

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 719.030
Elapsed time for attention_key_query_prob (32x2048x777x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x777x2048): 74.848
Elapsed time for attention_prob_times_values (32x2048x2048x777): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x777): 58.843

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 465.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x778x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x778x2048): 75.454
Elapsed time for attention_prob_times_values (32x2048x2048x778): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x778): 58.334

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 465.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x779x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x779x2048): 75.099
Elapsed time for attention_prob_times_values (32x2048x2048x779): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x779): 60.942

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 476.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x780x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x780x2048): 76.114
Elapsed time for attention_prob_times_values (32x2048x2048x780): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x780): 64.149

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 493.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x781x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x781x2048): 75.233
Elapsed time for attention_prob_times_values (32x2048x2048x781): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x781): 61.052

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 478.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x782x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x782x2048): 75.478
Elapsed time for attention_prob_times_values (32x2048x2048x782): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x782): 63.857

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 491.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x783x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x783x2048): 75.207
Elapsed time for attention_prob_times_values (32x2048x2048x783): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x783): 61.200

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 480.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x784x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x784x2048): 77.126
Elapsed time for attention_prob_times_values (32x2048x2048x784): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x784): 80.789

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 562.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x785x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x785x2048): 74.497
Elapsed time for attention_prob_times_values (32x2048x2048x785): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x785): 57.941

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 464.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x786x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x786x2048): 68.777
Elapsed time for attention_prob_times_values (32x2048x2048x786): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x786): 64.318

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 474.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x237x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x237x2048): 68.847
Elapsed time for attention_prob_times_values (160x2048x2048x237): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x237): 58.690

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 649.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x238x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x238x2048): 70.924
Elapsed time for attention_prob_times_values (160x2048x2048x238): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x238): 61.381

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 677.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x239x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x239x2048): 69.821
Elapsed time for attention_prob_times_values (160x2048x2048x239): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x239): 59.122

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 661.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x240x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x240x2048): 55.761
Elapsed time for attention_prob_times_values (160x2048x2048x240): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x240): 59.522

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 597.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x241x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x241x2048): 52.153
Elapsed time for attention_prob_times_values (160x2048x2048x241): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x241): 59.641

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 579.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x242x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x242x2048): 54.040
Elapsed time for attention_prob_times_values (160x2048x2048x242): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x242): 59.909

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 593.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x243x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x243x2048): 53.018
Elapsed time for attention_prob_times_values (160x2048x2048x243): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x243): 60.028

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 590.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x244x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x244x2048): 54.874
Elapsed time for attention_prob_times_values (160x2048x2048x244): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x244): 62.016

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 613.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x245x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x245x2048): 53.727
Elapsed time for attention_prob_times_values (160x2048x2048x245): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x245): 59.740

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 598.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x787x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x787x2048): 74.819
Elapsed time for attention_prob_times_values (32x2048x2048x787): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x787): 61.397

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 482.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x788x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x788x2048): 67.376
Elapsed time for attention_prob_times_values (32x2048x2048x788): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x788): 64.562

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 471.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x789x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x789x2048): 74.902
Elapsed time for attention_prob_times_values (32x2048x2048x789): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x789): 61.451

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 483.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x790x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x790x2048): 75.468
Elapsed time for attention_prob_times_values (32x2048x2048x790): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x790): 58.188

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 471.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x791x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x791x2048): 71.516
Elapsed time for attention_prob_times_values (32x2048x2048x791): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x791): 59.203

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 465.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x792x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x792x2048): 76.634
Elapsed time for attention_prob_times_values (32x2048x2048x792): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x792): 81.331

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 567.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x793x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x793x2048): 71.772
Elapsed time for attention_prob_times_values (32x2048x2048x793): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x793): 60.696

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 473.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x794x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x794x2048): 73.174
Elapsed time for attention_prob_times_values (32x2048x2048x794): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x794): 63.627

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 490.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x795x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x795x2048): 74.610
Elapsed time for attention_prob_times_values (32x2048x2048x795): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x795): 60.275

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 480.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x417x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x417x2048): 66.486
Elapsed time for attention_prob_times_values (80x2048x2048x417): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x417): 61.692

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 585.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x418x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x418x2048): 69.508
Elapsed time for attention_prob_times_values (80x2048x2048x418): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x418): 66.312

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 621.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x419x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x419x2048): 67.513
Elapsed time for attention_prob_times_values (80x2048x2048x419): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x419): 63.356

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 600.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x420x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x420x2048): 67.786
Elapsed time for attention_prob_times_values (80x2048x2048x420): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x420): 66.768

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 619.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x421x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x421x2048): 68.605
Elapsed time for attention_prob_times_values (80x2048x2048x421): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x421): 60.935

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 595.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x422x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x422x2048): 64.342
Elapsed time for attention_prob_times_values (80x2048x2048x422): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x422): 63.381

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 590.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x423x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x423x2048): 68.239
Elapsed time for attention_prob_times_values (80x2048x2048x423): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x423): 63.047

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 607.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x424x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x424x2048): 67.691
Elapsed time for attention_prob_times_values (80x2048x2048x424): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x424): 77.522

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 670.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x425x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x425x2048): 61.121
Elapsed time for attention_prob_times_values (80x2048x2048x425): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x425): 63.794

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 580.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x796x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x796x2048): 76.105
Elapsed time for attention_prob_times_values (32x2048x2048x796): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x796): 65.085

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 506.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x797x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x797x2048): 70.813
Elapsed time for attention_prob_times_values (32x2048x2048x797): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x797): 59.819

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 468.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x798x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x798x2048): 72.126
Elapsed time for attention_prob_times_values (32x2048x2048x798): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x798): 64.960

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 494.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x799x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x799x2048): 69.606
Elapsed time for attention_prob_times_values (32x2048x2048x799): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x799): 58.920

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 462.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x800x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x800x2048): 88.680
Elapsed time for attention_prob_times_values (32x2048x2048x800): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x800): 82.863

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 621.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x801x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x801x2048): 76.936
Elapsed time for attention_prob_times_values (32x2048x2048x801): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x801): 62.624

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 501.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x802x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x802x2048): 77.615
Elapsed time for attention_prob_times_values (32x2048x2048x802): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x802): 63.176

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 506.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x803x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x803x2048): 73.153
Elapsed time for attention_prob_times_values (32x2048x2048x803): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x803): 62.632

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 490.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x804x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x804x2048): 73.884
Elapsed time for attention_prob_times_values (32x2048x2048x804): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x804): 65.620

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 506.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x805x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x805x2048): 74.163
Elapsed time for attention_prob_times_values (32x2048x2048x805): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x805): 62.727

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 495.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x806x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x806x2048): 77.403
Elapsed time for attention_prob_times_values (32x2048x2048x806): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x806): 65.206

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 516.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x807x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x807x2048): 73.834
Elapsed time for attention_prob_times_values (32x2048x2048x807): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x807): 62.744

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 495.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x808x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x808x2048): 78.068
Elapsed time for attention_prob_times_values (32x2048x2048x808): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x808): 83.042

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 588.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x809x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x809x2048): 75.740
Elapsed time for attention_prob_times_values (32x2048x2048x809): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x809): 62.748

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 502.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x810x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x810x2048): 76.399
Elapsed time for attention_prob_times_values (32x2048x2048x810): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x810): 65.634

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 517.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x811x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x811x2048): 74.943
Elapsed time for attention_prob_times_values (32x2048x2048x811): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x811): 61.771

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 496.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x812x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x812x2048): 71.241
Elapsed time for attention_prob_times_values (32x2048x2048x812): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x812): 66.095

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 503.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x813x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x813x2048): 74.034
Elapsed time for attention_prob_times_values (32x2048x2048x813): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x813): 63.246

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 501.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x814x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x814x2048): 77.061
Elapsed time for attention_prob_times_values (32x2048x2048x814): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x814): 77.701

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 569.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x246x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x246x2048): 54.625
Elapsed time for attention_prob_times_values (160x2048x2048x246): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x246): 59.961

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 606.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x247x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x247x2048): 53.987
Elapsed time for attention_prob_times_values (160x2048x2048x247): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x247): 60.062

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 605.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x248x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x248x2048): 55.822
Elapsed time for attention_prob_times_values (160x2048x2048x248): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x248): 62.039

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 628.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x249x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x249x2048): 51.819
Elapsed time for attention_prob_times_values (160x2048x2048x249): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x249): 59.720

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 595.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x250x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x250x2048): 45.462
Elapsed time for attention_prob_times_values (160x2048x2048x250): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x250): 63.832

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 571.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x251x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x251x2048): 52.943
Elapsed time for attention_prob_times_values (160x2048x2048x251): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x251): 62.137

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 617.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x252x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x252x2048): 54.181
Elapsed time for attention_prob_times_values (160x2048x2048x252): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x252): 64.157

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 637.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x253x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x253x2048): 52.432
Elapsed time for attention_prob_times_values (160x2048x2048x253): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x253): 56.357

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 591.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x254x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x254x2048): 54.737
Elapsed time for attention_prob_times_values (160x2048x2048x254): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x254): 60.826

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 629.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x255x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x255x2048): 53.069
Elapsed time for attention_prob_times_values (160x2048x2048x255): 0.0052
num_attention_heads: 20, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x426x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x426x2048): 64.648
Elapsed time for attention_prob_times_values (80x2048x2048x426): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x426): 67.078

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 613.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x427x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x427x2048): 67.416
Elapsed time for attention_prob_times_values (80x2048x2048x427): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x427): 64.187

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 614.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x428x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x428x2048): 67.164
Elapsed time for attention_prob_times_values (80x2048x2048x428): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x428): 66.208

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 624.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x429x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x429x2048): 66.818
Elapsed time for attention_prob_times_values (80x2048x2048x429): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x429): 61.700

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 601.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x430x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x430x2048): 65.587
Elapsed time for attention_prob_times_values (80x2048x2048x430): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x430): 63.921

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 608.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x431x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x431x2048): 64.885
Elapsed time for attention_prob_times_values (80x2048x2048x431): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x431): 63.460

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 604.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x432x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x432x2048): 69.611
Elapsed time for attention_prob_times_values (80x2048x2048x432): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x432): 81.418

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 708.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x433x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x433x2048): 67.147
Elapsed time for attention_prob_times_values (80x2048x2048x433): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x433): 65.235

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 625.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x434x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x434x2048): 65.250
Elapsed time for attention_prob_times_values (80x2048x2048x434): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x434): 68.491

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 633.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x435x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x435x2048): 67.298
Elapsed time for attention_prob_times_values (80x2048x2048x435): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x114x2048): 54.112
Elapsed time for attention_prob_times_values (384x2048x2048x114): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x114): 54.777

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 636.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x115x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x115x2048): 55.710
Elapsed time for attention_prob_times_values (384x2048x2048x115): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x115): 52.470

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 636.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x116x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x116x2048): 57.012
Elapsed time for attention_prob_times_values (384x2048x2048x116): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x116): 54.820

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 663.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x117x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x117x2048): 55.990
Elapsed time for attention_prob_times_values (384x2048x2048x117): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x117): 52.480

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 648.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x118x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x118x2048): 57.441
Elapsed time for attention_prob_times_values (384x2048x2048x118): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x118): 54.888

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 677.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x119x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x119x2048): 57.103
Elapsed time for attention_prob_times_values (384x2048x2048x119): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x119): 53.529

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 671.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x120x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x120x2048): 57.891
Elapsed time for attention_prob_times_values (384x2048x2048x120): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x120): 56.413

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 699.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x121x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x121x2048): 53.824
Elapsed time for attention_prob_times_values (384x2048x2048x121): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x121): 43.242

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 591.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x122x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x122x2048): 57.531
Elapsed time for attention_prob_times_values (384x2048x2048x122): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x122): 56.884

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 711.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x123x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x123x2048): 55.982
Elapsed time for attention_prob_times_values (384x2048x2048x123): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x123): 43.804

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 615.908
MLP duration (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x815x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x815x2048): 76.391
Elapsed time for attention_prob_times_values (32x2048x2048x815): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x815): 75.300

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 558.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x816x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x816x2048): 76.301
Elapsed time for attention_prob_times_values (32x2048x2048x816): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x816): 83.895

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 589.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x817x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x817x2048): 76.110
Elapsed time for attention_prob_times_values (32x2048x2048x817): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x817): 75.504

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 559.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x818x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x818x2048): 76.723
Elapsed time for attention_prob_times_values (32x2048x2048x818): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x818): 78.043

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 571.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x819x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x819x2048): 75.925
Elapsed time for attention_prob_times_values (32x2048x2048x819): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x819): 75.577

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 560.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x820x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x820x2048): 75.638
Elapsed time for attention_prob_times_values (32x2048x2048x820): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x820): 75.719

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 560.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x821x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x821x2048): 78.230
Elapsed time for attention_prob_times_values (32x2048x2048x821): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x821): 75.501

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 569.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x822x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x822x2048): 77.118
Elapsed time for attention_prob_times_values (32x2048x2048x822): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x822): 78.219

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 576.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x823x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x823x2048): 76.346
Elapsed time for attention_prob_times_values (32x2048x2048x823): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x823): 68.033

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 534.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x824x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x824x2048): 78.034
Elapsed time for attention_prob_times_values (32x2048x2048x824): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x824): 82.203

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 595.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x825x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x825x2048): 75.469
Elapsed time for attention_prob_times_values (32x2048x2048x825): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x825): 76.199

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 564.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x826x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x826x2048): 76.005
Elapsed time for attention_prob_times_values (32x2048x2048x826): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x826): 76.317

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 567.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x827x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x827x2048): 75.529
Elapsed time for attention_prob_times_values (32x2048x2048x827): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x827): 76.409

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 566.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x828x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x828x2048): 77.128
Elapsed time for attention_prob_times_values (32x2048x2048x828): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x828): 78.957

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 582.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x829x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x829x2048): 75.681
Elapsed time for attention_prob_times_values (32x2048x2048x829): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x829): 76.764

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 569.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x830x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x830x2048): 76.323
Elapsed time for attention_prob_times_values (32x2048x2048x830): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x830): 78.754

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 580.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x831x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x831x2048): 75.661
Elapsed time for attention_prob_times_values (32x2048x2048x831): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x831): 75.600

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 566.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x832x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x832x2048): 88.876
Elapsed time for attention_prob_times_values (32x2048x2048x832): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x832): 86.544

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 657.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x833x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x833x2048): 77.223
Elapsed time for attention_prob_times_values (32x2048x2048x833): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x833): 77.052

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 579.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x435): 65.417

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 630.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x436x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x436x2048): 69.066
Elapsed time for attention_prob_times_values (80x2048x2048x436): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x436): 67.154

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 647.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x437x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x437x2048): 63.988
Elapsed time for attention_prob_times_values (80x2048x2048x437): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x437): 63.287

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 606.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x438x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x438x2048): 68.431
Elapsed time for attention_prob_times_values (80x2048x2048x438): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x438): 66.031

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 642.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x439x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x439x2048): 66.146
Elapsed time for attention_prob_times_values (80x2048x2048x439): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x439): 66.242

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 633.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x440x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x440x2048): 66.137
Elapsed time for attention_prob_times_values (80x2048x2048x440): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x440): 81.207

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 699.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x441x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x441x2048): 64.864
Elapsed time for attention_prob_times_values (80x2048x2048x441): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x441): 63.742

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 618.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x442x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x442x2048): 67.799
Elapsed time for attention_prob_times_values (80x2048x2048x442): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x442): 69.535

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 661.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x443x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x443x2048): 65.535
Elapsed time for attention_prob_times_values (80x2048x2048x443): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x443): 65.360

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 631.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x444x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x444x2048): 68.724
Elapsed time for attention_prob_times_values (80x2048x2048x444): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x444): 68.588

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 664.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x255): 65.895

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 644.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x256x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x256x2048): 72.782
Elapsed time for attention_prob_times_values (160x2048x2048x256): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x256): 67.093

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 768.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x257x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x257x2048): 55.575
Elapsed time for attention_prob_times_values (160x2048x2048x257): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x257): 53.232

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 600.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x258x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x258x2048): 56.123
Elapsed time for attention_prob_times_values (160x2048x2048x258): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x258): 55.300

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 617.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x259x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x259x2048): 56.775
Elapsed time for attention_prob_times_values (160x2048x2048x259): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x259): 53.984

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 615.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x260x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x260x2048): 56.635
Elapsed time for attention_prob_times_values (160x2048x2048x260): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x260): 56.676

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 632.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x261x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x261x2048): 54.110
Elapsed time for attention_prob_times_values (160x2048x2048x261): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x261): 54.388

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 607.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x262x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x262x2048): 57.268
Elapsed time for attention_prob_times_values (160x2048x2048x262): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x262): 56.969

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 641.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x263x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x263x2048): 54.980
Elapsed time for attention_prob_times_values (160x2048x2048x263): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x263): 54.901

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 619.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x264x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x264x2048): 57.225
Elapsed time for attention_prob_times_values (160x2048x2048x264): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x264): 52.143

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 617.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x834x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x834x2048): 77.990
Elapsed time for attention_prob_times_values (32x2048x2048x834): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x834): 79.381

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 591.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x835x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x835x2048): 75.126
Elapsed time for attention_prob_times_values (32x2048x2048x835): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x835): 77.091

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 572.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x836x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x836x2048): 78.871
Elapsed time for attention_prob_times_values (32x2048x2048x836): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x836): 79.686

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 597.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x837x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x837x2048): 76.697
Elapsed time for attention_prob_times_values (32x2048x2048x837): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x837): 77.250

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 580.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x838x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x838x2048): 75.571
Elapsed time for attention_prob_times_values (32x2048x2048x838): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x838): 79.688

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 585.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x839x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x839x2048): 76.818
Elapsed time for attention_prob_times_values (32x2048x2048x839): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x839): 76.470

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 579.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x840x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x840x2048): 77.597
Elapsed time for attention_prob_times_values (32x2048x2048x840): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x840): 85.134

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 614.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x841x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x841x2048): 75.878
Elapsed time for attention_prob_times_values (32x2048x2048x841): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x841): 78.035

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 582.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x842x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x842x2048): 76.942
Elapsed time for attention_prob_times_values (32x2048x2048x842): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x842): 79.638

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 593.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x843x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x843x2048): 76.297
========================================================================================================================
num_attention_heads: 20, hidden_size: 8900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x445x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x445x2048): 65.126
Elapsed time for attention_prob_times_values (80x2048x2048x445): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x445): 67.007

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 640.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x446x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x446x2048): 65.360
Elapsed time for attention_prob_times_values (80x2048x2048x446): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x446): 70.098

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 656.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x447x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x447x2048): 65.144
Elapsed time for attention_prob_times_values (80x2048x2048x447): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x447): 67.100

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 643.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x448x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x448x2048): 82.457
Elapsed time for attention_prob_times_values (80x2048x2048x448): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x448): 17.960

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 287.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x449x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x449x2048): 70.134
Elapsed time for attention_prob_times_values (80x2048x2048x449): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x449): 60.612

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 635.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x450x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x450x2048): 71.372
Elapsed time for attention_prob_times_values (80x2048x2048x450): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x450): 61.199

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 645.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x451x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x451x2048): 70.279
Elapsed time for attention_prob_times_values (80x2048x2048x451): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x451): 61.528

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 643.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x452x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x452x2048): 71.916
Elapsed time for attention_prob_times_values (80x2048x2048x452): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x452): 63.287

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 661.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x453x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x453x2048): 68.457
Elapsed time for attention_prob_times_values (80x2048x2048x453): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x453): 60.471

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 632.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x454x2048): 0.0044
Elapsed time for attention_prob_times_values (32x2048x2048x843): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x843): 77.983

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 585.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x844x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x844x2048): 72.861
Elapsed time for attention_prob_times_values (32x2048x2048x844): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x844): 74.031

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 557.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x845x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x845x2048): 76.441
Elapsed time for attention_prob_times_values (32x2048x2048x845): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x845): 77.976

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 586.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x846x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x846x2048): 77.133
Elapsed time for attention_prob_times_values (32x2048x2048x846): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x846): 76.421

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 584.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x847x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x847x2048): 76.419
Elapsed time for attention_prob_times_values (32x2048x2048x847): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x847): 78.109

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 588.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x848x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x848x2048): 78.665
Elapsed time for attention_prob_times_values (32x2048x2048x848): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x848): 84.875

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 622.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x849x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x849x2048): 75.943
Elapsed time for attention_prob_times_values (32x2048x2048x849): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x849): 78.564

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 589.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x850x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x850x2048): 76.580
Elapsed time for attention_prob_times_values (32x2048x2048x850): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x850): 80.638

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 600.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x851x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x851x2048): 73.208
Elapsed time for attention_prob_times_values (32x2048x2048x851): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x851): 78.898

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 580.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x852x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x852x2048): 77.404
Elapsed time for attention_prob_times_values (32x2048x2048x852): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x852): 80.685

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 604.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x853x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x853x2048): 76.107
Elapsed time for attention_prob_times_values (32x2048x2048x853): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x853): 76.942

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 586.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x854x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x854x2048): 76.928
Elapsed time for attention_prob_times_values (32x2048x2048x854): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x854): 77.554

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 592.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x855x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x855x2048): 76.291
Elapsed time for attention_prob_times_values (32x2048x2048x855): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x855): 78.983

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 596.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x856x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x856x2048): 78.234
Elapsed time for attention_prob_times_values (32x2048x2048x856): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x856): 87.385

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 634.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x857x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x857x2048): 72.093
Elapsed time for attention_prob_times_values (32x2048x2048x857): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x857): 78.313

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 577.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x858x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x858x2048): 76.602
Elapsed time for attention_prob_times_values (32x2048x2048x858): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x858): 81.315

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 607.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x859x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x859x2048): 75.665
Elapsed time for attention_prob_times_values (32x2048x2048x859): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x859): 78.147

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 592.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x860x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x860x2048): 77.493
Elapsed time for attention_prob_times_values (32x2048x2048x860): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x860): 76.914

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 595.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x861x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x861x2048): 75.829
Elapsed time for attention_prob_times_values (32x2048x2048x861): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x861): 79.426

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 599.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x862x2048): 0.0030
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x265x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x265x2048): 54.667
Elapsed time for attention_prob_times_values (160x2048x2048x265): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x265): 54.874

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 621.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x266x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x266x2048): 54.472
Elapsed time for attention_prob_times_values (160x2048x2048x266): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x266): 55.827

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 628.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x267x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x267x2048): 53.656
Elapsed time for attention_prob_times_values (160x2048x2048x267): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x267): 55.368

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 622.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x268x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x268x2048): 56.688
Elapsed time for attention_prob_times_values (160x2048x2048x268): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x268): 57.825

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 656.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x269x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x269x2048): 54.324
Elapsed time for attention_prob_times_values (160x2048x2048x269): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x269): 55.699

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 632.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x270x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x270x2048): 55.095
Elapsed time for attention_prob_times_values (160x2048x2048x270): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x270): 57.459

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 649.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x271x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x271x2048): 54.034
Elapsed time for attention_prob_times_values (160x2048x2048x271): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x271): 54.614

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 629.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x272x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x272x2048): 57.931
Elapsed time for attention_prob_times_values (160x2048x2048x272): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x272): 72.378

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 748.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x273x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x273x2048): 54.580
Elapsed time for attention_prob_times_values (160x2048x2048x273): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x273): 55.752

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 643.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x124x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x124x2048): 56.988
Elapsed time for attention_prob_times_values (384x2048x2048x124): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x124): 57.050

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 719.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x125x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x125x2048): 57.497
Elapsed time for attention_prob_times_values (384x2048x2048x125): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x125): 43.741

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 631.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x126x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x126x2048): 56.252
Elapsed time for attention_prob_times_values (384x2048x2048x126): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x126): 56.600

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 722.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x127x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x127x2048): 55.505
Elapsed time for attention_prob_times_values (384x2048x2048x127): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x127): 43.732

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 631.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x128x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x128x2048): 69.021
Elapsed time for attention_prob_times_values (384x2048x2048x128): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x128): 61.256

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 843.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x129x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x129x2048): 56.002
Elapsed time for attention_prob_times_values (384x2048x2048x129): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x129): 43.477

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 640.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x130x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x130x2048): 58.286
Elapsed time for attention_prob_times_values (384x2048x2048x130): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x130): 45.785

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 676.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x131x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x131x2048): 57.829
Elapsed time for attention_prob_times_values (384x2048x2048x131): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x131): 44.919

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 671.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x132x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x132x2048): 58.836
Elapsed time for attention_prob_times_values (384x2048x2048x132): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x132): 46.229

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 692.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x454x2048): 68.923
Elapsed time for attention_prob_times_values (80x2048x2048x454): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x454): 62.951

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 649.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x455x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x455x2048): 69.843
Elapsed time for attention_prob_times_values (80x2048x2048x455): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x455): 61.715

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 647.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x456x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x456x2048): 71.514
Elapsed time for attention_prob_times_values (80x2048x2048x456): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x456): 82.521

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 759.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x457x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x457x2048): 68.583
Elapsed time for attention_prob_times_values (80x2048x2048x457): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x457): 59.852

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 634.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x458x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x458x2048): 66.585
Elapsed time for attention_prob_times_values (80x2048x2048x458): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x458): 63.883

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 648.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x459x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x459x2048): 69.160
Elapsed time for attention_prob_times_values (80x2048x2048x459): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x459): 60.243

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 641.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x460x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x460x2048): 70.784
Elapsed time for attention_prob_times_values (80x2048x2048x460): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x460): 65.552

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 679.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x461x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x461x2048): 66.807
Elapsed time for attention_prob_times_values (80x2048x2048x461): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x461): 62.469

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 645.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x462x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x462x2048): 70.062
Elapsed time for attention_prob_times_values (80x2048x2048x462): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x462): 65.522

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 678.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x463x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x463x2048): 69.383
Elapsed time for attention_prob_times_values (80x2048x2048x463): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x463): 60.508

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 649.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x862x2048): 76.679
Elapsed time for attention_prob_times_values (32x2048x2048x862): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x862): 76.474

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 592.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x863x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x863x2048): 76.089
Elapsed time for attention_prob_times_values (32x2048x2048x863): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x863): 77.158

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 593.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x864x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x864x2048): 89.640
Elapsed time for attention_prob_times_values (32x2048x2048x864): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x864): 89.090

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 692.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x865x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x865x2048): 70.778
Elapsed time for attention_prob_times_values (32x2048x2048x865): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x865): 77.577

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 574.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x866x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x866x2048): 72.043
Elapsed time for attention_prob_times_values (32x2048x2048x866): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x866): 81.110

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 592.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x867x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x867x2048): 77.313
Elapsed time for attention_prob_times_values (32x2048x2048x867): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x867): 79.954

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 611.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x868x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x868x2048): 73.813
Elapsed time for attention_prob_times_values (32x2048x2048x868): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x868): 82.118

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 604.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x869x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x869x2048): 71.689
Elapsed time for attention_prob_times_values (32x2048x2048x869): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x869): 74.688

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 569.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x870x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x870x2048): 77.988
Elapsed time for attention_prob_times_values (32x2048x2048x870): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x870): 70.633

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 577.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x871x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x871x2048): 76.794
Elapsed time for attention_prob_times_values (32x2048x2048x871): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x871): 80.165

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 612.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x872x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x872x2048): 78.795
Elapsed time for attention_prob_times_values (32x2048x2048x872): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x872): 82.213

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 628.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x873x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x873x2048): 72.406
Elapsed time for attention_prob_times_values (32x2048x2048x873): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x873): 80.251

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 595.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 6992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x874x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x874x2048): 73.097
Elapsed time for attention_prob_times_values (32x2048x2048x874): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x874): 82.401

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 606.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x875x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x875x2048): 76.744
Elapsed time for attention_prob_times_values (32x2048x2048x875): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x875): 80.218

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 614.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x876x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x876x2048): 71.945
Elapsed time for attention_prob_times_values (32x2048x2048x876): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x876): 77.551

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 585.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x877x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x877x2048): 76.764
Elapsed time for attention_prob_times_values (32x2048x2048x877): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x877): 74.443

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 593.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x878x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x878x2048): 77.496
Elapsed time for attention_prob_times_values (32x2048x2048x878): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x878): 82.883

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 629.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x879x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x879x2048): 76.712
Elapsed time for attention_prob_times_values (32x2048x2048x879): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x879): 80.493

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 618.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x880x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x880x2048): 78.985
Elapsed time for attention_prob_times_values (32x2048x2048x880): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x880): 89.990

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 662.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x464x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x464x2048): 71.151
Elapsed time for attention_prob_times_values (80x2048x2048x464): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x464): 81.409

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 764.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x465x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x465x2048): 67.659
Elapsed time for attention_prob_times_values (80x2048x2048x465): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x465): 62.673

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 656.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x466x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x466x2048): 69.430
Elapsed time for attention_prob_times_values (80x2048x2048x466): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x466): 65.831

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 682.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x467x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x467x2048): 68.699
Elapsed time for attention_prob_times_values (80x2048x2048x467): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x467): 63.096

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 665.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x468x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x468x2048): 70.418
Elapsed time for attention_prob_times_values (80x2048x2048x468): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x468): 66.527

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 693.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x469x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x469x2048): 68.635
Elapsed time for attention_prob_times_values (80x2048x2048x469): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x469): 63.010

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 667.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x470x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x470x2048): 69.676
Elapsed time for attention_prob_times_values (80x2048x2048x470): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x470): 64.311

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 680.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x471x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x471x2048): 66.987
Elapsed time for attention_prob_times_values (80x2048x2048x471): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x471): 61.207

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 652.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x472x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x472x2048): 71.153
Elapsed time for attention_prob_times_values (80x2048x2048x472): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x472): 85.908

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 795.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x881x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x881x2048): 70.649
Elapsed time for attention_prob_times_values (32x2048x2048x881): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x881): 80.927

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 594.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x882x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x882x2048): 77.172
Elapsed time for attention_prob_times_values (32x2048x2048x882): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x882): 79.552

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 618.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x883x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x883x2048): 76.445
Elapsed time for attention_prob_times_values (32x2048x2048x883): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x883): 80.912

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 620.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x884x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x884x2048): 77.515
Elapsed time for attention_prob_times_values (32x2048x2048x884): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x884): 83.589

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 635.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x885x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x885x2048): 76.530
Elapsed time for attention_prob_times_values (32x2048x2048x885): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x885): 81.162

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 623.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x886x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x886x2048): 77.221
Elapsed time for attention_prob_times_values (32x2048x2048x886): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x886): 83.589

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 635.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x887x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x887x2048): 73.839
Elapsed time for attention_prob_times_values (32x2048x2048x887): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x887): 81.173

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 613.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x888x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x888x2048): 78.042
Elapsed time for attention_prob_times_values (32x2048x2048x888): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x888): 90.668

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 665.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x889x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x889x2048): 76.360
Elapsed time for attention_prob_times_values (32x2048x2048x889): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x889): 75.382

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 602.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x890x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x890x2048): 76.951
Elapsed time for attention_prob_times_values (32x2048x2048x890): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x890): 83.504

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 636.994
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (160x2048x274x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x274x2048): 54.534
Elapsed time for attention_prob_times_values (160x2048x2048x274): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x274): 59.079

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 663.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x275x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x275x2048): 53.983
Elapsed time for attention_prob_times_values (160x2048x2048x275): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x275): 56.782

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 649.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x276x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x276x2048): 56.155
Elapsed time for attention_prob_times_values (160x2048x2048x276): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x276): 59.073

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 678.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x277x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x277x2048): 55.789
Elapsed time for attention_prob_times_values (160x2048x2048x277): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x277): 56.485

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 663.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x278x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x278x2048): 54.926
Elapsed time for attention_prob_times_values (160x2048x2048x278): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x278): 57.389

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 665.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x279x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x279x2048): 55.296
Elapsed time for attention_prob_times_values (160x2048x2048x279): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x279): 57.676

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 671.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x280x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x280x2048): 57.883
Elapsed time for attention_prob_times_values (160x2048x2048x280): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x280): 74.932

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 779.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x281x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x281x2048): 55.814
Elapsed time for attention_prob_times_values (160x2048x2048x281): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x281): 27.097

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 436.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x282x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x282x2048): 55.413
Elapsed time for attention_prob_times_values (160x2048x2048x282): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x282): 60.678

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 696.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x283x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x283x2048): 56.431
Elapsed time for attention_prob_times_values (160x2048x2048x283): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x283): 58.400

Attention duration (in seconds): 0.0132
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x891x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x891x2048): 76.241
Elapsed time for attention_prob_times_values (32x2048x2048x891): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x891): 81.699

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 627.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x892x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x892x2048): 74.642
Elapsed time for attention_prob_times_values (32x2048x2048x892): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x892): 80.267

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 616.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x893x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x893x2048): 72.825
Elapsed time for attention_prob_times_values (32x2048x2048x893): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x893): 81.742

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 614.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x894x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x894x2048): 73.103
Elapsed time for attention_prob_times_values (32x2048x2048x894): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x894): 84.365

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 625.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x895x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x895x2048): 71.290
Elapsed time for attention_prob_times_values (32x2048x2048x895): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x895): 81.363

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 607.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x896x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x896x2048): 86.223
Elapsed time for attention_prob_times_values (32x2048x2048x896): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x896): 85.601

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 687.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x897x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x897x2048): 74.020
Elapsed time for attention_prob_times_values (32x2048x2048x897): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x897): 73.739

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 591.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x898x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x898x2048): 73.596
Elapsed time for attention_prob_times_values (32x2048x2048x898): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x898): 76.134

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 599.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x899x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x899x2048): 76.978
Elapsed time for attention_prob_times_values (32x2048x2048x899): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x899): 74.077

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 605.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Elapsed time for attention_key_query_prob (80x2048x473x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x473x2048): 68.578
Elapsed time for attention_prob_times_values (80x2048x2048x473): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x473): 63.696

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 676.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x474x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x474x2048): 69.058
Elapsed time for attention_prob_times_values (80x2048x2048x474): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x474): 62.931

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 675.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x475x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x475x2048): 68.571
Elapsed time for attention_prob_times_values (80x2048x2048x475): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x475): 63.816

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 679.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x476x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x476x2048): 70.126
Elapsed time for attention_prob_times_values (80x2048x2048x476): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x476): 67.423

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 707.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x477x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x477x2048): 68.800
Elapsed time for attention_prob_times_values (80x2048x2048x477): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x477): 63.671

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 682.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x478x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x478x2048): 69.075
Elapsed time for attention_prob_times_values (80x2048x2048x478): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x478): 67.390

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 705.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x479x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x479x2048): 68.894
Elapsed time for attention_prob_times_values (80x2048x2048x479): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x479): 61.321

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 671.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x480x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x480x2048): 86.089
Elapsed time for attention_prob_times_values (80x2048x2048x480): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x480): 88.688

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 906.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x481x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x481x2048): 67.752
Elapsed time for attention_prob_times_values (80x2048x2048x481): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x481): 62.325

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 674.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x482x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x482x2048): 72.227
Elapsed time for attention_prob_times_values (80x2048x2048x482): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x482): 64.372

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 708.924


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x900x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x900x2048): 79.152
Elapsed time for attention_prob_times_values (32x2048x2048x900): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x900): 76.396

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 624.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x901x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x901x2048): 75.143
Elapsed time for attention_prob_times_values (32x2048x2048x901): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x901): 69.028

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 578.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x902x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x902x2048): 75.066
Elapsed time for attention_prob_times_values (32x2048x2048x902): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x902): 76.238

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 608.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x903x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x903x2048): 74.222
Elapsed time for attention_prob_times_values (32x2048x2048x903): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x903): 74.298

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 598.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x904x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x904x2048): 78.986
Elapsed time for attention_prob_times_values (32x2048x2048x904): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x904): 77.031

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 628.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x905x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x905x2048): 77.111
Elapsed time for attention_prob_times_values (32x2048x2048x905): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x905): 74.527

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 611.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x906x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x906x2048): 78.202
Elapsed time for attention_prob_times_values (32x2048x2048x906): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x906): 71.055

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 601.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x907x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x907x2048): 74.939
Elapsed time for attention_prob_times_values (32x2048x2048x907): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x907): 74.567

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 604.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x908x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x908x2048): 77.378
Elapsed time for attention_prob_times_values (32x2048x2048x908): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x908): 76.831

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 624.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x909x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x909x2048): 77.177
Elapsed time for attention_prob_times_values (32x2048x2048x909): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x909): 74.671

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 614.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x483x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x483x2048): 71.131
Elapsed time for attention_prob_times_values (80x2048x2048x483): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x483): 64.962

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 708.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x484x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x484x2048): 69.567
Elapsed time for attention_prob_times_values (80x2048x2048x484): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x484): 66.088

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 708.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x485x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x485x2048): 69.936
Elapsed time for attention_prob_times_values (80x2048x2048x485): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x485): 64.955

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 705.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x486x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x486x2048): 70.536
Elapsed time for attention_prob_times_values (80x2048x2048x486): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x486): 66.284

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 717.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x487x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x487x2048): 69.141
Elapsed time for attention_prob_times_values (80x2048x2048x487): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x487): 65.269

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 705.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x488x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x488x2048): 72.359
Elapsed time for attention_prob_times_values (80x2048x2048x488): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x488): 88.734

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 839.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x489x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x489x2048): 69.299
Elapsed time for attention_prob_times_values (80x2048x2048x489): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x489): 64.369

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 704.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x490x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x490x2048): 69.630
Elapsed time for attention_prob_times_values (80x2048x2048x490): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x490): 66.325

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 718.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x491x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x491x2048): 69.948
Elapsed time for attention_prob_times_values (80x2048x2048x491): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x491): 63.539

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 705.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x133x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x133x2048): 58.144
Elapsed time for attention_prob_times_values (384x2048x2048x133): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x133): 46.105

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 692.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x134x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x134x2048): 57.722
Elapsed time for attention_prob_times_values (384x2048x2048x134): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x134): 48.163

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 712.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x135x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x135x2048): 56.786
Elapsed time for attention_prob_times_values (384x2048x2048x135): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x135): 46.360

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 697.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x136x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x136x2048): 24.730
Elapsed time for attention_prob_times_values (384x2048x2048x136): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x136): 44.847

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 438.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x137x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x137x2048): 58.872
Elapsed time for attention_prob_times_values (384x2048x2048x137): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x137): 45.335

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 709.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x138x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x138x2048): 58.759
Elapsed time for attention_prob_times_values (384x2048x2048x138): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x138): 49.324

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 747.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x139x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x139x2048): 58.778
Elapsed time for attention_prob_times_values (384x2048x2048x139): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x139): 46.269

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 726.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x140x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x140x2048): 60.347
Elapsed time for attention_prob_times_values (384x2048x2048x140): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x140): 49.428

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 767.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x141x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x141x2048): 59.100
Elapsed time for attention_prob_times_values (384x2048x2048x141): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x141): 47.045

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 744.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x142x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x142x2048): 59.032
Attention throughput (in TFLOP/s): 691.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x284x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x284x2048): 57.506
Elapsed time for attention_prob_times_values (160x2048x2048x284): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x284): 61.366

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 718.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x285x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x285x2048): 54.518
Elapsed time for attention_prob_times_values (160x2048x2048x285): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x285): 57.255

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 677.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x286x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x286x2048): 54.698
Elapsed time for attention_prob_times_values (160x2048x2048x286): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x286): 61.454

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 704.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x287x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x287x2048): 54.392
Elapsed time for attention_prob_times_values (160x2048x2048x287): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x287): 59.165

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 692.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x288x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x288x2048): 76.071
Elapsed time for attention_prob_times_values (160x2048x2048x288): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x288): 73.375

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 915.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x289x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x289x2048): 55.238
Elapsed time for attention_prob_times_values (160x2048x2048x289): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x289): 59.759

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 705.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x290x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x290x2048): 61.317
Elapsed time for attention_prob_times_values (160x2048x2048x290): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x290): 62.138

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 760.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x291x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x291x2048): 60.512
Elapsed time for attention_prob_times_values (160x2048x2048x291): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x291): 59.936

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 744.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x292x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x292x2048): 62.209
Elapsed time for attention_prob_times_values (160x2048x2048x292): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x292): 62.736

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 775.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x910x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x910x2048): 78.164
Elapsed time for attention_prob_times_values (32x2048x2048x910): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x910): 76.827

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 628.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x911x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x911x2048): 77.411
Elapsed time for attention_prob_times_values (32x2048x2048x911): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x911): 69.341

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 593.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x912x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x912x2048): 79.084
Elapsed time for attention_prob_times_values (32x2048x2048x912): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x912): 82.139

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 654.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x913x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x913x2048): 77.249
Elapsed time for attention_prob_times_values (32x2048x2048x913): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x913): 75.214

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 619.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x914x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x914x2048): 77.972
Elapsed time for attention_prob_times_values (32x2048x2048x914): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x914): 77.240

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 631.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x915x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x915x2048): 73.188
Elapsed time for attention_prob_times_values (32x2048x2048x915): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x915): 75.329

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 604.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x916x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x916x2048): 78.273
Elapsed time for attention_prob_times_values (32x2048x2048x916): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x916): 72.060

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 612.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x917x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x917x2048): 77.106
Elapsed time for attention_prob_times_values (32x2048x2048x917): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x917): 75.213

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 621.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x918x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x918x2048): 77.520
Elapsed time for attention_prob_times_values (32x2048x2048x918): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x918): 77.450

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 633.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x919x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x919x2048): 77.240
Elapsed time for attention_prob_times_values (32x2048x2048x919): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x919): 71.334

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 606.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x920x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x920x2048): 78.944
Elapsed time for attention_prob_times_values (32x2048x2048x920): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x920): 79.289

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 647.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x921x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x921x2048): 76.488
Elapsed time for attention_prob_times_values (32x2048x2048x921): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x921): 73.045

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 612.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x922x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x922x2048): 72.981
Elapsed time for attention_prob_times_values (32x2048x2048x922): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x922): 77.718

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 617.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x923x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x923x2048): 76.581
Elapsed time for attention_prob_times_values (32x2048x2048x923): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x923): 71.753

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 608.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x924x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x924x2048): 78.226
Elapsed time for attention_prob_times_values (32x2048x2048x924): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x924): 77.974

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 641.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x925x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x925x2048): 76.320
Elapsed time for attention_prob_times_values (32x2048x2048x925): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x925): 75.897

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 626.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x926x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x926x2048): 72.517
Elapsed time for attention_prob_times_values (32x2048x2048x926): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x926): 71.759

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 593.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x927x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x927x2048): 72.159
Elapsed time for attention_prob_times_values (32x2048x2048x927): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x927): 76.133

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 610.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x928x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x928x2048): 84.295
Elapsed time for attention_prob_times_values (32x2048x2048x928): 0.0030
num_attention_heads: 20, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x492x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x492x2048): 67.832
Elapsed time for attention_prob_times_values (80x2048x2048x492): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x492): 66.535

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 712.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x493x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x493x2048): 70.127
Elapsed time for attention_prob_times_values (80x2048x2048x493): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x493): 63.463

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 708.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x494x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x494x2048): 67.720
Elapsed time for attention_prob_times_values (80x2048x2048x494): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x494): 66.372

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 713.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x495x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x495x2048): 66.934
Elapsed time for attention_prob_times_values (80x2048x2048x495): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x495): 63.549

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 695.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x496x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x496x2048): 70.761
Elapsed time for attention_prob_times_values (80x2048x2048x496): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x496): 89.089

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 842.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x497x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x497x2048): 68.568
Elapsed time for attention_prob_times_values (80x2048x2048x497): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x497): 62.584

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 700.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x498x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x498x2048): 68.092
Elapsed time for attention_prob_times_values (80x2048x2048x498): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x498): 67.082

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 724.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x499x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x499x2048): 69.698
Elapsed time for attention_prob_times_values (80x2048x2048x499): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x499): 64.229

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 718.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x500x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x500x2048): 69.579
Elapsed time for attention_prob_times_values (80x2048x2048x500): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x500): 66.326

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 731.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x501x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x501x2048): 69.794
Elapsed time for attention_prob_times_values (80x2048x2048x501): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x928): 83.607

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 692.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x929x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x929x2048): 78.768
Elapsed time for attention_prob_times_values (32x2048x2048x929): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x929): 76.381

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 640.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x930x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x930x2048): 79.626
Elapsed time for attention_prob_times_values (32x2048x2048x930): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x930): 76.530

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 645.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x931x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x931x2048): 78.184
Elapsed time for attention_prob_times_values (32x2048x2048x931): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x931): 75.098

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 633.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x932x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x932x2048): 79.399
Elapsed time for attention_prob_times_values (32x2048x2048x932): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x932): 78.614

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 654.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x933x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x933x2048): 73.722
Elapsed time for attention_prob_times_values (32x2048x2048x933): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x933): 76.592

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 622.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x934x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x934x2048): 79.234
Elapsed time for attention_prob_times_values (32x2048x2048x934): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x934): 78.571

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 654.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x935x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x935x2048): 77.623
Elapsed time for attention_prob_times_values (32x2048x2048x935): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x935): 76.411

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 639.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x936x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x936x2048): 80.116
Elapsed time for attention_prob_times_values (32x2048x2048x936): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x936): 83.572

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 680.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x937x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x937x2048): 77.703
Elapsed time for attention_prob_times_values (32x2048x2048x937): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x937): 76.469

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 641.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 40, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x293x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x293x2048): 57.697
Elapsed time for attention_prob_times_values (160x2048x2048x293): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x293): 60.537

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 735.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x294x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x294x2048): 60.014
Elapsed time for attention_prob_times_values (160x2048x2048x294): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x294): 61.978

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 761.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x295x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x295x2048): 60.159
Elapsed time for attention_prob_times_values (160x2048x2048x295): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x295): 60.952

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 758.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x296x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x296x2048): 62.077
Elapsed time for attention_prob_times_values (160x2048x2048x296): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x296): 79.082

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 873.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x297x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x297x2048): 59.205
Elapsed time for attention_prob_times_values (160x2048x2048x297): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x297): 61.085

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 757.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x298x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x298x2048): 57.020
Elapsed time for attention_prob_times_values (160x2048x2048x298): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x298): 61.938

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 750.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x299x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x299x2048): 58.152
Elapsed time for attention_prob_times_values (160x2048x2048x299): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x299): 61.461

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 757.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x300x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x300x2048): 56.444
Elapsed time for attention_prob_times_values (160x2048x2048x300): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x300): 64.554

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 766.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x301x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x301x2048): 59.926
Elapsed time for attention_prob_times_values (160x2048x2048x301): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x301): 62.003

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 777.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x302x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x302x2048): 60.625
========================================================================================================================
num_attention_heads: 8, hidden_size: 7504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x938x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x938x2048): 78.380
Elapsed time for attention_prob_times_values (32x2048x2048x938): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x938): 77.688

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 649.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x939x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x939x2048): 73.118
Elapsed time for attention_prob_times_values (32x2048x2048x939): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x939): 76.861

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 624.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x940x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x940x2048): 79.659
Elapsed time for attention_prob_times_values (32x2048x2048x940): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x940): 79.158

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 662.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x941x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x941x2048): 78.017
Elapsed time for attention_prob_times_values (32x2048x2048x941): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x941): 76.243

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 644.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x942x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x942x2048): 74.653
Elapsed time for attention_prob_times_values (32x2048x2048x942): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x942): 77.191

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 634.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x943x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x943x2048): 77.822
Elapsed time for attention_prob_times_values (32x2048x2048x943): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x943): 77.099

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 648.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x944x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x944x2048): 79.841
Elapsed time for attention_prob_times_values (32x2048x2048x944): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x944): 81.407

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 675.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x945x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x945x2048): 77.713
Elapsed time for attention_prob_times_values (32x2048x2048x945): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x945): 72.547

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 629.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x946x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x946x2048): 71.835
Elapsed time for attention_prob_times_values (32x2048x2048x946): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x946): 77.164

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 624.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x947x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x947x2048): 77.606
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x501): 64.827

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 724.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x502x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x502x2048): 70.502
Elapsed time for attention_prob_times_values (80x2048x2048x502): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x502): 68.018

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 748.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x503x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x503x2048): 70.052
Elapsed time for attention_prob_times_values (80x2048x2048x503): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x503): 64.364

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 726.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x504x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x504x2048): 69.503
Elapsed time for attention_prob_times_values (80x2048x2048x504): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x504): 90.083

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 850.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x505x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x505x2048): 67.454
Elapsed time for attention_prob_times_values (80x2048x2048x505): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x505): 60.811

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 694.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x506x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x506x2048): 67.712
Elapsed time for attention_prob_times_values (80x2048x2048x506): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x506): 65.556

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 724.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x507x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x507x2048): 69.215
Elapsed time for attention_prob_times_values (80x2048x2048x507): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x507): 61.493

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 710.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x508x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x508x2048): 69.551
Elapsed time for attention_prob_times_values (80x2048x2048x508): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x508): 67.483

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 748.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x509x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x509x2048): 66.990
Elapsed time for attention_prob_times_values (80x2048x2048x509): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x509): 61.052

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 698.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x510x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x510x2048): 69.642
Elapsed time for attention_prob_times_values (80x2048x2048x510): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x510): 68.063

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 754.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (32x2048x2048x947): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x947): 75.384

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 642.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x948x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x948x2048): 77.547
Elapsed time for attention_prob_times_values (32x2048x2048x948): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x948): 79.683

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 660.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x949x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x949x2048): 69.077
Elapsed time for attention_prob_times_values (32x2048x2048x949): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x949): 71.874

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 592.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x950x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x950x2048): 78.676
Elapsed time for attention_prob_times_values (32x2048x2048x950): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x950): 74.777

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 645.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x951x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x951x2048): 77.962
Elapsed time for attention_prob_times_values (32x2048x2048x951): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x951): 77.148

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 653.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x952x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x952x2048): 79.769
Elapsed time for attention_prob_times_values (32x2048x2048x952): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x952): 81.703

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 681.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x953x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x953x2048): 77.617
Elapsed time for attention_prob_times_values (32x2048x2048x953): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x953): 77.820

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 656.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x954x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x954x2048): 75.297
Elapsed time for attention_prob_times_values (32x2048x2048x954): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x954): 78.197

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 648.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x955x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x955x2048): 77.617
Elapsed time for attention_prob_times_values (32x2048x2048x955): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x955): 74.062

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 641.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x956x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x956x2048): 79.121
Elapsed time for attention_prob_times_values (32x2048x2048x956): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x956): 80.289

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 674.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Elapsed time for attention_prob_times_values (384x2048x2048x142): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x142): 49.824

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 773.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x143x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x143x2048): 59.587
Elapsed time for attention_prob_times_values (384x2048x2048x143): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x143): 47.159

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 758.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x144x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x144x2048): 61.830
Elapsed time for attention_prob_times_values (384x2048x2048x144): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x144): 49.076

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 793.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x145x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x145x2048): 58.907
Elapsed time for attention_prob_times_values (384x2048x2048x145): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x145): 48.342

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 774.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x146x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x146x2048): 59.526
Elapsed time for attention_prob_times_values (384x2048x2048x146): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x146): 51.268

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 809.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x147x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x147x2048): 60.483
Elapsed time for attention_prob_times_values (384x2048x2048x147): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x147): 48.823

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 798.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x148x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x148x2048): 61.450
Elapsed time for attention_prob_times_values (384x2048x2048x148): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x148): 51.067

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 829.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x149x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x149x2048): 61.492
Elapsed time for attention_prob_times_values (384x2048x2048x149): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x149): 49.326

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 819.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x150x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x150x2048): 62.779
Elapsed time for attention_prob_times_values (384x2048x2048x150): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x150): 49.793

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 836.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x151x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x151x2048): 59.751
Elapsed time for attention_prob_times_values (384x2048x2048x151): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x151): 51.138

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 835.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
========================================================================================================================
num_attention_heads: 20, hidden_size: 10220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x511x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x511x2048): 69.411
Elapsed time for attention_prob_times_values (80x2048x2048x511): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x511): 64.581

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 734.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x512x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x512x2048): 79.604
Elapsed time for attention_prob_times_values (80x2048x2048x512): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x512): 89.803

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 928.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x513x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x513x2048): 71.597
Elapsed time for attention_prob_times_values (80x2048x2048x513): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x513): 58.420

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 709.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x514x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x514x2048): 74.404
Elapsed time for attention_prob_times_values (80x2048x2048x514): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x514): 65.241

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 767.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x515x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x515x2048): 71.366
Elapsed time for attention_prob_times_values (80x2048x2048x515): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x515): 59.965

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 720.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x516x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x516x2048): 71.284
Elapsed time for attention_prob_times_values (80x2048x2048x516): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x516): 65.195

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 754.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x517x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x517x2048): 71.538
Elapsed time for attention_prob_times_values (80x2048x2048x517): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x517): 61.459

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 733.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x518x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x518x2048): 72.062
Elapsed time for attention_prob_times_values (80x2048x2048x518): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x518): 64.961

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 759.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x519x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x519x2048): 71.306
Elapsed time for attention_prob_times_values (80x2048x2048x519): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x519): 61.603

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 736.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x520x2048): 0.0048
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x957x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x957x2048): 76.859
Elapsed time for attention_prob_times_values (32x2048x2048x957): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x957): 77.904

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 655.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x958x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x958x2048): 78.513
Elapsed time for attention_prob_times_values (32x2048x2048x958): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x958): 80.350

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 673.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x959x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x959x2048): 74.501
Elapsed time for attention_prob_times_values (32x2048x2048x959): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x959): 77.064

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 643.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x960x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x960x2048): 90.411
Elapsed time for attention_prob_times_values (32x2048x2048x960): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x960): 86.872

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 753.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x961x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x961x2048): 68.637
Elapsed time for attention_prob_times_values (32x2048x2048x961): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x961): 73.600

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 604.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x962x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x962x2048): 80.160
Elapsed time for attention_prob_times_values (32x2048x2048x962): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x962): 75.279

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 661.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x963x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x963x2048): 79.049
Elapsed time for attention_prob_times_values (32x2048x2048x963): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x963): 78.597

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 671.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x964x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x964x2048): 80.877
Elapsed time for attention_prob_times_values (32x2048x2048x964): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x964): 80.913

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 690.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x965x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x965x2048): 77.474
Elapsed time for attention_prob_times_values (32x2048x2048x965): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x965): 78.726

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 666.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x966x2048): 0.0035
Elapsed time for attention_prob_times_values (160x2048x2048x302): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x302): 64.709

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 801.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x303x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x303x2048): 60.076
Elapsed time for attention_prob_times_values (160x2048x2048x303): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x303): 63.103

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 790.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x304x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x304x2048): 58.994
Elapsed time for attention_prob_times_values (160x2048x2048x304): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x304): 77.602

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 863.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x305x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x305x2048): 59.287
Elapsed time for attention_prob_times_values (160x2048x2048x305): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x305): 60.953

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 776.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x306x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x306x2048): 58.881
Elapsed time for attention_prob_times_values (160x2048x2048x306): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x306): 64.076

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 794.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x307x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x307x2048): 57.809
Elapsed time for attention_prob_times_values (160x2048x2048x307): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x307): 61.403

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 773.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x308x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x308x2048): 61.259
Elapsed time for attention_prob_times_values (160x2048x2048x308): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x308): 66.179

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 829.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x309x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x309x2048): 58.761
Elapsed time for attention_prob_times_values (160x2048x2048x309): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x309): 62.554

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 792.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x310x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x310x2048): 60.669
Elapsed time for attention_prob_times_values (160x2048x2048x310): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x310): 62.523

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 807.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x311x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x311x2048): 59.459
Elapsed time for attention_prob_times_values (160x2048x2048x311): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x311): 63.972

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 810.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x966x2048): 73.902
Elapsed time for attention_prob_times_values (32x2048x2048x966): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x966): 77.224

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 645.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x967x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x967x2048): 78.940
Elapsed time for attention_prob_times_values (32x2048x2048x967): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x967): 78.723

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 674.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x968x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x968x2048): 80.833
Elapsed time for attention_prob_times_values (32x2048x2048x968): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x968): 83.823

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 704.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x969x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x969x2048): 78.129
Elapsed time for attention_prob_times_values (32x2048x2048x969): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x969): 76.160

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 661.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x970x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x970x2048): 79.059
Elapsed time for attention_prob_times_values (32x2048x2048x970): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x970): 81.235

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 687.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x971x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x971x2048): 78.240
Elapsed time for attention_prob_times_values (32x2048x2048x971): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x971): 79.215

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 675.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x972x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x972x2048): 79.946
Elapsed time for attention_prob_times_values (32x2048x2048x972): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x972): 81.282

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 692.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x973x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x973x2048): 78.239
Elapsed time for attention_prob_times_values (32x2048x2048x973): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x973): 79.223

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 677.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x974x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x974x2048): 79.431
Elapsed time for attention_prob_times_values (32x2048x2048x974): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x974): 81.515

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 692.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x975x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x975x2048): 76.064
Elapsed time for attention_prob_times_values (32x2048x2048x975): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x975): 79.003

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 667.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x520x2048): 72.776
Elapsed time for attention_prob_times_values (80x2048x2048x520): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x520): 75.740

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 828.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x521x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x521x2048): 70.504
Elapsed time for attention_prob_times_values (80x2048x2048x521): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x521): 61.323

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 733.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x522x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x522x2048): 71.177
Elapsed time for attention_prob_times_values (80x2048x2048x522): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x522): 65.997

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 766.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x523x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x523x2048): 70.751
Elapsed time for attention_prob_times_values (80x2048x2048x523): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x523): 61.433

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 737.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x524x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x524x2048): 71.738
Elapsed time for attention_prob_times_values (80x2048x2048x524): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x524): 65.622

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 770.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x525x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x525x2048): 70.960
Elapsed time for attention_prob_times_values (80x2048x2048x525): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x525): 61.712

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 742.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x526x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x526x2048): 70.145
Elapsed time for attention_prob_times_values (80x2048x2048x526): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x526): 66.551

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 769.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x527x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x527x2048): 71.046
Elapsed time for attention_prob_times_values (80x2048x2048x527): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x527): 61.467

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 744.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x528x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x528x2048): 73.003
Elapsed time for attention_prob_times_values (80x2048x2048x528): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x528): 77.285

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 849.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x529x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x529x2048): 70.170
Elapsed time for attention_prob_times_values (80x2048x2048x529): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x529): 62.016

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 746.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x976x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x976x2048): 77.342
Elapsed time for attention_prob_times_values (32x2048x2048x976): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x976): 84.031

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 694.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x977x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x977x2048): 77.987
Elapsed time for attention_prob_times_values (32x2048x2048x977): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x977): 79.699

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 680.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x978x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x978x2048): 78.844
Elapsed time for attention_prob_times_values (32x2048x2048x978): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x978): 79.902

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 685.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x979x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x979x2048): 75.575
Elapsed time for attention_prob_times_values (32x2048x2048x979): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x979): 79.891

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 671.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x980x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x980x2048): 74.841
Elapsed time for attention_prob_times_values (32x2048x2048x980): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x980): 82.024

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 677.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x981x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x981x2048): 75.470
Elapsed time for attention_prob_times_values (32x2048x2048x981): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x981): 80.048

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 673.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x982x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x982x2048): 76.224
Elapsed time for attention_prob_times_values (32x2048x2048x982): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x982): 82.045

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 685.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x983x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x983x2048): 78.233
Elapsed time for attention_prob_times_values (32x2048x2048x983): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x983): 76.661

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 672.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x984x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x984x2048): 80.165
Elapsed time for attention_prob_times_values (32x2048x2048x984): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x984): 84.256

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 713.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x312x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x312x2048): 60.974
Elapsed time for attention_prob_times_values (160x2048x2048x312): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x312): 81.011

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 917.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x313x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x313x2048): 58.660
Elapsed time for attention_prob_times_values (160x2048x2048x313): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x313): 63.472

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 806.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x314x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x314x2048): 58.197
Elapsed time for attention_prob_times_values (160x2048x2048x314): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x314): 66.389

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 822.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x315x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x315x2048): 58.025
Elapsed time for attention_prob_times_values (160x2048x2048x315): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x315): 64.861

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 814.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x316x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x316x2048): 61.410
Elapsed time for attention_prob_times_values (160x2048x2048x316): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x316): 67.185

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 856.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x317x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x317x2048): 59.073
Elapsed time for attention_prob_times_values (160x2048x2048x317): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x317): 65.061

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 828.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x318x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x318x2048): 59.701
Elapsed time for attention_prob_times_values (160x2048x2048x318): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x318): 67.733

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 851.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x319x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x319x2048): 60.141
Elapsed time for attention_prob_times_values (160x2048x2048x319): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x319): 64.403

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 837.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x320x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x320x2048): 78.313
Elapsed time for attention_prob_times_values (160x2048x2048x320): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x320): 80.540

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1072.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x985x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x985x2048): 77.635
Elapsed time for attention_prob_times_values (32x2048x2048x985): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x985): 80.288

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 686.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x986x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x986x2048): 77.066
Elapsed time for attention_prob_times_values (32x2048x2048x986): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x986): 82.165

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 692.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x987x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x987x2048): 76.135
Elapsed time for attention_prob_times_values (32x2048x2048x987): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x987): 78.263

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 672.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x988x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x988x2048): 79.417
Elapsed time for attention_prob_times_values (32x2048x2048x988): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x988): 82.607

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 706.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x989x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x989x2048): 77.977
Elapsed time for attention_prob_times_values (32x2048x2048x989): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x989): 80.497

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 691.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x990x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x990x2048): 78.634
Elapsed time for attention_prob_times_values (32x2048x2048x990): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x990): 74.944

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 670.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x991x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x991x2048): 72.774
Elapsed time for attention_prob_times_values (32x2048x2048x991): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x991): 80.596

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 668.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x992x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x992x2048): 90.123
Elapsed time for attention_prob_times_values (32x2048x2048x992): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x992): 89.237

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 784.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x993x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x993x2048): 79.518
Elapsed time for attention_prob_times_values (32x2048x2048x993): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x993): 80.764

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 701.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x994x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x994x2048): 80.266
Elapsed time for attention_prob_times_values (32x2048x2048x994): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x994): 82.826

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 714.624
MLP duration (in seconds): 0.0000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x530x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x530x2048): 69.213
Elapsed time for attention_prob_times_values (80x2048x2048x530): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x530): 66.498

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 769.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x531x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x531x2048): 66.128
Elapsed time for attention_prob_times_values (80x2048x2048x531): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x531): 62.168

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 728.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x532x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x532x2048): 69.039
Elapsed time for attention_prob_times_values (80x2048x2048x532): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x532): 65.955

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 768.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x533x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x533x2048): 70.807
Elapsed time for attention_prob_times_values (80x2048x2048x533): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x533): 62.371

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 756.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x534x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x534x2048): 70.810
Elapsed time for attention_prob_times_values (80x2048x2048x534): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x534): 66.821

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 785.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x535x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x535x2048): 68.797
Elapsed time for attention_prob_times_values (80x2048x2048x535): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x535): 61.307

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 742.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x536x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x536x2048): 72.628
Elapsed time for attention_prob_times_values (80x2048x2048x536): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x536): 78.062

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 862.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x537x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x537x2048): 68.687
Elapsed time for attention_prob_times_values (80x2048x2048x537): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x537): 62.664

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 752.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x538x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x538x2048): 70.952
Elapsed time for attention_prob_times_values (80x2048x2048x538): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x538): 67.367

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 795.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x995x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x995x2048): 76.391
Elapsed time for attention_prob_times_values (32x2048x2048x995): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x995): 80.851

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 689.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x996x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x996x2048): 80.810
Elapsed time for attention_prob_times_values (32x2048x2048x996): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x996): 83.122

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 719.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x997x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x997x2048): 76.435
Elapsed time for attention_prob_times_values (32x2048x2048x997): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x997): 78.372

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 680.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x998x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x998x2048): 79.365
Elapsed time for attention_prob_times_values (32x2048x2048x998): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x998): 83.129

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 714.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 7992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x999x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x999x2048): 78.705
Elapsed time for attention_prob_times_values (32x2048x2048x999): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x999): 81.001

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 702.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1000x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1000x2048): 80.594
Elapsed time for attention_prob_times_values (32x2048x2048x1000): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1000): 89.053

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 745.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1001x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1001x2048): 78.054
Elapsed time for attention_prob_times_values (32x2048x2048x1001): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1001): 81.040

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 701.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1002x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1002x2048): 78.975
Elapsed time for attention_prob_times_values (32x2048x2048x1002): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1002): 83.357

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 716.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1003x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1003x2048): 78.308
Elapsed time for attention_prob_times_values (32x2048x2048x1003): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1003): 81.255

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 704.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x152x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x152x2048): 60.919
Elapsed time for attention_prob_times_values (384x2048x2048x152): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x152): 51.049

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 847.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x153x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x153x2048): 61.027
Elapsed time for attention_prob_times_values (384x2048x2048x153): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x153): 51.006

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 852.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x154x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x154x2048): 58.776
Elapsed time for attention_prob_times_values (384x2048x2048x154): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x154): 53.371

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 863.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x155x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x155x2048): 61.615
Elapsed time for attention_prob_times_values (384x2048x2048x155): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x155): 51.304

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 869.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x156x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x156x2048): 62.675
Elapsed time for attention_prob_times_values (384x2048x2048x156): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x156): 54.095

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 907.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x157x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x157x2048): 62.027
Elapsed time for attention_prob_times_values (384x2048x2048x157): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x157): 50.971

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 879.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x158x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x158x2048): 63.789
Elapsed time for attention_prob_times_values (384x2048x2048x158): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x158): 54.356

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 928.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x159x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x159x2048): 63.114
Elapsed time for attention_prob_times_values (384x2048x2048x159): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x159): 53.323

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 919.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x160x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x160x2048): 76.383
Elapsed time for attention_prob_times_values (384x2048x2048x160): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x160): 55.450

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1028.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 8, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1004x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1004x2048): 80.074
Elapsed time for attention_prob_times_values (32x2048x2048x1004): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1004): 83.690

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 723.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1005x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1005x2048): 76.290
Elapsed time for attention_prob_times_values (32x2048x2048x1005): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1005): 81.370

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 697.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1006x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1006x2048): 75.055
Elapsed time for attention_prob_times_values (32x2048x2048x1006): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1006): 83.665

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 701.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1007x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1007x2048): 78.479
Elapsed time for attention_prob_times_values (32x2048x2048x1007): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1007): 81.158

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 707.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1008x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1008x2048): 80.679
Elapsed time for attention_prob_times_values (32x2048x2048x1008): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1008): 87.381

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 744.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1009x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1009x2048): 77.917
Elapsed time for attention_prob_times_values (32x2048x2048x1009): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1009): 81.635

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 708.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1010x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1010x2048): 70.195
Elapsed time for attention_prob_times_values (32x2048x2048x1010): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1010): 83.750

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 679.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1011x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1011x2048): 77.981
Elapsed time for attention_prob_times_values (32x2048x2048x1011): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1011): 81.691

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 710.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1012x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1012x2048): 79.540
Elapsed time for attention_prob_times_values (32x2048x2048x1012): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1012): 84.027

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 727.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1013x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1013x2048): 78.344
Elapsed time for attention_prob_times_values (32x2048x2048x1013): 0.0034

Elapsed time for attention_key_query_prob (80x2048x539x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x539x2048): 68.842
Elapsed time for attention_prob_times_values (80x2048x2048x539): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x539): 62.219

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 753.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x540x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x540x2048): 71.846
Elapsed time for attention_prob_times_values (80x2048x2048x540): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x540): 68.087

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 807.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x541x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x541x2048): 68.659
Elapsed time for attention_prob_times_values (80x2048x2048x541): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x541): 63.186

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 761.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x542x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x542x2048): 71.376
Elapsed time for attention_prob_times_values (80x2048x2048x542): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x542): 66.449

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 797.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x543x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x543x2048): 70.766
Elapsed time for attention_prob_times_values (80x2048x2048x543): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x543): 61.886

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 766.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x544x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x544x2048): 87.394
Elapsed time for attention_prob_times_values (80x2048x2048x544): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x544): 80.152

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 972.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x545x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x545x2048): 71.238
Elapsed time for attention_prob_times_values (80x2048x2048x545): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x545): 63.764

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 783.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x546x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x546x2048): 74.090
Elapsed time for attention_prob_times_values (80x2048x2048x546): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x546): 66.519

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 817.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x547x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x547x2048): 73.408
Elapsed time for attention_prob_times_values (80x2048x2048x547): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x547): 63.929

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 798.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x548x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x548x2048): 74.538
Elapsed time for attention_prob_times_values (80x2048x2048x548): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x548): 67.178

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 827.024
Elapsed time for attention_key_query_prob (160x2048x321x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x321x2048): 61.482
Elapsed time for attention_prob_times_values (160x2048x2048x321): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x321): 57.130

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 801.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x322x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x322x2048): 61.949
Elapsed time for attention_prob_times_values (160x2048x2048x322): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x322): 58.701

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 818.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x323x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x323x2048): 62.861
Elapsed time for attention_prob_times_values (160x2048x2048x323): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x323): 58.484

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 825.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x324x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x324x2048): 64.553
Elapsed time for attention_prob_times_values (160x2048x2048x324): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x324): 61.166

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 857.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x325x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x325x2048): 62.664
Elapsed time for attention_prob_times_values (160x2048x2048x325): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x325): 58.791

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 830.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x326x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x326x2048): 63.251
Elapsed time for attention_prob_times_values (160x2048x2048x326): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x326): 59.810

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 844.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x327x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x327x2048): 61.848
Elapsed time for attention_prob_times_values (160x2048x2048x327): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x327): 57.226

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 818.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x328x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x328x2048): 64.404
Elapsed time for attention_prob_times_values (160x2048x2048x328): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x328): 70.302

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 928.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x329x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x329x2048): 59.634
Elapsed time for attention_prob_times_values (160x2048x2048x329): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x329): 58.007

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 814.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x330x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x330x2048): 61.484
Elapsed time for attention_prob_times_values (160x2048x2048x330): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x330): 60.115

Attention duration (in seconds): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1013): 80.581

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 708.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1014x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1014x2048): 78.959
Elapsed time for attention_prob_times_values (32x2048x2048x1014): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1014): 82.885

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 721.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1015x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1015x2048): 78.209
Elapsed time for attention_prob_times_values (32x2048x2048x1015): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1015): 81.558

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 713.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1016x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1016x2048): 80.016
Elapsed time for attention_prob_times_values (32x2048x2048x1016): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1016): 90.072

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 757.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1017x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1017x2048): 77.830
Elapsed time for attention_prob_times_values (32x2048x2048x1017): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1017): 82.088

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 714.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1018x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1018x2048): 78.317
Elapsed time for attention_prob_times_values (32x2048x2048x1018): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1018): 84.209

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 726.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1019x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1019x2048): 78.233
Elapsed time for attention_prob_times_values (32x2048x2048x1019): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1019): 81.472

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 715.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1020x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1020x2048): 81.454
Elapsed time for attention_prob_times_values (32x2048x2048x1020): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1020): 84.029

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 741.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1021x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1021x2048): 77.202
Elapsed time for attention_prob_times_values (32x2048x2048x1021): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1021): 81.812

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 713.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1022x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1022x2048): 78.732
Elapsed time for attention_prob_times_values (32x2048x2048x1022): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1022): 84.546

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 732.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x549x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x549x2048): 72.427
Elapsed time for attention_prob_times_values (80x2048x2048x549): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x549): 62.968

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 789.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x550x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x550x2048): 72.518
Elapsed time for attention_prob_times_values (80x2048x2048x550): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x550): 67.394

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 820.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x551x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x551x2048): 72.575
Elapsed time for attention_prob_times_values (80x2048x2048x551): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x551): 64.485

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 803.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x552x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x552x2048): 74.800
Elapsed time for attention_prob_times_values (80x2048x2048x552): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x552): 80.264

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 912.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x553x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x553x2048): 72.391
Elapsed time for attention_prob_times_values (80x2048x2048x553): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x553): 60.967

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 781.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x554x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x554x2048): 70.043
Elapsed time for attention_prob_times_values (80x2048x2048x554): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x554): 67.631

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 813.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x555x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x555x2048): 72.424
Elapsed time for attention_prob_times_values (80x2048x2048x555): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x555): 64.612

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 808.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x556x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x556x2048): 72.757
Elapsed time for attention_prob_times_values (80x2048x2048x556): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x556): 68.698

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 838.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x557x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x557x2048): 71.216
Elapsed time for attention_prob_times_values (80x2048x2048x557): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x557): 65.119

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 808.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 8, hidden_size: 8184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1023x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1023x2048): 78.061
Elapsed time for attention_prob_times_values (32x2048x2048x1023): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1023): 81.932

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 718.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1024x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1024x2048): 82.228
Elapsed time for attention_prob_times_values (32x2048x2048x1024): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1024): 90.503

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 775.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1025x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1025x2048): 74.551
Elapsed time for attention_prob_times_values (32x2048x2048x1025): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1025): 69.862

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 649.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1026x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1026x2048): 80.472
Elapsed time for attention_prob_times_values (32x2048x2048x1026): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1026): 72.607

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 688.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1027x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1027x2048): 79.074
Elapsed time for attention_prob_times_values (32x2048x2048x1027): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1027): 75.422

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 696.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1028x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1028x2048): 84.293
Elapsed time for attention_prob_times_values (32x2048x2048x1028): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1028): 74.552

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 714.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1029x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1029x2048): 76.873
Elapsed time for attention_prob_times_values (32x2048x2048x1029): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1029): 75.196

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 687.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1030x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1030x2048): 76.044
Elapsed time for attention_prob_times_values (32x2048x2048x1030): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1030): 77.664

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 695.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1031x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1031x2048): 79.138
Elapsed time for attention_prob_times_values (32x2048x2048x1031): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1031): 72.478

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 685.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1032x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1032x2048): 80.796
Elapsed time for attention_prob_times_values (32x2048x2048x1032): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1032): 81.648

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 736.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1033x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1033x2048): 78.484
Elapsed time for attention_prob_times_values (32x2048x2048x1033): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1033): 73.309

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 687.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1034x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1034x2048): 76.884
Elapsed time for attention_prob_times_values (32x2048x2048x1034): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1034): 78.039

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 703.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1035x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1035x2048): 75.503
Elapsed time for attention_prob_times_values (32x2048x2048x1035): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1035): 76.089

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 688.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1036x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1036x2048): 80.890
Elapsed time for attention_prob_times_values (32x2048x2048x1036): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1036): 78.295

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 723.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1037x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1037x2048): 78.820
Elapsed time for attention_prob_times_values (32x2048x2048x1037): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1037): 75.998

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 704.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1038x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1038x2048): 79.573
Elapsed time for attention_prob_times_values (32x2048x2048x1038): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1038): 78.230

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 718.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1039x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1039x2048): 78.400
Elapsed time for attention_prob_times_values (32x2048x2048x1039): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1039): 73.703

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 692.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1040x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1040x2048): 80.788
Elapsed time for attention_prob_times_values (32x2048x2048x1040): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1040): 82.793

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 746.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1041x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1041x2048): 78.373
Elapsed time for attention_prob_times_values (32x2048x2048x1041): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1041): 76.585

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 707.507
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 844.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x331x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x331x2048): 59.059
Elapsed time for attention_prob_times_values (160x2048x2048x331): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x331): 56.592

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 805.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x332x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x332x2048): 61.353
Elapsed time for attention_prob_times_values (160x2048x2048x332): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x332): 62.215

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 863.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x333x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x333x2048): 60.646
Elapsed time for attention_prob_times_values (160x2048x2048x333): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x333): 57.594

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 827.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x334x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x334x2048): 61.270
Elapsed time for attention_prob_times_values (160x2048x2048x334): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x334): 57.514

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 833.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x335x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x335x2048): 60.408
Elapsed time for attention_prob_times_values (160x2048x2048x335): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x335): 60.071

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 848.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x336x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x336x2048): 63.801
Elapsed time for attention_prob_times_values (160x2048x2048x336): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x336): 74.986

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 973.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x337x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x337x2048): 61.331
Elapsed time for attention_prob_times_values (160x2048x2048x337): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x337): 59.464

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 855.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x338x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x338x2048): 62.095
Elapsed time for attention_prob_times_values (160x2048x2048x338): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x338): 63.194

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 889.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x339x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x339x2048): 60.425
Elapsed time for attention_prob_times_values (160x2048x2048x339): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x339): 59.470

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 853.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x558x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x558x2048): 73.725
Elapsed time for attention_prob_times_values (80x2048x2048x558): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x558): 69.534

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 851.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x559x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x559x2048): 70.621
Elapsed time for attention_prob_times_values (80x2048x2048x559): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x559): 65.609

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 810.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x560x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x560x2048): 72.891
Elapsed time for attention_prob_times_values (80x2048x2048x560): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x560): 81.956

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 921.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x561x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x561x2048): 71.917
Elapsed time for attention_prob_times_values (80x2048x2048x561): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x561): 64.029

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 810.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x562x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x562x2048): 71.099
Elapsed time for attention_prob_times_values (80x2048x2048x562): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x562): 67.383

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 828.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x563x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x563x2048): 70.528
Elapsed time for attention_prob_times_values (80x2048x2048x563): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x563): 64.682

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 809.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x564x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x564x2048): 73.859
Elapsed time for attention_prob_times_values (80x2048x2048x564): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x564): 70.827

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 868.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x565x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x565x2048): 72.492
Elapsed time for attention_prob_times_values (80x2048x2048x565): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x565): 66.300

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 833.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x566x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x566x2048): 72.926
Elapsed time for attention_prob_times_values (80x2048x2048x566): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x566): 70.715

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 865.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x567x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x567x2048): 72.517
Elapsed time for attention_prob_times_values (80x2048x2048x567): 0.0057
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1042x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1042x2048): 74.370
Elapsed time for attention_prob_times_values (32x2048x2048x1042): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1042): 73.009

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 673.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1043x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1043x2048): 78.460
Elapsed time for attention_prob_times_values (32x2048x2048x1043): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1043): 76.742

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 709.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1044x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1044x2048): 80.131
Elapsed time for attention_prob_times_values (32x2048x2048x1044): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1044): 78.488

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 726.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1045x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1045x2048): 75.460
Elapsed time for attention_prob_times_values (32x2048x2048x1045): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1045): 73.338

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 681.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1046x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1046x2048): 78.744
Elapsed time for attention_prob_times_values (32x2048x2048x1046): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1046): 78.670

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 721.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1047x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1047x2048): 75.867
Elapsed time for attention_prob_times_values (32x2048x2048x1047): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1047): 69.989

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 668.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1048x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1048x2048): 80.400
Elapsed time for attention_prob_times_values (32x2048x2048x1048): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1048): 82.989

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 750.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1049x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1049x2048): 78.284
Elapsed time for attention_prob_times_values (32x2048x2048x1049): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1049): 77.159

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 714.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1050x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1050x2048): 78.992
Elapsed time for attention_prob_times_values (32x2048x2048x1050): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1050): 79.151

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 727.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (384x2048x161x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x161x2048): 62.528
Elapsed time for attention_prob_times_values (384x2048x2048x161): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x161): 53.707

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 929.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x162x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x162x2048): 63.728
Elapsed time for attention_prob_times_values (384x2048x2048x162): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x162): 56.043

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 965.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x163x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x163x2048): 62.947
Elapsed time for attention_prob_times_values (384x2048x2048x163): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x163): 54.055

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 946.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x164x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x164x2048): 64.702
Elapsed time for attention_prob_times_values (384x2048x2048x164): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x164): 57.262

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 994.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x165x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x165x2048): 63.433
Elapsed time for attention_prob_times_values (384x2048x2048x165): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x165): 55.521

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 975.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x166x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x166x2048): 63.383
Elapsed time for attention_prob_times_values (384x2048x2048x166): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x166): 57.601

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 999.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x167x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x167x2048): 63.867
Elapsed time for attention_prob_times_values (384x2048x2048x167): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x167): 54.356

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 978.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x168x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x168x2048): 63.539
Elapsed time for attention_prob_times_values (384x2048x2048x168): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x168): 55.107

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 988.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x169x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x169x2048): 62.050
Elapsed time for attention_prob_times_values (384x2048x2048x169): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x169): 54.489

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 977.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x170x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x170x2048): 65.085
Elapsed time for attention_prob_times_values (384x2048x2048x170): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x170): 56.460

Attention duration (in seconds): 0.0181
num_attention_heads: 8, hidden_size: 8408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1051x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1051x2048): 78.102
Elapsed time for attention_prob_times_values (32x2048x2048x1051): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1051): 77.220

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 715.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1052x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1052x2048): 79.418
Elapsed time for attention_prob_times_values (32x2048x2048x1052): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1052): 75.003

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 711.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1053x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1053x2048): 78.416
Elapsed time for attention_prob_times_values (32x2048x2048x1053): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1053): 77.181

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 717.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1054x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1054x2048): 78.913
Elapsed time for attention_prob_times_values (32x2048x2048x1054): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1054): 79.505

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 731.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1055x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1055x2048): 78.690
Elapsed time for attention_prob_times_values (32x2048x2048x1055): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1055): 77.458

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 721.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1056x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1056x2048): 81.385
Elapsed time for attention_prob_times_values (32x2048x2048x1056): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1056): 84.515

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 767.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1057x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1057x2048): 80.188
Elapsed time for attention_prob_times_values (32x2048x2048x1057): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1057): 77.107

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 727.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1058x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1058x2048): 80.930
Elapsed time for attention_prob_times_values (32x2048x2048x1058): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1058): 79.541

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 743.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1059x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1059x2048): 80.006
Elapsed time for attention_prob_times_values (32x2048x2048x1059): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1059): 77.732

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 731.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1060x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1060x2048): 81.637
Elapsed time for attention_prob_times_values (32x2048x2048x1060): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x567): 66.201

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 835.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x568x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x568x2048): 73.819
Elapsed time for attention_prob_times_values (80x2048x2048x568): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x568): 82.740

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 943.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x569x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x569x2048): 71.496
Elapsed time for attention_prob_times_values (80x2048x2048x569): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x569): 66.486

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 834.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x570x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x570x2048): 72.552
Elapsed time for attention_prob_times_values (80x2048x2048x570): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x570): 70.166

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 865.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x571x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x571x2048): 69.779
Elapsed time for attention_prob_times_values (80x2048x2048x571): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x571): 66.340

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 826.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x572x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x572x2048): 73.703
Elapsed time for attention_prob_times_values (80x2048x2048x572): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x572): 69.530

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 870.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x573x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x573x2048): 69.463
Elapsed time for attention_prob_times_values (80x2048x2048x573): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x573): 66.540

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 828.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x574x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x574x2048): 69.737
Elapsed time for attention_prob_times_values (80x2048x2048x574): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x574): 67.023

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 834.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x575x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x575x2048): 72.138
Elapsed time for attention_prob_times_values (80x2048x2048x575): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x575): 67.132

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 850.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x576x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x576x2048): 88.047
Elapsed time for attention_prob_times_values (80x2048x2048x576): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x576): 85.098

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1060.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 40, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x340x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x340x2048): 61.180
Elapsed time for attention_prob_times_values (160x2048x2048x340): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x340): 62.304

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 881.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x341x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x341x2048): 61.130
Elapsed time for attention_prob_times_values (160x2048x2048x341): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x341): 58.972

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 859.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x342x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x342x2048): 61.861
Elapsed time for attention_prob_times_values (160x2048x2048x342): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x342): 62.220

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 890.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x343x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x343x2048): 62.312
Elapsed time for attention_prob_times_values (160x2048x2048x343): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x343): 61.420

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 890.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x344x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x344x2048): 62.949
Elapsed time for attention_prob_times_values (160x2048x2048x344): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x344): 75.384

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 990.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x345x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x345x2048): 61.461
Elapsed time for attention_prob_times_values (160x2048x2048x345): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x345): 59.539

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 875.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x346x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x346x2048): 59.883
Elapsed time for attention_prob_times_values (160x2048x2048x346): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x346): 62.304

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 886.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x347x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x347x2048): 59.483
Elapsed time for attention_prob_times_values (160x2048x2048x347): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x347): 59.143

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 863.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x348x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x348x2048): 61.509
Elapsed time for attention_prob_times_values (160x2048x2048x348): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x348): 62.795

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 906.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x349x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x349x2048): 59.764
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1060): 79.846

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 749.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1061x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1061x2048): 79.916
Elapsed time for attention_prob_times_values (32x2048x2048x1061): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1061): 77.676

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 731.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1062x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1062x2048): 79.661
Elapsed time for attention_prob_times_values (32x2048x2048x1062): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1062): 79.927

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 741.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1063x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1063x2048): 79.746
Elapsed time for attention_prob_times_values (32x2048x2048x1063): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1063): 77.772

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 732.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1064x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1064x2048): 77.143
Elapsed time for attention_prob_times_values (32x2048x2048x1064): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1064): 84.091

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 749.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1065x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1065x2048): 79.252
Elapsed time for attention_prob_times_values (32x2048x2048x1065): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1065): 78.035

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 732.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1066x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1066x2048): 77.184
Elapsed time for attention_prob_times_values (32x2048x2048x1066): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1066): 77.224

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 720.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1067x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1067x2048): 79.411
Elapsed time for attention_prob_times_values (32x2048x2048x1067): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1067): 78.085

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 735.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1068x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1068x2048): 80.664
Elapsed time for attention_prob_times_values (32x2048x2048x1068): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1068): 80.481

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 752.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1069x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1069x2048): 79.539
Elapsed time for attention_prob_times_values (32x2048x2048x1069): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1069): 78.206

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 737.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x577x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x577x2048): 74.357
Elapsed time for attention_prob_times_values (80x2048x2048x577): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x577): 62.741

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 835.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x578x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x578x2048): 75.523
Elapsed time for attention_prob_times_values (80x2048x2048x578): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x578): 64.364

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 854.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x579x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x579x2048): 70.945
Elapsed time for attention_prob_times_values (80x2048x2048x579): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x579): 62.457

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 817.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x580x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x580x2048): 76.334
Elapsed time for attention_prob_times_values (80x2048x2048x580): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x580): 62.586

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 847.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x581x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x581x2048): 74.355
Elapsed time for attention_prob_times_values (80x2048x2048x581): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x581): 61.142

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 828.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x582x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x582x2048): 75.288
Elapsed time for attention_prob_times_values (80x2048x2048x582): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x582): 65.696

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 867.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x583x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x583x2048): 75.737
Elapsed time for attention_prob_times_values (80x2048x2048x583): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x583): 63.327

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 854.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x584x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x584x2048): 76.603
Elapsed time for attention_prob_times_values (80x2048x2048x584): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x584): 84.763

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 998.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x585x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x585x2048): 73.258
Elapsed time for attention_prob_times_values (80x2048x2048x585): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x585): 62.748

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 839.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x586x2048): 0.0055
========================================================================================================================
num_attention_heads: 8, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1070x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1070x2048): 80.529
Elapsed time for attention_prob_times_values (32x2048x2048x1070): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1070): 80.459

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 753.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1071x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1071x2048): 77.411
Elapsed time for attention_prob_times_values (32x2048x2048x1071): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1071): 78.357

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 729.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1072x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1072x2048): 81.376
Elapsed time for attention_prob_times_values (32x2048x2048x1072): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1072): 84.854

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 778.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1073x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1073x2048): 75.496
Elapsed time for attention_prob_times_values (32x2048x2048x1073): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1073): 78.519

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 722.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1074x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1074x2048): 80.059
Elapsed time for attention_prob_times_values (32x2048x2048x1074): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1074): 74.240

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 723.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1075x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1075x2048): 79.258
Elapsed time for attention_prob_times_values (32x2048x2048x1075): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1075): 71.322

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 705.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1076x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1076x2048): 78.048
Elapsed time for attention_prob_times_values (32x2048x2048x1076): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1076): 81.040

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 747.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1077x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1077x2048): 76.737
Elapsed time for attention_prob_times_values (32x2048x2048x1077): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1077): 78.873

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 732.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1078x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1078x2048): 75.083
Elapsed time for attention_prob_times_values (32x2048x2048x1078): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1078): 81.012

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 734.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1079x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1079x2048): 79.459
Elapsed time for attention_prob_times_values (32x2048x2048x1079): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1079): 77.448

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 739.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1080x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1080x2048): 81.472
Elapsed time for attention_prob_times_values (32x2048x2048x1080): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1080): 81.603

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 769.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1081x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1081x2048): 76.571
Elapsed time for attention_prob_times_values (32x2048x2048x1081): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1081): 75.051

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 715.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1082x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1082x2048): 77.402
Elapsed time for attention_prob_times_values (32x2048x2048x1082): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1082): 81.389

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 750.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1083x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1083x2048): 79.122
Elapsed time for attention_prob_times_values (32x2048x2048x1083): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1083): 79.158

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 748.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1084x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1084x2048): 75.431
Elapsed time for attention_prob_times_values (32x2048x2048x1084): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1084): 81.566

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 742.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1085x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1085x2048): 73.484
Elapsed time for attention_prob_times_values (32x2048x2048x1085): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1085): 79.314

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 722.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1086x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1086x2048): 79.948
Elapsed time for attention_prob_times_values (32x2048x2048x1086): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1086): 76.751

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 742.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1087x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1087x2048): 79.481
Elapsed time for attention_prob_times_values (32x2048x2048x1087): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1087): 79.209

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 753.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1088x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1088x2048): 91.532
Elapsed time for attention_prob_times_values (32x2048x2048x1088): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1088): 87.655

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 850.742
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (160x2048x2048x349): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x349): 60.391

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 879.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x350x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x350x2048): 62.042
Elapsed time for attention_prob_times_values (160x2048x2048x350): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x350): 62.767

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 915.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x351x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x351x2048): 60.176
Elapsed time for attention_prob_times_values (160x2048x2048x351): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x351): 62.045

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 898.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x352x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x352x2048): 78.194
Elapsed time for attention_prob_times_values (160x2048x2048x352): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x352): 74.036

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1121.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x353x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x353x2048): 62.246
Elapsed time for attention_prob_times_values (160x2048x2048x353): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x353): 59.558

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 900.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x354x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x354x2048): 65.679
Elapsed time for attention_prob_times_values (160x2048x2048x354): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x354): 63.349

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 956.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x355x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x355x2048): 63.700
Elapsed time for attention_prob_times_values (160x2048x2048x355): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x355): 63.113

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 942.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x356x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x356x2048): 64.401
Elapsed time for attention_prob_times_values (160x2048x2048x356): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x356): 63.916

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 956.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x357x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x357x2048): 64.503
Elapsed time for attention_prob_times_values (160x2048x2048x357): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x357): 63.864

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 959.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x358x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x358x2048): 64.876
Elapsed time for attention_prob_times_values (160x2048x2048x358): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x358): 63.990

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 965.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x586x2048): 71.830
Elapsed time for attention_prob_times_values (80x2048x2048x586): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x586): 62.614

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 832.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x587x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x587x2048): 73.587
Elapsed time for attention_prob_times_values (80x2048x2048x587): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x587): 62.907

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 845.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x588x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x588x2048): 73.821
Elapsed time for attention_prob_times_values (80x2048x2048x588): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x588): 66.453

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 873.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x589x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x589x2048): 72.778
Elapsed time for attention_prob_times_values (80x2048x2048x589): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x589): 63.718

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 849.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x590x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x590x2048): 72.129
Elapsed time for attention_prob_times_values (80x2048x2048x590): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x590): 66.190

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 864.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x591x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x591x2048): 73.568
Elapsed time for attention_prob_times_values (80x2048x2048x591): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x591): 63.761

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 856.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x592x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x592x2048): 75.653
Elapsed time for attention_prob_times_values (80x2048x2048x592): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x592): 86.536

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1014.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x593x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x593x2048): 70.928
Elapsed time for attention_prob_times_values (80x2048x2048x593): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x593): 61.848

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 831.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x594x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x594x2048): 73.746
Elapsed time for attention_prob_times_values (80x2048x2048x594): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x594): 66.506

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 881.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x595x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x595x2048): 73.077
Elapsed time for attention_prob_times_values (80x2048x2048x595): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x595): 64.212

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 862.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Attention throughput (in TFLOP/s): 1024.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x171x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x171x2048): 64.084
Elapsed time for attention_prob_times_values (384x2048x2048x171): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x171): 55.958

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1017.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x172x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x172x2048): 64.627
Elapsed time for attention_prob_times_values (384x2048x2048x172): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x172): 58.866

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1055.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x173x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x173x2048): 64.206
Elapsed time for attention_prob_times_values (384x2048x2048x173): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x173): 56.572

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1035.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x174x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x174x2048): 65.409
Elapsed time for attention_prob_times_values (384x2048x2048x174): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x174): 60.004

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1083.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x175x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x175x2048): 64.831
Elapsed time for attention_prob_times_values (384x2048x2048x175): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x175): 58.311

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1068.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x176x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x176x2048): 67.793
Elapsed time for attention_prob_times_values (384x2048x2048x176): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x176): 56.775

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1081.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x177x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x177x2048): 64.108
Elapsed time for attention_prob_times_values (384x2048x2048x177): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x177): 58.231

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1073.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x178x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x178x2048): 66.594
Elapsed time for attention_prob_times_values (384x2048x2048x178): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x178): 61.108

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1127.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x179x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x179x2048): 63.489
Elapsed time for attention_prob_times_values (384x2048x2048x179): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x179): 58.401

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1081.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1089x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1089x2048): 80.755
Elapsed time for attention_prob_times_values (32x2048x2048x1089): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1089): 79.385

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 761.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1090x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1090x2048): 70.353
Elapsed time for attention_prob_times_values (32x2048x2048x1090): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1090): 81.854

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 720.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1091x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1091x2048): 80.416
Elapsed time for attention_prob_times_values (32x2048x2048x1091): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1091): 79.665

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 762.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1092x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1092x2048): 81.856
Elapsed time for attention_prob_times_values (32x2048x2048x1092): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1092): 81.998

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 780.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1093x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1093x2048): 77.877
Elapsed time for attention_prob_times_values (32x2048x2048x1093): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1093): 77.460

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 740.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1094x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1094x2048): 81.083
Elapsed time for attention_prob_times_values (32x2048x2048x1094): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1094): 81.534

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 776.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1095x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1095x2048): 80.270
Elapsed time for attention_prob_times_values (32x2048x2048x1095): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1095): 80.061

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 765.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1096x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1096x2048): 82.304
Elapsed time for attention_prob_times_values (32x2048x2048x1096): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1096): 86.602

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 807.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1097x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1097x2048): 76.806
Elapsed time for attention_prob_times_values (32x2048x2048x1097): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1097): 80.192

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 750.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1098x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1098x2048): 79.711
Elapsed time for attention_prob_times_values (32x2048x2048x1098): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1098): 77.903

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 754.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1099x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1099x2048): 79.851
Elapsed time for attention_prob_times_values (32x2048x2048x1099): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1099): 76.522

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 749.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1100x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1100x2048): 81.399
Elapsed time for attention_prob_times_values (32x2048x2048x1100): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1100): 82.516

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 786.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1101x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1101x2048): 79.816
Elapsed time for attention_prob_times_values (32x2048x2048x1101): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1101): 80.411

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 769.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1102x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1102x2048): 80.777
Elapsed time for attention_prob_times_values (32x2048x2048x1102): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1102): 82.551

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 784.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1103x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1103x2048): 79.883
Elapsed time for attention_prob_times_values (32x2048x2048x1103): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1103): 80.515

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 771.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1104x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1104x2048): 82.073
Elapsed time for attention_prob_times_values (32x2048x2048x1104): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1104): 87.553

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 815.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1105x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1105x2048): 78.488
Elapsed time for attention_prob_times_values (32x2048x2048x1105): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1105): 77.573

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 751.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1106x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1106x2048): 76.424
Elapsed time for attention_prob_times_values (32x2048x2048x1106): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1106): 82.889

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 766.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1107x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1107x2048): 79.508
Elapsed time for attention_prob_times_values (32x2048x2048x1107): 0.0039
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x596x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x596x2048): 74.417
Elapsed time for attention_prob_times_values (80x2048x2048x596): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x596): 66.946

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 890.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x597x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x597x2048): 73.055
Elapsed time for attention_prob_times_values (80x2048x2048x597): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x597): 62.639

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 853.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x598x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x598x2048): 71.148
Elapsed time for attention_prob_times_values (80x2048x2048x598): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x598): 66.673

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 872.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x599x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x599x2048): 71.433
Elapsed time for attention_prob_times_values (80x2048x2048x599): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x599): 64.572

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 861.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x600x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x600x2048): 75.382
Elapsed time for attention_prob_times_values (80x2048x2048x600): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x600): 86.996

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1027.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x601x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x601x2048): 72.557
Elapsed time for attention_prob_times_values (80x2048x2048x601): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x601): 63.150

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 860.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x602x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x602x2048): 73.357
Elapsed time for attention_prob_times_values (80x2048x2048x602): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x602): 67.193

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 894.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x603x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x603x2048): 73.005
Elapsed time for attention_prob_times_values (80x2048x2048x603): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x603): 62.886

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 863.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x604x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x604x2048): 72.869
Elapsed time for attention_prob_times_values (80x2048x2048x604): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x604): 66.468

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 889.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1107): 75.910

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 749.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1108x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1108x2048): 81.249
Elapsed time for attention_prob_times_values (32x2048x2048x1108): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1108): 83.056

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 793.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1109x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1109x2048): 74.115
Elapsed time for attention_prob_times_values (32x2048x2048x1109): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1109): 80.985

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 747.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1110x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1110x2048): 80.359
Elapsed time for attention_prob_times_values (32x2048x2048x1110): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1110): 83.095

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 790.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1111x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1111x2048): 79.785
Elapsed time for attention_prob_times_values (32x2048x2048x1111): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1111): 81.036

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 778.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1112x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1112x2048): 81.980
Elapsed time for attention_prob_times_values (32x2048x2048x1112): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1112): 87.878

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 821.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1113x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1113x2048): 79.407
Elapsed time for attention_prob_times_values (32x2048x2048x1113): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1113): 81.151

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 778.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1114x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1114x2048): 77.485
Elapsed time for attention_prob_times_values (32x2048x2048x1114): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1114): 83.333

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 779.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1115x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1115x2048): 77.287
Elapsed time for attention_prob_times_values (32x2048x2048x1115): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1115): 81.160

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 768.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1116x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1116x2048): 81.359
Elapsed time for attention_prob_times_values (32x2048x2048x1116): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1116): 83.478

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 800.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x359x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x359x2048): 63.410
Elapsed time for attention_prob_times_values (160x2048x2048x359): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x359): 63.591

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 953.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x360x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x360x2048): 63.582
Elapsed time for attention_prob_times_values (160x2048x2048x360): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x360): 79.898

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1066.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x361x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x361x2048): 61.233
Elapsed time for attention_prob_times_values (160x2048x2048x361): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x361): 63.411

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 940.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x362x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x362x2048): 63.994
Elapsed time for attention_prob_times_values (160x2048x2048x362): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x362): 64.582

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 973.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x363x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x363x2048): 63.570
Elapsed time for attention_prob_times_values (160x2048x2048x363): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x363): 62.239

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 954.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x364x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x364x2048): 64.154
Elapsed time for attention_prob_times_values (160x2048x2048x364): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x364): 65.164

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 983.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x365x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x365x2048): 62.347
Elapsed time for attention_prob_times_values (160x2048x2048x365): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x365): 62.972

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 956.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x366x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x366x2048): 64.516
Elapsed time for attention_prob_times_values (160x2048x2048x366): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x366): 62.932

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 974.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x367x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x367x2048): 62.226
Elapsed time for attention_prob_times_values (160x2048x2048x367): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x367): 64.426

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 970.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 8, hidden_size: 8936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1117x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1117x2048): 79.441
Elapsed time for attention_prob_times_values (32x2048x2048x1117): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1117): 81.384

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 782.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1118x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1118x2048): 80.360
Elapsed time for attention_prob_times_values (32x2048x2048x1118): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1118): 80.736

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 784.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1119x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1119x2048): 79.775
Elapsed time for attention_prob_times_values (32x2048x2048x1119): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1119): 79.118

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 773.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1120x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1120x2048): 91.616
Elapsed time for attention_prob_times_values (32x2048x2048x1120): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1120): 89.554

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 883.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1121x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1121x2048): 80.870
Elapsed time for attention_prob_times_values (32x2048x2048x1121): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1121): 81.607

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 792.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1122x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1122x2048): 81.696
Elapsed time for attention_prob_times_values (32x2048x2048x1122): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1122): 83.345

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 805.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1123x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1123x2048): 80.714
Elapsed time for attention_prob_times_values (32x2048x2048x1123): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1123): 79.024

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 780.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1124x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1124x2048): 81.926
Elapsed time for attention_prob_times_values (32x2048x2048x1124): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1124): 84.048

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 811.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1125x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1125x2048): 80.534
Elapsed time for attention_prob_times_values (32x2048x2048x1125): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1125): 81.888

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 794.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1126x2048): 0.0037
--------
Elapsed time for attention_key_query_prob (80x2048x605x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x605x2048): 72.987
Elapsed time for attention_prob_times_values (80x2048x2048x605): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x605): 63.713

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 871.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x606x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x606x2048): 72.671
Elapsed time for attention_prob_times_values (80x2048x2048x606): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x606): 66.623

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 892.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x607x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x607x2048): 73.184
Elapsed time for attention_prob_times_values (80x2048x2048x607): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x607): 65.157

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 886.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x608x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x608x2048): 86.638
Elapsed time for attention_prob_times_values (80x2048x2048x608): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x608): 89.329

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1132.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x609x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x609x2048): 75.029
Elapsed time for attention_prob_times_values (80x2048x2048x609): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x609): 62.903

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 882.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x610x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x610x2048): 73.935
Elapsed time for attention_prob_times_values (80x2048x2048x610): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x610): 64.758

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 891.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x611x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x611x2048): 74.931
Elapsed time for attention_prob_times_values (80x2048x2048x611): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x611): 65.755

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 905.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x612x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x612x2048): 76.412
Elapsed time for attention_prob_times_values (80x2048x2048x612): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x612): 65.669

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 914.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x613x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x613x2048): 74.491
Elapsed time for attention_prob_times_values (80x2048x2048x613): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x613): 64.045

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 893.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x614x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x614x2048): 75.522
Elapsed time for attention_prob_times_values (80x2048x2048x614): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x614): 67.526

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 926.351
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1126x2048): 81.251
Elapsed time for attention_prob_times_values (32x2048x2048x1126): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1126): 83.924

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 808.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1127x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1127x2048): 77.803
Elapsed time for attention_prob_times_values (32x2048x2048x1127): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1127): 81.927

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 782.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1128x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1128x2048): 76.465
Elapsed time for attention_prob_times_values (32x2048x2048x1128): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1128): 85.812

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 793.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1129x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1129x2048): 79.765
Elapsed time for attention_prob_times_values (32x2048x2048x1129): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1129): 74.327

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 755.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1130x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1130x2048): 80.416
Elapsed time for attention_prob_times_values (32x2048x2048x1130): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1130): 84.134

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 808.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1131x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1131x2048): 79.952
Elapsed time for attention_prob_times_values (32x2048x2048x1131): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1131): 82.001

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 796.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1132x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1132x2048): 80.876
Elapsed time for attention_prob_times_values (32x2048x2048x1132): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1132): 84.066

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 811.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1133x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1133x2048): 79.873
Elapsed time for attention_prob_times_values (32x2048x2048x1133): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1133): 82.135

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 797.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1134x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1134x2048): 80.614
Elapsed time for attention_prob_times_values (32x2048x2048x1134): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1134): 76.651

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 774.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1135x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1135x2048): 80.079
Elapsed time for attention_prob_times_values (32x2048x2048x1135): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1135): 78.601

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 782.794
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x180x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x180x2048): 67.261
Elapsed time for attention_prob_times_values (384x2048x2048x180): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x180): 61.735

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1150.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x181x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x181x2048): 63.182
Elapsed time for attention_prob_times_values (384x2048x2048x181): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x181): 56.925

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1076.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x182x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x182x2048): 67.586
Elapsed time for attention_prob_times_values (384x2048x2048x182): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x182): 61.232

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1160.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x183x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x183x2048): 65.479
Elapsed time for attention_prob_times_values (384x2048x2048x183): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x183): 58.797

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1124.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x184x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x184x2048): 67.055
Elapsed time for attention_prob_times_values (384x2048x2048x184): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x184): 59.961

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1155.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x185x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x185x2048): 66.391
Elapsed time for attention_prob_times_values (384x2048x2048x185): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x185): 60.274

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1159.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x186x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x186x2048): 67.629
Elapsed time for attention_prob_times_values (384x2048x2048x186): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x186): 63.098

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1203.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x187x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x187x2048): 63.105
Elapsed time for attention_prob_times_values (384x2048x2048x187): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x187): 60.332

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1143.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x188x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x188x2048): 66.154
Elapsed time for attention_prob_times_values (384x2048x2048x188): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x188): 61.678

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1188.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x189x2048): 0.0092
Elapsed time for attention_key_query_prob (160x2048x368x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x368x2048): 63.739
Elapsed time for attention_prob_times_values (160x2048x2048x368): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x368): 81.520

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1099.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x369x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x369x2048): 63.103
Elapsed time for attention_prob_times_values (160x2048x2048x369): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x369): 63.869

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 978.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x370x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x370x2048): 63.502
Elapsed time for attention_prob_times_values (160x2048x2048x370): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x370): 63.597

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 982.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x371x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x371x2048): 61.667
Elapsed time for attention_prob_times_values (160x2048x2048x371): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x371): 64.154

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 974.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x372x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x372x2048): 62.453
Elapsed time for attention_prob_times_values (160x2048x2048x372): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x372): 66.284

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 998.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x373x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x373x2048): 62.283
Elapsed time for attention_prob_times_values (160x2048x2048x373): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x373): 64.425

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 986.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x374x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x374x2048): 61.908
Elapsed time for attention_prob_times_values (160x2048x2048x374): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x374): 65.343

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 992.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x375x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x375x2048): 63.792
Elapsed time for attention_prob_times_values (160x2048x2048x375): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x375): 63.923

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 999.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x376x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x376x2048): 63.954
Elapsed time for attention_prob_times_values (160x2048x2048x376): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x376): 81.118

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1121.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x377x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x377x2048): 61.074
Elapsed time for attention_prob_times_values (160x2048x2048x377): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x377): 64.393

Attention duration (in seconds): 0.0161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x615x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x615x2048): 73.603
Elapsed time for attention_prob_times_values (80x2048x2048x615): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x615): 64.474

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 894.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x616x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x616x2048): 76.238
Elapsed time for attention_prob_times_values (80x2048x2048x616): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x616): 89.616

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1073.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x617x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x617x2048): 73.567
Elapsed time for attention_prob_times_values (80x2048x2048x617): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x617): 66.168

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 909.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x618x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x618x2048): 74.373
Elapsed time for attention_prob_times_values (80x2048x2048x618): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x618): 68.337

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 930.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x619x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x619x2048): 73.774
Elapsed time for attention_prob_times_values (80x2048x2048x619): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x619): 66.471

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 915.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x620x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x620x2048): 70.528
Elapsed time for attention_prob_times_values (80x2048x2048x620): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x620): 68.103

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 908.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x621x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x621x2048): 73.651
Elapsed time for attention_prob_times_values (80x2048x2048x621): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x621): 65.961

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 913.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x622x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x622x2048): 68.720
Elapsed time for attention_prob_times_values (80x2048x2048x622): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x622): 65.393

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 881.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x623x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x623x2048): 72.218
Elapsed time for attention_prob_times_values (80x2048x2048x623): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x623): 66.722

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 913.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1136x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1136x2048): 82.310
Elapsed time for attention_prob_times_values (32x2048x2048x1136): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1136): 89.430

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 846.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1137x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1137x2048): 79.291
Elapsed time for attention_prob_times_values (32x2048x2048x1137): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1137): 82.293

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 798.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1138x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1138x2048): 80.170
Elapsed time for attention_prob_times_values (32x2048x2048x1138): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1138): 84.057

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 811.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1139x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1139x2048): 79.642
Elapsed time for attention_prob_times_values (32x2048x2048x1139): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1139): 81.780

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 798.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1140x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1140x2048): 80.939
Elapsed time for attention_prob_times_values (32x2048x2048x1140): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1140): 84.732

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 820.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1141x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1141x2048): 79.757
Elapsed time for attention_prob_times_values (32x2048x2048x1141): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1141): 74.031

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 761.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1142x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1142x2048): 80.431
Elapsed time for attention_prob_times_values (32x2048x2048x1142): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1142): 84.391

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 817.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1143x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1143x2048): 79.704
Elapsed time for attention_prob_times_values (32x2048x2048x1143): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1143): 82.679

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 805.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1144x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1144x2048): 81.671
Elapsed time for attention_prob_times_values (32x2048x2048x1144): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1144): 90.072

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 851.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1145x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1145x2048): 76.372
Elapsed time for attention_prob_times_values (32x2048x2048x1145): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1145): 82.474

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 788.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1146x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1146x2048): 77.161
Elapsed time for attention_prob_times_values (32x2048x2048x1146): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1146): 81.313

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 788.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1147x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1147x2048): 79.344
Elapsed time for attention_prob_times_values (32x2048x2048x1147): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1147): 82.224

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 804.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1148x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1148x2048): 80.758
Elapsed time for attention_prob_times_values (32x2048x2048x1148): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1148): 84.812

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 824.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1149x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1149x2048): 78.939
Elapsed time for attention_prob_times_values (32x2048x2048x1149): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1149): 82.672

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 805.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1150x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1150x2048): 79.999
Elapsed time for attention_prob_times_values (32x2048x2048x1150): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1150): 84.815

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 822.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1151x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1151x2048): 79.149
Elapsed time for attention_prob_times_values (32x2048x2048x1151): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1151): 83.114

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 810.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1152x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1152x2048): 88.553
Elapsed time for attention_prob_times_values (32x2048x2048x1152): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1152): 88.414

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 884.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1153x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1153x2048): 80.726
Elapsed time for attention_prob_times_values (32x2048x2048x1153): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1153): 76.430

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 785.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1154x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1154x2048): 81.732
Elapsed time for attention_prob_times_values (32x2048x2048x1154): 0.0041
num_attention_heads: 20, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x624x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x624x2048): 76.048
Elapsed time for attention_prob_times_values (80x2048x2048x624): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x624): 91.161

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1093.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x625x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x625x2048): 71.339
Elapsed time for attention_prob_times_values (80x2048x2048x625): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x625): 65.287

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 900.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x626x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x626x2048): 72.709
Elapsed time for attention_prob_times_values (80x2048x2048x626): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x626): 68.808

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 935.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x627x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x627x2048): 73.303
Elapsed time for attention_prob_times_values (80x2048x2048x627): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x627): 67.258

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 929.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x628x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x628x2048): 70.834
Elapsed time for attention_prob_times_values (80x2048x2048x628): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x628): 69.141

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 928.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x629x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x629x2048): 73.708
Elapsed time for attention_prob_times_values (80x2048x2048x629): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x629): 66.848

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 931.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x630x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x630x2048): 71.371
Elapsed time for attention_prob_times_values (80x2048x2048x630): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x630): 66.234

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 914.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x631x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x631x2048): 72.672
Elapsed time for attention_prob_times_values (80x2048x2048x631): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x631): 66.831

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 927.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x632x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x632x2048): 74.719
Elapsed time for attention_prob_times_values (80x2048x2048x632): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x632): 91.996

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1100.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x633x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x633x2048): 72.659
Elapsed time for attention_prob_times_values (80x2048x2048x633): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1154): 75.827

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 787.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1155x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1155x2048): 77.995
Elapsed time for attention_prob_times_values (32x2048x2048x1155): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1155): 76.743

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 775.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1156x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1156x2048): 82.178
Elapsed time for attention_prob_times_values (32x2048x2048x1156): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1156): 78.915

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 807.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1157x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1157x2048): 80.541
Elapsed time for attention_prob_times_values (32x2048x2048x1157): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1157): 76.740

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 789.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1158x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1158x2048): 79.840
Elapsed time for attention_prob_times_values (32x2048x2048x1158): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1158): 76.784

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 786.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1159x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1159x2048): 80.550
Elapsed time for attention_prob_times_values (32x2048x2048x1159): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1159): 75.529

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 783.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1160x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1160x2048): 79.579
Elapsed time for attention_prob_times_values (32x2048x2048x1160): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1160): 83.405

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 819.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1161x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1161x2048): 79.900
Elapsed time for attention_prob_times_values (32x2048x2048x1161): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1161): 77.028

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 789.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1162x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1162x2048): 80.764
Elapsed time for attention_prob_times_values (32x2048x2048x1162): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1162): 79.118

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 805.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1163x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1163x2048): 75.567
Elapsed time for attention_prob_times_values (32x2048x2048x1163): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1163): 77.111

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 769.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 985.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x378x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x378x2048): 60.880
Elapsed time for attention_prob_times_values (160x2048x2048x378): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x378): 64.780

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 989.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x379x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x379x2048): 60.921
Elapsed time for attention_prob_times_values (160x2048x2048x379): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x379): 63.273

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 981.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x380x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x380x2048): 63.358
Elapsed time for attention_prob_times_values (160x2048x2048x380): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x380): 65.521

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1020.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x381x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x381x2048): 60.791
Elapsed time for attention_prob_times_values (160x2048x2048x381): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x381): 64.822

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 996.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x382x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x382x2048): 64.061
Elapsed time for attention_prob_times_values (160x2048x2048x382): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x382): 66.810

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1041.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x383x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x383x2048): 63.145
Elapsed time for attention_prob_times_values (160x2048x2048x383): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x383): 64.672

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1019.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x384x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x384x2048): 78.015
Elapsed time for attention_prob_times_values (160x2048x2048x384): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x384): 79.513

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1260.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x385x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x385x2048): 59.882
Elapsed time for attention_prob_times_values (160x2048x2048x385): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x385): 57.893

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 944.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x386x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x386x2048): 65.296
Elapsed time for attention_prob_times_values (160x2048x2048x386): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x386): 61.015

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1014.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1164x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1164x2048): 81.370
Elapsed time for attention_prob_times_values (32x2048x2048x1164): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1164): 71.369

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 767.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1165x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1165x2048): 80.550
Elapsed time for attention_prob_times_values (32x2048x2048x1165): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1165): 77.203

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 796.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1166x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1166x2048): 82.214
Elapsed time for attention_prob_times_values (32x2048x2048x1166): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1166): 79.366

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 816.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1167x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1167x2048): 80.728
Elapsed time for attention_prob_times_values (32x2048x2048x1167): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1167): 77.440

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 799.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1168x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1168x2048): 82.500
Elapsed time for attention_prob_times_values (32x2048x2048x1168): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1168): 84.880

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 847.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1169x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1169x2048): 79.986
Elapsed time for attention_prob_times_values (32x2048x2048x1169): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1169): 77.522

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 797.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1170x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1170x2048): 78.453
Elapsed time for attention_prob_times_values (32x2048x2048x1170): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1170): 79.427

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 800.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1171x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1171x2048): 80.094
Elapsed time for attention_prob_times_values (32x2048x2048x1171): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1171): 77.696

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 800.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1172x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1172x2048): 77.516
Elapsed time for attention_prob_times_values (32x2048x2048x1172): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1172): 79.780

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 798.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1173x2048): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x633): 65.406

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 919.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x634x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x634x2048): 72.240
Elapsed time for attention_prob_times_values (80x2048x2048x634): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x634): 69.281

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 946.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x635x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x635x2048): 71.868
Elapsed time for attention_prob_times_values (80x2048x2048x635): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x635): 67.660

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 934.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x636x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x636x2048): 74.318
Elapsed time for attention_prob_times_values (80x2048x2048x636): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x636): 68.800

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 959.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x637x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x637x2048): 68.750
Elapsed time for attention_prob_times_values (80x2048x2048x637): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x637): 67.809

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 917.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x638x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x638x2048): 71.045
Elapsed time for attention_prob_times_values (80x2048x2048x638): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x638): 68.175

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 936.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x639x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x639x2048): 69.760
Elapsed time for attention_prob_times_values (80x2048x2048x639): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x639): 65.512

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 910.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x640x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x640x2048): 81.966
Elapsed time for attention_prob_times_values (80x2048x2048x640): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x640): 94.983

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1187.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x641x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x641x2048): 72.393
Elapsed time for attention_prob_times_values (80x2048x2048x641): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x641): 59.535

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 883.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x642x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x642x2048): 75.867
Elapsed time for attention_prob_times_values (80x2048x2048x642): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x642): 64.879

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 946.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1173x2048): 80.280
Elapsed time for attention_prob_times_values (32x2048x2048x1173): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1173): 75.779

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 792.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1174x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1174x2048): 81.053
Elapsed time for attention_prob_times_values (32x2048x2048x1174): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1174): 76.493

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 800.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1175x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1175x2048): 81.936
Elapsed time for attention_prob_times_values (32x2048x2048x1175): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1175): 77.783

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 812.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1176x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1176x2048): 77.854
Elapsed time for attention_prob_times_values (32x2048x2048x1176): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1176): 84.383

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 825.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1177x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1177x2048): 79.874
Elapsed time for attention_prob_times_values (32x2048x2048x1177): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1177): 71.492

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 769.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1178x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1178x2048): 80.480
Elapsed time for attention_prob_times_values (32x2048x2048x1178): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1178): 74.013

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 786.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1179x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1179x2048): 79.793
Elapsed time for attention_prob_times_values (32x2048x2048x1179): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1179): 74.312

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 785.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1180x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1180x2048): 74.105
Elapsed time for attention_prob_times_values (32x2048x2048x1180): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1180): 80.188

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 787.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1181x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1181x2048): 73.104
Elapsed time for attention_prob_times_values (32x2048x2048x1181): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1181): 78.286

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 773.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1182x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1182x2048): 80.636
Elapsed time for attention_prob_times_values (32x2048x2048x1182): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1182): 73.578

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 787.491
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x189x2048): 66.274
Elapsed time for attention_prob_times_values (384x2048x2048x189): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x189): 61.964

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1198.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x190x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x190x2048): 67.340
Elapsed time for attention_prob_times_values (384x2048x2048x190): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x190): 60.724

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1201.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x191x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x191x2048): 65.932
Elapsed time for attention_prob_times_values (384x2048x2048x191): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x191): 61.232

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1200.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x192x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x192x2048): 75.300
Elapsed time for attention_prob_times_values (384x2048x2048x192): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x192): 64.792

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1323.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x193x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x193x2048): 66.501
Elapsed time for attention_prob_times_values (384x2048x2048x193): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x193): 51.648

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1110.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x194x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x194x2048): 64.241
Elapsed time for attention_prob_times_values (384x2048x2048x194): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x194): 53.592

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1121.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x195x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x195x2048): 66.291
Elapsed time for attention_prob_times_values (384x2048x2048x195): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x195): 51.820

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1121.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x196x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x196x2048): 68.229
Elapsed time for attention_prob_times_values (384x2048x2048x196): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x196): 53.253

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1158.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x197x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x197x2048): 67.272
Elapsed time for attention_prob_times_values (384x2048x2048x197): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x197): 53.187

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1156.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x198x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x198x2048): 65.208
Elapsed time for attention_prob_times_values (384x2048x2048x198): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x198): 53.698

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1152.150
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x387x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x387x2048): 65.479
Elapsed time for attention_prob_times_values (160x2048x2048x387): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x387): 58.197

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 993.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x388x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x388x2048): 65.684
Elapsed time for attention_prob_times_values (160x2048x2048x388): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x388): 30.830

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 677.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x389x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x389x2048): 63.567
Elapsed time for attention_prob_times_values (160x2048x2048x389): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x389): 58.625

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 987.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x390x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x390x2048): 64.623
Elapsed time for attention_prob_times_values (160x2048x2048x390): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x390): 61.798

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1025.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x391x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x391x2048): 42.828
Elapsed time for attention_prob_times_values (160x2048x2048x391): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x391): 56.447

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 792.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x392x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x392x2048): 66.704
Elapsed time for attention_prob_times_values (160x2048x2048x392): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x392): 73.375

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1139.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x393x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x393x2048): 62.584
Elapsed time for attention_prob_times_values (160x2048x2048x393): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x393): 57.562

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 980.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x394x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x394x2048): 64.264
Elapsed time for attention_prob_times_values (160x2048x2048x394): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x394): 61.725

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1032.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x395x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x395x2048): 62.375
Elapsed time for attention_prob_times_values (160x2048x2048x395): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x395): 57.893

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 986.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x396x2048): 0.0083
========================================================================================================================
num_attention_heads: 20, hidden_size: 12860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x643x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x643x2048): 74.696
Elapsed time for attention_prob_times_values (80x2048x2048x643): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x643): 62.593

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 923.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x644x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x644x2048): 76.476
Elapsed time for attention_prob_times_values (80x2048x2048x644): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x644): 64.904

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 953.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x645x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x645x2048): 74.922
Elapsed time for attention_prob_times_values (80x2048x2048x645): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x645): 62.524

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 926.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x646x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x646x2048): 75.324
Elapsed time for attention_prob_times_values (80x2048x2048x646): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x646): 65.311

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 952.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x647x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x647x2048): 70.230
Elapsed time for attention_prob_times_values (80x2048x2048x647): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x647): 63.114

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 906.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x648x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x648x2048): 76.460
Elapsed time for attention_prob_times_values (80x2048x2048x648): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x648): 77.655

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1052.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x649x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x649x2048): 74.096
Elapsed time for attention_prob_times_values (80x2048x2048x649): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x649): 62.780

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 929.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x650x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x650x2048): 74.756
Elapsed time for attention_prob_times_values (80x2048x2048x650): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x650): 65.319

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 954.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x651x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x651x2048): 74.147
Elapsed time for attention_prob_times_values (80x2048x2048x651): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x651): 63.128

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 935.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x652x2048): 0.0058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1183x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1183x2048): 80.011
Elapsed time for attention_prob_times_values (32x2048x2048x1183): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1183): 78.340

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 810.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1184x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1184x2048): 91.702
Elapsed time for attention_prob_times_values (32x2048x2048x1184): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1184): 86.193

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 910.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1185x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1185x2048): 81.358
Elapsed time for attention_prob_times_values (32x2048x2048x1185): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1185): 76.788

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 810.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1186x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1186x2048): 78.994
Elapsed time for attention_prob_times_values (32x2048x2048x1186): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1186): 75.267

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 791.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1187x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1187x2048): 81.302
Elapsed time for attention_prob_times_values (32x2048x2048x1187): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1187): 78.536

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 820.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1188x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1188x2048): 79.556
Elapsed time for attention_prob_times_values (32x2048x2048x1188): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1188): 80.664

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 823.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1189x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1189x2048): 76.866
Elapsed time for attention_prob_times_values (32x2048x2048x1189): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1189): 78.622

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 799.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1190x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1190x2048): 78.999
Elapsed time for attention_prob_times_values (32x2048x2048x1190): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1190): 77.657

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 806.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1191x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1191x2048): 80.892
Elapsed time for attention_prob_times_values (32x2048x2048x1191): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1191): 73.352

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 792.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1192x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1192x2048): 82.900
Elapsed time for attention_prob_times_values (32x2048x2048x1192): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1192): 85.937

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 870.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1193x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1193x2048): 79.583
Elapsed time for attention_prob_times_values (32x2048x2048x1193): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1193): 78.462

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 815.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1194x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1194x2048): 81.159
Elapsed time for attention_prob_times_values (32x2048x2048x1194): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1194): 81.008

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 837.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1195x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1195x2048): 80.354
Elapsed time for attention_prob_times_values (32x2048x2048x1195): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1195): 78.097

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 818.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1196x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1196x2048): 82.390
Elapsed time for attention_prob_times_values (32x2048x2048x1196): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1196): 80.532

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 842.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1197x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1197x2048): 80.768
Elapsed time for attention_prob_times_values (32x2048x2048x1197): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1197): 78.864

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 826.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1198x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1198x2048): 81.688
Elapsed time for attention_prob_times_values (32x2048x2048x1198): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1198): 81.260

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 844.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1199x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1199x2048): 76.619
Elapsed time for attention_prob_times_values (32x2048x2048x1199): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1199): 79.019

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 806.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1200x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1200x2048): 81.200
Elapsed time for attention_prob_times_values (32x2048x2048x1200): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1200): 86.819

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 870.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1201x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1201x2048): 75.648
Elapsed time for attention_prob_times_values (32x2048x2048x1201): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x652x2048): 75.378
Elapsed time for attention_prob_times_values (80x2048x2048x652): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x652): 63.415

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 946.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x653x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x653x2048): 71.885
Elapsed time for attention_prob_times_values (80x2048x2048x653): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x653): 62.546

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 920.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x654x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x654x2048): 73.610
Elapsed time for attention_prob_times_values (80x2048x2048x654): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x654): 65.394

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 953.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x655x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x655x2048): 71.206
Elapsed time for attention_prob_times_values (80x2048x2048x655): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x655): 63.632

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 926.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x656x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x656x2048): 76.595
Elapsed time for attention_prob_times_values (80x2048x2048x656): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x656): 79.463

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1077.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x657x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x657x2048): 72.286
Elapsed time for attention_prob_times_values (80x2048x2048x657): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x657): 63.777

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 937.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x658x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x658x2048): 74.787
Elapsed time for attention_prob_times_values (80x2048x2048x658): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x658): 66.455

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 974.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x659x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x659x2048): 73.881
Elapsed time for attention_prob_times_values (80x2048x2048x659): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x659): 63.892

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 950.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x660x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x660x2048): 75.346
Elapsed time for attention_prob_times_values (80x2048x2048x660): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x660): 66.490

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 981.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x661x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x661x2048): 72.570
Elapsed time for attention_prob_times_values (80x2048x2048x661): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x661): 64.141

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 947.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1201): 77.260

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 793.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1202x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1202x2048): 79.956
Elapsed time for attention_prob_times_values (32x2048x2048x1202): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1202): 80.868

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 835.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1203x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1203x2048): 80.503
Elapsed time for attention_prob_times_values (32x2048x2048x1203): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1203): 79.357

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 831.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1204x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1204x2048): 82.206
Elapsed time for attention_prob_times_values (32x2048x2048x1204): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1204): 81.691

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 852.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1205x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1205x2048): 77.314
Elapsed time for attention_prob_times_values (32x2048x2048x1205): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1205): 76.746

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 802.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1206x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1206x2048): 79.713
Elapsed time for attention_prob_times_values (32x2048x2048x1206): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1206): 78.802

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 825.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1207x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1207x2048): 80.450
Elapsed time for attention_prob_times_values (32x2048x2048x1207): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1207): 79.690

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 835.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1208x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1208x2048): 82.685
Elapsed time for attention_prob_times_values (32x2048x2048x1208): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1208): 86.322

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 881.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1209x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1209x2048): 80.469
Elapsed time for attention_prob_times_values (32x2048x2048x1209): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1209): 74.739

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 809.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1210x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1210x2048): 77.476
Elapsed time for attention_prob_times_values (32x2048x2048x1210): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1210): 81.886

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 832.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x396x2048): 64.180
Elapsed time for attention_prob_times_values (160x2048x2048x396): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x396): 62.204

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1040.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x397x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x397x2048): 63.299
Elapsed time for attention_prob_times_values (160x2048x2048x397): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x397): 59.669

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1014.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x398x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x398x2048): 64.742
Elapsed time for attention_prob_times_values (160x2048x2048x398): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x398): 62.889

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1055.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x399x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x399x2048): 65.157
Elapsed time for attention_prob_times_values (160x2048x2048x399): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x399): 59.163

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1028.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x400x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x400x2048): 66.912
Elapsed time for attention_prob_times_values (160x2048x2048x400): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x400): 76.265

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1185.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x401x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x401x2048): 64.415
Elapsed time for attention_prob_times_values (160x2048x2048x401): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x401): 60.201

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1037.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x402x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x402x2048): 65.090
Elapsed time for attention_prob_times_values (160x2048x2048x402): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x402): 63.371

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1072.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x403x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x403x2048): 62.886
Elapsed time for attention_prob_times_values (160x2048x2048x403): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x403): 59.314

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1022.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x404x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x404x2048): 65.728
Elapsed time for attention_prob_times_values (160x2048x2048x404): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x404): 63.858

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1087.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x405x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x405x2048): 64.932
Elapsed time for attention_prob_times_values (160x2048x2048x405): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x405): 59.868

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1047.856
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1211x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1211x2048): 76.116
Elapsed time for attention_prob_times_values (32x2048x2048x1211): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1211): 79.917

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 815.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1212x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1212x2048): 81.913
Elapsed time for attention_prob_times_values (32x2048x2048x1212): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1212): 78.660

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 840.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1213x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1213x2048): 80.502
Elapsed time for attention_prob_times_values (32x2048x2048x1213): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1213): 77.133

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 825.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1214x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1214x2048): 78.563
Elapsed time for attention_prob_times_values (32x2048x2048x1214): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1214): 78.949

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 825.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1215x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1215x2048): 77.818
Elapsed time for attention_prob_times_values (32x2048x2048x1215): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1215): 76.714

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 810.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1216x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1216x2048): 91.997
Elapsed time for attention_prob_times_values (32x2048x2048x1216): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1216): 89.472

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 952.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1217x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1217x2048): 81.838
Elapsed time for attention_prob_times_values (32x2048x2048x1217): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1217): 78.428

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 841.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1218x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1218x2048): 82.826
Elapsed time for attention_prob_times_values (32x2048x2048x1218): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1218): 80.549

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 858.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1219x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1219x2048): 77.370
Elapsed time for attention_prob_times_values (32x2048x2048x1219): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1219): 80.215

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 828.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1220x2048): 0.0040
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x662x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x662x2048): 74.924
Elapsed time for attention_prob_times_values (80x2048x2048x662): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x662): 66.175

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 978.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x663x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x663x2048): 73.885
Elapsed time for attention_prob_times_values (80x2048x2048x663): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x663): 64.203

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 958.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x664x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x664x2048): 75.628
Elapsed time for attention_prob_times_values (80x2048x2048x664): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x664): 79.481

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1082.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x665x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x665x2048): 73.775
Elapsed time for attention_prob_times_values (80x2048x2048x665): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x665): 64.412

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 962.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x666x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x666x2048): 74.490
Elapsed time for attention_prob_times_values (80x2048x2048x666): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x666): 66.478

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 984.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x667x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x667x2048): 73.770
Elapsed time for attention_prob_times_values (80x2048x2048x667): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x667): 64.599

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 966.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x668x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x668x2048): 71.235
Elapsed time for attention_prob_times_values (80x2048x2048x668): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x668): 65.503

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 958.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x669x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x669x2048): 72.242
Elapsed time for attention_prob_times_values (80x2048x2048x669): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x669): 63.673

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 952.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x670x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x670x2048): 74.511
Elapsed time for attention_prob_times_values (80x2048x2048x670): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x670): 67.116

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 994.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x199x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x199x2048): 67.763
Elapsed time for attention_prob_times_values (384x2048x2048x199): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x199): 52.174

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1158.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x200x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x200x2048): 68.729
Elapsed time for attention_prob_times_values (384x2048x2048x200): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x200): 50.243

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1146.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x201x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x201x2048): 64.731
Elapsed time for attention_prob_times_values (384x2048x2048x201): 0.0258
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x201): 25.119

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 718.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x202x2048): 67.927
Elapsed time for attention_prob_times_values (384x2048x2048x202): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x202): 54.550

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1206.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x203x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x203x2048): 67.373
Elapsed time for attention_prob_times_values (384x2048x2048x203): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x203): 50.526

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1156.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x204x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x204x2048): 69.428
Elapsed time for attention_prob_times_values (384x2048x2048x204): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x204): 55.148

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1237.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x205x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x205x2048): 68.305
Elapsed time for attention_prob_times_values (384x2048x2048x205): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x205): 51.164

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1182.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x206x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x206x2048): 67.579
Elapsed time for attention_prob_times_values (384x2048x2048x206): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x206): 54.053

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1220.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x207x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x207x2048): 68.001
Elapsed time for attention_prob_times_values (384x2048x2048x207): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x207): 52.588

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1210.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1220x2048): 81.461
Elapsed time for attention_prob_times_values (32x2048x2048x1220): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1220): 82.588

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 863.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1221x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1221x2048): 81.659
Elapsed time for attention_prob_times_values (32x2048x2048x1221): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1221): 80.360

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 853.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1222x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1222x2048): 82.072
Elapsed time for attention_prob_times_values (32x2048x2048x1222): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1222): 80.880

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 859.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1223x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1223x2048): 81.468
Elapsed time for attention_prob_times_values (32x2048x2048x1223): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1223): 75.896

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 829.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1224x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1224x2048): 83.923
Elapsed time for attention_prob_times_values (32x2048x2048x1224): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1224): 85.650

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 895.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1225x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1225x2048): 78.436
Elapsed time for attention_prob_times_values (32x2048x2048x1225): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1225): 80.813

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 841.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1226x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1226x2048): 82.203
Elapsed time for attention_prob_times_values (32x2048x2048x1226): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1226): 82.775

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 872.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1227x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1227x2048): 81.225
Elapsed time for attention_prob_times_values (32x2048x2048x1227): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1227): 75.223

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 826.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1228x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1228x2048): 77.679
Elapsed time for attention_prob_times_values (32x2048x2048x1228): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1228): 82.955

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 849.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1229x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1229x2048): 78.953
Elapsed time for attention_prob_times_values (32x2048x2048x1229): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1229): 77.698

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 830.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x406x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x406x2048): 65.674
Elapsed time for attention_prob_times_values (160x2048x2048x406): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x406): 62.974

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1083.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x407x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x407x2048): 65.293
Elapsed time for attention_prob_times_values (160x2048x2048x407): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x407): 60.953

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1065.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x408x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x408x2048): 66.747
Elapsed time for attention_prob_times_values (160x2048x2048x408): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x408): 75.082

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1196.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x409x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x409x2048): 62.734
Elapsed time for attention_prob_times_values (160x2048x2048x409): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x409): 57.079

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1014.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x410x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x410x2048): 63.512
Elapsed time for attention_prob_times_values (160x2048x2048x410): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x410): 60.531

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1054.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x411x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x411x2048): 60.302
Elapsed time for attention_prob_times_values (160x2048x2048x411): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x411): 61.092

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1035.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x412x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x412x2048): 64.298
Elapsed time for attention_prob_times_values (160x2048x2048x412): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x412): 63.332

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1090.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x413x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x413x2048): 64.971
Elapsed time for attention_prob_times_values (160x2048x2048x413): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x413): 60.644

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1074.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x414x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x414x2048): 63.543
Elapsed time for attention_prob_times_values (160x2048x2048x414): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x414): 64.838

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1102.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1230x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1230x2048): 82.124
Elapsed time for attention_prob_times_values (32x2048x2048x1230): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1230): 83.015

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 875.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1231x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1231x2048): 81.265
Elapsed time for attention_prob_times_values (32x2048x2048x1231): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1231): 78.481

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 847.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1232x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1232x2048): 83.415
Elapsed time for attention_prob_times_values (32x2048x2048x1232): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1232): 84.492

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 891.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1233x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1233x2048): 80.824
Elapsed time for attention_prob_times_values (32x2048x2048x1233): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1233): 75.333

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 829.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1234x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1234x2048): 39.604
Elapsed time for attention_prob_times_values (32x2048x2048x1234): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1234): 83.380

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 571.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1235x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1235x2048): 78.786
Elapsed time for attention_prob_times_values (32x2048x2048x1235): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1235): 81.286

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 852.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1236x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1236x2048): 82.432
Elapsed time for attention_prob_times_values (32x2048x2048x1236): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1236): 83.515

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 884.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1237x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1237x2048): 80.720
Elapsed time for attention_prob_times_values (32x2048x2048x1237): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1237): 69.425

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 796.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1238x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1238x2048): 81.603
Elapsed time for attention_prob_times_values (32x2048x2048x1238): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1238): 76.431

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 842.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (80x2048x671x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x671x2048): 73.636
Elapsed time for attention_prob_times_values (80x2048x2048x671): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x671): 63.668

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 963.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x672x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x672x2048): 87.086
Elapsed time for attention_prob_times_values (80x2048x2048x672): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x672): 82.065

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1193.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x673x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x673x2048): 76.437
Elapsed time for attention_prob_times_values (80x2048x2048x673): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x673): 65.131

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 994.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x674x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x674x2048): 76.239
Elapsed time for attention_prob_times_values (80x2048x2048x674): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x674): 65.509

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 998.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x675x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x675x2048): 76.007
Elapsed time for attention_prob_times_values (80x2048x2048x675): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x675): 65.141

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 995.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x676x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x676x2048): 78.005
Elapsed time for attention_prob_times_values (80x2048x2048x676): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x676): 65.329

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1009.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x677x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x677x2048): 76.263
Elapsed time for attention_prob_times_values (80x2048x2048x677): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x677): 63.611

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 986.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x678x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x678x2048): 77.138
Elapsed time for attention_prob_times_values (80x2048x2048x678): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x678): 67.342

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1024.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x679x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x679x2048): 76.336
Elapsed time for attention_prob_times_values (80x2048x2048x679): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x679): 63.031

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 984.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x680x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x680x2048): 81.803
Elapsed time for attention_prob_times_values (80x2048x2048x680): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x680): 81.845

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1168.551
num_attention_heads: 8, hidden_size: 9912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1239x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1239x2048): 78.164
Elapsed time for attention_prob_times_values (32x2048x2048x1239): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1239): 71.788

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 799.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1240x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1240x2048): 82.226
Elapsed time for attention_prob_times_values (32x2048x2048x1240): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1240): 88.766

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 912.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1241x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1241x2048): 80.477
Elapsed time for attention_prob_times_values (32x2048x2048x1241): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1241): 69.674

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 798.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1242x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1242x2048): 81.032
Elapsed time for attention_prob_times_values (32x2048x2048x1242): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1242): 73.525

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 825.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1243x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1243x2048): 80.621
Elapsed time for attention_prob_times_values (32x2048x2048x1243): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1243): 70.245

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 804.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1244x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1244x2048): 78.589
Elapsed time for attention_prob_times_values (32x2048x2048x1244): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1244): 77.374

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 835.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1245x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1245x2048): 78.279
Elapsed time for attention_prob_times_values (32x2048x2048x1245): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1245): 72.573

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 807.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1246x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1246x2048): 79.151
Elapsed time for attention_prob_times_values (32x2048x2048x1246): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1246): 77.242

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 839.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1247x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1247x2048): 79.286
Elapsed time for attention_prob_times_values (32x2048x2048x1247): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1247): 71.385

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 807.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1248x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1248x2048): 91.035
Elapsed time for attention_prob_times_values (32x2048x2048x1248): 0.0037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x681x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x681x2048): 75.681
Elapsed time for attention_prob_times_values (80x2048x2048x681): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x681): 64.993

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1000.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x682x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x682x2048): 71.792
Elapsed time for attention_prob_times_values (80x2048x2048x682): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x682): 65.668

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 982.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x683x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x683x2048): 75.056
Elapsed time for attention_prob_times_values (80x2048x2048x683): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x683): 63.003

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 982.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x684x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x684x2048): 77.445
Elapsed time for attention_prob_times_values (80x2048x2048x684): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x684): 66.642

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1028.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x685x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x685x2048): 75.790
Elapsed time for attention_prob_times_values (80x2048x2048x685): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x685): 63.099

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 990.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x686x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x686x2048): 76.825
Elapsed time for attention_prob_times_values (80x2048x2048x686): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x686): 65.548

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1018.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x687x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x687x2048): 76.559
Elapsed time for attention_prob_times_values (80x2048x2048x687): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x687): 65.684

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1019.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x688x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x688x2048): 77.499
Elapsed time for attention_prob_times_values (80x2048x2048x688): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x688): 84.022

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1164.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x689x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x689x2048): 75.254
Elapsed time for attention_prob_times_values (80x2048x2048x689): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x689): 63.732

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 997.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1248): 90.315

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 974.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 9992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1249x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1249x2048): 79.324
Elapsed time for attention_prob_times_values (32x2048x2048x1249): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1249): 73.003

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 817.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1250x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1250x2048): 78.397
Elapsed time for attention_prob_times_values (32x2048x2048x1250): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1250): 77.524

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 839.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1251x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1251x2048): 80.477
Elapsed time for attention_prob_times_values (32x2048x2048x1251): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1251): 70.002

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 806.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1252x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1252x2048): 80.188
Elapsed time for attention_prob_times_values (32x2048x2048x1252): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1252): 77.773

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 851.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1253x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1253x2048): 81.740
Elapsed time for attention_prob_times_values (32x2048x2048x1253): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1253): 72.775

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 830.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1254x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1254x2048): 81.796
Elapsed time for attention_prob_times_values (32x2048x2048x1254): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1254): 73.639

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 836.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1255x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1255x2048): 77.783
Elapsed time for attention_prob_times_values (32x2048x2048x1255): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1255): 72.674

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 811.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1256x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1256x2048): 76.303
Elapsed time for attention_prob_times_values (32x2048x2048x1256): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1256): 89.763

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 891.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1257x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1257x2048): 79.665
Elapsed time for attention_prob_times_values (32x2048x2048x1257): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1257): 72.768

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 822.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 40, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x415x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x415x2048): 63.028
Elapsed time for attention_prob_times_values (160x2048x2048x415): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x415): 61.987

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1075.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x416x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x416x2048): 80.067
Elapsed time for attention_prob_times_values (160x2048x2048x416): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x416): 75.958

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1344.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x417x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x417x2048): 68.146
Elapsed time for attention_prob_times_values (160x2048x2048x417): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x417): 62.588

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1128.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x418x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x418x2048): 66.714
Elapsed time for attention_prob_times_values (160x2048x2048x418): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x418): 64.041

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1132.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x419x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x419x2048): 68.176
Elapsed time for attention_prob_times_values (160x2048x2048x419): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x419): 62.843

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1135.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x420x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x420x2048): 69.853
Elapsed time for attention_prob_times_values (160x2048x2048x420): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x420): 66.070

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1182.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x421x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x421x2048): 68.032
Elapsed time for attention_prob_times_values (160x2048x2048x421): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x421): 61.092

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1123.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x422x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x422x2048): 68.712
Elapsed time for attention_prob_times_values (160x2048x2048x422): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x422): 66.078

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1177.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x423x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x423x2048): 67.740
Elapsed time for attention_prob_times_values (160x2048x2048x423): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x423): 62.176

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1136.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x424x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x424x2048): 69.705
========================================================================================================================
num_attention_heads: 8, hidden_size: 10064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1258x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1258x2048): 81.612
Elapsed time for attention_prob_times_values (32x2048x2048x1258): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1258): 77.943

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 863.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1259x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1259x2048): 77.076
Elapsed time for attention_prob_times_values (32x2048x2048x1259): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1259): 72.861

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 811.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1260x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1260x2048): 81.780
Elapsed time for attention_prob_times_values (32x2048x2048x1260): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1260): 74.099

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 843.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1261x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1261x2048): 79.423
Elapsed time for attention_prob_times_values (32x2048x2048x1261): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1261): 73.116

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 826.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1262x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1262x2048): 79.392
Elapsed time for attention_prob_times_values (32x2048x2048x1262): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1262): 78.187

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 855.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1263x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1263x2048): 81.249
Elapsed time for attention_prob_times_values (32x2048x2048x1263): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1263): 71.289

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 825.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1264x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1264x2048): 81.696
Elapsed time for attention_prob_times_values (32x2048x2048x1264): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1264): 90.738

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 935.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1265x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1265x2048): 80.353
Elapsed time for attention_prob_times_values (32x2048x2048x1265): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1265): 73.095

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 833.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1266x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1266x2048): 81.656
Elapsed time for attention_prob_times_values (32x2048x2048x1266): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1266): 78.370

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 871.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1267x2048): 0.0042
num_attention_heads: 96, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x208x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x208x2048): 70.348
Elapsed time for attention_prob_times_values (384x2048x2048x208): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x208): 52.996

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1239.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x209x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x209x2048): 68.375
Elapsed time for attention_prob_times_values (384x2048x2048x209): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x209): 52.920

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1228.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x210x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x210x2048): 67.178
Elapsed time for attention_prob_times_values (384x2048x2048x210): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x210): 54.876

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1249.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x211x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x211x2048): 67.180
Elapsed time for attention_prob_times_values (384x2048x2048x211): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x211): 53.556

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1238.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x212x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x212x2048): 70.341
Elapsed time for attention_prob_times_values (384x2048x2048x212): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x212): 54.225

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1278.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x213x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x213x2048): 67.849
Elapsed time for attention_prob_times_values (384x2048x2048x213): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x213): 53.370

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1252.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x214x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x214x2048): 69.533
Elapsed time for attention_prob_times_values (384x2048x2048x214): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x214): 55.246

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1296.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x215x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x215x2048): 35.743
Elapsed time for attention_prob_times_values (384x2048x2048x215): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x215): 54.390

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 912.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x216x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x216x2048): 71.130
Elapsed time for attention_prob_times_values (384x2048x2048x216): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x216): 54.179

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1307.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x217x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x217x2048): 69.235
num_attention_heads: 20, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x690x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x690x2048): 75.692
Elapsed time for attention_prob_times_values (80x2048x2048x690): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x690): 68.507

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1041.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x691x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x691x2048): 75.469
Elapsed time for attention_prob_times_values (80x2048x2048x691): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x691): 63.904

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1003.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x692x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x692x2048): 76.964
Elapsed time for attention_prob_times_values (80x2048x2048x692): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x692): 66.198

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1033.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x693x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x693x2048): 75.723
Elapsed time for attention_prob_times_values (80x2048x2048x693): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x693): 66.170

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1026.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x694x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x694x2048): 76.258
Elapsed time for attention_prob_times_values (80x2048x2048x694): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x694): 66.697

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1035.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x695x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x695x2048): 75.665
Elapsed time for attention_prob_times_values (80x2048x2048x695): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x695): 65.591

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1024.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x696x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x696x2048): 77.485
Elapsed time for attention_prob_times_values (80x2048x2048x696): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x696): 84.724

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1181.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x697x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x697x2048): 75.067
Elapsed time for attention_prob_times_values (80x2048x2048x697): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x697): 65.218

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1019.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x698x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x698x2048): 74.417
Elapsed time for attention_prob_times_values (80x2048x2048x698): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x698): 69.226

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1049.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x699x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x699x2048): 75.251
Elapsed time for attention_prob_times_values (80x2048x2048x699): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1267x2048): 81.093
Elapsed time for attention_prob_times_values (32x2048x2048x1267): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1267): 73.042

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 837.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1268x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1268x2048): 81.997
Elapsed time for attention_prob_times_values (32x2048x2048x1268): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1268): 78.749

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 876.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1269x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1269x2048): 81.143
Elapsed time for attention_prob_times_values (32x2048x2048x1269): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1269): 69.693

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 818.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1270x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1270x2048): 81.853
Elapsed time for attention_prob_times_values (32x2048x2048x1270): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1270): 78.622

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 875.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1271x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1271x2048): 77.646
Elapsed time for attention_prob_times_values (32x2048x2048x1271): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1271): 72.922

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 822.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1272x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1272x2048): 79.102
Elapsed time for attention_prob_times_values (32x2048x2048x1272): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1272): 86.206

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 902.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1273x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1273x2048): 80.598
Elapsed time for attention_prob_times_values (32x2048x2048x1273): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1273): 69.950

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 819.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1274x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1274x2048): 78.511
Elapsed time for attention_prob_times_values (32x2048x2048x1274): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1274): 78.735

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 861.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1275x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1275x2048): 78.607
Elapsed time for attention_prob_times_values (32x2048x2048x1275): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1275): 72.780

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 828.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1276x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1276x2048): 77.842
Elapsed time for attention_prob_times_values (32x2048x2048x1276): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1276): 79.124

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 860.800
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (160x2048x2048x424): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x424): 77.409

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1288.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x425x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x425x2048): 65.983
Elapsed time for attention_prob_times_values (160x2048x2048x425): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x425): 63.098

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1135.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x426x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x426x2048): 66.645
Elapsed time for attention_prob_times_values (160x2048x2048x426): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x426): 64.850

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1159.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x427x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x427x2048): 64.705
Elapsed time for attention_prob_times_values (160x2048x2048x427): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x427): 61.926

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1118.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x428x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x428x2048): 67.593
Elapsed time for attention_prob_times_values (160x2048x2048x428): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x428): 66.643

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1189.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x429x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x429x2048): 66.426
Elapsed time for attention_prob_times_values (160x2048x2048x429): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x429): 64.097

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1158.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x430x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x430x2048): 68.279
Elapsed time for attention_prob_times_values (160x2048x2048x430): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x430): 65.696

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1191.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x431x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x431x2048): 66.765
Elapsed time for attention_prob_times_values (160x2048x2048x431): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x431): 64.321

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1168.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x432x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x432x2048): 68.319
Elapsed time for attention_prob_times_values (160x2048x2048x432): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x432): 79.152

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1310.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x433x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x433x2048): 65.724
Elapsed time for attention_prob_times_values (160x2048x2048x433): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x433): 63.608

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1158.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1277x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1277x2048): 75.323
Elapsed time for attention_prob_times_values (32x2048x2048x1277): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1277): 72.268

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 809.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1278x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1278x2048): 80.978
Elapsed time for attention_prob_times_values (32x2048x2048x1278): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1278): 78.995

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 878.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1279x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1279x2048): 79.035
Elapsed time for attention_prob_times_values (32x2048x2048x1279): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1279): 72.887

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 833.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1280x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1280x2048): 84.565
Elapsed time for attention_prob_times_values (32x2048x2048x1280): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1280): 94.292

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 980.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1281x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1281x2048): 79.294
Elapsed time for attention_prob_times_values (32x2048x2048x1281): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1281): 67.668

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 803.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1282x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1282x2048): 82.565
Elapsed time for attention_prob_times_values (32x2048x2048x1282): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1282): 70.700

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 839.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1283x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1283x2048): 76.531
Elapsed time for attention_prob_times_values (32x2048x2048x1283): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1283): 68.116

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 794.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1284x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1284x2048): 83.065
Elapsed time for attention_prob_times_values (32x2048x2048x1284): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1284): 73.724

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 861.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1285x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1285x2048): 78.510
Elapsed time for attention_prob_times_values (32x2048x2048x1285): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1285): 68.328

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 806.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x699): 66.797

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1036.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x700x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x700x2048): 74.011
Elapsed time for attention_prob_times_values (80x2048x2048x700): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x700): 65.734

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1021.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x701x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x701x2048): 73.052
Elapsed time for attention_prob_times_values (80x2048x2048x701): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x701): 65.734

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1016.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x702x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x702x2048): 76.277
Elapsed time for attention_prob_times_values (80x2048x2048x702): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x702): 69.603

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1070.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x703x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x703x2048): 73.112
Elapsed time for attention_prob_times_values (80x2048x2048x703): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x703): 65.946

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1021.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x704x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x704x2048): 89.446
Elapsed time for attention_prob_times_values (80x2048x2048x704): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x704): 84.531

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1282.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x705x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x705x2048): 75.178
Elapsed time for attention_prob_times_values (80x2048x2048x705): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x705): 64.098

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1022.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x706x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x706x2048): 78.416
Elapsed time for attention_prob_times_values (80x2048x2048x706): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x706): 66.822

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1067.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x707x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x707x2048): 75.101
Elapsed time for attention_prob_times_values (80x2048x2048x707): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x707): 61.305

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 999.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x708x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x708x2048): 79.160
Elapsed time for attention_prob_times_values (80x2048x2048x708): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x708): 65.640

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1064.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 10288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1286x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1286x2048): 77.849
Elapsed time for attention_prob_times_values (32x2048x2048x1286): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1286): 73.546

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 835.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1287x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1287x2048): 81.443
Elapsed time for attention_prob_times_values (32x2048x2048x1287): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1287): 64.640

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 796.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1288x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1288x2048): 83.366
Elapsed time for attention_prob_times_values (32x2048x2048x1288): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1288): 84.518

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 928.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1289x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1289x2048): 80.587
Elapsed time for attention_prob_times_values (32x2048x2048x1289): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1289): 68.628

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 820.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1290x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1290x2048): 82.057
Elapsed time for attention_prob_times_values (32x2048x2048x1290): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1290): 71.597

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 847.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1291x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1291x2048): 80.133
Elapsed time for attention_prob_times_values (32x2048x2048x1291): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1291): 66.719

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 807.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1292x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1292x2048): 81.535
Elapsed time for attention_prob_times_values (32x2048x2048x1292): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1292): 73.383

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 856.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1293x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1293x2048): 80.846
Elapsed time for attention_prob_times_values (32x2048x2048x1293): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1293): 69.050

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 826.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1294x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1294x2048): 82.020
Elapsed time for attention_prob_times_values (32x2048x2048x1294): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1294): 73.091

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 858.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1295x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1295x2048): 80.782
Elapsed time for attention_prob_times_values (32x2048x2048x1295): 0.0050
========================================================================================================================
num_attention_heads: 20, hidden_size: 14180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x709x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x709x2048): 75.121
Elapsed time for attention_prob_times_values (80x2048x2048x709): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x709): 62.279

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1011.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x710x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x710x2048): 78.099
Elapsed time for attention_prob_times_values (80x2048x2048x710): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x710): 64.494

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1050.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x711x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x711x2048): 77.088
Elapsed time for attention_prob_times_values (80x2048x2048x711): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x711): 62.666

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1029.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x712x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x712x2048): 76.176
Elapsed time for attention_prob_times_values (80x2048x2048x712): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x712): 86.544

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1207.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x713x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x713x2048): 76.294
Elapsed time for attention_prob_times_values (80x2048x2048x713): 0.0195
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x713): 24.530

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 554.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x714x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x714x2048): 72.627
Elapsed time for attention_prob_times_values (80x2048x2048x714): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x714): 67.568

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1046.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x715x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x715x2048): 73.709
Elapsed time for attention_prob_times_values (80x2048x2048x715): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x715): 62.967

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1016.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x716x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x716x2048): 78.012
Elapsed time for attention_prob_times_values (80x2048x2048x716): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x716): 66.595

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1076.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x717x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x717x2048): 73.877
Elapsed time for attention_prob_times_values (80x2048x2048x717): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x717): 62.950

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1019.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x718x2048): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1295): 68.997

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 827.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1296x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1296x2048): 80.890
Elapsed time for attention_prob_times_values (32x2048x2048x1296): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1296): 79.714

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 893.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1297x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1297x2048): 78.540
Elapsed time for attention_prob_times_values (32x2048x2048x1297): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1297): 66.516

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 801.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1298x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1298x2048): 82.098
Elapsed time for attention_prob_times_values (32x2048x2048x1298): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1298): 74.060

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 867.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1299x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1299x2048): 76.242
Elapsed time for attention_prob_times_values (32x2048x2048x1299): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1299): 68.190

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 802.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1300x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1300x2048): 78.675
Elapsed time for attention_prob_times_values (32x2048x2048x1300): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1300): 74.238

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 852.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1301x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1301x2048): 79.830
Elapsed time for attention_prob_times_values (32x2048x2048x1301): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1301): 69.399

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 828.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1302x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1302x2048): 82.314
Elapsed time for attention_prob_times_values (32x2048x2048x1302): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1302): 72.534

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 861.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1303x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1303x2048): 81.254
Elapsed time for attention_prob_times_values (32x2048x2048x1303): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1303): 67.342

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 823.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1304x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1304x2048): 83.381
Elapsed time for attention_prob_times_values (32x2048x2048x1304): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1304): 85.404

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 944.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x434x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x434x2048): 65.486
Elapsed time for attention_prob_times_values (160x2048x2048x434): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x434): 65.161

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1172.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x435x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x435x2048): 65.052
Elapsed time for attention_prob_times_values (160x2048x2048x435): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x435): 63.787

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1158.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x436x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x436x2048): 63.126
Elapsed time for attention_prob_times_values (160x2048x2048x436): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x436): 66.852

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1170.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x437x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x437x2048): 65.810
Elapsed time for attention_prob_times_values (160x2048x2048x437): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x437): 63.740

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1170.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x438x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x438x2048): 65.607
Elapsed time for attention_prob_times_values (160x2048x2048x438): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x438): 63.622

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1169.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x439x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x439x2048): 63.757
Elapsed time for attention_prob_times_values (160x2048x2048x439): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x439): 62.610

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1146.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x440x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x440x2048): 66.127
Elapsed time for attention_prob_times_values (160x2048x2048x440): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x440): 78.892

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1308.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x441x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x441x2048): 65.054
Elapsed time for attention_prob_times_values (160x2048x2048x441): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x441): 62.222

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1159.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x442x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x442x2048): 63.075
Elapsed time for attention_prob_times_values (160x2048x2048x442): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x442): 64.651

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1166.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (384x2048x2048x217): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x217): 54.787

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1305.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x218x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x218x2048): 69.586
Elapsed time for attention_prob_times_values (384x2048x2048x218): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x218): 56.982

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1343.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x219x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x219x2048): 69.831
Elapsed time for attention_prob_times_values (384x2048x2048x219): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x219): 55.709

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1334.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x220x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x220x2048): 69.804
Elapsed time for attention_prob_times_values (384x2048x2048x220): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x220): 57.777

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1367.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x221x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x221x2048): 69.354
Elapsed time for attention_prob_times_values (384x2048x2048x221): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x221): 54.845

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1330.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x222x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x222x2048): 70.938
Elapsed time for attention_prob_times_values (384x2048x2048x222): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x222): 58.808

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1402.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x223x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x223x2048): 70.398
Elapsed time for attention_prob_times_values (384x2048x2048x223): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x223): 55.841

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1364.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x224x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x224x2048): 79.912
Elapsed time for attention_prob_times_values (384x2048x2048x224): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x224): 59.605

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1502.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x225x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x225x2048): 68.288
Elapsed time for attention_prob_times_values (384x2048x2048x225): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x225): 57.123

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1374.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x226x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x226x2048): 70.686
Elapsed time for attention_prob_times_values (384x2048x2048x226): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x226): 59.660

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1435.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
========================================================================================================================
num_attention_heads: 8, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1305x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1305x2048): 76.782
Elapsed time for attention_prob_times_values (32x2048x2048x1305): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1305): 68.317

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 809.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1306x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1306x2048): 81.302
Elapsed time for attention_prob_times_values (32x2048x2048x1306): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1306): 74.403

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 870.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1307x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1307x2048): 78.642
Elapsed time for attention_prob_times_values (32x2048x2048x1307): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1307): 68.228

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 819.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1308x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1308x2048): 81.775
Elapsed time for attention_prob_times_values (32x2048x2048x1308): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1308): 72.707

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 863.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1309x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1309x2048): 75.402
Elapsed time for attention_prob_times_values (32x2048x2048x1309): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1309): 65.270

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 785.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1310x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1310x2048): 81.845
Elapsed time for attention_prob_times_values (32x2048x2048x1310): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1310): 74.604

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 876.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1311x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1311x2048): 80.641
Elapsed time for attention_prob_times_values (32x2048x2048x1311): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1311): 70.473

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 845.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1312x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1312x2048): 91.859
Elapsed time for attention_prob_times_values (32x2048x2048x1312): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1312): 86.959

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1005.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1313x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1313x2048): 82.647
Elapsed time for attention_prob_times_values (32x2048x2048x1313): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1313): 69.672

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 851.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1314x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x718x2048): 75.843
Elapsed time for attention_prob_times_values (80x2048x2048x718): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x718): 67.689

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1074.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x719x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x719x2048): 76.648
Elapsed time for attention_prob_times_values (80x2048x2048x719): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x719): 63.262

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1042.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x720x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x720x2048): 76.100
Elapsed time for attention_prob_times_values (80x2048x2048x720): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x720): 85.819

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1215.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x721x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x721x2048): 73.384
Elapsed time for attention_prob_times_values (80x2048x2048x721): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x721): 63.242

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1024.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x722x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x722x2048): 75.146
Elapsed time for attention_prob_times_values (80x2048x2048x722): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x722): 65.914

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1060.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x723x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x723x2048): 76.217
Elapsed time for attention_prob_times_values (80x2048x2048x723): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x723): 63.321

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1045.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x724x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x724x2048): 75.223
Elapsed time for attention_prob_times_values (80x2048x2048x724): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x724): 65.087

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1056.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x725x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x725x2048): 75.070
Elapsed time for attention_prob_times_values (80x2048x2048x725): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x725): 63.456

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1042.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x726x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x726x2048): 76.151
Elapsed time for attention_prob_times_values (80x2048x2048x726): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x726): 66.536

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1078.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x727x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x727x2048): 72.490
Elapsed time for attention_prob_times_values (80x2048x2048x727): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x727): 61.925

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1015.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1314x2048): 83.367
Elapsed time for attention_prob_times_values (32x2048x2048x1314): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1314): 74.745

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 887.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1315x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1315x2048): 82.308
Elapsed time for attention_prob_times_values (32x2048x2048x1315): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1315): 70.329

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 855.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1316x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1316x2048): 83.848
Elapsed time for attention_prob_times_values (32x2048x2048x1316): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1316): 74.783

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 891.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1317x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1317x2048): 81.806
Elapsed time for attention_prob_times_values (32x2048x2048x1317): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1317): 70.365

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 854.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1318x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1318x2048): 82.514
Elapsed time for attention_prob_times_values (32x2048x2048x1318): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1318): 73.126

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 875.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1319x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1319x2048): 82.084
Elapsed time for attention_prob_times_values (32x2048x2048x1319): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1319): 70.060

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 854.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1320x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1320x2048): 83.885
Elapsed time for attention_prob_times_values (32x2048x2048x1320): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1320): 86.542

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 963.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1321x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1321x2048): 77.760
Elapsed time for attention_prob_times_values (32x2048x2048x1321): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1321): 70.269

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 835.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1322x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1322x2048): 76.038
Elapsed time for attention_prob_times_values (32x2048x2048x1322): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1322): 72.671

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 841.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1323x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1323x2048): 81.007
Elapsed time for attention_prob_times_values (32x2048x2048x1323): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1323): 67.231

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 832.953
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (160x2048x443x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x443x2048): 64.685
Elapsed time for attention_prob_times_values (160x2048x2048x443): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x443): 64.220

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1179.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x444x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x444x2048): 65.776
Elapsed time for attention_prob_times_values (160x2048x2048x444): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x444): 68.709

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1232.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x445x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x445x2048): 65.916
Elapsed time for attention_prob_times_values (160x2048x2048x445): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x445): 64.147

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1195.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x446x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x446x2048): 67.568
Elapsed time for attention_prob_times_values (160x2048x2048x446): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x446): 69.136

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1259.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x447x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x447x2048): 66.054
Elapsed time for attention_prob_times_values (160x2048x2048x447): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x447): 28.131

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 728.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x448x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x448x2048): 83.251
Elapsed time for attention_prob_times_values (160x2048x2048x448): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x448): 80.772

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1516.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x449x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x449x2048): 68.748
Elapsed time for attention_prob_times_values (160x2048x2048x449): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x449): 59.692

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1184.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x450x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x450x2048): 70.669
Elapsed time for attention_prob_times_values (160x2048x2048x450): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x450): 63.367

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1241.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x451x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x451x2048): 68.403
Elapsed time for attention_prob_times_values (160x2048x2048x451): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x451): 59.154

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1181.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x452x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x452x2048): 71.413
Elapsed time for attention_prob_times_values (160x2048x2048x452): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x452): 63.795

Attention duration (in seconds): 0.0180
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x728x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x728x2048): 78.301
Elapsed time for attention_prob_times_values (80x2048x2048x728): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x728): 86.611

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1251.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x729x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x729x2048): 75.459
Elapsed time for attention_prob_times_values (80x2048x2048x729): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x729): 64.156

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1056.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x730x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x730x2048): 73.897
Elapsed time for attention_prob_times_values (80x2048x2048x730): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x730): 65.998

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1063.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x731x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x731x2048): 75.867
Elapsed time for attention_prob_times_values (80x2048x2048x731): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x731): 64.251

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1062.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x732x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x732x2048): 74.182
Elapsed time for attention_prob_times_values (80x2048x2048x732): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x732): 66.550

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1073.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x733x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x733x2048): 72.325
Elapsed time for attention_prob_times_values (80x2048x2048x733): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x733): 64.459

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1044.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x734x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x734x2048): 76.636
Elapsed time for attention_prob_times_values (80x2048x2048x734): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x734): 66.929

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1095.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x735x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x735x2048): 72.808
Elapsed time for attention_prob_times_values (80x2048x2048x735): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x735): 65.109

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1055.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x736x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x736x2048): 90.092
Elapsed time for attention_prob_times_values (80x2048x2048x736): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x736): 90.371

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1387.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateMLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1324x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1324x2048): 82.835
Elapsed time for attention_prob_times_values (32x2048x2048x1324): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1324): 75.383

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 895.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1325x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1325x2048): 81.232
Elapsed time for attention_prob_times_values (32x2048x2048x1325): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1325): 70.465

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 856.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1326x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1326x2048): 79.696
Elapsed time for attention_prob_times_values (32x2048x2048x1326): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1326): 75.166

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 878.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1327x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1327x2048): 77.133
Elapsed time for attention_prob_times_values (32x2048x2048x1327): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1327): 67.246

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 816.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1328x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1328x2048): 83.516
Elapsed time for attention_prob_times_values (32x2048x2048x1328): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1328): 82.795

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 945.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1329x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1329x2048): 81.468
Elapsed time for attention_prob_times_values (32x2048x2048x1329): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1329): 66.723

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 835.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1330x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1330x2048): 79.125
Elapsed time for attention_prob_times_values (32x2048x2048x1330): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1330): 75.507

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 880.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1331x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1331x2048): 77.182
Elapsed time for attention_prob_times_values (32x2048x2048x1331): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1331): 70.465

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 839.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1332x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1332x2048): 82.943
Elapsed time for attention_prob_times_values (32x2048x2048x1332): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1332): 75.449

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 901.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1333x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1333x2048): 81.740
Elapsed time for attention_prob_times_values (32x2048x2048x1333): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1333): 70.086

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 861.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1334x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1334x2048): 78.455
Elapsed time for attention_prob_times_values (32x2048x2048x1334): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1334): 75.562

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 879.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1335x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1335x2048): 79.343
Elapsed time for attention_prob_times_values (32x2048x2048x1335): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1335): 70.342

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 852.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1336x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1336x2048): 82.413
Elapsed time for attention_prob_times_values (32x2048x2048x1336): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1336): 86.063

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 963.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1337x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1337x2048): 77.272
Elapsed time for attention_prob_times_values (32x2048x2048x1337): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1337): 68.708

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 832.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1338x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1338x2048): 76.829
Elapsed time for attention_prob_times_values (32x2048x2048x1338): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1338): 69.534

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 836.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1339x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1339x2048): 81.022
Elapsed time for attention_prob_times_values (32x2048x2048x1339): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1339): 70.537

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 864.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1340x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1340x2048): 82.209
Elapsed time for attention_prob_times_values (32x2048x2048x1340): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1340): 75.860

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 904.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1341x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1341x2048): 79.161
Elapsed time for attention_prob_times_values (32x2048x2048x1341): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1341): 70.507

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 855.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1342x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1342x2048): 81.926
Elapsed time for attention_prob_times_values (32x2048x2048x1342): 0.0047

--------
Elapsed time for attention_key_query_prob (80x2048x737x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x737x2048): 76.125
Elapsed time for attention_prob_times_values (80x2048x2048x737): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x737): 65.708

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1085.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x738x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x738x2048): 78.308
Elapsed time for attention_prob_times_values (80x2048x2048x738): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x738): 65.055

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1095.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x739x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x739x2048): 77.711
Elapsed time for attention_prob_times_values (80x2048x2048x739): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x739): 65.795

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1099.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x740x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x740x2048): 79.343
Elapsed time for attention_prob_times_values (80x2048x2048x740): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x740): 67.245

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1124.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x741x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x741x2048): 76.135
Elapsed time for attention_prob_times_values (80x2048x2048x741): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x741): 63.972

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1075.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x742x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x742x2048): 76.790
Elapsed time for attention_prob_times_values (80x2048x2048x742): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x742): 66.518

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1104.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x743x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x743x2048): 74.935
Elapsed time for attention_prob_times_values (80x2048x2048x743): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x743): 64.524

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1075.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x744x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x744x2048): 76.822
Elapsed time for attention_prob_times_values (80x2048x2048x744): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x744): 89.991

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1287.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x745x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x745x2048): 76.132
Elapsed time for attention_prob_times_values (80x2048x2048x745): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x745): 65.586

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1095.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x746x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x746x2048): 77.199
Elapsed time for attention_prob_times_values (80x2048x2048x746): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x746): 64.389

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1093.264Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x227x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x227x2048): 66.957
Elapsed time for attention_prob_times_values (384x2048x2048x227): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x227): 57.587

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1379.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x228x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x228x2048): 70.768
Elapsed time for attention_prob_times_values (384x2048x2048x228): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x228): 60.208

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1455.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x229x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x229x2048): 67.724
Elapsed time for attention_prob_times_values (384x2048x2048x229): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x229): 55.985

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1377.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x230x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x230x2048): 71.157
Elapsed time for attention_prob_times_values (384x2048x2048x230): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x230): 58.672

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1451.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x231x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x231x2048): 68.629
Elapsed time for attention_prob_times_values (384x2048x2048x231): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x231): 57.627

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1419.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x232x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x232x2048): 72.496
Elapsed time for attention_prob_times_values (384x2048x2048x232): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x232): 57.773

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1462.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x233x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x233x2048): 66.968
Elapsed time for attention_prob_times_values (384x2048x2048x233): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x233): 57.078

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1407.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x234x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x234x2048): 68.626
Elapsed time for attention_prob_times_values (384x2048x2048x234): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x234): 60.308

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1472.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x235x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x235x2048): 67.195
Elapsed time for attention_prob_times_values (384x2048x2048x235): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x235): 57.073

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1421.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1342): 76.023

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 905.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1343x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1343x2048): 76.592
Elapsed time for attention_prob_times_values (32x2048x2048x1343): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1343): 68.597

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 831.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1344x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1344x2048): 91.504
Elapsed time for attention_prob_times_values (32x2048x2048x1344): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1344): 84.878

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1012.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1345x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1345x2048): 83.009
Elapsed time for attention_prob_times_values (32x2048x2048x1345): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1345): 70.526

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 877.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1346x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1346x2048): 79.004
Elapsed time for attention_prob_times_values (32x2048x2048x1346): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1346): 76.096

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 892.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1347x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1347x2048): 78.665
Elapsed time for attention_prob_times_values (32x2048x2048x1347): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1347): 67.850

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 839.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1348x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1348x2048): 83.764
Elapsed time for attention_prob_times_values (32x2048x2048x1348): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1348): 72.358

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 895.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1349x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1349x2048): 82.425
Elapsed time for attention_prob_times_values (32x2048x2048x1349): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1349): 70.631

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 877.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1350x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1350x2048): 83.122
Elapsed time for attention_prob_times_values (32x2048x2048x1350): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1350): 76.269

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 918.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1351x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1351x2048): 75.672
Elapsed time for attention_prob_times_values (32x2048x2048x1351): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1351): 70.870

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 845.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1257.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x453x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x453x2048): 67.988
Elapsed time for attention_prob_times_values (160x2048x2048x453): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x453): 60.360

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1195.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x454x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x454x2048): 70.139
Elapsed time for attention_prob_times_values (160x2048x2048x454): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x454): 62.368

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1236.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x455x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x455x2048): 69.218
Elapsed time for attention_prob_times_values (160x2048x2048x455): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x455): 61.170

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1219.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x456x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x456x2048): 71.382
Elapsed time for attention_prob_times_values (160x2048x2048x456): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x456): 79.631

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1416.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x457x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x457x2048): 66.816
Elapsed time for attention_prob_times_values (160x2048x2048x457): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x457): 60.160

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1193.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x458x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x458x2048): 67.382
Elapsed time for attention_prob_times_values (160x2048x2048x458): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x458): 62.624

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1226.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x459x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x459x2048): 67.203
Elapsed time for attention_prob_times_values (160x2048x2048x459): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x459): 60.387

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1204.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x460x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x460x2048): 68.656
Elapsed time for attention_prob_times_values (160x2048x2048x460): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x460): 63.278

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1249.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x461x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x461x2048): 67.425
Elapsed time for attention_prob_times_values (160x2048x2048x461): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x461): 60.359

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1210.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1352x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1352x2048): 80.443
Elapsed time for attention_prob_times_values (32x2048x2048x1352): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1352): 88.491

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 974.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1353x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1353x2048): 81.659
Elapsed time for attention_prob_times_values (32x2048x2048x1353): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1353): 70.899

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 878.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1354x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1354x2048): 82.377
Elapsed time for attention_prob_times_values (32x2048x2048x1354): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1354): 76.365

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 917.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1355x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1355x2048): 75.678
Elapsed time for attention_prob_times_values (32x2048x2048x1355): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1355): 69.333

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 838.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1356x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1356x2048): 79.335
Elapsed time for attention_prob_times_values (32x2048x2048x1356): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1356): 76.666

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 904.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1357x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1357x2048): 80.757
Elapsed time for attention_prob_times_values (32x2048x2048x1357): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1357): 68.606

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 860.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1358x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1358x2048): 83.119
Elapsed time for attention_prob_times_values (32x2048x2048x1358): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1358): 72.553

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 899.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1359x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1359x2048): 82.851
Elapsed time for attention_prob_times_values (32x2048x2048x1359): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1359): 71.207

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 889.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1360x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1360x2048): 85.524
Elapsed time for attention_prob_times_values (32x2048x2048x1360): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1360): 86.266

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 998.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1361x2048): 0.0043

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x747x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x747x2048): 76.867
Elapsed time for attention_prob_times_values (80x2048x2048x747): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x747): 64.541

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1093.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x748x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x748x2048): 78.170
Elapsed time for attention_prob_times_values (80x2048x2048x748): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x748): 65.380

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1111.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x749x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x749x2048): 76.758
Elapsed time for attention_prob_times_values (80x2048x2048x749): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x749): 63.559

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1086.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x750x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x750x2048): 75.277
Elapsed time for attention_prob_times_values (80x2048x2048x750): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x750): 68.072

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1118.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x751x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x751x2048): 76.877
Elapsed time for attention_prob_times_values (80x2048x2048x751): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x751): 65.488

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1108.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x752x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x752x2048): 78.644
Elapsed time for attention_prob_times_values (80x2048x2048x752): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x752): 84.022

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1274.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x753x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x753x2048): 76.181
Elapsed time for attention_prob_times_values (80x2048x2048x753): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x753): 65.858

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1109.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x754x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x754x2048): 76.546
Elapsed time for attention_prob_times_values (80x2048x2048x754): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x754): 68.365

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1135.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x755x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x755x2048): 76.192
Elapsed time for attention_prob_times_values (80x2048x2048x755): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x755): 66.150

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1115.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1361x2048): 85.041
Elapsed time for attention_prob_times_values (32x2048x2048x1361): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1361): 71.252

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 901.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1362x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1362x2048): 80.455
Elapsed time for attention_prob_times_values (32x2048x2048x1362): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1362): 76.704

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 914.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1363x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1363x2048): 79.890
Elapsed time for attention_prob_times_values (32x2048x2048x1363): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1363): 66.856

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 847.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1364x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1364x2048): 82.032
Elapsed time for attention_prob_times_values (32x2048x2048x1364): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1364): 76.507

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 922.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1365x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1365x2048): 76.893
Elapsed time for attention_prob_times_values (32x2048x2048x1365): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1365): 67.921

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 841.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1366x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1366x2048): 82.056
Elapsed time for attention_prob_times_values (32x2048x2048x1366): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1366): 76.784

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 925.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1367x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1367x2048): 82.712
Elapsed time for attention_prob_times_values (32x2048x2048x1367): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1367): 71.500

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 895.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1368x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1368x2048): 80.363
Elapsed time for attention_prob_times_values (32x2048x2048x1368): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1368): 89.392

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 989.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1369x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1369x2048): 82.000
Elapsed time for attention_prob_times_values (32x2048x2048x1369): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1369): 71.818

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 895.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1370x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1370x2048): 84.459
Elapsed time for attention_prob_times_values (32x2048x2048x1370): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1370): 74.040

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 923.464
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x462x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x462x2048): 66.674
Elapsed time for attention_prob_times_values (160x2048x2048x462): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x462): 62.012

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1223.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x463x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x463x2048): 68.701
Elapsed time for attention_prob_times_values (160x2048x2048x463): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x463): 61.908

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1243.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x464x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x464x2048): 68.051
Elapsed time for attention_prob_times_values (160x2048x2048x464): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x464): 34.095

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 868.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x465x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x465x2048): 67.917
Elapsed time for attention_prob_times_values (160x2048x2048x465): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x465): 62.257

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1244.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x466x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x466x2048): 68.769
Elapsed time for attention_prob_times_values (160x2048x2048x466): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x466): 63.856

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1271.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x467x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x467x2048): 66.186
Elapsed time for attention_prob_times_values (160x2048x2048x467): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x467): 60.424

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1215.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x468x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x468x2048): 69.660
Elapsed time for attention_prob_times_values (160x2048x2048x468): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x468): 63.244

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1278.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x469x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x469x2048): 66.647
Elapsed time for attention_prob_times_values (160x2048x2048x469): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x469): 61.555

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1236.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x470x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x470x2048): 66.606
Elapsed time for attention_prob_times_values (160x2048x2048x470): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x470): 63.951

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1263.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x471x2048): 0.0094
num_attention_heads: 20, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x756x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x756x2048): 77.425
Elapsed time for attention_prob_times_values (80x2048x2048x756): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x756): 68.683

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1147.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x757x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x757x2048): 74.414
Elapsed time for attention_prob_times_values (80x2048x2048x757): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x757): 66.031

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1104.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x758x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x758x2048): 75.253
Elapsed time for attention_prob_times_values (80x2048x2048x758): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x758): 68.388

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1132.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x759x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x759x2048): 76.404
Elapsed time for attention_prob_times_values (80x2048x2048x759): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x759): 65.863

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1119.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x760x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x760x2048): 74.999
Elapsed time for attention_prob_times_values (80x2048x2048x760): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x760): 91.466

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1305.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x761x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x761x2048): 76.061
Elapsed time for attention_prob_times_values (80x2048x2048x761): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x761): 62.693

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1090.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x762x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x762x2048): 76.736
Elapsed time for attention_prob_times_values (80x2048x2048x762): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x762): 67.580

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1141.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x763x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x763x2048): 76.085
Elapsed time for attention_prob_times_values (80x2048x2048x763): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x763): 66.271

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1126.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x764x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x764x2048): 77.413
Elapsed time for attention_prob_times_values (80x2048x2048x764): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x764): 68.925

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1161.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x765x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x765x2048): 75.996
Elapsed time for attention_prob_times_values (80x2048x2048x765): 0.0078
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1371x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1371x2048): 84.753
Elapsed time for attention_prob_times_values (32x2048x2048x1371): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1371): 72.195

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 913.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1372x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1372x2048): 83.450
Elapsed time for attention_prob_times_values (32x2048x2048x1372): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1372): 77.314

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 940.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1373x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1373x2048): 81.839
Elapsed time for attention_prob_times_values (32x2048x2048x1373): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1373): 72.199

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 899.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1374x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1374x2048): 84.748
Elapsed time for attention_prob_times_values (32x2048x2048x1374): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1374): 77.322

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 948.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1375x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1375x2048): 83.727
Elapsed time for attention_prob_times_values (32x2048x2048x1375): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1375): 71.655

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 906.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1376x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1376x2048): 91.390
Elapsed time for attention_prob_times_values (32x2048x2048x1376): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1376): 90.886

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1070.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1377x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1377x2048): 79.887
Elapsed time for attention_prob_times_values (32x2048x2048x1377): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1377): 71.799

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 889.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1378x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1378x2048): 80.526
Elapsed time for attention_prob_times_values (32x2048x2048x1378): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1378): 77.609

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 929.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1379x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1379x2048): 82.411
Elapsed time for attention_prob_times_values (32x2048x2048x1379): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1379): 72.715

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 909.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1380x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1380x2048): 83.499
Elapsed time for attention_prob_times_values (32x2048x2048x1380): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1380): 77.749

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 948.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1381x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1381x2048): 80.588
Elapsed time for attention_prob_times_values (32x2048x2048x1381): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1381): 72.749

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 901.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1382x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1382x2048): 81.079
Elapsed time for attention_prob_times_values (32x2048x2048x1382): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1382): 75.082

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 919.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1383x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1383x2048): 81.824
Elapsed time for attention_prob_times_values (32x2048x2048x1383): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1383): 72.644

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 908.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1384x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1384x2048): 80.663
Elapsed time for attention_prob_times_values (32x2048x2048x1384): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1384): 90.427

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1007.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1385x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1385x2048): 79.184
Elapsed time for attention_prob_times_values (32x2048x2048x1385): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1385): 72.667

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 895.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1386x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1386x2048): 81.607
Elapsed time for attention_prob_times_values (32x2048x2048x1386): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1386): 77.889

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 942.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1387x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1387x2048): 81.157
Elapsed time for attention_prob_times_values (32x2048x2048x1387): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1387): 72.827

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 908.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1388x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1388x2048): 82.764
Elapsed time for attention_prob_times_values (32x2048x2048x1388): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1388): 77.803

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 949.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1389x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1389x2048): 81.575
Elapsed time for attention_prob_times_values (32x2048x2048x1389): 0.0051
--------
Elapsed time for attention_key_query_prob (384x2048x236x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x236x2048): 70.319
Elapsed time for attention_prob_times_values (384x2048x2048x236): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x236): 59.840

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1495.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x237x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x237x2048): 70.128
Elapsed time for attention_prob_times_values (384x2048x2048x237): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x237): 58.994

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1487.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x238x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x238x2048): 71.742
Elapsed time for attention_prob_times_values (384x2048x2048x238): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x238): 61.943

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1549.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x239x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x239x2048): 70.519
Elapsed time for attention_prob_times_values (384x2048x2048x239): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x239): 57.751

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1486.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x240x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x240x2048): 55.933
Elapsed time for attention_prob_times_values (384x2048x2048x240): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x240): 61.076

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1372.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x241x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x241x2048): 53.249
Elapsed time for attention_prob_times_values (384x2048x2048x241): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x241): 59.145

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1322.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x242x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x242x2048): 54.408
Elapsed time for attention_prob_times_values (384x2048x2048x242): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x242): 62.123

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1374.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x243x2048): 52.806
Elapsed time for attention_prob_times_values (384x2048x2048x243): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x243): 59.719

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1332.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x244x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x244x2048): 54.913
Elapsed time for attention_prob_times_values (384x2048x2048x244): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x244): 63.271

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1403.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x245x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x245x2048): 53.422
Elapsed time for attention_prob_times_values (384x2048x2048x245): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x245): 60.872

Attention duration (in seconds): 0.0277
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x765): 65.461

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1121.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x766x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x766x2048): 76.842
Elapsed time for attention_prob_times_values (80x2048x2048x766): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x766): 69.130

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1161.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x767x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x767x2048): 75.911
Elapsed time for attention_prob_times_values (80x2048x2048x767): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x767): 65.451

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1123.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x768x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x768x2048): 86.893
Elapsed time for attention_prob_times_values (80x2048x2048x768): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x768): 96.743

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1464.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x769x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x769x2048): 75.917
Elapsed time for attention_prob_times_values (80x2048x2048x769): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x769): 61.606

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1089.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x770x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x770x2048): 78.196
Elapsed time for attention_prob_times_values (80x2048x2048x770): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x770): 64.537

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1134.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x771x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x771x2048): 76.965
Elapsed time for attention_prob_times_values (80x2048x2048x771): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x771): 62.438

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1107.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x772x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x772x2048): 79.297
Elapsed time for attention_prob_times_values (80x2048x2048x772): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x772): 65.003

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1148.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x773x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x773x2048): 77.181
Elapsed time for attention_prob_times_values (80x2048x2048x773): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x773): 61.950

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1106.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x774x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x774x2048): 75.578
Elapsed time for attention_prob_times_values (80x2048x2048x774): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x774): 65.544

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1131.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1389): 73.017

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 913.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1390x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1390x2048): 82.629
Elapsed time for attention_prob_times_values (32x2048x2048x1390): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1390): 78.090

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 952.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1391x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1391x2048): 81.539
Elapsed time for attention_prob_times_values (32x2048x2048x1391): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1391): 70.959

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 900.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1392x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1392x2048): 84.044
Elapsed time for attention_prob_times_values (32x2048x2048x1392): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1392): 87.351

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1017.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1393x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1393x2048): 79.820
Elapsed time for attention_prob_times_values (32x2048x2048x1393): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1393): 73.121

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 906.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1394x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1394x2048): 82.527
Elapsed time for attention_prob_times_values (32x2048x2048x1394): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1394): 78.373

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 955.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1395x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1395x2048): 81.477
Elapsed time for attention_prob_times_values (32x2048x2048x1395): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1395): 72.907

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 915.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1396x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1396x2048): 80.686
Elapsed time for attention_prob_times_values (32x2048x2048x1396): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1396): 78.655

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 948.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1397x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1397x2048): 81.231
Elapsed time for attention_prob_times_values (32x2048x2048x1397): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1397): 71.423

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 905.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1398x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1398x2048): 78.532
Elapsed time for attention_prob_times_values (32x2048x2048x1398): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1398): 78.539

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 936.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x471x2048): 67.200
Elapsed time for attention_prob_times_values (160x2048x2048x471): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x471): 60.178

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1231.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x472x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x472x2048): 67.957
Elapsed time for attention_prob_times_values (160x2048x2048x472): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x472): 82.532

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1448.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x473x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x473x2048): 65.512
Elapsed time for attention_prob_times_values (160x2048x2048x473): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x473): 61.900

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1239.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x474x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x474x2048): 66.919
Elapsed time for attention_prob_times_values (160x2048x2048x474): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x474): 65.234

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1289.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x475x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x475x2048): 67.968
Elapsed time for attention_prob_times_values (160x2048x2048x475): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x475): 63.260

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1281.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x476x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x476x2048): 68.334
Elapsed time for attention_prob_times_values (160x2048x2048x476): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x476): 63.254

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1287.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x477x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x477x2048): 68.159
Elapsed time for attention_prob_times_values (160x2048x2048x477): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x477): 61.783

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1272.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x478x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x478x2048): 67.187
Elapsed time for attention_prob_times_values (160x2048x2048x478): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x478): 65.552

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1305.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x479x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x479x2048): 66.614
Elapsed time for attention_prob_times_values (160x2048x2048x479): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x479): 63.490

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1281.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x480x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x480x2048): 83.459
Elapsed time for attention_prob_times_values (160x2048x2048x480): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x480): 83.169

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1645.449
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1399x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1399x2048): 81.470
Elapsed time for attention_prob_times_values (32x2048x2048x1399): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1399): 69.641

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 895.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1400x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1400x2048): 83.233
Elapsed time for attention_prob_times_values (32x2048x2048x1400): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1400): 91.340

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1039.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1401x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1401x2048): 81.075
Elapsed time for attention_prob_times_values (32x2048x2048x1401): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1401): 73.078

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 918.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1402x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1402x2048): 78.977
Elapsed time for attention_prob_times_values (32x2048x2048x1402): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1402): 78.361

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 940.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1403x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1403x2048): 80.961
Elapsed time for attention_prob_times_values (32x2048x2048x1403): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1403): 73.043

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 918.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1404x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1404x2048): 82.340
Elapsed time for attention_prob_times_values (32x2048x2048x1404): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1404): 77.032

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 952.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1405x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1405x2048): 81.143
Elapsed time for attention_prob_times_values (32x2048x2048x1405): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1405): 71.359

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 909.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1406x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1406x2048): 78.445
Elapsed time for attention_prob_times_values (32x2048x2048x1406): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1406): 79.127

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 944.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1407x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1407x2048): 77.791
Elapsed time for attention_prob_times_values (32x2048x2048x1407): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1407): 68.984

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 876.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1408x2048): 0.0042
========================================================================================================================
num_attention_heads: 20, hidden_size: 15500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x775x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x775x2048): 77.573
Elapsed time for attention_prob_times_values (80x2048x2048x775): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x775): 60.192

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1093.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x776x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x776x2048): 79.197
Elapsed time for attention_prob_times_values (80x2048x2048x776): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x776): 77.928

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1269.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x777x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x777x2048): 74.907
Elapsed time for attention_prob_times_values (80x2048x2048x777): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x777): 61.137

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1089.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x778x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x778x2048): 74.973
Elapsed time for attention_prob_times_values (80x2048x2048x778): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x778): 64.236

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1120.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x779x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x779x2048): 75.342
Elapsed time for attention_prob_times_values (80x2048x2048x779): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x779): 61.418

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1097.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x780x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x780x2048): 77.065
Elapsed time for attention_prob_times_values (80x2048x2048x780): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x780): 65.923

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1153.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x781x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x781x2048): 77.251
Elapsed time for attention_prob_times_values (80x2048x2048x781): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x781): 62.080

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1118.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x782x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x782x2048): 76.665
Elapsed time for attention_prob_times_values (80x2048x2048x782): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x782): 63.808

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1133.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x783x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x783x2048): 75.646
Elapsed time for attention_prob_times_values (80x2048x2048x783): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x783): 63.020

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1120.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x784x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1408x2048): 89.929
Elapsed time for attention_prob_times_values (32x2048x2048x1408): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1408): 94.020

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 1103.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1409x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1409x2048): 81.638
Elapsed time for attention_prob_times_values (32x2048x2048x1409): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1409): 67.240

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 885.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1410x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1410x2048): 82.834
Elapsed time for attention_prob_times_values (32x2048x2048x1410): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1410): 71.202

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 920.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1411x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1411x2048): 82.067
Elapsed time for attention_prob_times_values (32x2048x2048x1411): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1411): 67.510

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 890.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1412x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1412x2048): 83.575
Elapsed time for attention_prob_times_values (32x2048x2048x1412): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1412): 72.752

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 935.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1413x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1413x2048): 81.305
Elapsed time for attention_prob_times_values (32x2048x2048x1413): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1413): 64.005

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 862.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1414x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1414x2048): 83.052
Elapsed time for attention_prob_times_values (32x2048x2048x1414): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1414): 71.337

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 924.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1415x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1415x2048): 82.435
Elapsed time for attention_prob_times_values (32x2048x2048x1415): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1415): 66.578

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 887.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1416x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1416x2048): 83.958
Elapsed time for attention_prob_times_values (32x2048x2048x1416): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1416): 84.622

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1016.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1417x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1417x2048): 82.034
Elapsed time for attention_prob_times_values (32x2048x2048x1417): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1417): 67.950

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 897.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x481x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x481x2048): 68.844
Elapsed time for attention_prob_times_values (160x2048x2048x481): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x481): 63.414

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1306.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x482x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x482x2048): 67.912
Elapsed time for attention_prob_times_values (160x2048x2048x482): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x482): 64.542

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1312.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x483x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x483x2048): 67.200
Elapsed time for attention_prob_times_values (160x2048x2048x483): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x483): 63.304

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1295.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x484x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x484x2048): 68.464
Elapsed time for attention_prob_times_values (160x2048x2048x484): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x484): 67.578

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1353.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x485x2048): 0.0240
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x485x2048): 27.139
Elapsed time for attention_prob_times_values (160x2048x2048x485): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x485): 63.553

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 758.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x486x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x486x2048): 68.316
Elapsed time for attention_prob_times_values (160x2048x2048x486): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x486): 65.999

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1341.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x487x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x487x2048): 68.664
Elapsed time for attention_prob_times_values (160x2048x2048x487): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x487): 62.388

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1309.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x488x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x488x2048): 70.200
Elapsed time for attention_prob_times_values (160x2048x2048x488): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x488): 87.141

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1560.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x489x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x489x2048): 68.193
Elapsed time for attention_prob_times_values (160x2048x2048x489): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x489): 63.285

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1319.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x784x2048): 76.557
Elapsed time for attention_prob_times_values (80x2048x2048x784): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x784): 82.759

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1297.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x785x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x785x2048): 75.761
Elapsed time for attention_prob_times_values (80x2048x2048x785): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x785): 62.912

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1122.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x786x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x786x2048): 76.715
Elapsed time for attention_prob_times_values (80x2048x2048x786): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x786): 64.851

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1149.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x787x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x787x2048): 76.898
Elapsed time for attention_prob_times_values (80x2048x2048x787): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x787): 61.960

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1123.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x788x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x788x2048): 78.287
Elapsed time for attention_prob_times_values (80x2048x2048x788): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x788): 66.655

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1180.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x789x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x789x2048): 75.219
Elapsed time for attention_prob_times_values (80x2048x2048x789): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x789): 63.511

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1130.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x790x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x790x2048): 76.994
Elapsed time for attention_prob_times_values (80x2048x2048x790): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x790): 66.660

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1173.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x791x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x791x2048): 74.594
Elapsed time for attention_prob_times_values (80x2048x2048x791): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x791): 63.646

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1129.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x792x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x792x2048): 78.924
Elapsed time for attention_prob_times_values (80x2048x2048x792): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x792): 84.436

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1343.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x793x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x793x2048): 75.064
Elapsed time for attention_prob_times_values (80x2048x2048x793): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x793): 61.984

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1119.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1418x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1418x2048): 82.713
Elapsed time for attention_prob_times_values (32x2048x2048x1418): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1418): 72.954

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 936.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1419x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1419x2048): 82.163
Elapsed time for attention_prob_times_values (32x2048x2048x1419): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1419): 68.145

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 900.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1420x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1420x2048): 80.127
Elapsed time for attention_prob_times_values (32x2048x2048x1420): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1420): 73.352

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 926.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1421x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1421x2048): 82.147
Elapsed time for attention_prob_times_values (32x2048x2048x1421): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1421): 68.340

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 902.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1422x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1422x2048): 82.951
Elapsed time for attention_prob_times_values (32x2048x2048x1422): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1422): 71.750

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 931.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1423x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1423x2048): 82.086
Elapsed time for attention_prob_times_values (32x2048x2048x1423): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1423): 67.436

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 897.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1424x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1424x2048): 82.960
Elapsed time for attention_prob_times_values (32x2048x2048x1424): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1424): 85.476

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 1020.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1425x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1425x2048): 82.008
Elapsed time for attention_prob_times_values (32x2048x2048x1425): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1425): 68.644

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 906.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1426x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1426x2048): 82.534
Elapsed time for attention_prob_times_values (32x2048x2048x1426): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1426): 70.157

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 920.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1363.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x246x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x246x2048): 52.777
Elapsed time for attention_prob_times_values (384x2048x2048x246): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x246): 60.655

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1358.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x247x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x247x2048): 54.595
Elapsed time for attention_prob_times_values (384x2048x2048x247): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x247): 61.074

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1392.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x248x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x248x2048): 55.876
Elapsed time for attention_prob_times_values (384x2048x2048x248): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x248): 61.177

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1416.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x249x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x249x2048): 51.417
Elapsed time for attention_prob_times_values (384x2048x2048x249): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x249): 57.189

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1318.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x250x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x250x2048): 54.404
Elapsed time for attention_prob_times_values (384x2048x2048x250): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x250): 62.724

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1423.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x251x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x251x2048): 53.504
Elapsed time for attention_prob_times_values (384x2048x2048x251): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x251): 63.735

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1427.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x252x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x252x2048): 53.618
Elapsed time for attention_prob_times_values (384x2048x2048x252): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x252): 64.660

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1443.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x253x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x253x2048): 54.065
Elapsed time for attention_prob_times_values (384x2048x2048x253): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x253): 61.817

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1425.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x254x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x254x2048): 54.811
Elapsed time for attention_prob_times_values (384x2048x2048x254): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x254): 64.428

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1469.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 11416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1427x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1427x2048): 82.018
Elapsed time for attention_prob_times_values (32x2048x2048x1427): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1427): 67.529

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 899.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1428x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1428x2048): 83.111
Elapsed time for attention_prob_times_values (32x2048x2048x1428): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1428): 73.214

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 946.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1429x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1429x2048): 79.307
Elapsed time for attention_prob_times_values (32x2048x2048x1429): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1429): 68.813

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 896.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1430x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1430x2048): 82.731
Elapsed time for attention_prob_times_values (32x2048x2048x1430): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1430): 70.182

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 924.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1431x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1431x2048): 82.181
Elapsed time for attention_prob_times_values (32x2048x2048x1431): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1431): 65.912

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 890.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1432x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1432x2048): 83.799
Elapsed time for attention_prob_times_values (32x2048x2048x1432): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1432): 84.157

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1023.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1433x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1433x2048): 81.945
Elapsed time for attention_prob_times_values (32x2048x2048x1433): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1433): 66.524

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 895.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1434x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1434x2048): 78.008
Elapsed time for attention_prob_times_values (32x2048x2048x1434): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1434): 73.797

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 925.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1435x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1435x2048): 80.735
Elapsed time for attention_prob_times_values (32x2048x2048x1435): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1435): 67.649

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 898.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1436x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1436x2048): 83.151
Elapsed time for attention_prob_times_values (32x2048x2048x1436): 0.0054
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x794x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x794x2048): 77.367
Elapsed time for attention_prob_times_values (80x2048x2048x794): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x794): 66.819

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1183.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x795x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x795x2048): 76.750
Elapsed time for attention_prob_times_values (80x2048x2048x795): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x795): 61.649

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1130.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x796x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x796x2048): 76.378
Elapsed time for attention_prob_times_values (80x2048x2048x796): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x796): 65.509

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1167.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x797x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x797x2048): 73.875
Elapsed time for attention_prob_times_values (80x2048x2048x797): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x797): 62.746

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1124.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x798x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x798x2048): 75.171
Elapsed time for attention_prob_times_values (80x2048x2048x798): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x798): 65.377

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1159.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x799x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x799x2048): 74.725
Elapsed time for attention_prob_times_values (80x2048x2048x799): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x799): 62.479

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1130.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x800x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x800x2048): 91.507
Elapsed time for attention_prob_times_values (80x2048x2048x800): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x800): 83.872

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1455.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x801x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x801x2048): 79.123
Elapsed time for attention_prob_times_values (80x2048x2048x801): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x801): 62.785

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1165.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x802x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x802x2048): 77.304
Elapsed time for attention_prob_times_values (80x2048x2048x802): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x802): 67.416

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1200.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateThroughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1436): 71.813

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 941.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1437x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1437x2048): 82.190
Elapsed time for attention_prob_times_values (32x2048x2048x1437): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1437): 69.225

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 918.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1438x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1438x2048): 79.186
Elapsed time for attention_prob_times_values (32x2048x2048x1438): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1438): 74.047

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 936.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1439x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1439x2048): 82.142
Elapsed time for attention_prob_times_values (32x2048x2048x1439): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1439): 69.682

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 923.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1440x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1440x2048): 92.751
Elapsed time for attention_prob_times_values (32x2048x2048x1440): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1440): 87.101

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1100.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1441x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1441x2048): 83.385
Elapsed time for attention_prob_times_values (32x2048x2048x1441): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1441): 69.719

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 930.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1442x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1442x2048): 83.813
Elapsed time for attention_prob_times_values (32x2048x2048x1442): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1442): 73.500

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 960.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1443x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1443x2048): 83.347
Elapsed time for attention_prob_times_values (32x2048x2048x1443): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1443): 69.645

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 931.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1444x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1444x2048): 82.361
Elapsed time for attention_prob_times_values (32x2048x2048x1444): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1444): 74.329

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 959.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1445x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1445x2048): 83.227
Elapsed time for attention_prob_times_values (32x2048x2048x1445): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1445): 69.580

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 931.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 40, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x490x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x490x2048): 68.552
Elapsed time for attention_prob_times_values (160x2048x2048x490): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x490): 66.718

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1361.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x491x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x491x2048): 68.526
Elapsed time for attention_prob_times_values (160x2048x2048x491): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x491): 63.587

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1331.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x492x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x492x2048): 69.213
Elapsed time for attention_prob_times_values (160x2048x2048x492): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x492): 67.101

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1377.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x493x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x493x2048): 65.729
Elapsed time for attention_prob_times_values (160x2048x2048x493): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x493): 62.556

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1298.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x494x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x494x2048): 67.908
Elapsed time for attention_prob_times_values (160x2048x2048x494): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x494): 67.050

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1369.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x495x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x495x2048): 67.958
Elapsed time for attention_prob_times_values (160x2048x2048x495): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x495): 63.925

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1339.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x496x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x496x2048): 71.605
Elapsed time for attention_prob_times_values (160x2048x2048x496): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x496): 88.786

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1615.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x497x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x497x2048): 66.273
Elapsed time for attention_prob_times_values (160x2048x2048x497): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x497): 63.604

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1325.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x498x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x498x2048): 68.288
Elapsed time for attention_prob_times_values (160x2048x2048x498): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x498): 67.159

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1385.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x499x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x499x2048): 66.935
========================================================================================================================
num_attention_heads: 8, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1446x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1446x2048): 83.704
Elapsed time for attention_prob_times_values (32x2048x2048x1446): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1446): 72.623

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 956.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1447x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1447x2048): 83.253
Elapsed time for attention_prob_times_values (32x2048x2048x1447): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1447): 69.377

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 931.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1448x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1448x2048): 82.853
Elapsed time for attention_prob_times_values (32x2048x2048x1448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1448): 86.145

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1040.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1449x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1449x2048): 82.571
Elapsed time for attention_prob_times_values (32x2048x2048x1449): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1449): 69.547

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 930.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1450x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1450x2048): 83.082
Elapsed time for attention_prob_times_values (32x2048x2048x1450): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1450): 73.048

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 958.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1451x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1451x2048): 80.059
Elapsed time for attention_prob_times_values (32x2048x2048x1451): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1451): 68.204

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 908.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1452x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1452x2048): 82.089
Elapsed time for attention_prob_times_values (32x2048x2048x1452): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1452): 74.905

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 966.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1453x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1453x2048): 83.030
Elapsed time for attention_prob_times_values (32x2048x2048x1453): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1453): 69.786

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 936.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1454x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1454x2048): 80.751
Elapsed time for attention_prob_times_values (32x2048x2048x1454): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1454): 70.850

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 932.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1455x2048): 0.0047

--------
Elapsed time for attention_key_query_prob (80x2048x803x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x803x2048): 76.401
Elapsed time for attention_prob_times_values (80x2048x2048x803): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x803): 64.453

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1166.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x804x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x804x2048): 80.128
Elapsed time for attention_prob_times_values (80x2048x2048x804): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x804): 67.233

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1221.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x805x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x805x2048): 78.613
Elapsed time for attention_prob_times_values (80x2048x2048x805): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x805): 63.966

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1179.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x806x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x806x2048): 79.553
Elapsed time for attention_prob_times_values (80x2048x2048x806): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x806): 65.082

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1198.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x807x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x807x2048): 78.716
Elapsed time for attention_prob_times_values (80x2048x2048x807): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x807): 64.658

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1190.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x808x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x808x2048): 80.467
Elapsed time for attention_prob_times_values (80x2048x2048x808): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x808): 84.673

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1384.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x809x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x809x2048): 77.784
Elapsed time for attention_prob_times_values (80x2048x2048x809): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x809): 64.724

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1187.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x810x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x810x2048): 75.695
Elapsed time for attention_prob_times_values (80x2048x2048x810): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x810): 66.030

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1186.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x811x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x811x2048): 77.913
Elapsed time for attention_prob_times_values (80x2048x2048x811): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x811): 61.523

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1157.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x812x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x812x2048): 76.448
Elapsed time for attention_prob_times_values (80x2048x2048x812): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x812): 67.928

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1212.810Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1455x2048): 82.387
Elapsed time for attention_prob_times_values (32x2048x2048x1455): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1455): 67.643

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 918.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1456x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1456x2048): 84.837
Elapsed time for attention_prob_times_values (32x2048x2048x1456): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1456): 87.435

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 1065.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1457x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1457x2048): 82.696
Elapsed time for attention_prob_times_values (32x2048x2048x1457): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1457): 69.773

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 937.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1458x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1458x2048): 79.963
Elapsed time for attention_prob_times_values (32x2048x2048x1458): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1458): 71.747

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 937.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1459x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1459x2048): 82.784
Elapsed time for attention_prob_times_values (32x2048x2048x1459): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1459): 66.056

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 911.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1460x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1460x2048): 79.193
Elapsed time for attention_prob_times_values (32x2048x2048x1460): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1460): 75.194

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 957.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1461x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1461x2048): 82.615
Elapsed time for attention_prob_times_values (32x2048x2048x1461): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1461): 69.713

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 938.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1462x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1462x2048): 80.733
Elapsed time for attention_prob_times_values (32x2048x2048x1462): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1462): 75.012

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 966.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1463x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1463x2048): 83.049
Elapsed time for attention_prob_times_values (32x2048x2048x1463): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1463): 69.838

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 943.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1464x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1464x2048): 81.859
Elapsed time for attention_prob_times_values (32x2048x2048x1464): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1464): 87.116

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1049.796
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (160x2048x2048x499): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x499): 65.071

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1352.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x500x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x500x2048): 68.992
Elapsed time for attention_prob_times_values (160x2048x2048x500): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x500): 69.094

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1417.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x501x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x501x2048): 68.611
Elapsed time for attention_prob_times_values (160x2048x2048x501): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x501): 65.149

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1374.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x502x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x502x2048): 69.023
Elapsed time for attention_prob_times_values (160x2048x2048x502): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x502): 67.177

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1403.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x503x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x503x2048): 68.296
Elapsed time for attention_prob_times_values (160x2048x2048x503): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x503): 63.426

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1358.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x504x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x504x2048): 68.115
Elapsed time for attention_prob_times_values (160x2048x2048x504): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x504): 89.442

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1599.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x505x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x505x2048): 68.049
Elapsed time for attention_prob_times_values (160x2048x2048x505): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x505): 60.158

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1323.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x506x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x506x2048): 69.480
Elapsed time for attention_prob_times_values (160x2048x2048x506): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x506): 67.429

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1421.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x507x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x507x2048): 68.275
Elapsed time for attention_prob_times_values (160x2048x2048x507): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x507): 60.716

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1337.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x508x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x508x2048): 69.834
Elapsed time for attention_prob_times_values (160x2048x2048x508): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x508): 66.780

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1423.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x813x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x813x2048): 74.872
Elapsed time for attention_prob_times_values (80x2048x2048x813): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x813): 62.223

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1147.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x814x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x814x2048): 79.316
Elapsed time for attention_prob_times_values (80x2048x2048x814): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x814): 80.884

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1353.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x815x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x815x2048): 78.252
Elapsed time for attention_prob_times_values (80x2048x2048x815): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x815): 78.884

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1329.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x816x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x816x2048): 83.573
Elapsed time for attention_prob_times_values (80x2048x2048x816): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x816): 87.107

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1444.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x817x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x817x2048): 75.594
Elapsed time for attention_prob_times_values (80x2048x2048x817): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x817): 77.284

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1296.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x818x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x818x2048): 76.484
Elapsed time for attention_prob_times_values (80x2048x2048x818): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x818): 77.833

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1309.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x819x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x819x2048): 75.926
Elapsed time for attention_prob_times_values (80x2048x2048x819): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x819): 76.228

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1293.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x820x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x820x2048): 78.259
Elapsed time for attention_prob_times_values (80x2048x2048x820): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x820): 79.796

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1344.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x821x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x821x2048): 77.459
Elapsed time for attention_prob_times_values (80x2048x2048x821): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x821): 75.495

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1302.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1465x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1465x2048): 82.497
Elapsed time for attention_prob_times_values (32x2048x2048x1465): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1465): 69.788

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 941.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1466x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1466x2048): 83.010
Elapsed time for attention_prob_times_values (32x2048x2048x1466): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1466): 75.252

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 983.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1467x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1467x2048): 82.564
Elapsed time for attention_prob_times_values (32x2048x2048x1467): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1467): 69.742

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 942.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1468x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1468x2048): 83.683
Elapsed time for attention_prob_times_values (32x2048x2048x1468): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1468): 75.682

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 991.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1469x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1469x2048): 82.487
Elapsed time for attention_prob_times_values (32x2048x2048x1469): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1469): 69.799

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 943.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1470x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1470x2048): 83.249
Elapsed time for attention_prob_times_values (32x2048x2048x1470): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1470): 75.498

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 988.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1471x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1471x2048): 82.566
Elapsed time for attention_prob_times_values (32x2048x2048x1471): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1471): 69.879

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 945.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1472x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1472x2048): 91.712
Elapsed time for attention_prob_times_values (32x2048x2048x1472): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1472): 84.821

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1101.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1473x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1473x2048): 81.593
Elapsed time for attention_prob_times_values (32x2048x2048x1473): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1473): 69.880

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 941.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 96, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x255x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x255x2048): 53.346
Elapsed time for attention_prob_times_values (384x2048x2048x255): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x255): 65.117

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 1460.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x256x2048): 0.0249
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x256x2048): 33.126
Elapsed time for attention_prob_times_values (384x2048x2048x256): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x256): 67.539

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 1111.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x257x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x257x2048): 54.561
Elapsed time for attention_prob_times_values (384x2048x2048x257): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x257): 53.368

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1354.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x258x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x258x2048): 55.703
Elapsed time for attention_prob_times_values (384x2048x2048x258): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x258): 56.246

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1409.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x259x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x259x2048): 56.481
Elapsed time for attention_prob_times_values (384x2048x2048x259): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x259): 52.623

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1377.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x260x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x260x2048): 58.078
Elapsed time for attention_prob_times_values (384x2048x2048x260): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x260): 57.218

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 1462.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x261x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x261x2048): 56.858
Elapsed time for attention_prob_times_values (384x2048x2048x261): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x261): 54.996

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1423.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x262x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x262x2048): 56.551
Elapsed time for attention_prob_times_values (384x2048x2048x262): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x262): 57.570

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1458.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x263x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (384x2048x263x2048): 54.734
Elapsed time for attention_prob_times_values (384x2048x2048x263): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (384x2048x2048x263): 55.399

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1412.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 96, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (384x2048x264x2048): 0.0147
num_attention_heads: 8, hidden_size: 11792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1474x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1474x2048): 81.304
Elapsed time for attention_prob_times_values (32x2048x2048x1474): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1474): 73.530

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 966.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1475x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1475x2048): 83.620
Elapsed time for attention_prob_times_values (32x2048x2048x1475): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1475): 66.999

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 931.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1476x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1476x2048): 85.087
Elapsed time for attention_prob_times_values (32x2048x2048x1476): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1476): 76.133

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1007.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1477x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1477x2048): 83.202
Elapsed time for attention_prob_times_values (32x2048x2048x1477): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1477): 68.389

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 941.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1478x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1478x2048): 80.332
Elapsed time for attention_prob_times_values (32x2048x2048x1478): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1478): 76.034

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 980.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1479x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1479x2048): 80.027
Elapsed time for attention_prob_times_values (32x2048x2048x1479): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1479): 68.513

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 926.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1480x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1480x2048): 83.425
Elapsed time for attention_prob_times_values (32x2048x2048x1480): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1480): 78.434

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1015.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1481x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1481x2048): 80.274
Elapsed time for attention_prob_times_values (32x2048x2048x1481): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1481): 69.203

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 934.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1482x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1482x2048): 83.588
Elapsed time for attention_prob_times_values (32x2048x2048x1482): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1482): 76.083

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1001.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1483x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1483x2048): 83.178
Elapsed time for attention_prob_times_values (32x2048x2048x1483): 0.0057
num_attention_heads: 20, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x822x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x822x2048): 74.888
Elapsed time for attention_prob_times_values (80x2048x2048x822): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x822): 78.370

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1306.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x823x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x823x2048): 78.482
Elapsed time for attention_prob_times_values (80x2048x2048x823): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x823): 76.373

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1321.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x824x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x824x2048): 77.719
Elapsed time for attention_prob_times_values (80x2048x2048x824): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x824): 87.878

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1410.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x825x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x825x2048): 72.310
Elapsed time for attention_prob_times_values (80x2048x2048x825): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x825): 77.092

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1277.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x826x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x826x2048): 75.965
Elapsed time for attention_prob_times_values (80x2048x2048x826): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x826): 77.435

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1313.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x827x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x827x2048): 73.077
Elapsed time for attention_prob_times_values (80x2048x2048x827): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x827): 77.503

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1290.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x828x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x828x2048): 74.245
Elapsed time for attention_prob_times_values (80x2048x2048x828): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x828): 82.023

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1338.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x829x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x829x2048): 77.828
Elapsed time for attention_prob_times_values (80x2048x2048x829): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x829): 80.002

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1356.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x830x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x830x2048): 76.701
Elapsed time for attention_prob_times_values (80x2048x2048x830): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x830): 82.667

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1369.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x831x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x831x2048): 74.876
Elapsed time for attention_prob_times_values (80x2048x2048x831): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x509x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x509x2048): 64.446
Elapsed time for attention_prob_times_values (160x2048x2048x509): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x509): 58.691

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1282.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x510x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x510x2048): 68.753
Elapsed time for attention_prob_times_values (160x2048x2048x510): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x510): 66.318

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1412.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x511x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x511x2048): 65.132
Elapsed time for attention_prob_times_values (160x2048x2048x511): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x511): 62.035

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1331.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x512x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x512x2048): 82.356
Elapsed time for attention_prob_times_values (160x2048x2048x512): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x512): 92.625

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1830.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x513x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x513x2048): 66.599
Elapsed time for attention_prob_times_values (160x2048x2048x513): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x513): 29.753

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 865.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x514x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x514x2048): 73.876
Elapsed time for attention_prob_times_values (160x2048x2048x514): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x514): 64.206

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1448.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x515x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x515x2048): 69.700
Elapsed time for attention_prob_times_values (160x2048x2048x515): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x515): 58.744

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1346.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x516x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x516x2048): 71.521
Elapsed time for attention_prob_times_values (160x2048x2048x516): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x516): 63.161

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1419.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x517x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x517x2048): 70.857
Elapsed time for attention_prob_times_values (160x2048x2048x517): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x517): 62.067

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1402.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1483): 70.238

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 958.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1484x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1484x2048): 83.936
Elapsed time for attention_prob_times_values (32x2048x2048x1484): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1484): 76.764

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1009.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1485x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1485x2048): 83.203
Elapsed time for attention_prob_times_values (32x2048x2048x1485): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1485): 70.843

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 964.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1486x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1486x2048): 78.280
Elapsed time for attention_prob_times_values (32x2048x2048x1486): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1486): 75.087

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 966.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1487x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1487x2048): 79.241
Elapsed time for attention_prob_times_values (32x2048x2048x1487): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1487): 69.017

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 930.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1488x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1488x2048): 85.038
Elapsed time for attention_prob_times_values (32x2048x2048x1488): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1488): 78.665

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1031.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1489x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1489x2048): 80.152
Elapsed time for attention_prob_times_values (32x2048x2048x1489): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1489): 69.520

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 940.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1490x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1490x2048): 83.972
Elapsed time for attention_prob_times_values (32x2048x2048x1490): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1490): 76.947

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1015.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1491x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1491x2048): 82.655
Elapsed time for attention_prob_times_values (32x2048x2048x1491): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1491): 71.304

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 968.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1492x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1492x2048): 83.017
Elapsed time for attention_prob_times_values (32x2048x2048x1492): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1492): 77.231

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1012.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x831): 79.590

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1329.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x832x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x832x2048): 91.831
Elapsed time for attention_prob_times_values (80x2048x2048x832): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x832): 88.761

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1557.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x833x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x833x2048): 79.621
Elapsed time for attention_prob_times_values (80x2048x2048x833): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x833): 78.544

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1365.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x834x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x834x2048): 79.655
Elapsed time for attention_prob_times_values (80x2048x2048x834): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x834): 83.006

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1405.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x835x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x835x2048): 79.523
Elapsed time for attention_prob_times_values (80x2048x2048x835): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x835): 79.981

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1380.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x836x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x836x2048): 81.358
Elapsed time for attention_prob_times_values (80x2048x2048x836): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x836): 82.994

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1423.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x837x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x837x2048): 79.327
Elapsed time for attention_prob_times_values (80x2048x2048x837): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x837): 80.819

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1388.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x838x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x838x2048): 80.235
Elapsed time for attention_prob_times_values (80x2048x2048x838): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x838): 80.416

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1395.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x839x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x839x2048): 79.160
Elapsed time for attention_prob_times_values (80x2048x2048x839): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x839): 81.020

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1392.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x840x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x840x2048): 81.172
Elapsed time for attention_prob_times_values (80x2048x2048x840): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x840): 89.415

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1481.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1493x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1493x2048): 82.502
Elapsed time for attention_prob_times_values (32x2048x2048x1493): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1493): 70.525

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 963.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1494x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1494x2048): 81.544
Elapsed time for attention_prob_times_values (32x2048x2048x1494): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1494): 75.312

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 992.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1495x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1495x2048): 82.382
Elapsed time for attention_prob_times_values (32x2048x2048x1495): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1495): 69.431

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 955.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1496x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1496x2048): 82.801
Elapsed time for attention_prob_times_values (32x2048x2048x1496): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1496): 79.763

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1030.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1497x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1497x2048): 79.481
Elapsed time for attention_prob_times_values (32x2048x2048x1497): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1497): 72.047

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 959.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1498x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1498x2048): 83.351
Elapsed time for attention_prob_times_values (32x2048x2048x1498): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1498): 77.112

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1017.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 11992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1499x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1499x2048): 82.874
Elapsed time for attention_prob_times_values (32x2048x2048x1499): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1499): 71.251

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 973.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1500x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1500x2048): 83.939
Elapsed time for attention_prob_times_values (32x2048x2048x1500): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1500): 77.659

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1026.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1501x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x1501x2048): 82.724
Elapsed time for attention_prob_times_values (32x2048x2048x1501): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x1501): 72.769

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 985.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 12016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x1502x2048): 0.0048
slurmstepd: error: *** JOB 1507035 ON frontier08302 CANCELLED AT 2023-11-22T14:05:29 DUE TO TIME LIMIT ***
