
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
2
4
6
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-22 23:09:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 23:09:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 23:09:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-22 23:09:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 24, hidden_size: 24, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1x2048): 1.000
Elapsed time for attention_prob_times_values (96x2048x2048x1): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1): 0.211

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 0.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x2x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x2x2048): 1.841
Elapsed time for attention_prob_times_values (96x2048x2048x2): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x2): 1.772

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 1.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 72, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x3x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x3x2048): 2.588
Elapsed time for attention_prob_times_values (96x2048x2048x3): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x3): 2.415

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 2.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x4x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x4x2048): 3.334
Elapsed time for attention_prob_times_values (96x2048x2048x4): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x4): 3.078

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 3.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x5x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x5x2048): 4.123
Elapsed time for attention_prob_times_values (96x2048x2048x5): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x5): 3.127

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 3.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x6x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x6x2048): 4.945
Elapsed time for attention_prob_times_values (96x2048x2048x6): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x6): 4.463

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 5.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x7x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x7x2048): 5.749
Elapsed time for attention_prob_times_values (96x2048x2048x7): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x7): 5.184

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 6.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x8x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x8x2048): 6.541
Elapsed time for attention_prob_times_values (96x2048x2048x8): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x8): 6.982

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 8.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x9x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x9x2048): 7.056
Elapsed time for attention_prob_times_values (96x2048x2048x9): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x9): 7.519

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 8.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x10x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x10x2048): 7.810
Elapsed time for attention_prob_times_values (96x2048x2048x10): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x10): 8.577

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 10.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x11x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x11x2048): 8.569
Elapsed time for attention_prob_times_values (96x2048x2048x11): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x11): 9.064

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 11.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x12x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x12x2048): 9.316
Elapsed time for attention_prob_times_values (96x2048x2048x12): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x12): 10.272

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 12.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x13x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x13x2048): 10.067
Elapsed time for attention_prob_times_values (96x2048x2048x13): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x13): 10.746

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 13.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x14x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x14x2048): 10.826
Elapsed time for attention_prob_times_values (96x2048x2048x14): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x14): 11.860

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 15.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x15x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x15x2048): 11.594
Elapsed time for attention_prob_times_values (96x2048x2048x15): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x15): 12.339

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 16.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x16x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x16x2048): 12.415
Elapsed time for attention_prob_times_values (96x2048x2048x16): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x16): 13.599

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 17.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x17x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x17x2048): 12.642
Elapsed time for attention_prob_times_values (96x2048x2048x17): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x17): 13.826

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 18.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x18x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x18x2048): 13.463
Elapsed time for attention_prob_times_values (96x2048x2048x18): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x18): 14.946

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 20.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x19x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x19x2048): 14.117
Elapsed time for attention_prob_times_values (96x2048x2048x19): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x19): 15.453

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 21.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
2.1.1+rocm5.6 

num_attention_heads: 64, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x1x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x1x2048): 1.038
Elapsed time for attention_prob_times_values (256x2048x2048x1): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x1): 0.210

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 0.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x2x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x2x2048): 1.917
Elapsed time for attention_prob_times_values (256x2048x2048x2): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x2): 1.814

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 2.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x3x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x3x2048): 2.548
Elapsed time for attention_prob_times_values (256x2048x2048x3): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x3): 2.096

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 2.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x4x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x4x2048): 3.426
Elapsed time for attention_prob_times_values (256x2048x2048x4): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x4): 2.810

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 3.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x5x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x5x2048): 4.227
Elapsed time for attention_prob_times_values (256x2048x2048x5): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x5): 3.769

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 5.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x6x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x6x2048): 5.075
Elapsed time for attention_prob_times_values (256x2048x2048x6): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x6): 4.316

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 6.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x7x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x7x2048): 5.895
Elapsed time for attention_prob_times_values (256x2048x2048x7): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x7): 5.250

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 7.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x8x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x8x2048): 6.632
Elapsed time for attention_prob_times_values (256x2048x2048x8): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x8): 7.173

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 10.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x9x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x9x2048): 7.215
Elapsed time for attention_prob_times_values (256x2048x2048x9): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x9): 7.469

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 11.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x10x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x10x2048): 7.981
Elapsed time for attention_prob_times_values (256x2048x2048x10): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x10): 8.806

2.1.1+rocm5.6 

num_attention_heads: 8, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3812x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3812x2048): 99.597
Elapsed time for attention_prob_times_values (32x2048x2048x3812): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3812): 102.656

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3112.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3813x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3813x2048): 102.464
Elapsed time for attention_prob_times_values (32x2048x2048x3813): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3813): 100.625

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3126.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3814x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3814x2048): 99.527
Elapsed time for attention_prob_times_values (32x2048x2048x3814): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3814): 104.291

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3136.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3815x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3815x2048): 99.178
Elapsed time for attention_prob_times_values (32x2048x2048x3815): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3815): 103.208

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3115.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3816x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3816x2048): 103.277
Elapsed time for attention_prob_times_values (32x2048x2048x3816): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3816): 101.508

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3154.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3817x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3817x2048): 99.382
Elapsed time for attention_prob_times_values (32x2048x2048x3817): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3817): 102.744

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3113.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3818x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3818x2048): 95.217
Elapsed time for attention_prob_times_values (32x2048x2048x3818): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3818): 103.685

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3060.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3819x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3819x2048): 101.920
Elapsed time for attention_prob_times_values (32x2048x2048x3819): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3819): 102.172

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3146.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3820x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3820x2048): 103.150
Elapsed time for attention_prob_times_values (32x2048x2048x3820): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3820): 100.292

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3136.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3821x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3821x2048): 102.262
num_attention_heads: 24, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x20x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x20x2048): 14.801
Elapsed time for attention_prob_times_values (96x2048x2048x20): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x20): 16.594

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 22.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x21x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x21x2048): 15.586
Elapsed time for attention_prob_times_values (96x2048x2048x21): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x21): 16.941

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 24.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x22x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x22x2048): 16.339
Elapsed time for attention_prob_times_values (96x2048x2048x22): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x22): 16.196

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 24.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x23x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x23x2048): 17.059
Elapsed time for attention_prob_times_values (96x2048x2048x23): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x23): 18.471

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 27.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x24x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x24x2048): 17.785
Elapsed time for attention_prob_times_values (96x2048x2048x24): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x24): 20.071

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 29.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x25x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x25x2048): 18.043
Elapsed time for attention_prob_times_values (96x2048x2048x25): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x25): 19.869

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 29.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x26x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x26x2048): 18.745
Elapsed time for attention_prob_times_values (96x2048x2048x26): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x26): 21.000

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 31.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x27x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x27x2048): 19.416
Elapsed time for attention_prob_times_values (96x2048x2048x27): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x27): 18.203

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 30.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x28x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x28x2048): 20.050
Elapsed time for attention_prob_times_values (96x2048x2048x28): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x28): 22.626

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 35.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x29x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x29x2048): 20.709
Elapsed time for attention_prob_times_values (96x2048x2048x29): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x29): 22.833

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 36.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x30x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x30x2048): 21.491
Elapsed time for attention_prob_times_values (96x2048x2048x30): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x30): 23.995

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 38.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x31x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x31x2048): 22.132
Elapsed time for attention_prob_times_values (96x2048x2048x31): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x31): 24.317

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 40.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x32x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x32x2048): 26.222
Elapsed time for attention_prob_times_values (96x2048x2048x32): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x32): 26.127

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 45.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x33x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x33x2048): 26.013
Elapsed time for attention_prob_times_values (96x2048x2048x33): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x33): 25.844

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 45.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x34x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x34x2048): 26.196
Elapsed time for attention_prob_times_values (96x2048x2048x34): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x34): 26.813

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 47.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x35x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x35x2048): 25.207
Elapsed time for attention_prob_times_values (96x2048x2048x35): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x35): 27.391

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 47.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x36x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x36x2048): 25.828
Elapsed time for attention_prob_times_values (96x2048x2048x36): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x36): 24.710

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 46.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x37x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x37x2048): 25.752
Elapsed time for attention_prob_times_values (96x2048x2048x37): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x37): 28.730

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 50.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x38x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x38x2048): 26.585
Elapsed time for attention_prob_times_values (96x2048x2048x38): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x38): 29.794

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 53.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x39x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x39x2048): 26.889
Elapsed time for attention_prob_times_values (96x2048x2048x39): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x39): 30.200

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 54.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x40x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x40x2048): 27.749
Elapsed time for attention_prob_times_values (96x2048x2048x40): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x40): 32.046

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 57.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x41x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x41x2048): 27.396
Elapsed time for attention_prob_times_values (96x2048x2048x41): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x41): 27.977

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 54.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x42x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x42x2048): 28.192
Elapsed time for attention_prob_times_values (96x2048x2048x42): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x42): 32.315

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 59.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x43x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x43x2048): 28.351
Elapsed time for attention_prob_times_values (96x2048x2048x43): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x43): 32.580

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 60.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x44x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x44x2048): 29.541
Elapsed time for attention_prob_times_values (96x2048x2048x44): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x44): 33.854

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 64.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x45x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x45x2048): 27.166
Elapsed time for attention_prob_times_values (96x2048x2048x45): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x45): 30.357

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 58.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x46x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x46x2048): 30.871
Elapsed time for attention_prob_times_values (96x2048x2048x46): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x46): 35.288

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 68.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x47x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x47x2048): 31.158
Elapsed time for attention_prob_times_values (96x2048x2048x47): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x47): 35.239

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 69.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x48x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x48x2048): 32.374
Elapsed time for attention_prob_times_values (96x2048x2048x48): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x48): 37.978

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 13.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x11x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x11x2048): 8.718
Elapsed time for attention_prob_times_values (256x2048x2048x11): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x11): 9.336

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 15.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x12x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x12x2048): 9.322
Elapsed time for attention_prob_times_values (256x2048x2048x12): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x12): 10.564

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 17.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x13x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x13x2048): 10.213
Elapsed time for attention_prob_times_values (256x2048x2048x13): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x13): 11.024

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 19.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x14x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x14x2048): 11.061
Elapsed time for attention_prob_times_values (256x2048x2048x14): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x14): 11.301

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 20.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x15x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x15x2048): 11.828
Elapsed time for attention_prob_times_values (256x2048x2048x15): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x15): 12.659

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 23.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x16x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x16x2048): 12.677
Elapsed time for attention_prob_times_values (256x2048x2048x16): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x16): 14.026

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 26.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x17x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x17x2048): 12.996
Elapsed time for attention_prob_times_values (256x2048x2048x17): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x17): 14.198

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 27.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x18x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x18x2048): 13.690
Elapsed time for attention_prob_times_values (256x2048x2048x18): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x18): 15.417

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 30.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x19x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x19x2048): 14.435
Elapsed time for attention_prob_times_values (256x2048x2048x19): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x19): 15.871

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 33.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x1x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x1x2048): 0.986
Elapsed time for attention_prob_times_values (512x2048x2048x1): 0.0196
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x1): 0.220

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 0.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x2x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x2x2048): 1.981
Elapsed time for attention_prob_times_values (512x2048x2048x2): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x2): 1.419

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x3x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x3x2048): 2.689
Elapsed time for attention_prob_times_values (512x2048x2048x3): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x3): 2.072

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x4x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x4x2048): 3.506
Elapsed time for attention_prob_times_values (512x2048x2048x4): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x4): 2.916

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 4.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x5x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x5x2048): 4.274
Elapsed time for attention_prob_times_values (512x2048x2048x5): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x5): 3.642

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 6.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x6x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x6x2048): 5.116
Elapsed time for attention_prob_times_values (512x2048x2048x6): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x6): 4.378

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 8.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x7x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x7x2048): 5.826
Elapsed time for attention_prob_times_values (512x2048x2048x7): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x7): 5.143

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 10.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x8x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x8x2048): 6.337
Elapsed time for attention_prob_times_values (512x2048x2048x8): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x8): 7.226

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 13.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x9x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x9x2048): 6.889
Elapsed time for attention_prob_times_values (512x2048x2048x9): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x9): 7.237

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 15.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x10x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x10x2048): 6.041
Elapsed time for attention_prob_times_values (512x2048x2048x10): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x10): 8.940

Elapsed time for attention_prob_times_values (32x2048x2048x3821): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3821): 99.470

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3111.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3822x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3822x2048): 102.908
Elapsed time for attention_prob_times_values (32x2048x2048x3822): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3822): 103.729

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3188.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3823x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3823x2048): 102.313
Elapsed time for attention_prob_times_values (32x2048x2048x3823): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3823): 101.668

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3148.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3824x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3824x2048): 100.113
Elapsed time for attention_prob_times_values (32x2048x2048x3824): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3824): 102.603

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3128.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3825x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3825x2048): 101.890
Elapsed time for attention_prob_times_values (32x2048x2048x3825): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3825): 101.169

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3135.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3826x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3826x2048): 98.536
Elapsed time for attention_prob_times_values (32x2048x2048x3826): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3826): 103.760

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3122.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3827x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3827x2048): 99.243
Elapsed time for attention_prob_times_values (32x2048x2048x3827): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3827): 96.769

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3027.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3828x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3828x2048): 103.057
Elapsed time for attention_prob_times_values (32x2048x2048x3828): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3828): 102.199

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3171.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3829x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3829x2048): 102.134
Elapsed time for attention_prob_times_values (32x2048x2048x3829): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3829): 101.645

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3149.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3830x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3830x2048): 102.335
Elapsed time for attention_prob_times_values (32x2048x2048x3830): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3830): 103.893

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3188.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 74.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x49x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x49x2048): 32.735
Elapsed time for attention_prob_times_values (96x2048x2048x49): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x49): 36.870

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 74.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x50x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x50x2048): 33.469
Elapsed time for attention_prob_times_values (96x2048x2048x50): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x50): 37.757

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 77.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x51x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x51x2048): 34.017
Elapsed time for attention_prob_times_values (96x2048x2048x51): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x51): 37.769

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 78.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x52x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x52x2048): 32.787
Elapsed time for attention_prob_times_values (96x2048x2048x52): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x52): 39.523

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 79.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x53x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x53x2048): 35.393
Elapsed time for attention_prob_times_values (96x2048x2048x53): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x53): 39.138

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 83.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x54x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x54x2048): 36.089
Elapsed time for attention_prob_times_values (96x2048x2048x54): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x54): 40.149

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 86.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x55x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x55x2048): 36.721
Elapsed time for attention_prob_times_values (96x2048x2048x55): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x55): 33.618

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 80.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x56x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x56x2048): 37.433
Elapsed time for attention_prob_times_values (96x2048x2048x56): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x56): 43.856

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 93.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x57x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x57x2048): 36.876
Elapsed time for attention_prob_times_values (96x2048x2048x57): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x57): 42.083

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 91.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x58x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x58x2048): 37.552
Elapsed time for attention_prob_times_values (96x2048x2048x58): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x58): 43.381

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 94.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x59x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x59x2048): 38.039
Elapsed time for attention_prob_times_values (96x2048x2048x59): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x59): 43.182

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 96.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x60x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x60x2048): 38.772
Elapsed time for attention_prob_times_values (96x2048x2048x60): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x60): 44.906

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 100.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x61x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x61x2048): 39.208
Elapsed time for attention_prob_times_values (96x2048x2048x61): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x61): 44.253

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 101.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x62x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x62x2048): 36.785
Elapsed time for attention_prob_times_values (96x2048x2048x62): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x62): 46.290

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 100.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x63x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x63x2048): 40.007
Elapsed time for attention_prob_times_values (96x2048x2048x63): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x63): 45.812

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 105.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x64x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x64x2048): 55.457
Elapsed time for attention_prob_times_values (96x2048x2048x64): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x64): 49.821

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 131.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x65x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x65x2048): 42.392
Elapsed time for attention_prob_times_values (96x2048x2048x65): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x65): 26.735

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 82.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x66x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x66x2048): 43.095
Elapsed time for attention_prob_times_values (96x2048x2048x66): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x66): 34.270

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 97.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x67x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x67x2048): 42.861
Elapsed time for attention_prob_times_values (96x2048x2048x67): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x67): 33.888

num_attention_heads: 64, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x20x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x20x2048): 15.117
Elapsed time for attention_prob_times_values (256x2048x2048x20): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x20): 15.791

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 34.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x21x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x21x2048): 15.918
Elapsed time for attention_prob_times_values (256x2048x2048x21): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x21): 17.379

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 38.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x22x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x22x2048): 16.652
Elapsed time for attention_prob_times_values (256x2048x2048x22): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x22): 18.650

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 41.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x23x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x23x2048): 17.421
Elapsed time for attention_prob_times_values (256x2048x2048x23): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x23): 18.828

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 44.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x24x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x24x2048): 18.070
Elapsed time for attention_prob_times_values (256x2048x2048x24): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x24): 20.483

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 48.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x25x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x25x2048): 18.426
Elapsed time for attention_prob_times_values (256x2048x2048x25): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x25): 17.651

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 46.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x26x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x26x2048): 19.060
Elapsed time for attention_prob_times_values (256x2048x2048x26): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x26): 20.387

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 51.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x27x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x27x2048): 19.814
Elapsed time for attention_prob_times_values (256x2048x2048x27): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x27): 21.949

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 55.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x28x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x28x2048): 19.449
Elapsed time for attention_prob_times_values (256x2048x2048x28): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x28): 23.161

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 58.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x29x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x29x2048): 21.171
Elapsed time for attention_prob_times_values (256x2048x2048x29): 0.0027
Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 97.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x68x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x68x2048): 43.858
Elapsed time for attention_prob_times_values (96x2048x2048x68): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x68): 35.385

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 101.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x69x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x69x2048): 43.490
Elapsed time for attention_prob_times_values (96x2048x2048x69): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x69): 34.853

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 101.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x70x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x70x2048): 44.468
Elapsed time for attention_prob_times_values (96x2048x2048x70): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x70): 35.840

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 104.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x71x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x71x2048): 44.304
Elapsed time for attention_prob_times_values (96x2048x2048x71): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x71): 35.581

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 105.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x72x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x72x2048): 45.474
Elapsed time for attention_prob_times_values (96x2048x2048x72): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x72): 35.560

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 107.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x73x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x73x2048): 44.141
Elapsed time for attention_prob_times_values (96x2048x2048x73): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x73): 36.535

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 108.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x74x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x74x2048): 39.518
Elapsed time for attention_prob_times_values (96x2048x2048x74): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x74): 38.073

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 106.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x75x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x75x2048): 38.932
Elapsed time for attention_prob_times_values (96x2048x2048x75): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x75): 37.183

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 104.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x76x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x76x2048): 46.187
Elapsed time for attention_prob_times_values (96x2048x2048x76): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x76): 39.054

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 117.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3831x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3831x2048): 95.200
Elapsed time for attention_prob_times_values (32x2048x2048x3831): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3831): 98.570

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2995.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3832x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3832x2048): 103.427
Elapsed time for attention_prob_times_values (32x2048x2048x3832): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3832): 100.203

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3149.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3833x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3833x2048): 102.047
Elapsed time for attention_prob_times_values (32x2048x2048x3833): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3833): 101.412

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3148.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3834x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3834x2048): 100.130
Elapsed time for attention_prob_times_values (32x2048x2048x3834): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3834): 104.213

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3161.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3835x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3835x2048): 98.590
Elapsed time for attention_prob_times_values (32x2048x2048x3835): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3835): 102.284

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3108.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3836x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3836x2048): 101.522
Elapsed time for attention_prob_times_values (32x2048x2048x3836): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3836): 102.132

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3153.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3837x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3837x2048): 101.973
Elapsed time for attention_prob_times_values (32x2048x2048x3837): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3837): 101.618

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3153.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3838x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3838x2048): 102.693
Elapsed time for attention_prob_times_values (32x2048x2048x3838): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3838): 104.191

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3204.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3839x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3839x2048): 102.147
Elapsed time for attention_prob_times_values (32x2048x2048x3839): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3839): 101.685

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3158.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 24, hidden_size: 1848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x77x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x77x2048): 45.838
Elapsed time for attention_prob_times_values (96x2048x2048x77): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x77): 38.102

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 116.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x78x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x78x2048): 41.720
Elapsed time for attention_prob_times_values (96x2048x2048x78): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x78): 39.505

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 114.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x79x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x79x2048): 46.904
Elapsed time for attention_prob_times_values (96x2048x2048x79): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x79): 39.029

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 121.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x80x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x80x2048): 48.617
Elapsed time for attention_prob_times_values (96x2048x2048x80): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x80): 36.290

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 119.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x81x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x81x2048): 46.013
Elapsed time for attention_prob_times_values (96x2048x2048x81): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x81): 39.784

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 123.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x82x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x82x2048): 46.409
Elapsed time for attention_prob_times_values (96x2048x2048x82): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x82): 41.475

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 127.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x83x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x83x2048): 41.950
Elapsed time for attention_prob_times_values (96x2048x2048x83): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x83): 40.624

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 121.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x84x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x84x2048): 47.725
Elapsed time for attention_prob_times_values (96x2048x2048x84): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x84): 42.430

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 133.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x85x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x85x2048): 47.369
Elapsed time for attention_prob_times_values (96x2048x2048x85): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x85): 41.501

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 132.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x86x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x86x2048): 48.357
Elapsed time for attention_prob_times_values (96x2048x2048x86): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x86): 42.618

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 16.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x11x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x11x2048): 8.740
Elapsed time for attention_prob_times_values (512x2048x2048x11): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x11): 9.415

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 21.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x12x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x12x2048): 9.517
Elapsed time for attention_prob_times_values (512x2048x2048x12): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x12): 10.619

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 25.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x13x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x13x2048): 10.349
Elapsed time for attention_prob_times_values (512x2048x2048x13): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x13): 10.980

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 27.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x14x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x14x2048): 11.107
Elapsed time for attention_prob_times_values (512x2048x2048x14): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x14): 11.582

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 31.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x15x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x15x2048): 11.922
Elapsed time for attention_prob_times_values (512x2048x2048x15): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x15): 12.780

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 35.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x16x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x16x2048): 12.742
Elapsed time for attention_prob_times_values (512x2048x2048x16): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x16): 14.187

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 40.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x17x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x17x2048): 13.031
Elapsed time for attention_prob_times_values (512x2048x2048x17): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x17): 14.423

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 42.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x18x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x18x2048): 13.839
Elapsed time for attention_prob_times_values (512x2048x2048x18): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x18): 15.650

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 47.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x19x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x19x2048): 14.482
Elapsed time for attention_prob_times_values (512x2048x2048x19): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x19): 16.053

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 51.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 136.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x87x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x87x2048): 48.564
Elapsed time for attention_prob_times_values (96x2048x2048x87): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x87): 42.288

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 137.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x88x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x88x2048): 49.664
Elapsed time for attention_prob_times_values (96x2048x2048x88): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x88): 42.505

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 140.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x89x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x89x2048): 48.098
Elapsed time for attention_prob_times_values (96x2048x2048x89): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x89): 43.093

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 140.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x90x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x90x2048): 48.897
Elapsed time for attention_prob_times_values (96x2048x2048x90): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x90): 43.496

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 143.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x91x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x91x2048): 48.828
Elapsed time for attention_prob_times_values (96x2048x2048x91): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x91): 43.923

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 144.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x92x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x92x2048): 48.646
Elapsed time for attention_prob_times_values (96x2048x2048x92): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x92): 45.942

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 149.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x93x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x93x2048): 49.649
Elapsed time for attention_prob_times_values (96x2048x2048x93): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x93): 44.525

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 149.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x94x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x94x2048): 50.397
Elapsed time for attention_prob_times_values (96x2048x2048x94): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x94): 45.209

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 152.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x95x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x95x2048): 49.268
Elapsed time for attention_prob_times_values (96x2048x2048x95): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x95): 45.470

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 152.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x29): 23.279

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 62.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x30x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x30x2048): 21.933
Elapsed time for attention_prob_times_values (256x2048x2048x30): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x30): 24.655

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 66.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x31x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x31x2048): 22.585
Elapsed time for attention_prob_times_values (256x2048x2048x31): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x31): 24.629

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 69.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x32x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x32x2048): 40.682
Elapsed time for attention_prob_times_values (256x2048x2048x32): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x32): 26.575

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 96.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x33x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x33x2048): 25.657
Elapsed time for attention_prob_times_values (256x2048x2048x33): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x33): 26.398

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 79.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x34x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x34x2048): 27.106
Elapsed time for attention_prob_times_values (256x2048x2048x34): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x34): 27.431

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 85.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x35x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x35x2048): 26.125
Elapsed time for attention_prob_times_values (256x2048x2048x35): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x35): 27.844

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 85.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x36x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x36x2048): 25.579
Elapsed time for attention_prob_times_values (256x2048x2048x36): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x36): 29.181

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 88.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x37x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x37x2048): 26.740
Elapsed time for attention_prob_times_values (256x2048x2048x37): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x37): 27.912

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 90.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x38x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x38x2048): 27.552
Elapsed time for attention_prob_times_values (256x2048x2048x38): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x38): 30.443

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 97.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (32x2048x3840x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3840x2048): 106.269
Elapsed time for attention_prob_times_values (32x2048x2048x3840): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3840): 102.113

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3228.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3841x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3841x2048): 101.179
Elapsed time for attention_prob_times_values (32x2048x2048x3841): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3841): 98.393

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3093.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3842x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3842x2048): 103.426
Elapsed time for attention_prob_times_values (32x2048x2048x3842): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3842): 96.440

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3095.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3843x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3843x2048): 102.775
Elapsed time for attention_prob_times_values (32x2048x2048x3843): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3843): 99.952

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3144.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3844x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3844x2048): 102.729
Elapsed time for attention_prob_times_values (32x2048x2048x3844): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3844): 101.763

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3172.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3845x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3845x2048): 102.643
Elapsed time for attention_prob_times_values (32x2048x2048x3845): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3845): 100.550

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3153.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3846x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3846x2048): 102.614
Elapsed time for attention_prob_times_values (32x2048x2048x3846): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3846): 102.707

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3187.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3847x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3847x2048): 102.471
Elapsed time for attention_prob_times_values (32x2048x2048x3847): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3847): 100.733

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3154.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3848x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3848x2048): 102.848
Elapsed time for attention_prob_times_values (32x2048x2048x3848): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3848): 96.500

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3092.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3849x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3849x2048): 100.447
Elapsed time for attention_prob_times_values (32x2048x2048x3849): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3849): 100.873

num_attention_heads: 24, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x96x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x96x2048): 64.222
Elapsed time for attention_prob_times_values (96x2048x2048x96): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x96): 47.008

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 176.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x97x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x97x2048): 50.939
Elapsed time for attention_prob_times_values (96x2048x2048x97): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x97): 46.467

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 159.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x98x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x98x2048): 51.989
Elapsed time for attention_prob_times_values (96x2048x2048x98): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x98): 48.148

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 164.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x99x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x99x2048): 51.485
Elapsed time for attention_prob_times_values (96x2048x2048x99): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x99): 46.553

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 162.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x100x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x100x2048): 52.801
Elapsed time for attention_prob_times_values (96x2048x2048x100): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x100): 49.066

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 170.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x101x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x101x2048): 52.076
Elapsed time for attention_prob_times_values (96x2048x2048x101): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x101): 48.246

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 168.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x102x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x102x2048): 53.110
Elapsed time for attention_prob_times_values (96x2048x2048x102): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x102): 49.859

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 174.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x103x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x103x2048): 52.721
Elapsed time for attention_prob_times_values (96x2048x2048x103): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x103): 48.849

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 173.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x104x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x104x2048): 54.101
Elapsed time for attention_prob_times_values (96x2048x2048x104): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x104): 48.898

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 176.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x105x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x105x2048): 52.379
Elapsed time for attention_prob_times_values (96x2048x2048x105): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x105): 49.361

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 175.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x106x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x106x2048): 53.200
Elapsed time for attention_prob_times_values (96x2048x2048x106): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x106): 51.346

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 182.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x107x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x107x2048): 53.157
Elapsed time for attention_prob_times_values (96x2048x2048x107): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x107): 50.134

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 181.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x108x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x108x2048): 54.430
Elapsed time for attention_prob_times_values (96x2048x2048x108): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x108): 50.602

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 185.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x109x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x109x2048): 53.747
Elapsed time for attention_prob_times_values (96x2048x2048x109): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x109): 50.817

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 185.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x110x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x110x2048): 50.216
Elapsed time for attention_prob_times_values (96x2048x2048x110): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x110): 52.777

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 184.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x111x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x111x2048): 53.584
Elapsed time for attention_prob_times_values (96x2048x2048x111): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x111): 51.388

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 188.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x112x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x112x2048): 56.438
Elapsed time for attention_prob_times_values (96x2048x2048x112): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x112): 53.267

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 198.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x113x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x113x2048): 54.316
Elapsed time for attention_prob_times_values (96x2048x2048x113): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x113): 51.641

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 193.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x114x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x114x2048): 55.436
Elapsed time for attention_prob_times_values (96x2048x2048x114): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x114): 54.387

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 201.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x39x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x39x2048): 27.835
Elapsed time for attention_prob_times_values (256x2048x2048x39): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x39): 30.790

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 100.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x40x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x40x2048): 28.660
Elapsed time for attention_prob_times_values (256x2048x2048x40): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x40): 31.398

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 104.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x41x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x41x2048): 27.373
Elapsed time for attention_prob_times_values (256x2048x2048x41): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x41): 31.861

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 104.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x42x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x42x2048): 29.190
Elapsed time for attention_prob_times_values (256x2048x2048x42): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x42): 30.168

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 107.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x43x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x43x2048): 29.550
Elapsed time for attention_prob_times_values (256x2048x2048x43): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x43): 33.180

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 115.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x44x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x44x2048): 30.451
Elapsed time for attention_prob_times_values (256x2048x2048x44): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x44): 34.667

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 121.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x45x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x45x2048): 30.877
Elapsed time for attention_prob_times_values (256x2048x2048x45): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x45): 34.251

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 123.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x46x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x46x2048): 31.619
Elapsed time for attention_prob_times_values (256x2048x2048x46): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x46): 35.963

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 130.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x47x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x47x2048): 32.232
Elapsed time for attention_prob_times_values (256x2048x2048x47): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x47): 36.164

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 134.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x48x2048): 0.0031
========================================================================================================================
num_attention_heads: 24, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x115x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x115x2048): 55.074
Elapsed time for attention_prob_times_values (96x2048x2048x115): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x115): 52.343

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 198.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x116x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x116x2048): 56.198
Elapsed time for attention_prob_times_values (96x2048x2048x116): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x116): 54.930

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 206.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x117x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x117x2048): 55.708
Elapsed time for attention_prob_times_values (96x2048x2048x117): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x117): 53.484

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 204.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x118x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x118x2048): 56.687
Elapsed time for attention_prob_times_values (96x2048x2048x118): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x118): 55.728

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 211.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x119x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x119x2048): 56.303
Elapsed time for attention_prob_times_values (96x2048x2048x119): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x119): 54.112

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 209.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x120x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x120x2048): 57.879
Elapsed time for attention_prob_times_values (96x2048x2048x120): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x120): 56.023

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 217.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x121x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x121x2048): 48.701
Elapsed time for attention_prob_times_values (96x2048x2048x121): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x121): 43.725

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 176.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x122x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x122x2048): 56.612
Elapsed time for attention_prob_times_values (96x2048x2048x122): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x122): 56.511

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 218.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x123x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x123x2048): 55.751
Elapsed time for attention_prob_times_values (96x2048x2048x123): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x123): 44.048

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 191.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x124x2048): 0.0017
Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3127.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3850x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3850x2048): 97.816
Elapsed time for attention_prob_times_values (32x2048x2048x3850): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3850): 102.772

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3115.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3851x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3851x2048): 102.248
Elapsed time for attention_prob_times_values (32x2048x2048x3851): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3851): 100.939

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3158.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3852x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3852x2048): 103.045
Elapsed time for attention_prob_times_values (32x2048x2048x3852): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3852): 102.357

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3193.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3853x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3853x2048): 102.256
Elapsed time for attention_prob_times_values (32x2048x2048x3853): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3853): 92.544

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3021.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3854x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3854x2048): 102.915
Elapsed time for attention_prob_times_values (32x2048x2048x3854): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3854): 97.891

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3121.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3855x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3855x2048): 102.393
Elapsed time for attention_prob_times_values (32x2048x2048x3855): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3855): 100.945

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3163.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3856x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3856x2048): 104.005
Elapsed time for attention_prob_times_values (32x2048x2048x3856): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3856): 97.770

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3137.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3857x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3857x2048): 102.123
Elapsed time for attention_prob_times_values (32x2048x2048x3857): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3857): 100.780

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3158.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3858x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3858x2048): 101.579
Elapsed time for attention_prob_times_values (32x2048x2048x3858): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3858): 102.966

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3184.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x124x2048): 57.509
Elapsed time for attention_prob_times_values (96x2048x2048x124): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x124): 57.667

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 224.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x125x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x125x2048): 56.696
Elapsed time for attention_prob_times_values (96x2048x2048x125): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x125): 44.026

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 194.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x126x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x126x2048): 55.735
Elapsed time for attention_prob_times_values (96x2048x2048x126): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x126): 58.525

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 225.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x127x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x127x2048): 56.802
Elapsed time for attention_prob_times_values (96x2048x2048x127): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x127): 43.563

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 196.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x128x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x128x2048): 67.883
Elapsed time for attention_prob_times_values (96x2048x2048x128): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x128): 61.994

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 259.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x129x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x129x2048): 56.477
Elapsed time for attention_prob_times_values (96x2048x2048x129): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x129): 43.986

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 198.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x130x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x130x2048): 57.234
Elapsed time for attention_prob_times_values (96x2048x2048x130): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x130): 46.703

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 208.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x131x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x131x2048): 57.146
Elapsed time for attention_prob_times_values (96x2048x2048x131): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x131): 44.772

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 204.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x132x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x132x2048): 58.712
Elapsed time for attention_prob_times_values (96x2048x2048x132): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x132): 47.231

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 214.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x133x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x133x2048): 57.601
Elapsed time for attention_prob_times_values (96x2048x2048x133): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x133): 45.391

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 209.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x20x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x20x2048): 15.223
Elapsed time for attention_prob_times_values (512x2048x2048x20): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x20): 17.297

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 56.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x21x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x21x2048): 16.037
Elapsed time for attention_prob_times_values (512x2048x2048x21): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x21): 17.573

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 60.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x22x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x22x2048): 16.782
Elapsed time for attention_prob_times_values (512x2048x2048x22): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x22): 18.848

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 66.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x23x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x23x2048): 17.525
Elapsed time for attention_prob_times_values (512x2048x2048x23): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x23): 19.030

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 70.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x24x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x24x2048): 18.275
Elapsed time for attention_prob_times_values (512x2048x2048x24): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x24): 20.677

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 77.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x25x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x25x2048): 17.919
Elapsed time for attention_prob_times_values (512x2048x2048x25): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x25): 19.913

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 77.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x26x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x26x2048): 19.229
Elapsed time for attention_prob_times_values (512x2048x2048x26): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x26): 21.555

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 86.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x27x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x27x2048): 19.879
Elapsed time for attention_prob_times_values (512x2048x2048x27): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x27): 21.576

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 90.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x28x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x28x2048): 20.506
Elapsed time for attention_prob_times_values (512x2048x2048x28): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x28): 22.882

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 97.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x29x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x29x2048): 21.133
Elapsed time for attention_prob_times_values (512x2048x2048x29): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x48x2048): 33.342
Elapsed time for attention_prob_times_values (256x2048x2048x48): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x48): 38.626

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 143.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x49x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x49x2048): 33.262
Elapsed time for attention_prob_times_values (256x2048x2048x49): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x49): 36.397

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 141.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x50x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x50x2048): 34.064
Elapsed time for attention_prob_times_values (256x2048x2048x50): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x50): 38.384

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 148.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x51x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x51x2048): 34.630
Elapsed time for attention_prob_times_values (256x2048x2048x51): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x51): 38.492

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 152.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x52x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x52x2048): 35.229
Elapsed time for attention_prob_times_values (256x2048x2048x52): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x52): 40.384

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 159.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x53x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x53x2048): 36.029
Elapsed time for attention_prob_times_values (256x2048x2048x53): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x53): 40.019

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 163.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x54x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x54x2048): 36.836
Elapsed time for attention_prob_times_values (256x2048x2048x54): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x54): 41.607

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 170.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x55x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x55x2048): 35.464
Elapsed time for attention_prob_times_values (256x2048x2048x55): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x55): 41.156

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 169.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x56x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x56x2048): 38.160
Elapsed time for attention_prob_times_values (256x2048x2048x56): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x56): 44.507

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 184.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x57x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x57x2048): 37.024
Elapsed time for attention_prob_times_values (256x2048x2048x57): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x57): 40.919

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 177.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x134x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x134x2048): 58.880
Elapsed time for attention_prob_times_values (96x2048x2048x134): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x134): 47.840

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 218.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x135x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x135x2048): 58.355
Elapsed time for attention_prob_times_values (96x2048x2048x135): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x135): 45.981

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 214.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x136x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x136x2048): 60.142
Elapsed time for attention_prob_times_values (96x2048x2048x136): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x136): 45.236

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 216.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x137x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x137x2048): 58.229
Elapsed time for attention_prob_times_values (96x2048x2048x137): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x137): 46.418

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 217.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x138x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x138x2048): 58.713
Elapsed time for attention_prob_times_values (96x2048x2048x138): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x138): 49.045

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 226.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x139x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x139x2048): 58.505
Elapsed time for attention_prob_times_values (96x2048x2048x139): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x139): 46.898

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 221.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x140x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x140x2048): 60.071
Elapsed time for attention_prob_times_values (96x2048x2048x140): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x140): 49.730

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 232.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x141x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x141x2048): 58.899
Elapsed time for attention_prob_times_values (96x2048x2048x141): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x141): 47.822

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 227.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x142x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x142x2048): 60.544
Elapsed time for attention_prob_times_values (96x2048x2048x142): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x142): 50.297

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 237.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 8, hidden_size: 30872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3859x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3859x2048): 95.961
Elapsed time for attention_prob_times_values (32x2048x2048x3859): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3859): 101.017

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3065.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3860x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3860x2048): 98.097
Elapsed time for attention_prob_times_values (32x2048x2048x3860): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3860): 102.112

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3117.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3861x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3861x2048): 101.994
Elapsed time for attention_prob_times_values (32x2048x2048x3861): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3861): 98.273

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3119.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3862x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3862x2048): 102.560
Elapsed time for attention_prob_times_values (32x2048x2048x3862): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3862): 102.917

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3202.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3863x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3863x2048): 101.966
Elapsed time for attention_prob_times_values (32x2048x2048x3863): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3863): 100.908

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3162.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3864x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3864x2048): 103.393
Elapsed time for attention_prob_times_values (32x2048x2048x3864): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3864): 96.527

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3113.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3865x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3865x2048): 101.479
Elapsed time for attention_prob_times_values (32x2048x2048x3865): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3865): 101.259

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3162.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3866x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3866x2048): 101.630
Elapsed time for attention_prob_times_values (32x2048x2048x3866): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3866): 103.094

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3193.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3867x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3867x2048): 101.650
Elapsed time for attention_prob_times_values (32x2048x2048x3867): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3867): 100.574

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3155.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3868x2048): 0.0101
Elapsed time for attention_key_query_prob (96x2048x143x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x143x2048): 56.302
Elapsed time for attention_prob_times_values (96x2048x2048x143): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x143): 48.495

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 226.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x144x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x144x2048): 59.871
Elapsed time for attention_prob_times_values (96x2048x2048x144): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x144): 47.309

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 231.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x145x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x145x2048): 59.394
Elapsed time for attention_prob_times_values (96x2048x2048x145): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x145): 48.777

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 235.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x146x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x146x2048): 60.575
Elapsed time for attention_prob_times_values (96x2048x2048x146): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x146): 51.241

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 245.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x147x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x147x2048): 56.862
Elapsed time for attention_prob_times_values (96x2048x2048x147): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x147): 49.494

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 235.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x148x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x148x2048): 61.564
Elapsed time for attention_prob_times_values (96x2048x2048x148): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x148): 52.247

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 252.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x149x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x149x2048): 60.775
Elapsed time for attention_prob_times_values (96x2048x2048x149): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x149): 50.232

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 247.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x150x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x150x2048): 60.692
Elapsed time for attention_prob_times_values (96x2048x2048x150): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x150): 51.784

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 252.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x151x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x151x2048): 61.472
Elapsed time for attention_prob_times_values (96x2048x2048x151): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x151): 50.714

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 252.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x152x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x152x2048): 63.306
Elapsed time for attention_prob_times_values (96x2048x2048x152): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x152): 46.534

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 244.730
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x58x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x58x2048): 38.371
Elapsed time for attention_prob_times_values (256x2048x2048x58): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x58): 44.172

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 189.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x59x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x59x2048): 38.847
Elapsed time for attention_prob_times_values (256x2048x2048x59): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x59): 43.667

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 192.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x60x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x60x2048): 37.392
Elapsed time for attention_prob_times_values (256x2048x2048x60): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x60): 45.687

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 195.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x61x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x61x2048): 39.067
Elapsed time for attention_prob_times_values (256x2048x2048x61): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x61): 45.225

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 201.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x62x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x62x2048): 40.742
Elapsed time for attention_prob_times_values (256x2048x2048x62): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x62): 47.447

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 213.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x63x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x63x2048): 40.909
Elapsed time for attention_prob_times_values (256x2048x2048x63): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x63): 46.781

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 215.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x64x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x64x2048): 55.994
Elapsed time for attention_prob_times_values (256x2048x2048x64): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x64): 50.736

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 266.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x65x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x65x2048): 43.568
Elapsed time for attention_prob_times_values (256x2048x2048x65): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x65): 33.993

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 193.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x66x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x66x2048): 44.624
Elapsed time for attention_prob_times_values (256x2048x2048x66): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x66): 35.358

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 202.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x153x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x153x2048): 61.180
Elapsed time for attention_prob_times_values (96x2048x2048x153): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x153): 51.258

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 255.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x154x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x154x2048): 62.109
Elapsed time for attention_prob_times_values (96x2048x2048x154): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x154): 53.697

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 265.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x155x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x155x2048): 61.564
Elapsed time for attention_prob_times_values (96x2048x2048x155): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x155): 51.996

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 261.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x156x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x156x2048): 62.954
Elapsed time for attention_prob_times_values (96x2048x2048x156): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x156): 54.654

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 272.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x157x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x157x2048): 62.417
Elapsed time for attention_prob_times_values (96x2048x2048x157): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x157): 52.349

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 266.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x158x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x158x2048): 63.427
Elapsed time for attention_prob_times_values (96x2048x2048x158): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x158): 55.001

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 277.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x159x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x159x2048): 62.784
Elapsed time for attention_prob_times_values (96x2048x2048x159): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x159): 53.079

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 271.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x160x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x160x2048): 76.182
Elapsed time for attention_prob_times_values (96x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x160): 55.079

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 303.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x161x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x161x2048): 61.747
Elapsed time for attention_prob_times_values (96x2048x2048x161): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x161): 54.008

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 275.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3868x2048): 102.984
Elapsed time for attention_prob_times_values (32x2048x2048x3868): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3868): 97.177

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3121.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3869x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3869x2048): 102.012
Elapsed time for attention_prob_times_values (32x2048x2048x3869): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3869): 96.476

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3096.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3870x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3870x2048): 102.649
Elapsed time for attention_prob_times_values (32x2048x2048x3870): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3870): 99.066

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3149.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3871x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3871x2048): 101.397
Elapsed time for attention_prob_times_values (32x2048x2048x3871): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3871): 100.027

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3146.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3872x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3872x2048): 107.440
Elapsed time for attention_prob_times_values (32x2048x2048x3872): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3872): 97.091

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3187.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3873x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3873x2048): 102.845
Elapsed time for attention_prob_times_values (32x2048x2048x3873): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3873): 101.240

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3189.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3874x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3874x2048): 101.175
Elapsed time for attention_prob_times_values (32x2048x2048x3874): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3874): 102.825

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3188.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3875x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3875x2048): 102.721
Elapsed time for attention_prob_times_values (32x2048x2048x3875): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3875): 101.337

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3190.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3876x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3876x2048): 103.554
Elapsed time for attention_prob_times_values (32x2048x2048x3876): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3876): 103.280

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3235.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3877x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3877x2048): 97.249
Elapsed time for attention_prob_times_values (32x2048x2048x3877): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3877): 97.686

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3049.655
MLP duration (in seconds): 0.0000
num_attention_heads: 24, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x162x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x162x2048): 63.135
Elapsed time for attention_prob_times_values (96x2048x2048x162): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x162): 56.070

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 284.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x163x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x163x2048): 62.373
Elapsed time for attention_prob_times_values (96x2048x2048x163): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x163): 54.571

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 280.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x164x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x164x2048): 64.129
Elapsed time for attention_prob_times_values (96x2048x2048x164): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x164): 53.985

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 283.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x165x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x165x2048): 62.889
Elapsed time for attention_prob_times_values (96x2048x2048x165): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x165): 55.197

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 286.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x166x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x166x2048): 63.963
Elapsed time for attention_prob_times_values (96x2048x2048x166): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x166): 57.214

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 295.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x167x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x167x2048): 63.223
Elapsed time for attention_prob_times_values (96x2048x2048x167): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x167): 51.547

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 279.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x168x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x168x2048): 60.833
Elapsed time for attention_prob_times_values (96x2048x2048x168): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x168): 55.616

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 286.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x169x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x169x2048): 63.047
Elapsed time for attention_prob_times_values (96x2048x2048x169): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x169): 55.838

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 293.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x170x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x170x2048): 56.729
Elapsed time for attention_prob_times_values (96x2048x2048x170): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x170): 58.462

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 287.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x171x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x171x2048): 63.646
Elapsed time for attention_prob_times_values (96x2048x2048x171): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x29): 23.571

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 103.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x30x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x30x2048): 20.904
Elapsed time for attention_prob_times_values (512x2048x2048x30): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x30): 24.890

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 107.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x31x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x31x2048): 22.669
Elapsed time for attention_prob_times_values (512x2048x2048x31): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x31): 25.056

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 116.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x32x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x32x2048): 40.959
Elapsed time for attention_prob_times_values (512x2048x2048x32): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x32): 26.103

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 159.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x33x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x33x2048): 27.042
Elapsed time for attention_prob_times_values (512x2048x2048x33): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x33): 26.645

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 137.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x34x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x34x2048): 27.373
Elapsed time for attention_prob_times_values (512x2048x2048x34): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x34): 27.629

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 144.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x35x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x35x2048): 24.991
Elapsed time for attention_prob_times_values (512x2048x2048x35): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x35): 28.064

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 142.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x36x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x36x2048): 26.336
Elapsed time for attention_prob_times_values (512x2048x2048x36): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x36): 29.381

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 152.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x37x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x37x2048): 25.454
Elapsed time for attention_prob_times_values (512x2048x2048x37): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x37): 27.780

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 149.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x38x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x38x2048): 26.340
Elapsed time for attention_prob_times_values (512x2048x2048x38): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x38): 30.804

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 163.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x171): 56.011

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 298.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x172x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x172x2048): 65.182
Elapsed time for attention_prob_times_values (96x2048x2048x172): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x172): 59.179

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 312.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x173x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x173x2048): 64.199
Elapsed time for attention_prob_times_values (96x2048x2048x173): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x173): 57.272

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 306.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x174x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x174x2048): 65.455
Elapsed time for attention_prob_times_values (96x2048x2048x174): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x174): 59.631

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 316.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x175x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x175x2048): 64.742
Elapsed time for attention_prob_times_values (96x2048x2048x175): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x175): 57.736

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 311.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x176x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x176x2048): 67.493
Elapsed time for attention_prob_times_values (96x2048x2048x176): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x176): 51.572

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 299.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x177x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x177x2048): 64.396
Elapsed time for attention_prob_times_values (96x2048x2048x177): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x177): 58.236

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 314.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x178x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x178x2048): 59.404
Elapsed time for attention_prob_times_values (96x2048x2048x178): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x178): 60.779

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 310.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x179x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x179x2048): 57.200
Elapsed time for attention_prob_times_values (96x2048x2048x179): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x179): 59.090

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 302.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x180x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x180x2048): 66.425
Elapsed time for attention_prob_times_values (96x2048x2048x180): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x180): 61.332

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 332.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (256x2048x67x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x67x2048): 44.309
Elapsed time for attention_prob_times_values (256x2048x2048x67): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x67): 33.259

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 197.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x68x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x68x2048): 45.043
Elapsed time for attention_prob_times_values (256x2048x2048x68): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x68): 36.141

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 210.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x69x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x69x2048): 44.961
Elapsed time for attention_prob_times_values (256x2048x2048x69): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x69): 35.714

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 211.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x70x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x70x2048): 45.995
Elapsed time for attention_prob_times_values (256x2048x2048x70): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x70): 37.374

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 221.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x71x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x71x2048): 45.666
Elapsed time for attention_prob_times_values (256x2048x2048x71): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x71): 36.885

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 221.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x72x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x72x2048): 46.916
Elapsed time for attention_prob_times_values (256x2048x2048x72): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x72): 36.775

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 226.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x73x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x73x2048): 45.681
Elapsed time for attention_prob_times_values (256x2048x2048x73): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x73): 37.573

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 229.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x74x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x74x2048): 46.804
Elapsed time for attention_prob_times_values (256x2048x2048x74): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x74): 39.229

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 240.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x75x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x75x2048): 42.984
Elapsed time for attention_prob_times_values (256x2048x2048x75): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x75): 38.536

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 231.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x76x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x76x2048): 47.820
Elapsed time for attention_prob_times_values (256x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x76): 40.279

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 251.431
========================================================================================================================
num_attention_heads: 24, hidden_size: 4344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x181x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x181x2048): 65.471
Elapsed time for attention_prob_times_values (96x2048x2048x181): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x181): 59.454

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 326.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x182x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x182x2048): 66.849
Elapsed time for attention_prob_times_values (96x2048x2048x182): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x182): 60.470

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 334.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x183x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x183x2048): 65.876
Elapsed time for attention_prob_times_values (96x2048x2048x183): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x183): 58.913

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 328.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x184x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x184x2048): 67.886
Elapsed time for attention_prob_times_values (96x2048x2048x184): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x184): 60.398

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 339.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x185x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x185x2048): 64.673
Elapsed time for attention_prob_times_values (96x2048x2048x185): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x185): 60.433

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 333.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x186x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x186x2048): 63.441
Elapsed time for attention_prob_times_values (96x2048x2048x186): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x186): 62.769

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 338.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x187x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x187x2048): 66.125
Elapsed time for attention_prob_times_values (96x2048x2048x187): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x187): 58.172

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 333.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x188x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x188x2048): 65.337
Elapsed time for attention_prob_times_values (96x2048x2048x188): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x188): 63.204

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 347.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x189x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x189x2048): 66.464
Elapsed time for attention_prob_times_values (96x2048x2048x189): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x189): 61.590

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 347.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x190x2048): 0.0024
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3878x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3878x2048): 101.458
Elapsed time for attention_prob_times_values (32x2048x2048x3878): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3878): 102.934

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3198.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3879x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3879x2048): 102.422
Elapsed time for attention_prob_times_values (32x2048x2048x3879): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3879): 101.409

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3190.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3880x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3880x2048): 103.595
Elapsed time for attention_prob_times_values (32x2048x2048x3880): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3880): 96.991

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3137.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3881x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3881x2048): 101.654
Elapsed time for attention_prob_times_values (32x2048x2048x3881): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3881): 98.317

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3130.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3882x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3882x2048): 102.662
Elapsed time for attention_prob_times_values (32x2048x2048x3882): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3882): 102.424

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3212.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3883x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3883x2048): 102.323
Elapsed time for attention_prob_times_values (32x2048x2048x3883): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3883): 101.173

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3188.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3884x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3884x2048): 103.227
Elapsed time for attention_prob_times_values (32x2048x2048x3884): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3884): 103.351

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3237.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3885x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3885x2048): 100.754
Elapsed time for attention_prob_times_values (32x2048x2048x3885): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3885): 100.770

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3159.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3886x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3886x2048): 102.619
Elapsed time for attention_prob_times_values (32x2048x2048x3886): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3886): 102.807

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3221.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x190x2048): 64.434
Elapsed time for attention_prob_times_values (96x2048x2048x190): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x190): 64.035

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 350.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x191x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x191x2048): 67.118
Elapsed time for attention_prob_times_values (96x2048x2048x191): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x191): 61.783

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 352.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x192x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x192x2048): 77.278
Elapsed time for attention_prob_times_values (96x2048x2048x192): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x192): 65.573

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 390.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x193x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x193x2048): 65.961
Elapsed time for attention_prob_times_values (96x2048x2048x193): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x193): 50.909

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 317.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x194x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x194x2048): 67.325
Elapsed time for attention_prob_times_values (96x2048x2048x194): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x194): 52.649

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 327.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x195x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x195x2048): 66.316
Elapsed time for attention_prob_times_values (96x2048x2048x195): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x195): 51.062

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 321.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x196x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x196x2048): 67.859
Elapsed time for attention_prob_times_values (96x2048x2048x196): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x196): 52.916

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 332.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x197x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x197x2048): 66.701
Elapsed time for attention_prob_times_values (96x2048x2048x197): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x197): 51.542

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 326.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x198x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x198x2048): 68.077
Elapsed time for attention_prob_times_values (96x2048x2048x198): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x198): 53.260

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 337.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x199x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x199x2048): 67.069
Elapsed time for attention_prob_times_values (96x2048x2048x199): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x199): 51.717

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 330.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x77x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x77x2048): 45.192
Elapsed time for attention_prob_times_values (256x2048x2048x77): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x77): 39.324

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 244.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x78x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x78x2048): 48.681
Elapsed time for attention_prob_times_values (256x2048x2048x78): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x78): 41.042

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 261.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x79x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x79x2048): 46.786
Elapsed time for attention_prob_times_values (256x2048x2048x79): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x79): 40.213

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 256.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x80x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x80x2048): 50.400
Elapsed time for attention_prob_times_values (256x2048x2048x80): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x80): 39.324

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 265.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x81x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x81x2048): 47.471
Elapsed time for attention_prob_times_values (256x2048x2048x81): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x81): 39.155

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 260.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x82x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x82x2048): 48.029
Elapsed time for attention_prob_times_values (256x2048x2048x82): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x82): 42.802

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 277.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x83x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x83x2048): 48.310
Elapsed time for attention_prob_times_values (256x2048x2048x83): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x83): 41.740

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 277.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x84x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x84x2048): 48.820
Elapsed time for attention_prob_times_values (256x2048x2048x84): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x84): 43.724

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 288.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x85x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x85x2048): 48.858
Elapsed time for attention_prob_times_values (256x2048x2048x85): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x85): 42.671

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 287.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x200x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x200x2048): 69.379
Elapsed time for attention_prob_times_values (96x2048x2048x200): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x200): 51.201

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 335.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x201x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x201x2048): 63.085
Elapsed time for attention_prob_times_values (96x2048x2048x201): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x201): 51.480

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 323.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x202x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x202x2048): 66.378
Elapsed time for attention_prob_times_values (96x2048x2048x202): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x202): 54.258

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 342.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x203x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x203x2048): 59.396
Elapsed time for attention_prob_times_values (96x2048x2048x203): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x203): 52.028

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 319.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x204x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x204x2048): 66.217
Elapsed time for attention_prob_times_values (96x2048x2048x204): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x204): 54.931

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 347.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x205x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x205x2048): 67.660
Elapsed time for attention_prob_times_values (96x2048x2048x205): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x205): 52.524

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 343.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x206x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x206x2048): 68.994
Elapsed time for attention_prob_times_values (96x2048x2048x206): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x206): 55.087

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 357.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x207x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x207x2048): 68.140
Elapsed time for attention_prob_times_values (96x2048x2048x207): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x207): 52.896

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 348.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x208x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x208x2048): 71.102
Elapsed time for attention_prob_times_values (96x2048x2048x208): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x208): 53.886

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 360.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 8, hidden_size: 31096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3887x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3887x2048): 99.583
Elapsed time for attention_prob_times_values (32x2048x2048x3887): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3887): 101.216

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3149.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3888x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3888x2048): 103.979
Elapsed time for attention_prob_times_values (32x2048x2048x3888): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3888): 98.419

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3172.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3889x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3889x2048): 99.634
Elapsed time for attention_prob_times_values (32x2048x2048x3889): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3889): 101.327

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3153.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3890x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3890x2048): 101.494
Elapsed time for attention_prob_times_values (32x2048x2048x3890): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3890): 102.052

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3194.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3891x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3891x2048): 97.457
Elapsed time for attention_prob_times_values (32x2048x2048x3891): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3891): 100.661

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3109.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3892x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3892x2048): 102.338
Elapsed time for attention_prob_times_values (32x2048x2048x3892): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3892): 103.366

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3230.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3893x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3893x2048): 102.187
Elapsed time for attention_prob_times_values (32x2048x2048x3893): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3893): 101.222

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3194.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3894x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3894x2048): 102.820
Elapsed time for attention_prob_times_values (32x2048x2048x3894): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3894): 102.893

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3231.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3895x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3895x2048): 102.185
Elapsed time for attention_prob_times_values (32x2048x2048x3895): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3895): 101.323

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3198.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3896x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3896x2048): 103.438
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x39x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x39x2048): 27.695
Elapsed time for attention_prob_times_values (512x2048x2048x39): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x39): 31.077

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 172.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x40x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x40x2048): 27.786
Elapsed time for attention_prob_times_values (512x2048x2048x40): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x40): 32.941

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 180.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x41x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x41x2048): 26.851
Elapsed time for attention_prob_times_values (512x2048x2048x41): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x41): 31.662

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 177.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x42x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x42x2048): 28.990
Elapsed time for attention_prob_times_values (512x2048x2048x42): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x42): 33.131

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 193.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x43x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x43x2048): 29.358
Elapsed time for attention_prob_times_values (512x2048x2048x43): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x43): 33.594

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 199.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x44x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x44x2048): 30.142
Elapsed time for attention_prob_times_values (512x2048x2048x44): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x44): 34.912

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 210.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x45x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x45x2048): 30.602
Elapsed time for attention_prob_times_values (512x2048x2048x45): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x45): 32.746

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 209.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x46x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x46x2048): 31.703
Elapsed time for attention_prob_times_values (512x2048x2048x46): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x46): 36.409

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 228.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x47x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x47x2048): 30.684
Elapsed time for attention_prob_times_values (512x2048x2048x47): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x47): 36.326

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 228.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x48x2048): 0.0062
Elapsed time for attention_key_query_prob (96x2048x209x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x209x2048): 67.729
Elapsed time for attention_prob_times_values (96x2048x2048x209): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x209): 50.256

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 340.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x210x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x210x2048): 68.973
Elapsed time for attention_prob_times_values (96x2048x2048x210): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x210): 56.033

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 366.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x211x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x211x2048): 68.031
Elapsed time for attention_prob_times_values (96x2048x2048x211): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x211): 50.197

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 343.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x212x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x212x2048): 69.576
Elapsed time for attention_prob_times_values (96x2048x2048x212): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x212): 50.357

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 348.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x213x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x213x2048): 68.604
Elapsed time for attention_prob_times_values (96x2048x2048x213): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x213): 53.875

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 361.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x214x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x214x2048): 69.751
Elapsed time for attention_prob_times_values (96x2048x2048x214): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x214): 56.750

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 376.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x215x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x215x2048): 68.920
Elapsed time for attention_prob_times_values (96x2048x2048x215): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x215): 52.437

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 359.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x216x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x216x2048): 70.978
Elapsed time for attention_prob_times_values (96x2048x2048x216): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x216): 55.242

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 376.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x217x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x217x2048): 68.616
Elapsed time for attention_prob_times_values (96x2048x2048x217): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x217): 54.842

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 371.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x218x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x218x2048): 66.475
Elapsed time for attention_prob_times_values (96x2048x2048x218): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x218): 57.612

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 377.112
num_attention_heads: 64, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x86x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x86x2048): 47.807
Elapsed time for attention_prob_times_values (256x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x86): 44.437

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 293.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x87x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x87x2048): 49.893
Elapsed time for attention_prob_times_values (256x2048x2048x87): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x87): 43.489

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 299.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x88x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x88x2048): 49.986
Elapsed time for attention_prob_times_values (256x2048x2048x88): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x88): 43.219

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 301.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x89x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x89x2048): 49.271
Elapsed time for attention_prob_times_values (256x2048x2048x89): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x89): 43.667

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 303.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x90x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x90x2048): 49.103
Elapsed time for attention_prob_times_values (256x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x90): 46.212

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 315.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x91x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x91x2048): 50.172
Elapsed time for attention_prob_times_values (256x2048x2048x91): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x91): 44.270

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 314.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x92x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x92x2048): 51.079
Elapsed time for attention_prob_times_values (256x2048x2048x92): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x92): 46.385

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 328.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x93x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x93x2048): 51.025
Elapsed time for attention_prob_times_values (256x2048x2048x93): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x93): 45.880

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 329.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x94x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x94x2048): 51.838
Elapsed time for attention_prob_times_values (256x2048x2048x94): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x94): 47.916

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 342.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x95x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x95x2048): 51.882
Elapsed time for attention_prob_times_values (256x2048x2048x95): 0.0044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x219x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x219x2048): 68.638
Elapsed time for attention_prob_times_values (96x2048x2048x219): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x219): 54.212

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 371.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x220x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x220x2048): 70.489
Elapsed time for attention_prob_times_values (96x2048x2048x220): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x220): 58.314

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 392.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x221x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x221x2048): 69.497
Elapsed time for attention_prob_times_values (96x2048x2048x221): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x221): 55.836

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 382.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x222x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x222x2048): 67.804
Elapsed time for attention_prob_times_values (96x2048x2048x222): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x222): 51.785

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 364.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x223x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x223x2048): 64.175
Elapsed time for attention_prob_times_values (96x2048x2048x223): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x223): 56.156

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 372.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x224x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x224x2048): 79.651
Elapsed time for attention_prob_times_values (96x2048x2048x224): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x224): 56.124

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 411.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x225x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x225x2048): 68.442
Elapsed time for attention_prob_times_values (96x2048x2048x225): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x225): 54.312

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 379.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x226x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x226x2048): 65.553
Elapsed time for attention_prob_times_values (96x2048x2048x226): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x226): 57.499

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 385.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x227x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x227x2048): 64.394
Elapsed time for attention_prob_times_values (96x2048x2048x227): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x227): 57.356

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 383.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (32x2048x2048x3896): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3896): 96.623

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3141.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3897x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3897x2048): 102.040
Elapsed time for attention_prob_times_values (32x2048x2048x3897): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3897): 101.035

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3192.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3898x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3898x2048): 102.431
Elapsed time for attention_prob_times_values (32x2048x2048x3898): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3898): 102.539

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3223.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3899x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3899x2048): 102.159
Elapsed time for attention_prob_times_values (32x2048x2048x3899): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3899): 101.350

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3201.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3900x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3900x2048): 103.146
Elapsed time for attention_prob_times_values (32x2048x2048x3900): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3900): 103.540

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3252.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3901x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3901x2048): 102.166
Elapsed time for attention_prob_times_values (32x2048x2048x3901): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3901): 101.278

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3201.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3902x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3902x2048): 100.650
Elapsed time for attention_prob_times_values (32x2048x2048x3902): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3902): 101.487

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3182.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3903x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3903x2048): 102.352
Elapsed time for attention_prob_times_values (32x2048x2048x3903): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3903): 101.398

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3208.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3904x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3904x2048): 107.562
Elapsed time for attention_prob_times_values (32x2048x2048x3904): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3904): 99.338

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3253.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3905x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3905x2048): 102.675
Elapsed time for attention_prob_times_values (32x2048x2048x3905): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3905): 101.223

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3212.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
num_attention_heads: 24, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x228x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x228x2048): 63.614
Elapsed time for attention_prob_times_values (96x2048x2048x228): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x228): 59.847

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 391.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x229x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x229x2048): 69.099
Elapsed time for attention_prob_times_values (96x2048x2048x229): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x229): 57.702

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 400.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x230x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x230x2048): 70.509
Elapsed time for attention_prob_times_values (96x2048x2048x230): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x230): 58.765

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 409.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x231x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x231x2048): 69.392
Elapsed time for attention_prob_times_values (96x2048x2048x231): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x231): 54.207

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 390.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x232x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x232x2048): 71.813
Elapsed time for attention_prob_times_values (96x2048x2048x232): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x232): 58.506

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 415.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x233x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x233x2048): 68.830
Elapsed time for attention_prob_times_values (96x2048x2048x233): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x233): 58.017

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 406.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x234x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x234x2048): 70.375
Elapsed time for attention_prob_times_values (96x2048x2048x234): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x234): 60.580

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 422.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x235x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x235x2048): 69.161
Elapsed time for attention_prob_times_values (96x2048x2048x235): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x235): 58.464

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 412.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x236x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x236x2048): 71.052
Elapsed time for attention_prob_times_values (96x2048x2048x236): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x236): 59.170

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 421.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x237x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x237x2048): 69.507
Elapsed time for attention_prob_times_values (96x2048x2048x237): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x95): 46.677

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 340.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x96x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x96x2048): 61.126
Elapsed time for attention_prob_times_values (256x2048x2048x96): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x96): 48.515

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 378.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x97x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x97x2048): 48.821
Elapsed time for attention_prob_times_values (256x2048x2048x97): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x97): 47.714

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 340.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x98x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x98x2048): 51.933
Elapsed time for attention_prob_times_values (256x2048x2048x98): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x98): 46.610

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 350.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x99x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x99x2048): 52.571
Elapsed time for attention_prob_times_values (256x2048x2048x99): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x99): 48.568

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 362.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x100x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x100x2048): 54.875
Elapsed time for attention_prob_times_values (256x2048x2048x100): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x100): 50.168

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 380.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x101x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x101x2048): 53.852
Elapsed time for attention_prob_times_values (256x2048x2048x101): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x101): 49.042

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 375.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x102x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x102x2048): 53.214
Elapsed time for attention_prob_times_values (256x2048x2048x102): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x102): 50.364

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 381.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x103x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x103x2048): 54.599
Elapsed time for attention_prob_times_values (256x2048x2048x103): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x103): 50.216

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 389.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x104x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x104x2048): 56.039
Elapsed time for attention_prob_times_values (256x2048x2048x104): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x104): 50.600

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 398.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x237): 58.818

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 417.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x238x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x238x2048): 71.097
Elapsed time for attention_prob_times_values (96x2048x2048x238): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x238): 61.601

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 434.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x239x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x239x2048): 69.984
Elapsed time for attention_prob_times_values (96x2048x2048x239): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x239): 59.328

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 423.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x240x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x240x2048): 55.665
Elapsed time for attention_prob_times_values (96x2048x2048x240): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x240): 61.660

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 387.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x241x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x241x2048): 53.246
Elapsed time for attention_prob_times_values (96x2048x2048x241): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x241): 59.864

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 374.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x242x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x242x2048): 53.692
Elapsed time for attention_prob_times_values (96x2048x2048x242): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x242): 62.236

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 384.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x243x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x243x2048): 52.007
Elapsed time for attention_prob_times_values (96x2048x2048x243): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x243): 59.615

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 371.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x244x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x244x2048): 54.725
Elapsed time for attention_prob_times_values (96x2048x2048x244): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x244): 62.990

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 393.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x245x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x245x2048): 53.510
Elapsed time for attention_prob_times_values (96x2048x2048x245): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x245): 60.563

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 383.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x246x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x246x2048): 54.687
Elapsed time for attention_prob_times_values (96x2048x2048x246): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x246): 63.139

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 396.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3906x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3906x2048): 99.854
Elapsed time for attention_prob_times_values (32x2048x2048x3906): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3906): 100.399

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3155.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3907x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3907x2048): 99.244
Elapsed time for attention_prob_times_values (32x2048x2048x3907): 0.0214
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3907): 49.078

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 2070.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3908x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3908x2048): 103.732
Elapsed time for attention_prob_times_values (32x2048x2048x3908): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3908): 102.864

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3257.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3909x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3909x2048): 96.154
Elapsed time for attention_prob_times_values (32x2048x2048x3909): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3909): 98.791

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3073.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3910x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3910x2048): 99.983
Elapsed time for attention_prob_times_values (32x2048x2048x3910): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3910): 100.077

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3155.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3911x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3911x2048): 100.670
Elapsed time for attention_prob_times_values (32x2048x2048x3911): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3911): 101.564

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3190.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3912x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3912x2048): 103.765
Elapsed time for attention_prob_times_values (32x2048x2048x3912): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3912): 96.510

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3156.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3913x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3913x2048): 101.534
Elapsed time for attention_prob_times_values (32x2048x2048x3913): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3913): 101.759

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3208.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3914x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3914x2048): 102.885
Elapsed time for attention_prob_times_values (32x2048x2048x3914): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3914): 103.755

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3262.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x48x2048): 33.301
Elapsed time for attention_prob_times_values (512x2048x2048x48): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x48): 37.352

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 246.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x49x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x49x2048): 33.503
Elapsed time for attention_prob_times_values (512x2048x2048x49): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x49): 38.012

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 253.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x50x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x50x2048): 14.977
Elapsed time for attention_prob_times_values (512x2048x2048x50): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x50): 38.881

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 156.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x51x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x51x2048): 34.834
Elapsed time for attention_prob_times_values (512x2048x2048x51): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x51): 38.872

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 270.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x52x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x52x2048): 35.627
Elapsed time for attention_prob_times_values (512x2048x2048x52): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x52): 40.721

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 285.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x53x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x53x2048): 35.270
Elapsed time for attention_prob_times_values (512x2048x2048x53): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x53): 38.091

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 279.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x54x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x54x2048): 36.194
Elapsed time for attention_prob_times_values (512x2048x2048x54): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x54): 42.174

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 301.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x55x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x55x2048): 36.339
Elapsed time for attention_prob_times_values (512x2048x2048x55): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x55): 41.283

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 304.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x56x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x56x2048): 38.340
Elapsed time for attention_prob_times_values (512x2048x2048x56): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x56): 43.884

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 327.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x57x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x57x2048): 37.880
Elapsed time for attention_prob_times_values (512x2048x2048x57): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x57): 40.904

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 319.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x247x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x247x2048): 52.137
Elapsed time for attention_prob_times_values (96x2048x2048x247): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x247): 60.973

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 381.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x248x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x248x2048): 55.711
Elapsed time for attention_prob_times_values (96x2048x2048x248): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x248): 60.503

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 395.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x249x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x249x2048): 53.652
Elapsed time for attention_prob_times_values (96x2048x2048x249): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x249): 62.760

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 395.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x250x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x250x2048): 54.130
Elapsed time for attention_prob_times_values (96x2048x2048x250): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x250): 64.006

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 402.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x251x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x251x2048): 53.873
Elapsed time for attention_prob_times_values (96x2048x2048x251): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x251): 59.192

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 388.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x252x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x252x2048): 51.908
Elapsed time for attention_prob_times_values (96x2048x2048x252): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x252): 64.435

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 397.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x253x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x253x2048): 53.803
Elapsed time for attention_prob_times_values (96x2048x2048x253): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x253): 60.427

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 394.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x254x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x254x2048): 54.576
Elapsed time for attention_prob_times_values (96x2048x2048x254): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x254): 65.010

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 412.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x255x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x255x2048): 53.325
Elapsed time for attention_prob_times_values (96x2048x2048x255): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x255): 61.827

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 399.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x256x2048): 0.0027
========================================================================================================================
num_attention_heads: 64, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x105x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x105x2048): 52.356
Elapsed time for attention_prob_times_values (256x2048x2048x105): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x105): 49.816

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 386.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x106x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x106x2048): 55.300
Elapsed time for attention_prob_times_values (256x2048x2048x106): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x106): 51.417

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 406.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x107x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x107x2048): 52.957
Elapsed time for attention_prob_times_values (256x2048x2048x107): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x107): 50.362

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 396.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x108x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x108x2048): 56.236
Elapsed time for attention_prob_times_values (256x2048x2048x108): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x108): 52.320

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 420.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x109x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x109x2048): 53.196
Elapsed time for attention_prob_times_values (256x2048x2048x109): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x109): 50.232

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 403.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x110x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x110x2048): 56.743
Elapsed time for attention_prob_times_values (256x2048x2048x110): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x110): 54.350

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 437.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x111x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x111x2048): 56.339
Elapsed time for attention_prob_times_values (256x2048x2048x111): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x111): 52.816

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 432.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x112x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x112x2048): 58.394
Elapsed time for attention_prob_times_values (256x2048x2048x112): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x112): 54.913

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 452.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x113x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x113x2048): 54.385
Elapsed time for attention_prob_times_values (256x2048x2048x113): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x113): 53.420

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 434.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x114x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x256x2048): 76.433
Elapsed time for attention_prob_times_values (96x2048x2048x256): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x256): 69.098

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 508.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x257x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x257x2048): 55.032
Elapsed time for attention_prob_times_values (96x2048x2048x257): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x257): 53.357

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 380.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x258x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x258x2048): 57.182
Elapsed time for attention_prob_times_values (96x2048x2048x258): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x258): 55.851

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 398.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x259x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x259x2048): 55.828
Elapsed time for attention_prob_times_values (96x2048x2048x259): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x259): 54.045

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 388.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x260x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x260x2048): 57.693
Elapsed time for attention_prob_times_values (96x2048x2048x260): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x260): 55.663

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 401.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x261x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x261x2048): 55.092
Elapsed time for attention_prob_times_values (96x2048x2048x261): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x261): 54.355

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 389.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x262x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x262x2048): 56.794
Elapsed time for attention_prob_times_values (96x2048x2048x262): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x262): 57.142

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 406.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x263x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x263x2048): 55.615
Elapsed time for attention_prob_times_values (96x2048x2048x263): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x263): 55.176

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 396.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x264x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x264x2048): 57.653
Elapsed time for attention_prob_times_values (96x2048x2048x264): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x264): 54.254

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 401.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x265x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x265x2048): 55.259
Elapsed time for attention_prob_times_values (96x2048x2048x265): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x265): 55.011

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 397.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
--------
Elapsed time for attention_key_query_prob (32x2048x3915x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3915x2048): 102.238
Elapsed time for attention_prob_times_values (32x2048x2048x3915): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3915): 101.863

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3223.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3916x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3916x2048): 102.832
Elapsed time for attention_prob_times_values (32x2048x2048x3916): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3916): 103.903

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3265.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3917x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3917x2048): 98.736
Elapsed time for attention_prob_times_values (32x2048x2048x3917): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3917): 101.173

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3158.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3918x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3918x2048): 98.181
Elapsed time for attention_prob_times_values (32x2048x2048x3918): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3918): 99.479

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3123.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3919x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3919x2048): 96.895
Elapsed time for attention_prob_times_values (32x2048x2048x3919): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3919): 101.945

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3141.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3920x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3920x2048): 96.977
Elapsed time for attention_prob_times_values (32x2048x2048x3920): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3920): 99.116

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3100.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3921x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3921x2048): 102.158
Elapsed time for attention_prob_times_values (32x2048x2048x3921): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3921): 102.104

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3230.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3922x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3922x2048): 102.793
Elapsed time for attention_prob_times_values (32x2048x2048x3922): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3922): 103.803

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3268.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3923x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3923x2048): 102.158
Elapsed time for attention_prob_times_values (32x2048x2048x3923): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3923): 102.116

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3232.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3924x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3924x2048): 102.833
Elapsed time for attention_prob_times_values (32x2048x2048x3924): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3924): 104.101

Attention duration (in seconds): 0.0204Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x266x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x266x2048): 56.018
Elapsed time for attention_prob_times_values (96x2048x2048x266): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x266): 57.835

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 411.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x267x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x267x2048): 55.800
Elapsed time for attention_prob_times_values (96x2048x2048x267): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x267): 55.491

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 403.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x268x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x268x2048): 56.807
Elapsed time for attention_prob_times_values (96x2048x2048x268): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x268): 58.549

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 419.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x269x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x269x2048): 56.170
Elapsed time for attention_prob_times_values (96x2048x2048x269): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x269): 55.976

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 409.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x270x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x270x2048): 56.726
Elapsed time for attention_prob_times_values (96x2048x2048x270): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x270): 58.758

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 423.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x271x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x271x2048): 56.344
Elapsed time for attention_prob_times_values (96x2048x2048x271): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x271): 56.267

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 413.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x272x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x272x2048): 58.258
Elapsed time for attention_prob_times_values (96x2048x2048x272): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x272): 73.223

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 478.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x273x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x273x2048): 55.673
Elapsed time for attention_prob_times_values (96x2048x2048x273): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x273): 54.064

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 405.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x274x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x274x2048): 55.144
Elapsed time for attention_prob_times_values (96x2048x2048x274): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x274): 59.247

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 423.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x58x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x58x2048): 38.208
Elapsed time for attention_prob_times_values (512x2048x2048x58): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x58): 44.781

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 340.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x59x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x59x2048): 37.821
Elapsed time for attention_prob_times_values (512x2048x2048x59): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x59): 43.635

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 339.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x60x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x60x2048): 39.742
Elapsed time for attention_prob_times_values (512x2048x2048x60): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x60): 46.453

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 364.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x61x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x61x2048): 39.800
Elapsed time for attention_prob_times_values (512x2048x2048x61): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x61): 44.326

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 361.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x62x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x62x2048): 39.458
Elapsed time for attention_prob_times_values (512x2048x2048x62): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x62): 45.804

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 370.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x63x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x63x2048): 41.217
Elapsed time for attention_prob_times_values (512x2048x2048x63): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x63): 45.176

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 382.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 55.214
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 51.186

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 478.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 13.788
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 33.455

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 178.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 44.192
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 34.952

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 361.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x114x2048): 57.435
Elapsed time for attention_prob_times_values (256x2048x2048x114): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x114): 55.576

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 458.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x115x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x115x2048): 56.711
Elapsed time for attention_prob_times_values (256x2048x2048x115): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x115): 51.971

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 444.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x116x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x116x2048): 58.130
Elapsed time for attention_prob_times_values (256x2048x2048x116): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x116): 56.386

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 472.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x117x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x117x2048): 57.562
Elapsed time for attention_prob_times_values (256x2048x2048x117): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x117): 54.174

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 463.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x118x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x118x2048): 56.749
Elapsed time for attention_prob_times_values (256x2048x2048x118): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x118): 57.330

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 477.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x119x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x119x2048): 58.247
Elapsed time for attention_prob_times_values (256x2048x2048x119): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x119): 55.577

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 479.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x120x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x120x2048): 57.429
Elapsed time for attention_prob_times_values (256x2048x2048x120): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x120): 57.377

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 487.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x121x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x121x2048): 58.009
Elapsed time for attention_prob_times_values (256x2048x2048x121): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x121): 42.641

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 420.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x122x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x122x2048): 58.997
Elapsed time for attention_prob_times_values (256x2048x2048x122): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x122): 56.781

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 499.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x123x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x123x2048): 57.772
Elapsed time for attention_prob_times_values (256x2048x2048x123): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x123): 42.433

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 425.065
MLP duration (in seconds): 0.0000

Attention throughput (in TFLOP/s): 3275.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3925x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3925x2048): 102.094
Elapsed time for attention_prob_times_values (32x2048x2048x3925): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3925): 101.970

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3230.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3926x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3926x2048): 101.815
Elapsed time for attention_prob_times_values (32x2048x2048x3926): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3926): 97.297

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3151.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3927x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3927x2048): 101.960
Elapsed time for attention_prob_times_values (32x2048x2048x3927): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3927): 97.605

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3159.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3928x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3928x2048): 103.275
Elapsed time for attention_prob_times_values (32x2048x2048x3928): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3928): 90.184

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 3051.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3929x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3929x2048): 101.939
Elapsed time for attention_prob_times_values (32x2048x2048x3929): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3929): 101.600

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3225.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3930x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3930x2048): 102.490
Elapsed time for attention_prob_times_values (32x2048x2048x3930): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3930): 103.951

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3272.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3931x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3931x2048): 102.069
Elapsed time for attention_prob_times_values (32x2048x2048x3931): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3931): 102.187

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3238.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3932x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3932x2048): 102.475
Elapsed time for attention_prob_times_values (32x2048x2048x3932): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3932): 104.180

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3277.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3933x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3933x2048): 101.673
Elapsed time for attention_prob_times_values (32x2048x2048x3933): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3933): 99.335

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3188.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (96x2048x275x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x275x2048): 55.990
Elapsed time for attention_prob_times_values (96x2048x2048x275): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x275): 56.905

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 420.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x276x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x276x2048): 55.786
Elapsed time for attention_prob_times_values (96x2048x2048x276): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x276): 58.757

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 427.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x277x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x277x2048): 55.105
Elapsed time for attention_prob_times_values (96x2048x2048x277): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x277): 57.376

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 421.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x278x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x278x2048): 56.906
Elapsed time for attention_prob_times_values (96x2048x2048x278): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x278): 56.682

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 426.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x279x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x279x2048): 56.780
Elapsed time for attention_prob_times_values (96x2048x2048x279): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x279): 57.712

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 431.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x280x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x280x2048): 58.068
Elapsed time for attention_prob_times_values (96x2048x2048x280): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x280): 70.526

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 481.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x281x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x281x2048): 56.075
Elapsed time for attention_prob_times_values (96x2048x2048x281): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x281): 58.101

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 432.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x282x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x282x2048): 56.519
Elapsed time for attention_prob_times_values (96x2048x2048x282): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x282): 60.778

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 445.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x283x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x283x2048): 56.298
Elapsed time for attention_prob_times_values (96x2048x2048x283): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x283): 58.554

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 438.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x284x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x284x2048): 57.451
Elapsed time for attention_prob_times_values (96x2048x2048x284): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x284): 61.030

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 453.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x285x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x285x2048): 55.433
Elapsed time for attention_prob_times_values (96x2048x2048x285): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x285): 58.222

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 436.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x286x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x286x2048): 54.888
Elapsed time for attention_prob_times_values (96x2048x2048x286): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x286): 61.660

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 447.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x287x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x287x2048): 56.446
Elapsed time for attention_prob_times_values (96x2048x2048x287): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x287): 59.421

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 447.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x288x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x288x2048): 77.370
Elapsed time for attention_prob_times_values (96x2048x2048x288): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x288): 76.254

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 595.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x289x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x289x2048): 60.398
Elapsed time for attention_prob_times_values (96x2048x2048x289): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x289): 59.872

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 467.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x290x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x290x2048): 60.223
Elapsed time for attention_prob_times_values (96x2048x2048x290): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x290): 62.153

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 476.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x291x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x291x2048): 60.442
Elapsed time for attention_prob_times_values (96x2048x2048x291): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x291): 60.102

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 471.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x292x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x292x2048): 61.386
Elapsed time for attention_prob_times_values (96x2048x2048x292): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x292): 62.949

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 487.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x293x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x293x2048): 60.467
Elapsed time for attention_prob_times_values (96x2048x2048x293): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x293): 60.608

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 476.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x124x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x124x2048): 57.468
Elapsed time for attention_prob_times_values (256x2048x2048x124): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x124): 58.519

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 507.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x125x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x125x2048): 57.344
Elapsed time for attention_prob_times_values (256x2048x2048x125): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x125): 44.068

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 439.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x126x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x126x2048): 59.707
Elapsed time for attention_prob_times_values (256x2048x2048x126): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x126): 60.118

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 531.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x127x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x127x2048): 58.387
Elapsed time for attention_prob_times_values (256x2048x2048x127): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x127): 43.803

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 447.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 69.650
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 63.386

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 597.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x129x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x129x2048): 56.024
Elapsed time for attention_prob_times_values (256x2048x2048x129): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x129): 45.167

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 453.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 59.538
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 45.720

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 471.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x131x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x131x2048): 59.303
Elapsed time for attention_prob_times_values (256x2048x2048x131): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x131): 46.287

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 477.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 59.093
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 48.818

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 494.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 8, hidden_size: 31472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3934x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3934x2048): 101.001
Elapsed time for attention_prob_times_values (32x2048x2048x3934): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3934): 104.304

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3256.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3935x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3935x2048): 101.961
Elapsed time for attention_prob_times_values (32x2048x2048x3935): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3935): 98.537

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3181.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3936x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3936x2048): 107.567
Elapsed time for attention_prob_times_values (32x2048x2048x3936): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3936): 99.845

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3288.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3937x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3937x2048): 102.853
Elapsed time for attention_prob_times_values (32x2048x2048x3937): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3937): 102.549

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3261.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3938x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3938x2048): 103.215
Elapsed time for attention_prob_times_values (32x2048x2048x3938): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3938): 104.359

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3296.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3939x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3939x2048): 102.708
Elapsed time for attention_prob_times_values (32x2048x2048x3939): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3939): 60.388

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2416.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3940x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3940x2048): 103.697
Elapsed time for attention_prob_times_values (32x2048x2048x3940): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3940): 103.847

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3298.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3941x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3941x2048): 102.056
Elapsed time for attention_prob_times_values (32x2048x2048x3941): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3941): 102.719

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3254.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3942x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3942x2048): 103.175
Elapsed time for attention_prob_times_values (32x2048x2048x3942): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3942): 103.648

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3288.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3943x2048): 0.0103
num_attention_heads: 24, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x294x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x294x2048): 58.673
Elapsed time for attention_prob_times_values (96x2048x2048x294): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x294): 62.986

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 479.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x295x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x295x2048): 58.386
Elapsed time for attention_prob_times_values (96x2048x2048x295): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x295): 61.141

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 472.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x296x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x296x2048): 59.505
Elapsed time for attention_prob_times_values (96x2048x2048x296): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x296): 78.917

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 538.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x297x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x297x2048): 59.188
Elapsed time for attention_prob_times_values (96x2048x2048x297): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x297): 58.192

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 467.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x298x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x298x2048): 56.311
Elapsed time for attention_prob_times_values (96x2048x2048x298): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x298): 61.518

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 469.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x299x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x299x2048): 56.298
Elapsed time for attention_prob_times_values (96x2048x2048x299): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x299): 61.604

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 471.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x300x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x300x2048): 60.958
Elapsed time for attention_prob_times_values (96x2048x2048x300): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x300): 64.852

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 504.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x301x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x301x2048): 59.869
Elapsed time for attention_prob_times_values (96x2048x2048x301): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x301): 62.316

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 491.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x302x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x302x2048): 55.441
Elapsed time for attention_prob_times_values (96x2048x2048x302): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x302): 65.018

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 483.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x303x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x303x2048): 60.033
Elapsed time for attention_prob_times_values (96x2048x2048x303): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x303): 63.137

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 498.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x304x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x304x2048): 61.350
Elapsed time for attention_prob_times_values (96x2048x2048x304): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x304): 80.820

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 566.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x305x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x305x2048): 59.227
Elapsed time for attention_prob_times_values (96x2048x2048x305): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x305): 63.121

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 497.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x306x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x306x2048): 60.057
Elapsed time for attention_prob_times_values (96x2048x2048x306): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x306): 65.678

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 512.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x307x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x307x2048): 59.674
Elapsed time for attention_prob_times_values (96x2048x2048x307): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x307): 63.492

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 504.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x308x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x308x2048): 61.189
Elapsed time for attention_prob_times_values (96x2048x2048x308): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x308): 64.770

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x309x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x309x2048): 60.059
Elapsed time for attention_prob_times_values (96x2048x2048x309): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x309): 64.907

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 514.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x310x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x310x2048): 60.571
Elapsed time for attention_prob_times_values (96x2048x2048x310): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x310): 66.465

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 523.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x311x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x311x2048): 60.306
Elapsed time for attention_prob_times_values (96x2048x2048x311): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x311): 63.858

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 514.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x312x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x312x2048): 61.969
Elapsed time for attention_prob_times_values (96x2048x2048x312): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x312): 81.296

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 584.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000

Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 42.458
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 34.087

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 354.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 44.965
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 34.629

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 371.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 43.483
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 35.516

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 376.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 45.641
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 36.873

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 397.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 44.104
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 36.468

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 394.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 46.178
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 35.501

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 401.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 43.552
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 35.942

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 398.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 45.250
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 37.315

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 419.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 46.183
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 38.000

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 432.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 45.664
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 38.734

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 440.102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3943x2048): 102.449
Elapsed time for attention_prob_times_values (32x2048x2048x3943): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3943): 94.953

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3134.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3944x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3944x2048): 103.727
Elapsed time for attention_prob_times_values (32x2048x2048x3944): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3944): 98.495

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3214.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3945x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3945x2048): 102.150
Elapsed time for attention_prob_times_values (32x2048x2048x3945): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3945): 98.065

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3184.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3946x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3946x2048): 99.229
Elapsed time for attention_prob_times_values (32x2048x2048x3946): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3946): 100.652

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3180.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3947x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3947x2048): 96.496
Elapsed time for attention_prob_times_values (32x2048x2048x3947): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3947): 102.395

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3163.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3948x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3948x2048): 99.089
Elapsed time for attention_prob_times_values (32x2048x2048x3948): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3948): 104.604

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3240.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3949x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3949x2048): 101.857
Elapsed time for attention_prob_times_values (32x2048x2048x3949): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3949): 101.109

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3232.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3950x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3950x2048): 99.379
Elapsed time for attention_prob_times_values (32x2048x2048x3950): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3950): 103.602

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3232.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3951x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3951x2048): 101.960
Elapsed time for attention_prob_times_values (32x2048x2048x3951): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3951): 102.572

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3258.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3952x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3952x2048): 104.200
Elapsed time for attention_prob_times_values (32x2048x2048x3952): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3952): 99.616

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3246.680
MLP duration (in seconds): 0.0000
num_attention_heads: 64, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x133x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x133x2048): 59.458
Elapsed time for attention_prob_times_values (256x2048x2048x133): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x133): 47.017

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 489.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 58.679
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 47.153

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 490.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x135x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x135x2048): 58.996
Elapsed time for attention_prob_times_values (256x2048x2048x135): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x135): 47.670

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 497.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 62.071
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 46.341

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 504.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x137x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x137x2048): 60.171
Elapsed time for attention_prob_times_values (256x2048x2048x137): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x137): 45.871

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 497.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 61.072
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 49.823

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 528.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x139x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x139x2048): 60.583
Elapsed time for attention_prob_times_values (256x2048x2048x139): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x139): 48.293

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 520.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 60.065
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 51.514

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 540.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x141x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x141x2048): 60.979
Elapsed time for attention_prob_times_values (256x2048x2048x141): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x141): 49.170

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 534.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 61.095
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0059
========================================================================================================================
num_attention_heads: 24, hidden_size: 7512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x313x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x313x2048): 59.521
Elapsed time for attention_prob_times_values (96x2048x2048x313): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x313): 64.575

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 516.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x314x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x314x2048): 59.541
Elapsed time for attention_prob_times_values (96x2048x2048x314): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x314): 67.097

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 527.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x315x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x315x2048): 59.804
Elapsed time for attention_prob_times_values (96x2048x2048x315): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x315): 65.347

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 523.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x316x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x316x2048): 59.268
Elapsed time for attention_prob_times_values (96x2048x2048x316): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x316): 67.218

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 529.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x317x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x317x2048): 57.554
Elapsed time for attention_prob_times_values (96x2048x2048x317): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x317): 65.520

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 516.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x318x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x318x2048): 60.657
Elapsed time for attention_prob_times_values (96x2048x2048x318): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x318): 67.908

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 541.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x319x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x319x2048): 59.164
Elapsed time for attention_prob_times_values (96x2048x2048x319): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x319): 66.099

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 529.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x320x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x320x2048): 78.380
Elapsed time for attention_prob_times_values (96x2048x2048x320): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x320): 81.840

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 680.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x321x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x321x2048): 62.869
Elapsed time for attention_prob_times_values (96x2048x2048x321): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x321): 58.227

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 515.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x322x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x322x2048): 63.726
Elapsed time for attention_prob_times_values (96x2048x2048x322): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x322): 60.578

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 530.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x323x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x323x2048): 62.903
Elapsed time for attention_prob_times_values (96x2048x2048x323): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x323): 58.524

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 519.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x324x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x324x2048): 64.002
Elapsed time for attention_prob_times_values (96x2048x2048x324): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x324): 59.995

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 532.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x325x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x325x2048): 62.783
Elapsed time for attention_prob_times_values (96x2048x2048x325): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x325): 58.820

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 523.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x326x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x326x2048): 63.167
Elapsed time for attention_prob_times_values (96x2048x2048x326): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x326): 61.194

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 537.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x327x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x327x2048): 62.451
Elapsed time for attention_prob_times_values (96x2048x2048x327): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x327): 59.204

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 526.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x328x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x328x2048): 62.571
Elapsed time for attention_prob_times_values (96x2048x2048x328): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x328): 73.007

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 585.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x329x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x329x2048): 61.474
Elapsed time for attention_prob_times_values (96x2048x2048x329): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x329): 57.626

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 518.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x330x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x330x2048): 57.482
Elapsed time for attention_prob_times_values (96x2048x2048x330): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x330): 61.295

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 518.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x331x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x331x2048): 61.054
Elapsed time for attention_prob_times_values (96x2048x2048x331): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x331): 59.422

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 527.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3953x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3953x2048): 101.968
Elapsed time for attention_prob_times_values (32x2048x2048x3953): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3953): 102.479

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3259.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3954x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3954x2048): 102.854
Elapsed time for attention_prob_times_values (32x2048x2048x3954): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3954): 104.711

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3309.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3955x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3955x2048): 98.418
Elapsed time for attention_prob_times_values (32x2048x2048x3955): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3955): 103.013

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3210.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3956x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3956x2048): 103.299
Elapsed time for attention_prob_times_values (32x2048x2048x3956): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3956): 104.838

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3320.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3957x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3957x2048): 99.242
Elapsed time for attention_prob_times_values (32x2048x2048x3957): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3957): 102.932

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3225.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3958x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3958x2048): 102.908
Elapsed time for attention_prob_times_values (32x2048x2048x3958): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3958): 104.743

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3314.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3959x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3959x2048): 99.436
Elapsed time for attention_prob_times_values (32x2048x2048x3959): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3959): 102.336

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3220.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3960x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3960x2048): 103.714
Elapsed time for attention_prob_times_values (32x2048x2048x3960): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3960): 98.447

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3226.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3961x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3961x2048): 102.258
Elapsed time for attention_prob_times_values (32x2048x2048x3961): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3961): 95.938

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3162.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 51.873

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 554.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x143x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x143x2048): 60.040
Elapsed time for attention_prob_times_values (256x2048x2048x143): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x143): 47.241

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 525.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 64.200
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 50.046

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 562.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x145x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x145x2048): 61.389
Elapsed time for attention_prob_times_values (256x2048x2048x145): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x145): 50.543

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 557.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 61.873
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 49.583

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 557.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x147x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x147x2048): 61.898
Elapsed time for attention_prob_times_values (256x2048x2048x147): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x147): 51.082

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 570.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 62.860
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 53.848

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 594.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x149x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x149x2048): 62.946
Elapsed time for attention_prob_times_values (256x2048x2048x149): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x149): 51.724

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 585.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 64.212
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 54.243

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 610.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x151x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x151x2048): 61.359
Elapsed time for attention_prob_times_values (256x2048x2048x151): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x151): 50.621

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 579.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x332x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x332x2048): 63.485
Elapsed time for attention_prob_times_values (96x2048x2048x332): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x332): 62.627

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 553.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x333x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x333x2048): 60.293
Elapsed time for attention_prob_times_values (96x2048x2048x333): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x333): 57.874

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 519.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x334x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x334x2048): 62.850
Elapsed time for attention_prob_times_values (96x2048x2048x334): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x334): 62.724

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 554.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x335x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x335x2048): 57.778
Elapsed time for attention_prob_times_values (96x2048x2048x335): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x335): 58.468

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 514.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x336x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x336x2048): 63.852
Elapsed time for attention_prob_times_values (96x2048x2048x336): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x336): 74.862

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 611.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x337x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x337x2048): 61.294
Elapsed time for attention_prob_times_values (96x2048x2048x337): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x337): 59.347

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 536.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x338x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x338x2048): 62.131
Elapsed time for attention_prob_times_values (96x2048x2048x338): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x338): 63.254

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 559.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x339x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x339x2048): 61.874
Elapsed time for attention_prob_times_values (96x2048x2048x339): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x339): 56.091

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 526.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x340x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x340x2048): 62.858
Elapsed time for attention_prob_times_values (96x2048x2048x340): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x340): 63.684

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 567.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 45.397
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 37.128

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 434.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 48.096
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 39.617

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 467.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 46.023
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 39.752

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 463.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 49.915
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 38.500

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 478.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 47.314
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 38.529

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 472.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 47.821
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 42.247

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 504.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 48.438
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 40.697

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 503.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 47.668
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 42.006

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 513.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 48.150
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 40.696

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 512.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (96x2048x341x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x341x2048): 62.252
Elapsed time for attention_prob_times_values (96x2048x2048x341): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x341): 59.484

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 547.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x342x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x342x2048): 60.269
Elapsed time for attention_prob_times_values (96x2048x2048x342): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x342): 62.503

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 553.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x343x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x343x2048): 60.225
Elapsed time for attention_prob_times_values (96x2048x2048x343): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x343): 60.883

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 547.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x344x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x344x2048): 61.752
Elapsed time for attention_prob_times_values (96x2048x2048x344): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x344): 76.488

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 619.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x345x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x345x2048): 59.039
Elapsed time for attention_prob_times_values (96x2048x2048x345): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x345): 59.664

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 539.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x346x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x346x2048): 61.947
Elapsed time for attention_prob_times_values (96x2048x2048x346): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x346): 62.507

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 566.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x347x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x347x2048): 60.103
Elapsed time for attention_prob_times_values (96x2048x2048x347): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x347): 60.547

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 550.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x348x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x348x2048): 63.101
Elapsed time for attention_prob_times_values (96x2048x2048x348): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x348): 63.114

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 577.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x349x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x349x2048): 61.893
Elapsed time for attention_prob_times_values (96x2048x2048x349): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x349): 60.319

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 560.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x350x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x350x2048): 62.659
Elapsed time for attention_prob_times_values (96x2048x2048x350): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x350): 63.020

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 578.318
num_attention_heads: 8, hidden_size: 31696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3962x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3962x2048): 102.793
Elapsed time for attention_prob_times_values (32x2048x2048x3962): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3962): 104.931

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3318.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3963x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3963x2048): 100.181
Elapsed time for attention_prob_times_values (32x2048x2048x3963): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3963): 102.762

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3242.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3964x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3964x2048): 101.270
Elapsed time for attention_prob_times_values (32x2048x2048x3964): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3964): 105.127

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3297.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3965x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3965x2048): 102.235
Elapsed time for attention_prob_times_values (32x2048x2048x3965): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3965): 102.828

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3278.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3966x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3966x2048): 102.343
Elapsed time for attention_prob_times_values (32x2048x2048x3966): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3966): 102.277

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3272.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3967x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3967x2048): 97.788
Elapsed time for attention_prob_times_values (32x2048x2048x3967): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3967): 102.904

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3208.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3968x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3968x2048): 107.559
Elapsed time for attention_prob_times_values (32x2048x2048x3968): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3968): 101.605

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 3343.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3969x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3969x2048): 101.290
Elapsed time for attention_prob_times_values (32x2048x2048x3969): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3969): 100.871

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3235.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3970x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3970x2048): 102.121
Elapsed time for attention_prob_times_values (32x2048x2048x3970): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3970): 102.270

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3271.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3971x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3971x2048): 99.589
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 63.413
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 50.051

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 587.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 60.153
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 52.884

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 594.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 64.364
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 55.299

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 632.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 60.867
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 51.670

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 597.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 65.263
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 56.209

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 649.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 64.634
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 52.676

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 627.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 65.822
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 54.740

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 650.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 65.047
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 54.566

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 649.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 78.635
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 56.511

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 723.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x351x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x351x2048): 62.098
Elapsed time for attention_prob_times_values (96x2048x2048x351): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x351): 61.096

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 568.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x352x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x352x2048): 80.799
Elapsed time for attention_prob_times_values (96x2048x2048x352): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x352): 75.957

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 724.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x353x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x353x2048): 64.304
Elapsed time for attention_prob_times_values (96x2048x2048x353): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x353): 62.883

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 589.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x354x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x354x2048): 63.846
Elapsed time for attention_prob_times_values (96x2048x2048x354): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x354): 63.625

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 592.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x355x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x355x2048): 61.930
Elapsed time for attention_prob_times_values (96x2048x2048x355): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x355): 63.326

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 583.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x356x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x356x2048): 64.073
Elapsed time for attention_prob_times_values (96x2048x2048x356): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x356): 61.871

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 588.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x357x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x357x2048): 61.965
Elapsed time for attention_prob_times_values (96x2048x2048x357): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x357): 62.501

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 582.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x358x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x358x2048): 65.117
Elapsed time for attention_prob_times_values (96x2048x2048x358): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x358): 62.251

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 597.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x359x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x359x2048): 64.013
Elapsed time for attention_prob_times_values (96x2048x2048x359): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x359): 64.182

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 603.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (32x2048x2048x3971): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3971): 101.169

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3214.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3972x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3972x2048): 103.859
Elapsed time for attention_prob_times_values (32x2048x2048x3972): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3972): 101.721

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 3292.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3973x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3973x2048): 101.424
Elapsed time for attention_prob_times_values (32x2048x2048x3973): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3973): 98.013

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3193.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3974x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3974x2048): 100.744
Elapsed time for attention_prob_times_values (32x2048x2048x3974): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3974): 99.609

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3210.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3975x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3975x2048): 101.886
Elapsed time for attention_prob_times_values (32x2048x2048x3975): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3975): 101.282

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3256.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3976x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3976x2048): 103.047
Elapsed time for attention_prob_times_values (32x2048x2048x3976): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3976): 94.664

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3163.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3977x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3977x2048): 99.953
Elapsed time for attention_prob_times_values (32x2048x2048x3977): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3977): 101.271

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3226.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3978x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3978x2048): 102.910
Elapsed time for attention_prob_times_values (32x2048x2048x3978): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3978): 97.235

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3207.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3979x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3979x2048): 102.317
Elapsed time for attention_prob_times_values (32x2048x2048x3979): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3979): 99.823

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3242.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3980x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3980x2048): 102.047
Elapsed time for attention_prob_times_values (32x2048x2048x3980): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3980): 98.360

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3214.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
num_attention_heads: 24, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x360x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x360x2048): 62.447
Elapsed time for attention_prob_times_values (96x2048x2048x360): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x360): 79.361

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 659.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x361x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x361x2048): 63.159
Elapsed time for attention_prob_times_values (96x2048x2048x361): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x361): 62.269

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 593.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x362x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x362x2048): 63.867
Elapsed time for attention_prob_times_values (96x2048x2048x362): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x362): 62.083

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 597.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x363x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x363x2048): 63.483
Elapsed time for attention_prob_times_values (96x2048x2048x363): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x363): 62.166

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 597.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x364x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x364x2048): 64.682
Elapsed time for attention_prob_times_values (96x2048x2048x364): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x364): 65.398

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 619.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x365x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x365x2048): 60.580
Elapsed time for attention_prob_times_values (96x2048x2048x365): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x365): 62.969

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 590.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x366x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x366x2048): 64.444
Elapsed time for attention_prob_times_values (96x2048x2048x366): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x366): 62.295

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 606.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x367x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x367x2048): 63.717
Elapsed time for attention_prob_times_values (96x2048x2048x367): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x367): 63.766

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 612.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x368x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x368x2048): 65.690
Elapsed time for attention_prob_times_values (96x2048x2048x368): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x368): 80.552

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 696.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x369x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x369x2048): 62.853
Elapsed time for attention_prob_times_values (96x2048x2048x369): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 63.124
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 55.364

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 652.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 65.346
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 57.577

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 681.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 64.422
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 53.957

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 657.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 66.319
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 58.449

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 699.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 64.970
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 56.820

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 685.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 65.958
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 57.980

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 701.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 65.193
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 56.316

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 691.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 67.482
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 53.721

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 687.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 62.371
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 56.523

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 685.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 66.379
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 57.496

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 716.317
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x369): 63.675

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 610.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x370x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x370x2048): 63.923
Elapsed time for attention_prob_times_values (96x2048x2048x370): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x370): 65.787

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 627.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x371x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x371x2048): 63.222
Elapsed time for attention_prob_times_values (96x2048x2048x371): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x371): 63.902

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 616.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x372x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x372x2048): 64.463
Elapsed time for attention_prob_times_values (96x2048x2048x372): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x372): 63.176

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 620.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x373x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x373x2048): 63.190
Elapsed time for attention_prob_times_values (96x2048x2048x373): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x373): 63.413

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 616.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x374x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x374x2048): 64.242
Elapsed time for attention_prob_times_values (96x2048x2048x374): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x374): 64.298

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 627.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x375x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x375x2048): 63.775
Elapsed time for attention_prob_times_values (96x2048x2048x375): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x375): 64.424

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 627.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x376x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x376x2048): 61.565
Elapsed time for attention_prob_times_values (96x2048x2048x376): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x376): 80.036

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 682.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x377x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x377x2048): 62.111
Elapsed time for attention_prob_times_values (96x2048x2048x377): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x377): 64.692

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 623.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x378x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x378x2048): 60.126
Elapsed time for attention_prob_times_values (96x2048x2048x378): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x378): 67.142

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 625.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 49.198
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 43.198

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 540.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 49.520
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 41.622

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 537.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 49.695
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 41.007

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 539.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 47.781
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 43.988

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 555.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 47.583
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 45.454

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 569.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 48.809
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 44.180

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 573.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0247
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 15.989
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 45.750

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 296.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0333
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 49.280
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 42.921

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 579.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 49.755
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 47.453

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 619.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 50.361
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3981x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3981x2048): 102.415
Elapsed time for attention_prob_times_values (32x2048x2048x3981): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3981): 101.373

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3270.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3982x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3982x2048): 102.883
Elapsed time for attention_prob_times_values (32x2048x2048x3982): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3982): 103.034

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3305.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3983x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3983x2048): 101.741
Elapsed time for attention_prob_times_values (32x2048x2048x3983): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3983): 101.061

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3256.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3984x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3984x2048): 101.440
Elapsed time for attention_prob_times_values (32x2048x2048x3984): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3984): 100.248

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3239.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3985x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3985x2048): 101.806
Elapsed time for attention_prob_times_values (32x2048x2048x3985): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3985): 101.397

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3264.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3986x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3986x2048): 102.816
Elapsed time for attention_prob_times_values (32x2048x2048x3986): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3986): 103.069

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3308.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3987x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3987x2048): 101.110
Elapsed time for attention_prob_times_values (32x2048x2048x3987): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3987): 100.548

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3241.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3988x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3988x2048): 100.544
Elapsed time for attention_prob_times_values (32x2048x2048x3988): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3988): 102.845

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3269.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3989x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3989x2048): 99.652
Elapsed time for attention_prob_times_values (32x2048x2048x3989): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3989): 101.538

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3235.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
========================================================================================================================
num_attention_heads: 24, hidden_size: 9096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x379x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x379x2048): 63.223
Elapsed time for attention_prob_times_values (96x2048x2048x379): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x379): 62.854

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 622.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x380x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x380x2048): 64.476
Elapsed time for attention_prob_times_values (96x2048x2048x380): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x380): 67.409

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 652.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x381x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x381x2048): 62.954
Elapsed time for attention_prob_times_values (96x2048x2048x381): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x381): 65.209

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 636.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x382x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x382x2048): 60.491
Elapsed time for attention_prob_times_values (96x2048x2048x382): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x382): 67.749

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 636.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x383x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x383x2048): 62.998
Elapsed time for attention_prob_times_values (96x2048x2048x383): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x383): 60.849

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 617.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x384x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x384x2048): 77.909
Elapsed time for attention_prob_times_values (96x2048x2048x384): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x384): 82.320

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 800.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x385x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x385x2048): 63.618
Elapsed time for attention_prob_times_values (96x2048x2048x385): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x385): 57.948

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 607.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x386x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x386x2048): 66.428
Elapsed time for attention_prob_times_values (96x2048x2048x386): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x386): 60.877

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 638.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x387x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x387x2048): 65.483
Elapsed time for attention_prob_times_values (96x2048x2048x387): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x387): 58.431

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 621.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x388x2048): 0.0047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 65.785
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 55.858

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 706.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 67.376
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 60.886

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 751.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 64.706
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 58.871

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 728.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 67.639
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 61.218

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 763.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 66.822
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 59.412

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 750.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 69.319
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 58.579

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 761.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 65.762
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 59.612

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 754.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 66.438
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 62.051

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 778.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 65.227
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 57.828

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 747.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x388x2048): 66.834
Elapsed time for attention_prob_times_values (96x2048x2048x388): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x388): 61.859

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 648.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x389x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x389x2048): 60.488
Elapsed time for attention_prob_times_values (96x2048x2048x389): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x389): 58.899

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 603.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x390x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x390x2048): 63.298
Elapsed time for attention_prob_times_values (96x2048x2048x390): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x390): 62.008

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 635.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x391x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x391x2048): 65.568
Elapsed time for attention_prob_times_values (96x2048x2048x391): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x391): 59.298

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 632.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x392x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x392x2048): 66.848
Elapsed time for attention_prob_times_values (96x2048x2048x392): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x392): 74.364

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 717.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x393x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x393x2048): 64.417
Elapsed time for attention_prob_times_values (96x2048x2048x393): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x393): 56.036

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 611.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x394x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x394x2048): 65.236
Elapsed time for attention_prob_times_values (96x2048x2048x394): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x394): 62.553

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 653.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x395x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x395x2048): 64.861
Elapsed time for attention_prob_times_values (96x2048x2048x395): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x395): 59.451

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 636.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x396x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x396x2048): 65.880
Elapsed time for attention_prob_times_values (96x2048x2048x396): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x396): 63.036

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 662.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x397x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x397x2048): 64.899
Elapsed time for attention_prob_times_values (96x2048x2048x397): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x397): 59.733

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 641.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3990x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3990x2048): 102.317
Elapsed time for attention_prob_times_values (32x2048x2048x3990): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3990): 101.266

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3274.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3991x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3991x2048): 99.740
Elapsed time for attention_prob_times_values (32x2048x2048x3991): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3991): 100.478

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3221.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3992x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3992x2048): 99.816
Elapsed time for attention_prob_times_values (32x2048x2048x3992): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3992): 97.514

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3175.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3993x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3993x2048): 102.178
Elapsed time for attention_prob_times_values (32x2048x2048x3993): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3993): 101.328

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3275.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3994x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3994x2048): 101.299
Elapsed time for attention_prob_times_values (32x2048x2048x3994): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3994): 100.508

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3249.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3995x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3995x2048): 100.863
Elapsed time for attention_prob_times_values (32x2048x2048x3995): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3995): 101.688

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3262.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3996x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3996x2048): 103.136
Elapsed time for attention_prob_times_values (32x2048x2048x3996): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3996): 100.053

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3272.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3997x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3997x2048): 100.063
Elapsed time for attention_prob_times_values (32x2048x2048x3997): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3997): 96.921

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 3173.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3998x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3998x2048): 102.684
Elapsed time for attention_prob_times_values (32x2048x2048x3998): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3998): 99.618

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3259.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 31992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3999x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3999x2048): 100.383
Elapsed time for attention_prob_times_values (32x2048x2048x3999): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3999): 101.469

Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x398x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x398x2048): 65.808
Elapsed time for attention_prob_times_values (96x2048x2048x398): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x398): 63.061

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 665.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x399x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x399x2048): 65.188
Elapsed time for attention_prob_times_values (96x2048x2048x399): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x399): 60.037

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 647.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x400x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x400x2048): 67.038
Elapsed time for attention_prob_times_values (96x2048x2048x400): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x400): 71.042

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 715.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x401x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x401x2048): 64.532
Elapsed time for attention_prob_times_values (96x2048x2048x401): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x401): 60.268

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 648.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x402x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x402x2048): 65.215
Elapsed time for attention_prob_times_values (96x2048x2048x402): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x402): 61.178

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 657.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x403x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x403x2048): 64.885
Elapsed time for attention_prob_times_values (96x2048x2048x403): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x403): 60.453

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 653.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x404x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x404x2048): 65.754
Elapsed time for attention_prob_times_values (96x2048x2048x404): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x404): 63.458

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 676.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x405x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x405x2048): 65.033
Elapsed time for attention_prob_times_values (96x2048x2048x405): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x405): 60.790

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 659.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x406x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x406x2048): 63.932
Elapsed time for attention_prob_times_values (96x2048x2048x406): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x406): 62.883

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 666.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0184
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 21.036
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 63.110

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 386.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 67.586
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 57.155

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 762.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 69.088
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 59.862

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 793.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 65.983
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 61.629

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 792.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 70.345
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 61.526

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 820.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 67.732
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 61.971

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 813.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 67.737
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 64.525

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 834.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 66.164
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 62.687

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 816.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 69.895
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 64.798

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 857.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 66.325
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 46.292

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 621.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 64.213
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 47.799

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 712.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 52.160
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 47.328

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 651.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 52.220
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 49.060

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 670.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 52.635
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 47.003

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 664.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 53.890
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 49.972

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 700.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 52.389
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 47.332

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 677.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 54.305
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 49.220

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 710.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 53.176
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 47.902

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 699.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 55.419
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 49.075

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 728.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3253.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4000x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4000x2048): 101.595
Elapsed time for attention_prob_times_values (32x2048x2048x4000): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4000): 101.561

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3275.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4001x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4001x2048): 101.414
Elapsed time for attention_prob_times_values (32x2048x2048x4001): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4001): 96.154

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 3184.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4002x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4002x2048): 103.241
Elapsed time for attention_prob_times_values (32x2048x2048x4002): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4002): 103.014

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3327.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4003x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4003x2048): 102.287
Elapsed time for attention_prob_times_values (32x2048x2048x4003): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4003): 101.809

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3293.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4004x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4004x2048): 102.569
Elapsed time for attention_prob_times_values (32x2048x2048x4004): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4004): 103.616

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3327.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4005x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4005x2048): 101.844
Elapsed time for attention_prob_times_values (32x2048x2048x4005): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4005): 101.717

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3286.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4006x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4006x2048): 99.321
Elapsed time for attention_prob_times_values (32x2048x2048x4006): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4006): 103.482

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3273.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4007x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4007x2048): 100.647
Elapsed time for attention_prob_times_values (32x2048x2048x4007): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4007): 99.153

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3227.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4008x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4008x2048): 103.273
Elapsed time for attention_prob_times_values (32x2048x2048x4008): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4008): 99.733

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3278.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (96x2048x407x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x407x2048): 65.342
Elapsed time for attention_prob_times_values (96x2048x2048x407): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x407): 60.909

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 664.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x408x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x408x2048): 66.778
Elapsed time for attention_prob_times_values (96x2048x2048x408): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x408): 77.006

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 755.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x409x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x409x2048): 64.776
Elapsed time for attention_prob_times_values (96x2048x2048x409): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x409): 61.317

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 666.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x410x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x410x2048): 65.435
Elapsed time for attention_prob_times_values (96x2048x2048x410): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x410): 61.372

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 671.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x411x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x411x2048): 65.053
Elapsed time for attention_prob_times_values (96x2048x2048x411): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x411): 61.027

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 669.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x412x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x412x2048): 62.920
Elapsed time for attention_prob_times_values (96x2048x2048x412): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x412): 63.841

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 675.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x413x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x413x2048): 61.873
Elapsed time for attention_prob_times_values (96x2048x2048x413): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x413): 61.868

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 660.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x414x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x414x2048): 65.663
Elapsed time for attention_prob_times_values (96x2048x2048x414): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x414): 65.083

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 699.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x415x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x415x2048): 65.170
Elapsed time for attention_prob_times_values (96x2048x2048x415): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x415): 62.120

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 682.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x416x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x416x2048): 83.217
Elapsed time for attention_prob_times_values (96x2048x2048x416): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x416): 77.023

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 860.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x417x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x417x2048): 68.241
Elapsed time for attention_prob_times_values (96x2048x2048x417): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x417): 62.564

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 703.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x418x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x418x2048): 69.152
Elapsed time for attention_prob_times_values (96x2048x2048x418): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x418): 65.596

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 726.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x419x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x419x2048): 68.192
Elapsed time for attention_prob_times_values (96x2048x2048x419): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x419): 62.661

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 706.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x420x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x420x2048): 69.283
Elapsed time for attention_prob_times_values (96x2048x2048x420): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x420): 65.459

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 729.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x421x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x421x2048): 68.114
Elapsed time for attention_prob_times_values (96x2048x2048x421): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x421): 62.893

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 710.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x422x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x422x2048): 68.650
Elapsed time for attention_prob_times_values (96x2048x2048x422): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x422): 66.094

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 733.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x423x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x423x2048): 65.997
Elapsed time for attention_prob_times_values (96x2048x2048x423): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x423): 63.425

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 705.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x424x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x424x2048): 69.633
Elapsed time for attention_prob_times_values (96x2048x2048x424): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x424): 79.440

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 811.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x425x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x425x2048): 66.887
Elapsed time for attention_prob_times_values (96x2048x2048x425): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x425): 63.434

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 713.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 61.689

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 819.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 70.245
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 63.080

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 855.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 68.787
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 63.645

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 855.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 80.545
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 66.916

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 950.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 65.294
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 54.044

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 772.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 66.720
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 56.379

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 802.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 66.730
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 54.548

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 791.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 70.184
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 54.706

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 814.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 68.955
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 54.638

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 811.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 70.402
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 55.463

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 829.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
========================================================================================================================
num_attention_heads: 8, hidden_size: 32072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4009x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4009x2048): 102.218
Elapsed time for attention_prob_times_values (32x2048x2048x4009): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4009): 100.717

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3279.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4010x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4010x2048): 102.896
Elapsed time for attention_prob_times_values (32x2048x2048x4010): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4010): 98.515

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3254.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4011x2048): 0.0240
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4011x2048): 44.871
Elapsed time for attention_prob_times_values (32x2048x2048x4011): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4011): 101.865

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 2014.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4012x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4012x2048): 103.329
Elapsed time for attention_prob_times_values (32x2048x2048x4012): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4012): 103.796

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3349.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4013x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4013x2048): 100.921
Elapsed time for attention_prob_times_values (32x2048x2048x4013): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4013): 101.953

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3281.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4014x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4014x2048): 103.036
Elapsed time for attention_prob_times_values (32x2048x2048x4014): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4014): 103.712

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 3345.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4015x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4015x2048): 101.664
Elapsed time for attention_prob_times_values (32x2048x2048x4015): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4015): 95.728

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 3191.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4016x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4016x2048): 97.951
Elapsed time for attention_prob_times_values (32x2048x2048x4016): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4016): 101.216

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3223.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4017x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4017x2048): 100.467
Elapsed time for attention_prob_times_values (32x2048x2048x4017): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4017): 101.680

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3272.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4018x2048): 0.0105
num_attention_heads: 24, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x426x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x426x2048): 66.068
Elapsed time for attention_prob_times_values (96x2048x2048x426): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x426): 66.603

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 728.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x427x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x427x2048): 67.158
Elapsed time for attention_prob_times_values (96x2048x2048x427): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x427): 63.663

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 719.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x428x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x428x2048): 68.664
Elapsed time for attention_prob_times_values (96x2048x2048x428): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x428): 66.969

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 747.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x429x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x429x2048): 64.467
Elapsed time for attention_prob_times_values (96x2048x2048x429): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x429): 63.854

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 709.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x430x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x430x2048): 68.272
Elapsed time for attention_prob_times_values (96x2048x2048x430): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x430): 67.269

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 750.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x431x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x431x2048): 67.426
Elapsed time for attention_prob_times_values (96x2048x2048x431): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x431): 62.429

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 719.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x432x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x432x2048): 68.103
Elapsed time for attention_prob_times_values (96x2048x2048x432): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x432): 80.957

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 822.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x433x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x433x2048): 64.107
Elapsed time for attention_prob_times_values (96x2048x2048x433): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x433): 64.586

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 717.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x434x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x434x2048): 67.542
Elapsed time for attention_prob_times_values (96x2048x2048x434): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x434): 67.748

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 755.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x435x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x435x2048): 63.949
Elapsed time for attention_prob_times_values (96x2048x2048x435): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 52.125
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 49.490

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 717.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 51.882
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 52.380

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 742.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 54.286
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 50.924

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 755.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 54.764
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 53.210

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 782.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 53.528
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 51.149

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 765.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 54.747
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 53.799

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 800.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 55.743
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 52.309

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 802.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 55.820
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 52.821

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 814.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 55.385
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 50.901

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 802.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4018x2048): 102.682
Elapsed time for attention_prob_times_values (32x2048x2048x4018): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4018): 103.868

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3345.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4019x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4019x2048): 94.535
Elapsed time for attention_prob_times_values (32x2048x2048x4019): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4019): 102.052

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 3179.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4020x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4020x2048): 100.226
Elapsed time for attention_prob_times_values (32x2048x2048x4020): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4020): 100.739

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3256.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4021x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4021x2048): 97.671
Elapsed time for attention_prob_times_values (32x2048x2048x4021): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4021): 100.061

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 3204.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4022x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4022x2048): 102.768
Elapsed time for attention_prob_times_values (32x2048x2048x4022): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4022): 103.952

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3351.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4023x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4023x2048): 100.963
Elapsed time for attention_prob_times_values (32x2048x2048x4023): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4023): 102.080

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3292.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4024x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4024x2048): 103.630
Elapsed time for attention_prob_times_values (32x2048x2048x4024): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4024): 100.091

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3303.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4025x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4025x2048): 100.498
Elapsed time for attention_prob_times_values (32x2048x2048x4025): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4025): 101.949

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3284.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4026x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4026x2048): 102.826
Elapsed time for attention_prob_times_values (32x2048x2048x4026): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4026): 103.606

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3349.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4027x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4027x2048): 102.214
Elapsed time for attention_prob_times_values (32x2048x2048x4027): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4027): 100.463

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3289.295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 67.936
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 55.091

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 817.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 67.278
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 52.517

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 796.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 68.975
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 51.917

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 803.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 68.257
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 56.967

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 846.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 68.038
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 52.615

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 812.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 69.480
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 56.731

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 858.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 69.929
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 23.985

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 493.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 71.373
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 56.833

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 877.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0188
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 23.663
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 54.219

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 459.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x435): 64.735

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 720.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x436x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x436x2048): 65.249
Elapsed time for attention_prob_times_values (96x2048x2048x436): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x436): 65.528

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 733.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x437x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x437x2048): 67.341
Elapsed time for attention_prob_times_values (96x2048x2048x437): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x437): 65.174

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 744.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x438x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x438x2048): 67.995
Elapsed time for attention_prob_times_values (96x2048x2048x438): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x438): 68.136

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 766.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x439x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x439x2048): 67.520
Elapsed time for attention_prob_times_values (96x2048x2048x439): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x439): 65.513

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 750.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x440x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x440x2048): 65.255
Elapsed time for attention_prob_times_values (96x2048x2048x440): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x440): 82.406

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 823.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x441x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x441x2048): 63.594
Elapsed time for attention_prob_times_values (96x2048x2048x441): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x441): 65.686

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 732.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x442x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x442x2048): 64.739
Elapsed time for attention_prob_times_values (96x2048x2048x442): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x442): 68.790

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 757.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x443x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x443x2048): 66.633
Elapsed time for attention_prob_times_values (96x2048x2048x443): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x443): 66.063

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 755.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x444x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x444x2048): 68.486
Elapsed time for attention_prob_times_values (96x2048x2048x444): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x444): 66.180

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 767.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x445x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x445x2048): 66.899
Elapsed time for attention_prob_times_values (96x2048x2048x445): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x445): 66.335

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 761.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x446x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x446x2048): 67.867
Elapsed time for attention_prob_times_values (96x2048x2048x446): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x446): 69.390

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 785.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x447x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x447x2048): 67.235
Elapsed time for attention_prob_times_values (96x2048x2048x447): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x447): 66.527

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 767.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x448x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x448x2048): 80.619
Elapsed time for attention_prob_times_values (96x2048x2048x448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x448): 80.187

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 924.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x449x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x449x2048): 69.805
Elapsed time for attention_prob_times_values (96x2048x2048x449): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x449): 60.768

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 748.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x450x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x450x2048): 70.815
Elapsed time for attention_prob_times_values (96x2048x2048x450): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x450): 61.445

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 759.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x451x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x451x2048): 69.696
Elapsed time for attention_prob_times_values (96x2048x2048x451): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x451): 59.371

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 741.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x452x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x452x2048): 70.171
Elapsed time for attention_prob_times_values (96x2048x2048x452): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x452): 63.861

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 775.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x453x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x453x2048): 69.553
Elapsed time for attention_prob_times_values (96x2048x2048x453): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x453): 60.591

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 752.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x454x2048): 0.0054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4028x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4028x2048): 103.027
Elapsed time for attention_prob_times_values (32x2048x2048x4028): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4028): 104.079

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3362.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4029x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4029x2048): 101.557
Elapsed time for attention_prob_times_values (32x2048x2048x4029): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4029): 101.732

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3301.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4030x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4030x2048): 98.583
Elapsed time for attention_prob_times_values (32x2048x2048x4030): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4030): 104.063

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3289.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4031x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4031x2048): 102.140
Elapsed time for attention_prob_times_values (32x2048x2048x4031): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4031): 100.771

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3296.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4032x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4032x2048): 107.386
Elapsed time for attention_prob_times_values (32x2048x2048x4032): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4032): 102.876

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3415.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4033x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4033x2048): 103.152
Elapsed time for attention_prob_times_values (32x2048x2048x4033): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4033): 98.545

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3276.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4034x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4034x2048): 102.021
Elapsed time for attention_prob_times_values (32x2048x2048x4034): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4034): 100.375

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3290.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4035x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4035x2048): 102.925
Elapsed time for attention_prob_times_values (32x2048x2048x4035): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4035): 101.706

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3327.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4036x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4036x2048): 101.909
Elapsed time for attention_prob_times_values (32x2048x2048x4036): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4036): 100.204

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3287.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 73.641
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 55.357

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 884.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 68.012
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 54.050

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 847.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 70.147
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 57.561

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 893.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 68.977
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 55.148

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 869.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 69.564
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 58.315

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 904.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 70.920
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 53.492

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 872.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 72.041
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 58.458

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 927.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 71.420
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 55.981

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 906.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 73.810
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 56.422

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 927.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 71.230
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 53.980

Attention duration (in seconds): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x454x2048): 68.088
Elapsed time for attention_prob_times_values (96x2048x2048x454): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x454): 62.524

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 758.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x455x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x455x2048): 69.406
Elapsed time for attention_prob_times_values (96x2048x2048x455): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x455): 60.668

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 755.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x456x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x456x2048): 70.701
Elapsed time for attention_prob_times_values (96x2048x2048x456): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x456): 82.106

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 887.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x457x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x457x2048): 68.060
Elapsed time for attention_prob_times_values (96x2048x2048x457): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x457): 59.160

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 741.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x458x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x458x2048): 69.083
Elapsed time for attention_prob_times_values (96x2048x2048x458): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x458): 64.393

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 782.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x459x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x459x2048): 68.470
Elapsed time for attention_prob_times_values (96x2048x2048x459): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x459): 61.031

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 758.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x460x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x460x2048): 70.235
Elapsed time for attention_prob_times_values (96x2048x2048x460): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x460): 64.885

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 794.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x461x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x461x2048): 67.777
Elapsed time for attention_prob_times_values (96x2048x2048x461): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x461): 61.580

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 761.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x462x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x462x2048): 67.907
Elapsed time for attention_prob_times_values (96x2048x2048x462): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x462): 64.937

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 785.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x463x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x463x2048): 66.984
Elapsed time for attention_prob_times_values (96x2048x2048x463): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x463): 62.039

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 763.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 54.483
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 54.348

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 829.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 55.384
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 53.436

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 836.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 56.218
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 55.013

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 861.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 55.737
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 54.428

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 860.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 56.127
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 56.719

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 888.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 57.508
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 53.335

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 878.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 59.225
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 56.898

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 928.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 56.017
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 43.425

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 788.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 57.073
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 53.982

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 901.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 56.905
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 44.448

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 817.292
MLP duration (in seconds): 0.0000
num_attention_heads: 8, hidden_size: 32296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4037x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4037x2048): 98.047
Elapsed time for attention_prob_times_values (32x2048x2048x4037): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4037): 101.590

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3246.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4038x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4038x2048): 103.303
Elapsed time for attention_prob_times_values (32x2048x2048x4038): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4038): 104.158

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3376.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4039x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4039x2048): 102.567
Elapsed time for attention_prob_times_values (32x2048x2048x4039): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4039): 99.451

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3287.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4040x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4040x2048): 101.627
Elapsed time for attention_prob_times_values (32x2048x2048x4040): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4040): 98.061

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3250.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4041x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4041x2048): 102.318
Elapsed time for attention_prob_times_values (32x2048x2048x4041): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4041): 102.481

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3335.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4042x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4042x2048): 100.174
Elapsed time for attention_prob_times_values (32x2048x2048x4042): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4042): 103.069

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3309.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4043x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4043x2048): 102.507
Elapsed time for attention_prob_times_values (32x2048x2048x4043): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4043): 102.504

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3340.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4044x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4044x2048): 101.762
Elapsed time for attention_prob_times_values (32x2048x2048x4044): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4044): 100.635

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3298.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4045x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4045x2048): 102.504
Elapsed time for attention_prob_times_values (32x2048x2048x4045): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4045): 102.659

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3344.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4046x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4046x2048): 77.552
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x464x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x464x2048): 69.668
Elapsed time for attention_prob_times_values (96x2048x2048x464): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x464): 83.824

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 903.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x465x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x465x2048): 67.644
Elapsed time for attention_prob_times_values (96x2048x2048x465): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x465): 62.302

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 771.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x466x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x466x2048): 68.878
Elapsed time for attention_prob_times_values (96x2048x2048x466): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x466): 63.119

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 785.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x467x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x467x2048): 67.568
Elapsed time for attention_prob_times_values (96x2048x2048x467): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x467): 59.661

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 756.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x468x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x468x2048): 67.667
Elapsed time for attention_prob_times_values (96x2048x2048x468): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x468): 65.857

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 798.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x469x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x469x2048): 67.244
Elapsed time for attention_prob_times_values (96x2048x2048x469): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x469): 62.733

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 778.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x470x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x470x2048): 69.116
Elapsed time for attention_prob_times_values (96x2048x2048x470): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x470): 65.858

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 810.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x471x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x471x2048): 68.671
Elapsed time for attention_prob_times_values (96x2048x2048x471): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x471): 63.021

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 791.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x472x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x472x2048): 70.574
Elapsed time for attention_prob_times_values (96x2048x2048x472): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x472): 83.235

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 921.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------Attention throughput (in TFLOP/s): 894.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 71.658
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 57.825

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 936.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 69.506
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 56.819

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 918.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 73.264
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 60.115

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 974.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 70.943
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 55.709

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 924.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 73.458
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 60.035

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 982.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 72.552
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 55.628

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 940.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 83.403
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 59.689

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1043.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 68.949
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 58.605

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 954.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 67.936
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 60.989

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 972.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

Elapsed time for attention_key_query_prob (96x2048x473x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x473x2048): 68.026
Elapsed time for attention_prob_times_values (96x2048x2048x473): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x473): 63.123

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 791.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x474x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x474x2048): 68.385
Elapsed time for attention_prob_times_values (96x2048x2048x474): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x474): 66.216

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 814.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x475x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x475x2048): 68.110
Elapsed time for attention_prob_times_values (96x2048x2048x475): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x475): 63.378

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 796.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x476x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x476x2048): 69.559
Elapsed time for attention_prob_times_values (96x2048x2048x476): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x476): 66.741

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 828.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x477x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x477x2048): 68.275
Elapsed time for attention_prob_times_values (96x2048x2048x477): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x477): 63.574

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 801.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x478x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x478x2048): 68.853
Elapsed time for attention_prob_times_values (96x2048x2048x478): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x478): 66.620

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 826.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x479x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x479x2048): 68.370
Elapsed time for attention_prob_times_values (96x2048x2048x479): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x479): 61.668

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 792.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x480x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x480x2048): 85.354
Elapsed time for attention_prob_times_values (96x2048x2048x480): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x480): 87.510

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1058.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x481x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x481x2048): 70.641
Elapsed time for attention_prob_times_values (96x2048x2048x481): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x481): 64.204

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 825.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x482x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x482x2048): 69.391
Elapsed time for attention_prob_times_values (96x2048x2048x482): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x482): 65.091

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 826.007
Elapsed time for attention_prob_times_values (32x2048x2048x4046): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4046): 102.826

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2883.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4047x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4047x2048): 98.362
Elapsed time for attention_prob_times_values (32x2048x2048x4047): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4047): 102.747

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3278.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4048x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4048x2048): 101.267
Elapsed time for attention_prob_times_values (32x2048x2048x4048): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4048): 100.177

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3285.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4049x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4049x2048): 99.820
Elapsed time for attention_prob_times_values (32x2048x2048x4049): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4049): 100.580

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3269.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4050x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4050x2048): 103.004
Elapsed time for attention_prob_times_values (32x2048x2048x4050): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4050): 104.319

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3383.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4051x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4051x2048): 99.664
Elapsed time for attention_prob_times_values (32x2048x2048x4051): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4051): 102.686

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3302.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4052x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4052x2048): 103.373
Elapsed time for attention_prob_times_values (32x2048x2048x4052): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4052): 104.639

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3396.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4053x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4053x2048): 99.661
Elapsed time for attention_prob_times_values (32x2048x2048x4053): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4053): 102.862

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3306.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4054x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4054x2048): 103.045
Elapsed time for attention_prob_times_values (32x2048x2048x4054): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4054): 102.647

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3360.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4055x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4055x2048): 102.458
Elapsed time for attention_prob_times_values (32x2048x2048x4055): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4055): 100.005

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3307.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x483x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x483x2048): 70.500
Elapsed time for attention_prob_times_values (96x2048x2048x483): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x483): 64.364

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 829.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x484x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x484x2048): 72.062
Elapsed time for attention_prob_times_values (96x2048x2048x484): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x484): 66.058

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 850.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x485x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x485x2048): 70.206
Elapsed time for attention_prob_times_values (96x2048x2048x485): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x485): 62.655

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 818.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x486x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x486x2048): 70.990
Elapsed time for attention_prob_times_values (96x2048x2048x486): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x486): 65.550

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 844.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x487x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x487x2048): 69.380
Elapsed time for attention_prob_times_values (96x2048x2048x487): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x487): 64.700

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 831.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x488x2048): 71.635
Elapsed time for attention_prob_times_values (96x2048x2048x488): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x488): 87.789

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 981.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x489x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x489x2048): 68.743
Elapsed time for attention_prob_times_values (96x2048x2048x489): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x489): 64.744

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 830.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x490x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x490x2048): 69.902
Elapsed time for attention_prob_times_values (96x2048x2048x490): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x490): 67.811

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 859.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x491x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x491x2048): 67.755
Elapsed time for attention_prob_times_values (96x2048x2048x491): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x491): 65.076

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 830.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 71.197
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 58.821

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 978.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 70.164
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 59.721

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 983.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 71.732
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 59.246

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 993.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 70.173
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 59.028

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 985.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 71.938
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 59.594

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1006.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 72.390
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 59.984

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1016.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 69.594
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 59.542

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 998.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 73.034
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 62.511

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1052.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 70.117
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 59.453

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1009.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 71.931
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 59.157
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 58.331

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 969.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 56.471
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 44.007

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 822.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 58.342
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 59.639

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 987.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 56.310
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 43.926

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 832.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 67.843
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 62.799

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1108.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 56.520
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 43.094

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 837.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 57.833
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 47.206

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 896.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 58.430
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 44.863

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 881.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 57.771
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 46.957

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 906.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4056x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4056x2048): 103.790
Elapsed time for attention_prob_times_values (32x2048x2048x4056): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4056): 101.267

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3350.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4057x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4057x2048): 102.302
Elapsed time for attention_prob_times_values (32x2048x2048x4057): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4057): 100.652

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3317.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4058x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4058x2048): 102.896
Elapsed time for attention_prob_times_values (32x2048x2048x4058): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4058): 103.448

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3374.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4059x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4059x2048): 99.305
Elapsed time for attention_prob_times_values (32x2048x2048x4059): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4059): 100.154

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 3262.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4060x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4060x2048): 103.303
Elapsed time for attention_prob_times_values (32x2048x2048x4060): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4060): 104.359

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3397.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4061x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4061x2048): 102.311
Elapsed time for attention_prob_times_values (32x2048x2048x4061): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4061): 102.725

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3355.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4062x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4062x2048): 101.359
Elapsed time for attention_prob_times_values (32x2048x2048x4062): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4062): 104.808

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3373.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4063x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4063x2048): 102.371
Elapsed time for attention_prob_times_values (32x2048x2048x4063): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4063): 103.146

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3364.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4064x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4064x2048): 105.715
Elapsed time for attention_prob_times_values (32x2048x2048x4064): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4064): 103.353

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 3423.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
num_attention_heads: 24, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x492x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x492x2048): 67.772
Elapsed time for attention_prob_times_values (96x2048x2048x492): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x492): 66.119

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 838.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x493x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x493x2048): 69.476
Elapsed time for attention_prob_times_values (96x2048x2048x493): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x493): 65.065

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 843.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x494x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x494x2048): 70.333
Elapsed time for attention_prob_times_values (96x2048x2048x494): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x494): 65.540

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 853.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x495x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x495x2048): 67.768
Elapsed time for attention_prob_times_values (96x2048x2048x495): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x495): 65.444

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 839.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x496x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x496x2048): 71.792
Elapsed time for attention_prob_times_values (96x2048x2048x496): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x496): 89.442

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1005.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x497x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x497x2048): 66.918
Elapsed time for attention_prob_times_values (96x2048x2048x497): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x497): 63.961

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 827.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x498x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x498x2048): 69.574
Elapsed time for attention_prob_times_values (96x2048x2048x498): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x498): 68.495

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 874.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x499x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x499x2048): 68.834
Elapsed time for attention_prob_times_values (96x2048x2048x499): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x499): 65.839

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 854.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x500x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x500x2048): 70.243
Elapsed time for attention_prob_times_values (96x2048x2048x500): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x500): 69.079

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 885.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x501x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x501x2048): 69.249
Elapsed time for attention_prob_times_values (96x2048x2048x501): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x501): 66.095

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 861.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x502x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x502x2048): 70.121
Elapsed time for attention_prob_times_values (96x2048x2048x502): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x502): 66.152

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 869.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x503x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x503x2048): 69.403
Elapsed time for attention_prob_times_values (96x2048x2048x503): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x503): 63.190

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 846.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x504x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x504x2048): 71.130
Elapsed time for attention_prob_times_values (96x2048x2048x504): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x504): 90.556

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1020.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x505x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x505x2048): 68.852
Elapsed time for attention_prob_times_values (96x2048x2048x505): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x505): 57.967

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 807.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x506x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x506x2048): 69.468
Elapsed time for attention_prob_times_values (96x2048x2048x506): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x506): 67.766

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 882.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x507x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x507x2048): 68.789
Elapsed time for attention_prob_times_values (96x2048x2048x507): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x507): 60.469

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 829.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x508x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x508x2048): 69.996
Elapsed time for attention_prob_times_values (96x2048x2048x508): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x508): 67.047

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 883.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x509x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x509x2048): 68.800
Elapsed time for attention_prob_times_values (96x2048x2048x509): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x509): 60.516

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 832.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x510x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x510x2048): 69.010
Elapsed time for attention_prob_times_values (96x2048x2048x510): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x510): 67.528

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 884.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4065x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4065x2048): 103.127
Elapsed time for attention_prob_times_values (32x2048x2048x4065): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4065): 101.553

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3352.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4066x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4066x2048): 102.478
Elapsed time for attention_prob_times_values (32x2048x2048x4066): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4066): 104.923

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3397.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4067x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4067x2048): 102.671
Elapsed time for attention_prob_times_values (32x2048x2048x4067): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4067): 103.152

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3372.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4068x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4068x2048): 101.174
Elapsed time for attention_prob_times_values (32x2048x2048x4068): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4068): 102.747

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3342.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4069x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4069x2048): 99.216
Elapsed time for attention_prob_times_values (32x2048x2048x4069): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4069): 102.927

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3312.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4070x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4070x2048): 101.385
Elapsed time for attention_prob_times_values (32x2048x2048x4070): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4070): 100.549

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3311.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4071x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4071x2048): 102.692
Elapsed time for attention_prob_times_values (32x2048x2048x4071): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4071): 102.172

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3360.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4072x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4072x2048): 101.604
Elapsed time for attention_prob_times_values (32x2048x2048x4072): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4072): 100.017

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3307.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4073x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4073x2048): 102.348
Elapsed time for attention_prob_times_values (32x2048x2048x4073): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4073): 100.330

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3325.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4074x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4074x2048): 100.357
Elapsed time for attention_prob_times_values (32x2048x2048x4074): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4074): 98.141

Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 63.143

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1059.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 72.333
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 58.929

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1026.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 73.690
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 63.088

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1079.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 70.764
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 59.317

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1028.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 56.886
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 62.445

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 952.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 54.432
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 61.381

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 926.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 53.778
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 61.800

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 927.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 53.876
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 60.053

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 919.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 55.839
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 61.895

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 954.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 52.460
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 61.082

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 920.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
========================================================================================================================
num_attention_heads: 24, hidden_size: 12264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x511x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x511x2048): 67.913
Elapsed time for attention_prob_times_values (96x2048x2048x511): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x511): 60.743

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 832.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x512x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x512x2048): 81.888
Elapsed time for attention_prob_times_values (96x2048x2048x512): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x512): 94.761

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1142.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x513x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x513x2048): 68.502
Elapsed time for attention_prob_times_values (96x2048x2048x513): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x513): 59.414

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 828.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x514x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x514x2048): 73.512
Elapsed time for attention_prob_times_values (96x2048x2048x514): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x514): 61.119

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 870.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x515x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x515x2048): 70.847
Elapsed time for attention_prob_times_values (96x2048x2048x515): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x515): 61.169

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 858.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x516x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x516x2048): 71.931
Elapsed time for attention_prob_times_values (96x2048x2048x516): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x516): 64.650

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 891.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x517x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x517x2048): 71.015
Elapsed time for attention_prob_times_values (96x2048x2048x517): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x517): 61.942

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 867.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x518x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x518x2048): 71.860
Elapsed time for attention_prob_times_values (96x2048x2048x518): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x518): 64.847

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 895.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x519x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x519x2048): 70.374
Elapsed time for attention_prob_times_values (96x2048x2048x519): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x519): 61.958

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 867.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x520x2048): 0.0058
Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 3257.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4075x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4075x2048): 100.836
Elapsed time for attention_prob_times_values (32x2048x2048x4075): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4075): 98.216

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 3267.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4076x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4076x2048): 102.471
Elapsed time for attention_prob_times_values (32x2048x2048x4076): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4076): 104.484

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3398.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4077x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4077x2048): 102.476
Elapsed time for attention_prob_times_values (32x2048x2048x4077): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4077): 101.575

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 3351.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4078x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4078x2048): 103.166
Elapsed time for attention_prob_times_values (32x2048x2048x4078): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4078): 101.387

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3360.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4079x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4079x2048): 101.783
Elapsed time for attention_prob_times_values (32x2048x2048x4079): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4079): 99.343

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 3304.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4080x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4080x2048): 98.776
Elapsed time for attention_prob_times_values (32x2048x2048x4080): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4080): 97.572

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 3227.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4081x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4081x2048): 102.325
Elapsed time for attention_prob_times_values (32x2048x2048x4081): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4081): 97.343

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 3280.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4082x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4082x2048): 102.967
Elapsed time for attention_prob_times_values (32x2048x2048x4082): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4082): 103.175

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 3390.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4083x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4083x2048): 102.143
Elapsed time for attention_prob_times_values (32x2048x2048x4083): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4083): 100.832

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3338.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 57.332
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 46.321

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 903.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 60.150
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 48.513

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 953.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 57.932
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 46.768

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 925.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 61.362
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 44.869

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 933.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 59.373
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 47.155

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 952.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 60.448
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 49.632

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 994.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 59.747
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 47.773

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 975.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 61.268
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 50.680

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1026.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 59.051
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 47.991

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 986.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 61.675
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x520x2048): 72.084
Elapsed time for attention_prob_times_values (96x2048x2048x520): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x520): 75.118

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 970.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x521x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x521x2048): 69.730
Elapsed time for attention_prob_times_values (96x2048x2048x521): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x521): 60.628

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 856.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x522x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x522x2048): 70.781
Elapsed time for attention_prob_times_values (96x2048x2048x522): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x522): 63.214

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 883.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x523x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x523x2048): 67.769
Elapsed time for attention_prob_times_values (96x2048x2048x523): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x523): 61.438

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 854.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x524x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x524x2048): 66.997
Elapsed time for attention_prob_times_values (96x2048x2048x524): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x524): 62.196

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 856.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x525x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x525x2048): 70.489
Elapsed time for attention_prob_times_values (96x2048x2048x525): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x525): 62.256

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 879.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x526x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x526x2048): 68.965
Elapsed time for attention_prob_times_values (96x2048x2048x526): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x526): 65.737

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 897.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x527x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x527x2048): 70.572
Elapsed time for attention_prob_times_values (96x2048x2048x527): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x527): 62.285

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 883.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x528x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x528x2048): 72.107
Elapsed time for attention_prob_times_values (96x2048x2048x528): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x528): 73.558

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 974.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x529x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x529x2048): 69.854
Elapsed time for attention_prob_times_values (96x2048x2048x529): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x529): 62.332

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 882.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 55.331
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 64.762

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 977.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 55.178
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 61.565

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 956.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 56.795
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 62.800

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 984.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 54.670
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 61.664

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 959.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 55.179
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 62.688

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 975.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 53.753
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 63.769

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 973.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 53.975
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 65.832

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 993.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 52.541
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 66.103

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 984.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 55.573
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 66.514

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1021.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x530x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x530x2048): 70.516
Elapsed time for attention_prob_times_values (96x2048x2048x530): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x530): 66.041

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 915.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x531x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x531x2048): 69.979
Elapsed time for attention_prob_times_values (96x2048x2048x531): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x531): 59.751

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 866.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x532x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x532x2048): 71.160
Elapsed time for attention_prob_times_values (96x2048x2048x532): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x532): 66.617

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 926.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x533x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x533x2048): 70.295
Elapsed time for attention_prob_times_values (96x2048x2048x533): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x533): 62.292

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 891.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x534x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x534x2048): 71.081
Elapsed time for attention_prob_times_values (96x2048x2048x534): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x534): 66.300

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 927.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x535x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x535x2048): 67.200
Elapsed time for attention_prob_times_values (96x2048x2048x535): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x535): 59.345

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 853.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x536x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x536x2048): 71.989
Elapsed time for attention_prob_times_values (96x2048x2048x536): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x536): 72.975

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 982.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x537x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x537x2048): 67.766
Elapsed time for attention_prob_times_values (96x2048x2048x537): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x537): 58.866

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 855.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x538x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x538x2048): 67.073
Elapsed time for attention_prob_times_values (96x2048x2048x538): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x538): 63.986

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 891.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------========================================================================================================================
num_attention_heads: 8, hidden_size: 32672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4084x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4084x2048): 103.262
Elapsed time for attention_prob_times_values (32x2048x2048x4084): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4084): 105.106

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 3428.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4085x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4085x2048): 101.908
Elapsed time for attention_prob_times_values (32x2048x2048x4085): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4085): 101.012

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3339.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4086x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4086x2048): 102.186
Elapsed time for attention_prob_times_values (32x2048x2048x4086): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4086): 102.679

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3372.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4087x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4087x2048): 102.407
Elapsed time for attention_prob_times_values (32x2048x2048x4087): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4087): 100.925

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3347.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4088x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4088x2048): 103.680
Elapsed time for attention_prob_times_values (32x2048x2048x4088): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4088): 101.345

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 3376.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4089x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4089x2048): 99.850
Elapsed time for attention_prob_times_values (32x2048x2048x4089): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4089): 101.129

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 3310.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4090x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4090x2048): 102.828
Elapsed time for attention_prob_times_values (32x2048x2048x4090): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4090): 105.006

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 3424.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4091x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4091x2048): 99.923
Elapsed time for attention_prob_times_values (32x2048x2048x4091): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4091): 99.696

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 3289.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4092x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4092x2048): 99.007
Elapsed time for attention_prob_times_values (32x2048x2048x4092): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4092): 103.948

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 3343.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4093x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4093x2048): 101.760
Elapsed time for attention_prob_times_values (32x2048x2048x4093): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4093): 101.282

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 3347.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4094x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4094x2048): 102.988
Elapsed time for attention_prob_times_values (32x2048x2048x4094): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4094): 104.001

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 3413.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x4095x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x4095x2048): 99.619
Elapsed time for attention_prob_times_values (32x2048x2048x4095): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x4095): 99.056

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 3277.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

Elapsed time for attention_key_query_prob (96x2048x539x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x539x2048): 70.038
Elapsed time for attention_prob_times_values (96x2048x2048x539): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x539): 60.619

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 885.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x540x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x540x2048): 66.707
Elapsed time for attention_prob_times_values (96x2048x2048x540): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x540): 67.208

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 914.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x541x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x541x2048): 70.207
Elapsed time for attention_prob_times_values (96x2048x2048x541): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x541): 62.531

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 904.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x542x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x542x2048): 67.950
Elapsed time for attention_prob_times_values (96x2048x2048x542): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x542): 64.811

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 909.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x543x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x543x2048): 70.428
Elapsed time for attention_prob_times_values (96x2048x2048x543): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x543): 62.552

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 909.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x544x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x544x2048): 86.759
Elapsed time for attention_prob_times_values (96x2048x2048x544): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x544): 79.297

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1139.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x545x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x545x2048): 70.018
Elapsed time for attention_prob_times_values (96x2048x2048x545): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x545): 62.610

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 910.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x546x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x546x2048): 73.807
Elapsed time for attention_prob_times_values (96x2048x2048x546): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x546): 65.832

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 960.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x547x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x547x2048): 72.629
Elapsed time for attention_prob_times_values (96x2048x2048x547): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x547): 63.858

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 939.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x548x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x548x2048): 74.414
Elapsed time for attention_prob_times_values (96x2048x2048x548): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x548): 66.274

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 970.570
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 54.707
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 65.020

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1006.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 77.263
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 68.749

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1236.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x257x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x257x2048): 57.184
Elapsed time for attention_prob_times_values (256x2048x2048x257): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x257): 54.869

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 955.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 56.929
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 57.668

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 981.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x259x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x259x2048): 58.024
Elapsed time for attention_prob_times_values (256x2048x2048x259): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x259): 55.775

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 977.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 59.219
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 58.490

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1015.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x261x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x261x2048): 55.154
Elapsed time for attention_prob_times_values (256x2048x2048x261): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x261): 55.223

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 955.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 58.688
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 58.909

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1021.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x263x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x263x2048): 57.822
Elapsed time for attention_prob_times_values (256x2048x2048x263): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x263): 56.938

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1000.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 56.426
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 55.522

Attention duration (in seconds): 0.0203
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 50.353

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1039.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 58.300
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 48.347

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 997.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 62.280
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 48.754

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1039.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 60.825
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 49.433

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1043.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 61.787
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 52.177

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1089.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 60.557
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 49.366

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1053.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 61.984
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 50.941

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1090.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 60.110
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 50.883

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1081.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 63.207
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 53.306

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1142.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 62.739
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 50.641

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1113.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x549x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x549x2048): 72.572
Elapsed time for attention_prob_times_values (96x2048x2048x549): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x549): 64.118

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 944.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x550x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x550x2048): 73.478
Elapsed time for attention_prob_times_values (96x2048x2048x550): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x550): 66.367

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 968.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x551x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x551x2048): 72.530
Elapsed time for attention_prob_times_values (96x2048x2048x551): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x551): 64.033

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 946.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x552x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x552x2048): 74.371
Elapsed time for attention_prob_times_values (96x2048x2048x552): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x552): 79.479

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1070.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x553x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x553x2048): 71.775
Elapsed time for attention_prob_times_values (96x2048x2048x553): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x553): 64.407

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 947.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x554x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x554x2048): 72.411
Elapsed time for attention_prob_times_values (96x2048x2048x554): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x554): 66.595

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 970.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x555x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x555x2048): 69.805
Elapsed time for attention_prob_times_values (96x2048x2048x555): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x555): 64.009

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 935.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x556x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x556x2048): 73.428
Elapsed time for attention_prob_times_values (96x2048x2048x556): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x556): 68.862

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 997.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x557x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x557x2048): 72.122
Elapsed time for attention_prob_times_values (96x2048x2048x557): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x557): 65.357

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 963.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 979.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x265x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x265x2048): 55.609
Elapsed time for attention_prob_times_values (256x2048x2048x265): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x265): 56.795

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 986.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 57.438
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 59.795

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1032.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x267x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x267x2048): 57.225
Elapsed time for attention_prob_times_values (256x2048x2048x267): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x267): 57.449

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1014.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 58.327
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 60.565

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1054.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x269x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x269x2048): 57.589
Elapsed time for attention_prob_times_values (256x2048x2048x269): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x269): 54.433

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 996.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 56.908
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 60.642

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1049.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x271x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x271x2048): 57.827
Elapsed time for attention_prob_times_values (256x2048x2048x271): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x271): 58.038

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1039.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 59.659
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 76.789

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1208.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x273x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x273x2048): 57.136
Elapsed time for attention_prob_times_values (256x2048x2048x273): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x273): 58.369

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1043.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 24, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x558x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x558x2048): 73.135
Elapsed time for attention_prob_times_values (96x2048x2048x558): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x558): 68.644

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 996.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x559x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x559x2048): 72.176
Elapsed time for attention_prob_times_values (96x2048x2048x559): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x559): 65.747

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 970.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x560x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x560x2048): 72.706
Elapsed time for attention_prob_times_values (96x2048x2048x560): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x560): 79.716

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1074.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x561x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x561x2048): 71.357
Elapsed time for attention_prob_times_values (96x2048x2048x561): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x561): 66.132

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 971.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x562x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x562x2048): 72.265
Elapsed time for attention_prob_times_values (96x2048x2048x562): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x562): 68.082

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 993.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x563x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x563x2048): 71.676
Elapsed time for attention_prob_times_values (96x2048x2048x563): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x563): 66.248

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 977.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x564x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x564x2048): 73.262
Elapsed time for attention_prob_times_values (96x2048x2048x564): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x564): 69.958

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1017.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x565x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x565x2048): 71.907
Elapsed time for attention_prob_times_values (96x2048x2048x565): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x565): 66.016

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 980.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x566x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x566x2048): 72.578
Elapsed time for attention_prob_times_values (96x2048x2048x566): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x566): 69.747

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1014.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x567x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x567x2048): 72.036
Elapsed time for attention_prob_times_values (96x2048x2048x567): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x567): 64.273

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 970.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x568x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x568x2048): 73.992
Elapsed time for attention_prob_times_values (96x2048x2048x568): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x568): 81.933

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1112.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x569x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x569x2048): 71.187
Elapsed time for attention_prob_times_values (96x2048x2048x569): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x569): 66.006

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 981.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x570x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x570x2048): 70.189
Elapsed time for attention_prob_times_values (96x2048x2048x570): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x570): 68.746

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 997.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x571x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x571x2048): 71.477
Elapsed time for attention_prob_times_values (96x2048x2048x571): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x571): 65.901

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 986.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x572x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x572x2048): 73.106
Elapsed time for attention_prob_times_values (96x2048x2048x572): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x572): 68.768

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1020.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x573x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x573x2048): 69.135
Elapsed time for attention_prob_times_values (96x2048x2048x573): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x573): 66.411

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 977.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x574x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x574x2048): 72.368
Elapsed time for attention_prob_times_values (96x2048x2048x574): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x574): 69.596

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1025.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x575x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x575x2048): 71.293
Elapsed time for attention_prob_times_values (96x2048x2048x575): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x575): 65.779

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 990.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x576x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x576x2048): 87.225
Elapsed time for attention_prob_times_values (96x2048x2048x576): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x576): 84.619

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1245.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 64.600
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 50.395

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1132.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 62.327
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 51.411

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1133.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 62.460
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 54.582

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1179.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 62.615
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 51.762

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1154.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 63.422
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 54.486

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1201.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 61.296
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 52.769

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1169.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 64.768
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 54.731

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1231.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 64.130
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 53.371

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1216.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 77.871
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 55.751

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1364.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 57.890
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 61.036

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1077.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x275x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x275x2048): 55.923
Elapsed time for attention_prob_times_values (256x2048x2048x275): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x275): 58.843

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1042.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 58.607
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 60.318

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1084.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x277x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x277x2048): 57.838
Elapsed time for attention_prob_times_values (256x2048x2048x277): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x277): 56.958

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1051.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 58.347
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 60.276

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1089.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x279x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x279x2048): 56.486
Elapsed time for attention_prob_times_values (256x2048x2048x279): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x279): 59.989

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1072.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 56.814
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 78.302

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1218.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x281x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x281x2048): 57.394
Elapsed time for attention_prob_times_values (256x2048x2048x281): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x281): 60.205

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1090.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 57.910
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 61.947

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1114.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x283x2048): 0.0106
========================================================================================================================
num_attention_heads: 24, hidden_size: 13848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x577x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x577x2048): 70.935
Elapsed time for attention_prob_times_values (96x2048x2048x577): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x577): 58.754

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 933.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x578x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x578x2048): 75.040
Elapsed time for attention_prob_times_values (96x2048x2048x578): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x578): 64.732

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1011.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x579x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x579x2048): 73.883
Elapsed time for attention_prob_times_values (96x2048x2048x579): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x579): 62.198

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 984.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x580x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x580x2048): 75.520
Elapsed time for attention_prob_times_values (96x2048x2048x580): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x580): 60.418

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 979.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x581x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x581x2048): 73.779
Elapsed time for attention_prob_times_values (96x2048x2048x581): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x581): 62.383

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 988.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x582x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x582x2048): 71.792
Elapsed time for attention_prob_times_values (96x2048x2048x582): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x582): 62.969

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 982.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x583x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x583x2048): 75.278
Elapsed time for attention_prob_times_values (96x2048x2048x583): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x583): 62.566

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1002.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x584x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x584x2048): 75.845
Elapsed time for attention_prob_times_values (96x2048x2048x584): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x584): 84.316

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1172.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x585x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x585x2048): 72.462
Elapsed time for attention_prob_times_values (96x2048x2048x585): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x585): 62.445

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 986.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x586x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x586x2048): 73.366
Elapsed time for attention_prob_times_values (96x2048x2048x586): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x586): 65.445

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1019.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x587x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x587x2048): 72.974
Elapsed time for attention_prob_times_values (96x2048x2048x587): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x587): 62.708

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 995.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x588x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x588x2048): 76.037
Elapsed time for attention_prob_times_values (96x2048x2048x588): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x588): 65.630

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1041.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x589x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x589x2048): 72.962
Elapsed time for attention_prob_times_values (96x2048x2048x589): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x589): 62.986

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1000.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x590x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x590x2048): 71.400
Elapsed time for attention_prob_times_values (96x2048x2048x590): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x590): 65.838

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1015.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x591x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x591x2048): 72.810
Elapsed time for attention_prob_times_values (96x2048x2048x591): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x591): 63.076

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1003.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x592x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x592x2048): 74.947
Elapsed time for attention_prob_times_values (96x2048x2048x592): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x592): 85.720

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1189.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x593x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x593x2048): 72.344
Elapsed time for attention_prob_times_values (96x2048x2048x593): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x593): 63.241

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1005.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x594x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x594x2048): 72.974
Elapsed time for attention_prob_times_values (96x2048x2048x594): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x594): 65.625

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1031.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x595x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x595x2048): 72.480
Elapsed time for attention_prob_times_values (96x2048x2048x595): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x595): 63.438

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1011.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x283x2048): 57.547
Elapsed time for attention_prob_times_values (256x2048x2048x283): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x283): 60.642

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1103.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 56.255
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 63.707

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1120.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x285x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x285x2048): 58.007
Elapsed time for attention_prob_times_values (256x2048x2048x285): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x285): 60.929

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1118.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 58.664
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 63.555

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1151.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x287x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x287x2048): 56.648
Elapsed time for attention_prob_times_values (256x2048x2048x287): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x287): 61.295

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1115.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 79.656
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 79.877

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1515.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x289x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x289x2048): 61.916
Elapsed time for attention_prob_times_values (256x2048x2048x289): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x289): 61.563

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1176.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 58.957
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 61.458

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1150.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x291x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x291x2048): 61.845
Elapsed time for attention_prob_times_values (256x2048x2048x291): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x291): 62.027

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1188.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 63.637
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 64.955

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1237.567
MLP duration (in seconds): 0.0000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x596x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x596x2048): 73.649
Elapsed time for attention_prob_times_values (96x2048x2048x596): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x596): 66.569

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1046.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x597x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x597x2048): 72.614
Elapsed time for attention_prob_times_values (96x2048x2048x597): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x597): 63.748

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1017.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x598x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x598x2048): 73.282
Elapsed time for attention_prob_times_values (96x2048x2048x598): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x598): 66.561

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1047.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x599x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x599x2048): 72.865
Elapsed time for attention_prob_times_values (96x2048x2048x599): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x599): 63.948

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1024.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x600x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x600x2048): 72.485
Elapsed time for attention_prob_times_values (96x2048x2048x600): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x600): 83.988

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1172.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x601x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x601x2048): 71.970
Elapsed time for attention_prob_times_values (96x2048x2048x601): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x601): 63.921

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1021.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x602x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x602x2048): 72.852
Elapsed time for attention_prob_times_values (96x2048x2048x602): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x602): 66.896

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1053.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x603x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x603x2048): 71.377
Elapsed time for attention_prob_times_values (96x2048x2048x603): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x603): 62.552

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1008.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x604x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x604x2048): 73.752
Elapsed time for attention_prob_times_values (96x2048x2048x604): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x604): 65.443

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1051.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate

Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 60.920
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 54.769

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1218.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 64.387
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 55.540

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1267.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 63.592
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 52.811

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1233.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 65.442
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 57.025

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1310.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 64.069
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 55.936

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1291.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 65.285
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 58.005

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1336.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 63.557
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 56.634

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1310.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 64.806
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 56.326

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1325.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 62.899
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 56.810

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1320.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 64.692
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 59.047


--------
Elapsed time for attention_key_query_prob (96x2048x605x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x605x2048): 72.352
Elapsed time for attention_prob_times_values (96x2048x2048x605): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x605): 61.538

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1009.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x606x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x606x2048): 73.145
Elapsed time for attention_prob_times_values (96x2048x2048x606): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x606): 66.587

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1059.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x607x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x607x2048): 70.601
Elapsed time for attention_prob_times_values (96x2048x2048x607): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x607): 64.563

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1026.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x608x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x608x2048): 88.431
Elapsed time for attention_prob_times_values (96x2048x2048x608): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x608): 84.522

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1318.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x609x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x609x2048): 72.458
Elapsed time for attention_prob_times_values (96x2048x2048x609): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x609): 64.790

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1044.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x610x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x610x2048): 75.271
Elapsed time for attention_prob_times_values (96x2048x2048x610): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x610): 66.785

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1082.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x611x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x611x2048): 74.098
Elapsed time for attention_prob_times_values (96x2048x2048x611): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x611): 61.879

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1033.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x612x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x612x2048): 75.648
Elapsed time for attention_prob_times_values (96x2048x2048x612): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x612): 66.827

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1088.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x613x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x613x2048): 73.887
Elapsed time for attention_prob_times_values (96x2048x2048x613): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x613): 65.273

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1065.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x614x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x614x2048): 74.743
Elapsed time for attention_prob_times_values (96x2048x2048x614): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x614): 66.354

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1081.954MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x293x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x293x2048): 61.878
Elapsed time for attention_prob_times_values (256x2048x2048x293): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x293): 62.629

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1202.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 61.641
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 65.026

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1226.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x295x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x295x2048): 60.413
Elapsed time for attention_prob_times_values (256x2048x2048x295): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x295): 62.899

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1197.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 61.358
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 79.846

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1353.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x297x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x297x2048): 60.439
Elapsed time for attention_prob_times_values (256x2048x2048x297): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x297): 64.523

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1220.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 60.996
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 64.461

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1230.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x299x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x299x2048): 60.672
Elapsed time for attention_prob_times_values (256x2048x2048x299): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x299): 62.398

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1211.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 61.351
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 66.760

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1262.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x301x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x301x2048): 59.948
Elapsed time for attention_prob_times_values (256x2048x2048x301): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x301): 65.717

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1242.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x615x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x615x2048): 73.733
Elapsed time for attention_prob_times_values (96x2048x2048x615): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x615): 65.379

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1068.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x616x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x616x2048): 74.077
Elapsed time for attention_prob_times_values (96x2048x2048x616): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x616): 88.728

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1246.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x617x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x617x2048): 72.810
Elapsed time for attention_prob_times_values (96x2048x2048x617): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x617): 63.777

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1051.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x618x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x618x2048): 73.620
Elapsed time for attention_prob_times_values (96x2048x2048x618): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x618): 68.127

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1095.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x619x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x619x2048): 73.051
Elapsed time for attention_prob_times_values (96x2048x2048x619): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x619): 65.662

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1072.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x620x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x620x2048): 74.254
Elapsed time for attention_prob_times_values (96x2048x2048x620): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x620): 68.540

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1107.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x621x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x621x2048): 70.582
Elapsed time for attention_prob_times_values (96x2048x2048x621): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x621): 65.911

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1060.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x622x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x622x2048): 73.997
Elapsed time for attention_prob_times_values (96x2048x2048x622): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x622): 68.436

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1107.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x623x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x623x2048): 73.234
Elapsed time for attention_prob_times_values (96x2048x2048x623): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x623): 65.957

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1082.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x624x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x624x2048): 73.176
Elapsed time for attention_prob_times_values (96x2048x2048x624): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x624): 90.246

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1262.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x625x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x625x2048): 71.827
Elapsed time for attention_prob_times_values (96x2048x2048x625): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x625): 65.563

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1072.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x626x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x626x2048): 73.446
Elapsed time for attention_prob_times_values (96x2048x2048x626): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x626): 68.841

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1113.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x627x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x627x2048): 72.692
Elapsed time for attention_prob_times_values (96x2048x2048x627): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x627): 66.502

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1090.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x628x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x628x2048): 74.005
Elapsed time for attention_prob_times_values (96x2048x2048x628): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x628): 68.632

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1119.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x629x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x629x2048): 71.958
Elapsed time for attention_prob_times_values (96x2048x2048x629): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x629): 66.771

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1090.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x630x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x630x2048): 73.849
Elapsed time for attention_prob_times_values (96x2048x2048x630): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x630): 69.256

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1126.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x631x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x631x2048): 73.121
Elapsed time for attention_prob_times_values (96x2048x2048x631): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x631): 66.737

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1101.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x632x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x632x2048): 74.815
Elapsed time for attention_prob_times_values (96x2048x2048x632): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x632): 91.013

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1298.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x633x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x633x2048): 72.522
Elapsed time for attention_prob_times_values (96x2048x2048x633): 0.0076
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 60.344
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 67.165

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1263.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x303x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x303x2048): 56.193
Elapsed time for attention_prob_times_values (256x2048x2048x303): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x303): 66.101

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1211.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 62.683
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 85.400

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1445.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x305x2048): 0.0293
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x305x2048): 22.391
Elapsed time for attention_prob_times_values (256x2048x2048x305): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x305): 66.571

Attention duration (in seconds): 0.0391
Attention throughput (in TFLOP/s): 672.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0391
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 59.773
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 67.820

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1278.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x307x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x307x2048): 60.865
Elapsed time for attention_prob_times_values (256x2048x2048x307): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x307): 66.902

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1286.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 61.451
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 69.648

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1322.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x309x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x309x2048): 61.279
Elapsed time for attention_prob_times_values (256x2048x2048x309): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x309): 67.408

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1304.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 61.804
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 69.163

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1330.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x311x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x311x2048): 58.990
Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1373.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 64.751
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 57.228

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1359.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 66.367
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 59.168

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1407.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 65.308
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 56.801

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1374.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 65.248
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 59.895

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1420.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 62.857
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 58.682

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1388.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 68.868
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 56.164

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1423.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 65.543
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 54.820

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1380.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 66.879
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 59.309

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1461.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 65.168
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 57.522

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1428.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x633): 67.155

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1104.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x634x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x634x2048): 72.995
Elapsed time for attention_prob_times_values (96x2048x2048x634): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x634): 69.042

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1125.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x635x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x635x2048): 72.463
Elapsed time for attention_prob_times_values (96x2048x2048x635): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x635): 67.119

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1106.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x636x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x636x2048): 73.708
Elapsed time for attention_prob_times_values (96x2048x2048x636): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x636): 68.434

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1128.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x637x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x637x2048): 72.284
Elapsed time for attention_prob_times_values (96x2048x2048x637): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x637): 65.601

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1095.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x638x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x638x2048): 73.214
Elapsed time for attention_prob_times_values (96x2048x2048x638): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x638): 69.094

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1134.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x639x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x639x2048): 72.319
Elapsed time for attention_prob_times_values (96x2048x2048x639): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x639): 65.148

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1095.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x640x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x640x2048): 82.834
Elapsed time for attention_prob_times_values (96x2048x2048x640): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x640): 93.626

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1406.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x641x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x641x2048): 74.226
Elapsed time for attention_prob_times_values (96x2048x2048x641): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x641): 61.063

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1073.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x642x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x642x2048): 73.673
Elapsed time for attention_prob_times_values (96x2048x2048x642): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x642): 64.600

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1104.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x643x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x643x2048): 74.072
Elapsed time for attention_prob_times_values (96x2048x2048x643): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x643): 60.234

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1067.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x644x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x644x2048): 75.808
Elapsed time for attention_prob_times_values (96x2048x2048x644): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x644): 64.497

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1121.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x645x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x645x2048): 74.534
Elapsed time for attention_prob_times_values (96x2048x2048x645): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x645): 60.706

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1078.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x646x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x646x2048): 75.210
Elapsed time for attention_prob_times_values (96x2048x2048x646): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x646): 64.985

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1125.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x647x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x647x2048): 74.211
Elapsed time for attention_prob_times_values (96x2048x2048x647): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x647): 62.345

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1095.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x648x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x648x2048): 75.725
Elapsed time for attention_prob_times_values (96x2048x2048x648): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x648): 77.634

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1241.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x649x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x649x2048): 73.468
Elapsed time for attention_prob_times_values (96x2048x2048x649): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x649): 62.505

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1094.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x650x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x650x2048): 74.165
Elapsed time for attention_prob_times_values (96x2048x2048x650): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x650): 65.288

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1127.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x651x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x651x2048): 73.625
Elapsed time for attention_prob_times_values (96x2048x2048x651): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x651): 62.730

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1101.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x652x2048): 0.0070
Elapsed time for attention_prob_times_values (256x2048x2048x311): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x311): 67.487

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1286.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 63.043
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 83.870

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1475.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x313x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x313x2048): 59.049
Elapsed time for attention_prob_times_values (256x2048x2048x313): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x313): 67.632

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1296.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 60.091
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 69.124

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1326.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x315x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x315x2048): 61.118
Elapsed time for attention_prob_times_values (256x2048x2048x315): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x315): 66.087

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1313.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 62.575
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 69.125

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1363.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x317x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x317x2048): 58.866
Elapsed time for attention_prob_times_values (256x2048x2048x317): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x317): 68.579

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1318.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 61.824
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 69.071

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1362.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x319x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x319x2048): 59.960
Elapsed time for attention_prob_times_values (256x2048x2048x319): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x319): 68.868

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1342.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 80.415
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 87.348

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1758.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x652x2048): 74.764
Elapsed time for attention_prob_times_values (96x2048x2048x652): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x652): 65.561

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1137.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x653x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x653x2048): 73.836
Elapsed time for attention_prob_times_values (96x2048x2048x653): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x653): 62.855

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1107.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x654x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x654x2048): 74.556
Elapsed time for attention_prob_times_values (96x2048x2048x654): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x654): 65.078

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1134.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x655x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x655x2048): 73.636
Elapsed time for attention_prob_times_values (96x2048x2048x655): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x655): 63.044

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1110.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x656x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x656x2048): 75.903
Elapsed time for attention_prob_times_values (96x2048x2048x656): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x656): 78.968

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1267.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x657x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x657x2048): 73.331
Elapsed time for attention_prob_times_values (96x2048x2048x657): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x657): 63.375

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1114.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x658x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x658x2048): 74.183
Elapsed time for attention_prob_times_values (96x2048x2048x658): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x658): 65.812

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1145.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x659x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x659x2048): 73.557
Elapsed time for attention_prob_times_values (96x2048x2048x659): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x659): 63.353

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1119.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x660x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x660x2048): 72.075
Elapsed time for attention_prob_times_values (96x2048x2048x660): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x660): 65.419

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1129.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x661x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x661x2048): 73.773
Elapsed time for attention_prob_times_values (96x2048x2048x661): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x661): 63.596

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1126.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 66.187
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 62.274

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1508.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 66.205
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 60.239

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1490.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 67.900
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 61.840

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1537.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 63.481
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 60.872

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1483.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 67.419
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 58.772

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1507.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 66.669
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 60.944

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1536.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 67.973
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 63.714

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1595.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 67.121
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 62.082

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1572.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 67.099
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 64.056

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1605.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x321x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x321x2048): 64.331
Elapsed time for attention_prob_times_values (256x2048x2048x321): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x321): 60.075

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1308.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 64.319
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 62.892

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1343.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x323x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x323x2048): 64.397
Elapsed time for attention_prob_times_values (256x2048x2048x323): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x323): 58.325

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1296.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 64.189
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 63.291

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1354.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x325x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x325x2048): 64.289
Elapsed time for attention_prob_times_values (256x2048x2048x325): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x325): 59.079

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1312.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 64.932
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 63.189

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1369.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x327x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x327x2048): 62.383
Elapsed time for attention_prob_times_values (256x2048x2048x327): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x327): 61.158

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1324.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 66.102
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 73.948

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1500.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x329x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x329x2048): 61.755
Elapsed time for attention_prob_times_values (256x2048x2048x329): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x329): 61.161

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1325.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x662x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x662x2048): 74.430
Elapsed time for attention_prob_times_values (96x2048x2048x662): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x662): 66.059

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1156.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x663x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x663x2048): 73.911
Elapsed time for attention_prob_times_values (96x2048x2048x663): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x663): 63.445

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1129.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x664x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x664x2048): 73.019
Elapsed time for attention_prob_times_values (96x2048x2048x664): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x664): 79.580

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1261.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x665x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x665x2048): 73.385
Elapsed time for attention_prob_times_values (96x2048x2048x665): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x665): 63.930

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1133.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x666x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x666x2048): 74.064
Elapsed time for attention_prob_times_values (96x2048x2048x666): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x666): 66.045

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1159.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x667x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x667x2048): 70.900
Elapsed time for attention_prob_times_values (96x2048x2048x667): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x667): 63.924

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1118.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x668x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x668x2048): 74.682
Elapsed time for attention_prob_times_values (96x2048x2048x668): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x668): 66.593

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1172.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x669x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x669x2048): 72.652
Elapsed time for attention_prob_times_values (96x2048x2048x669): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x669): 64.097

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1136.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x670x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x670x2048): 74.250
Elapsed time for attention_prob_times_values (96x2048x2048x670): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x670): 66.449

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1171.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x671x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x671x2048): 73.708
Elapsed time for attention_prob_times_values (96x2048x2048x671): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x671): 64.296

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1148.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x672x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x672x2048): 89.072
Elapsed time for attention_prob_times_values (96x2048x2048x672): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x672): 81.091

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1421.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x673x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x673x2048): 75.776
Elapsed time for attention_prob_times_values (96x2048x2048x673): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x673): 64.555

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1169.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x674x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x674x2048): 76.789
Elapsed time for attention_prob_times_values (96x2048x2048x674): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x674): 66.652

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1198.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x675x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x675x2048): 69.420
Elapsed time for attention_prob_times_values (96x2048x2048x675): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x675): 61.324

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1095.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x676x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x676x2048): 77.417
Elapsed time for attention_prob_times_values (96x2048x2048x676): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x676): 67.076

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1210.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x677x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x677x2048): 75.706
Elapsed time for attention_prob_times_values (96x2048x2048x677): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x677): 58.695

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1115.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x678x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x678x2048): 76.479
Elapsed time for attention_prob_times_values (96x2048x2048x678): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x678): 66.989

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1206.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x679x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x679x2048): 75.757
Elapsed time for attention_prob_times_values (96x2048x2048x679): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x679): 64.761

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1181.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x680x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x680x2048): 81.132
Elapsed time for attention_prob_times_values (96x2048x2048x680): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x680): 81.474

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1377.061--------
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 63.726
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 64.297

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1384.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x331x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x331x2048): 63.368
Elapsed time for attention_prob_times_values (256x2048x2048x331): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x331): 58.864

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1323.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 63.579
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 64.927

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1397.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x333x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x333x2048): 62.198
Elapsed time for attention_prob_times_values (256x2048x2048x333): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x333): 59.596

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1327.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 64.376
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 63.574

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1399.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x335x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x335x2048): 61.858
Elapsed time for attention_prob_times_values (256x2048x2048x335): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x335): 59.882

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1334.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 62.245
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 78.688

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1529.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x337x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x337x2048): 62.749
Elapsed time for attention_prob_times_values (256x2048x2048x337): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x337): 60.584

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1360.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 61.925
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 65.122

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1404.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x339x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x339x2048): 63.261
Elapsed time for attention_prob_times_values (256x2048x2048x339): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x339): 60.931

Attention duration (in seconds): 0.0235

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x681x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x681x2048): 75.062
Elapsed time for attention_prob_times_values (96x2048x2048x681): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x681): 64.528

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1177.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x682x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x682x2048): 75.761
Elapsed time for attention_prob_times_values (96x2048x2048x682): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x682): 66.864

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1206.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x683x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x683x2048): 75.128
Elapsed time for attention_prob_times_values (96x2048x2048x683): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x683): 61.939

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1154.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x684x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x684x2048): 76.779
Elapsed time for attention_prob_times_values (96x2048x2048x684): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x684): 67.459

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1223.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x685x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x685x2048): 75.383
Elapsed time for attention_prob_times_values (96x2048x2048x685): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x685): 64.916

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1189.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x686x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x686x2048): 76.230
Elapsed time for attention_prob_times_values (96x2048x2048x686): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x686): 67.831

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1225.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x687x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x687x2048): 75.581
Elapsed time for attention_prob_times_values (96x2048x2048x687): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x687): 65.320

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1198.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x688x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x688x2048): 77.036
Elapsed time for attention_prob_times_values (96x2048x2048x688): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x688): 82.838

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1367.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x689x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x689x2048): 74.541
Elapsed time for attention_prob_times_values (96x2048x2048x689): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x689): 65.554

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1196.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 65.787
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 60.952

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1558.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 67.409
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 64.756

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1634.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 65.815
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 58.807

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1545.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 78.551
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 66.461

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1800.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 65.407
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 52.614

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1465.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 66.968
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 55.203

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1528.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 67.244
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 52.429

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1495.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 69.153
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 53.426

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1537.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 67.552
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 54.045

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 1538.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 65.109
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 54.466

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 1527.324
num_attention_heads: 24, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x690x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x690x2048): 74.728
Elapsed time for attention_prob_times_values (96x2048x2048x690): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x690): 68.053

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1223.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x691x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x691x2048): 74.779
Elapsed time for attention_prob_times_values (96x2048x2048x691): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x691): 65.418

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1199.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x692x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x692x2048): 76.243
Elapsed time for attention_prob_times_values (96x2048x2048x692): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x692): 68.541

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1242.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x693x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x693x2048): 75.046
Elapsed time for attention_prob_times_values (96x2048x2048x693): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x693): 65.545

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1206.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x694x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x694x2048): 75.612
Elapsed time for attention_prob_times_values (96x2048x2048x694): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x694): 68.610

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1242.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x695x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x695x2048): 75.070
Elapsed time for attention_prob_times_values (96x2048x2048x695): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x695): 63.012

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1184.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x696x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x696x2048): 76.924
Elapsed time for attention_prob_times_values (96x2048x2048x696): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x696): 83.462

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1386.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x697x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x697x2048): 74.363
Elapsed time for attention_prob_times_values (96x2048x2048x697): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x697): 65.951

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1211.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x698x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x698x2048): 75.086
Elapsed time for attention_prob_times_values (96x2048x2048x698): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x698): 66.881

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1228.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x699x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x699x2048): 74.595
Elapsed time for attention_prob_times_values (96x2048x2048x699): 0.0085
Attention throughput (in TFLOP/s): 1377.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 64.291
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 65.764

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1446.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x341x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x341x2048): 62.419
Elapsed time for attention_prob_times_values (256x2048x2048x341): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x341): 62.414

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1392.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 62.039
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 65.094

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1421.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x343x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x343x2048): 63.791
Elapsed time for attention_prob_times_values (256x2048x2048x343): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x343): 62.909

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1421.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 65.655
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 80.226

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1624.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x345x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x345x2048): 61.220
Elapsed time for attention_prob_times_values (256x2048x2048x345): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x345): 61.486

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1384.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 62.063
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 64.020

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1425.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x347x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x347x2048): 60.735
Elapsed time for attention_prob_times_values (256x2048x2048x347): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x347): 63.838

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1412.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 64.446
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 64.964

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1472.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x699): 66.085

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1218.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x700x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x700x2048): 76.258
Elapsed time for attention_prob_times_values (96x2048x2048x700): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x700): 65.427

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1225.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x701x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x701x2048): 74.655
Elapsed time for attention_prob_times_values (96x2048x2048x701): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x701): 66.391

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1224.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x702x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x702x2048): 75.574
Elapsed time for attention_prob_times_values (96x2048x2048x702): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x702): 68.939

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1258.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x703x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x703x2048): 74.565
Elapsed time for attention_prob_times_values (96x2048x2048x703): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x703): 65.020

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1214.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x704x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x704x2048): 89.003
Elapsed time for attention_prob_times_values (96x2048x2048x704): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x704): 85.826

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1529.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x705x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x705x2048): 76.863
Elapsed time for attention_prob_times_values (96x2048x2048x705): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x705): 63.302

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1216.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x706x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x706x2048): 77.775
Elapsed time for attention_prob_times_values (96x2048x2048x706): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x706): 63.615

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1228.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x707x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x707x2048): 76.655
Elapsed time for attention_prob_times_values (96x2048x2048x707): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x707): 63.432

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1219.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x708x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x708x2048): 78.381
Elapsed time for attention_prob_times_values (96x2048x2048x708): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x708): 65.810

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1258.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x709x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x709x2048): 76.296
Elapsed time for attention_prob_times_values (96x2048x2048x709): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x709): 63.575

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1221.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x710x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x710x2048): 74.870
Elapsed time for attention_prob_times_values (96x2048x2048x710): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x710): 66.378

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1241.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x711x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x711x2048): 76.278
Elapsed time for attention_prob_times_values (96x2048x2048x711): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x711): 63.771

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1227.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x712x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x712x2048): 78.244
Elapsed time for attention_prob_times_values (96x2048x2048x712): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x712): 85.187

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1442.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x713x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x713x2048): 75.446
Elapsed time for attention_prob_times_values (96x2048x2048x713): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x713): 62.919

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1215.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x714x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x714x2048): 76.300
Elapsed time for attention_prob_times_values (96x2048x2048x714): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x714): 66.612

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1261.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x715x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x715x2048): 72.962
Elapsed time for attention_prob_times_values (96x2048x2048x715): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x715): 63.068

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1201.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x716x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x716x2048): 77.205
Elapsed time for attention_prob_times_values (96x2048x2048x716): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x716): 66.957

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1275.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x717x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x717x2048): 75.798
Elapsed time for attention_prob_times_values (96x2048x2048x717): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x717): 63.660

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1232.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x718x2048): 0.0078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0287
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 68.400
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 54.168

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1564.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 68.835
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 50.089

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1507.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 67.957
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 52.226

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1542.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 67.973
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 54.926

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1594.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 65.572
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 51.347

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1519.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 69.952
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 55.247

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1636.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 66.997
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 52.800

Attention duration (in seconds): 0.0298
Attention throughput (in TFLOP/s): 1572.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0298
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 70.087
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 55.646

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1659.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 67.205
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 52.889

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1590.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 64, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x349x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x349x2048): 63.182
Elapsed time for attention_prob_times_values (256x2048x2048x349): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x349): 59.211

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1394.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 63.701
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 64.827

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1469.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x351x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x351x2048): 63.345
Elapsed time for attention_prob_times_values (256x2048x2048x351): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x351): 62.927

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1448.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 79.862
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 78.127

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1816.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x353x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x353x2048): 66.249
Elapsed time for attention_prob_times_values (256x2048x2048x353): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x353): 61.297

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1468.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 66.770
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 65.382

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1527.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x355x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x355x2048): 65.929
Elapsed time for attention_prob_times_values (256x2048x2048x355): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x355): 65.246

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1520.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 67.655
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 64.800

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1539.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x357x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x357x2048): 65.850
Elapsed time for attention_prob_times_values (256x2048x2048x357): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x357): 63.748

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1510.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x718x2048): 74.201
Elapsed time for attention_prob_times_values (96x2048x2048x718): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x718): 66.863

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1254.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x719x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x719x2048): 75.798
Elapsed time for attention_prob_times_values (96x2048x2048x719): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x719): 61.753

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1214.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x720x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x720x2048): 77.882
Elapsed time for attention_prob_times_values (96x2048x2048x720): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x720): 86.380

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1464.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x721x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x721x2048): 69.906
Elapsed time for attention_prob_times_values (96x2048x2048x721): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x721): 60.039

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1156.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x722x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x722x2048): 76.000
Elapsed time for attention_prob_times_values (96x2048x2048x722): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x722): 67.130

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1277.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x723x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x723x2048): 75.405
Elapsed time for attention_prob_times_values (96x2048x2048x723): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x723): 64.049

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1242.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x724x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x724x2048): 76.713
Elapsed time for attention_prob_times_values (96x2048x2048x724): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x724): 66.303

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1278.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x725x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x725x2048): 74.716
Elapsed time for attention_prob_times_values (96x2048x2048x725): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x725): 64.303

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1243.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x726x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x726x2048): 76.061
Elapsed time for attention_prob_times_values (96x2048x2048x726): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x726): 67.341

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1286.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x727x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x727x2048): 75.378
Elapsed time for attention_prob_times_values (96x2048x2048x727): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x727): 63.472

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1243.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x728x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x728x2048): 76.009
Elapsed time for attention_prob_times_values (96x2048x2048x728): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x728): 86.140

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1458.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x729x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x729x2048): 74.902
Elapsed time for attention_prob_times_values (96x2048x2048x729): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x729): 64.362

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1252.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x730x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x730x2048): 75.668
Elapsed time for attention_prob_times_values (96x2048x2048x730): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x730): 67.576

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1292.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x731x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x731x2048): 75.195
Elapsed time for attention_prob_times_values (96x2048x2048x731): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x731): 64.358

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1257.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x732x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x732x2048): 76.516
Elapsed time for attention_prob_times_values (96x2048x2048x732): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x732): 66.609

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1293.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x733x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x733x2048): 75.265
Elapsed time for attention_prob_times_values (96x2048x2048x733): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x733): 64.777

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1265.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x734x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x734x2048): 75.945
Elapsed time for attention_prob_times_values (96x2048x2048x734): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x734): 66.985

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1295.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x735x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x735x2048): 75.215
Elapsed time for attention_prob_times_values (96x2048x2048x735): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x735): 64.688

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1267.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x736x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x736x2048): 89.821
Elapsed time for attention_prob_times_values (96x2048x2048x736): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x736): 89.027

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1631.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateThroughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 66.601
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 64.751

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1534.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x359x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x359x2048): 65.485
Elapsed time for attention_prob_times_values (256x2048x2048x359): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x359): 66.239

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1543.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 67.321
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 83.001

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1747.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x361x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x361x2048): 64.058
Elapsed time for attention_prob_times_values (256x2048x2048x361): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x361): 63.812

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1506.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 65.317
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 64.598

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1534.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x363x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x363x2048): 63.681
Elapsed time for attention_prob_times_values (256x2048x2048x363): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x363): 59.452

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1456.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 64.146
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 67.210

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1558.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x365x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x365x2048): 65.081
Elapsed time for attention_prob_times_values (256x2048x2048x365): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x365): 64.466

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1542.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 63.202
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 67.226

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1555.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x367x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x367x2048): 65.227
Elapsed time for attention_prob_times_values (256x2048x2048x367): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x367): 64.996

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1558.605
MLP duration (in seconds): 0.0000

--------
Elapsed time for attention_key_query_prob (96x2048x737x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x737x2048): 77.094
Elapsed time for attention_prob_times_values (96x2048x2048x737): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x737): 65.063

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1289.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x738x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x738x2048): 77.940
Elapsed time for attention_prob_times_values (96x2048x2048x738): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x738): 64.056

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1286.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x739x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x739x2048): 76.213
Elapsed time for attention_prob_times_values (96x2048x2048x739): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x739): 65.193

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1287.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x740x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x740x2048): 78.517
Elapsed time for attention_prob_times_values (96x2048x2048x740): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x740): 66.824

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1324.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x741x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x741x2048): 76.771
Elapsed time for attention_prob_times_values (96x2048x2048x741): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x741): 65.339

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1296.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x742x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x742x2048): 77.583
Elapsed time for attention_prob_times_values (96x2048x2048x742): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x742): 67.296

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1325.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x743x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x743x2048): 76.672
Elapsed time for attention_prob_times_values (96x2048x2048x743): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x743): 64.221

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1287.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x744x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x744x2048): 78.529
Elapsed time for attention_prob_times_values (96x2048x2048x744): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x744): 89.022

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1538.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x745x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x745x2048): 72.901
Elapsed time for attention_prob_times_values (96x2048x2048x745): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x745): 65.466

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1273.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x746x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x746x2048): 76.679
Elapsed time for attention_prob_times_values (96x2048x2048x746): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x746): 60.423

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1249.313num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 69.767
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 53.308

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 1631.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 68.769
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 53.888

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1639.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 67.653
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 56.553

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 1678.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 66.707
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 54.170

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1636.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 67.360
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 55.382

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1671.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 69.542
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 54.540

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1688.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 69.727
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 57.400

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1747.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 67.500
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 55.161

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1692.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 70.622
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 54.136

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1716.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 69.715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 67.162
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 82.526

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1777.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x369x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x369x2048): 64.327
Elapsed time for attention_prob_times_values (256x2048x2048x369): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x369): 65.120

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1557.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 65.038
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 67.745

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1601.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x371x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x371x2048): 64.680
Elapsed time for attention_prob_times_values (256x2048x2048x371): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x371): 66.812

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1589.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 65.343
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 68.459

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1621.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x373x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x373x2048): 64.494
Elapsed time for attention_prob_times_values (256x2048x2048x373): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x373): 66.033

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1586.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 64.765
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 66.690

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1601.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x375x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x375x2048): 65.276
Elapsed time for attention_prob_times_values (256x2048x2048x375): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x375): 64.204

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1581.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 66.572
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 87.084

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1848.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x747x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x747x2048): 76.136
Elapsed time for attention_prob_times_values (96x2048x2048x747): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x747): 65.659

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1304.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x748x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x748x2048): 72.399
Elapsed time for attention_prob_times_values (96x2048x2048x748): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x748): 63.184

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1250.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x749x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x749x2048): 75.786
Elapsed time for attention_prob_times_values (96x2048x2048x749): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x749): 61.991

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1265.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x750x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x750x2048): 76.981
Elapsed time for attention_prob_times_values (96x2048x2048x750): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x750): 67.381

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1335.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x751x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x751x2048): 76.046
Elapsed time for attention_prob_times_values (96x2048x2048x751): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x751): 60.233

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1250.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x752x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x752x2048): 70.891
Elapsed time for attention_prob_times_values (96x2048x2048x752): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x752): 89.782

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1475.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x753x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x753x2048): 75.396
Elapsed time for attention_prob_times_values (96x2048x2048x753): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x753): 66.061

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1313.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x754x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x754x2048): 76.164
Elapsed time for attention_prob_times_values (96x2048x2048x754): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x754): 65.797

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1318.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x755x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x755x2048): 75.610
Elapsed time for attention_prob_times_values (96x2048x2048x755): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x755): 64.361

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1299.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x756x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x756x2048): 76.963
Elapsed time for attention_prob_times_values (96x2048x2048x756): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x756): 67.991

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1351.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x757x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x757x2048): 75.841
Elapsed time for attention_prob_times_values (96x2048x2048x757): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x757): 66.574

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1328.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x758x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x758x2048): 76.520
Elapsed time for attention_prob_times_values (96x2048x2048x758): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x758): 67.804

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1349.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x759x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x759x2048): 75.851
Elapsed time for attention_prob_times_values (96x2048x2048x759): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x759): 66.103

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1327.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x760x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x760x2048): 77.601
Elapsed time for attention_prob_times_values (96x2048x2048x760): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x760): 90.959

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1575.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x761x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x761x2048): 55.857
Elapsed time for attention_prob_times_values (96x2048x2048x761): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x761): 66.136

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1140.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x762x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x762x2048): 71.632
Elapsed time for attention_prob_times_values (96x2048x2048x762): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x762): 65.647

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1292.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x763x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x763x2048): 75.321
Elapsed time for attention_prob_times_values (96x2048x2048x763): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x763): 66.071

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1329.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x764x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x764x2048): 74.678
Elapsed time for attention_prob_times_values (96x2048x2048x764): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x764): 67.071

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1336.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x765x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x765x2048): 75.375
Elapsed time for attention_prob_times_values (96x2048x2048x765): 0.0095
num_attention_heads: 64, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x377x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x377x2048): 64.621
Elapsed time for attention_prob_times_values (256x2048x2048x377): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x377): 66.250

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1607.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 64.065
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 69.047

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1636.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x379x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x379x2048): 61.425
Elapsed time for attention_prob_times_values (256x2048x2048x379): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x379): 65.398

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1563.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 63.087
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 69.250

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1634.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x381x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x381x2048): 61.743
Elapsed time for attention_prob_times_values (256x2048x2048x381): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x381): 66.814

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1592.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 62.818
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 69.731

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1644.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x383x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x383x2048): 63.361
Elapsed time for attention_prob_times_values (256x2048x2048x383): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x383): 66.551

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1618.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 77.769
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 85.189

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2032.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x385x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x385x2048): 67.234
Elapsed time for attention_prob_times_values (256x2048x2048x385): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x385): 57.904

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1559.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 68.277
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 53.355

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1700.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 70.975
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 57.346

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1792.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 70.081
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 55.313

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1754.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 71.624
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 58.862

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1841.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 70.585
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 56.496

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1796.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 71.732
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 58.760

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1857.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 70.427
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 56.860

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1816.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 80.142
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 57.313

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1938.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 69.524
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 56.811

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1821.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 70.615
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 59.998

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1897.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x765): 64.642

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1317.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x766x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x766x2048): 76.187
Elapsed time for attention_prob_times_values (96x2048x2048x766): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x766): 64.641

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1325.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x767x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x767x2048): 75.282
Elapsed time for attention_prob_times_values (96x2048x2048x767): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x767): 65.206

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1326.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x768x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x768x2048): 86.443
Elapsed time for attention_prob_times_values (96x2048x2048x768): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x768): 87.266

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1650.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x769x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x769x2048): 77.183
Elapsed time for attention_prob_times_values (96x2048x2048x769): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x769): 60.904

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1295.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x770x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x770x2048): 78.162
Elapsed time for attention_prob_times_values (96x2048x2048x770): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x770): 64.762

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1349.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x771x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x771x2048): 76.895
Elapsed time for attention_prob_times_values (96x2048x2048x771): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x771): 60.601

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1292.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x772x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x772x2048): 78.686
Elapsed time for attention_prob_times_values (96x2048x2048x772): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x772): 64.838

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1357.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x773x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x773x2048): 77.215
Elapsed time for attention_prob_times_values (96x2048x2048x773): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x773): 61.971

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1314.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x774x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x774x2048): 78.056
Elapsed time for attention_prob_times_values (96x2048x2048x774): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x774): 64.890

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1356.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x775x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x775x2048): 74.433
Elapsed time for attention_prob_times_values (96x2048x2048x775): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x775): 62.128

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1297.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x776x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x776x2048): 78.474
Elapsed time for attention_prob_times_values (96x2048x2048x776): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x776): 82.238

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1540.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x777x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x777x2048): 73.077
Elapsed time for attention_prob_times_values (96x2048x2048x777): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x777): 58.403

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1247.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x778x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x778x2048): 76.986
Elapsed time for attention_prob_times_values (96x2048x2048x778): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x778): 65.059

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1356.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x779x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x779x2048): 76.536
Elapsed time for attention_prob_times_values (96x2048x2048x779): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x779): 59.482

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1289.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x780x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x780x2048): 77.676
Elapsed time for attention_prob_times_values (96x2048x2048x780): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x780): 65.572

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1371.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x781x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x781x2048): 76.712
Elapsed time for attention_prob_times_values (96x2048x2048x781): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x781): 62.070

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1324.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x782x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x782x2048): 77.512
Elapsed time for attention_prob_times_values (96x2048x2048x782): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x782): 63.849

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1353.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x783x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x783x2048): 74.051
Elapsed time for attention_prob_times_values (96x2048x2048x783): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x783): 62.389

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1310.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x784x2048): 0.0080
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 60.783

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1615.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x387x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x387x2048): 67.231
Elapsed time for attention_prob_times_values (256x2048x2048x387): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x387): 60.219

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1600.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 68.782
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 62.642

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1655.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x389x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x389x2048): 64.667
Elapsed time for attention_prob_times_values (256x2048x2048x389): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x389): 60.789

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1586.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 68.255
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 63.852

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1674.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x391x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x391x2048): 67.256
Elapsed time for attention_prob_times_values (256x2048x2048x391): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x391): 60.955

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1626.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 68.850
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 77.971

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1864.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x393x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x393x2048): 66.010
Elapsed time for attention_prob_times_values (256x2048x2048x393): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x393): 59.894

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1605.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 64.229
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 64.238

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1645.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x395x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x395x2048): 66.579
Elapsed time for attention_prob_times_values (256x2048x2048x395): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x395): 59.063

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1607.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x784x2048): 78.724
Elapsed time for attention_prob_times_values (96x2048x2048x784): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x784): 83.384

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1569.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x785x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x785x2048): 73.555
Elapsed time for attention_prob_times_values (96x2048x2048x785): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x785): 62.593

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1311.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x786x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x786x2048): 74.625
Elapsed time for attention_prob_times_values (96x2048x2048x786): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x786): 65.580

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1355.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x787x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x787x2048): 76.297
Elapsed time for attention_prob_times_values (96x2048x2048x787): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x787): 62.571

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1336.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x788x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x788x2048): 77.562
Elapsed time for attention_prob_times_values (96x2048x2048x788): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x788): 65.931

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1387.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x789x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x789x2048): 76.484
Elapsed time for attention_prob_times_values (96x2048x2048x789): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x789): 62.572

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1341.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x790x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x790x2048): 77.154
Elapsed time for attention_prob_times_values (96x2048x2048x790): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x790): 65.746

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1385.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x791x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x791x2048): 76.498
Elapsed time for attention_prob_times_values (96x2048x2048x791): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x791): 62.846

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1348.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x792x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x792x2048): 78.115
Elapsed time for attention_prob_times_values (96x2048x2048x792): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x792): 83.750

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1581.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x793x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x793x2048): 75.963
Elapsed time for attention_prob_times_values (96x2048x2048x793): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x793): 62.981

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1348.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 69.839
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 58.103

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1863.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 71.470
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 57.971

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1888.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 68.018
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 58.452

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1862.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 70.251
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 59.367

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 1914.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 70.197
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 57.218

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1883.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 69.808
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 57.405

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1890.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 70.072
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 58.833

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1926.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 69.772
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 59.351

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1940.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 70.273
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 59.259

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1953.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x794x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x794x2048): 76.584
Elapsed time for attention_prob_times_values (96x2048x2048x794): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x794): 66.071

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1391.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x795x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x795x2048): 76.100
Elapsed time for attention_prob_times_values (96x2048x2048x795): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x795): 63.152

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1355.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x796x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x796x2048): 77.516
Elapsed time for attention_prob_times_values (96x2048x2048x796): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x796): 66.498

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1407.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x797x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x797x2048): 76.214
Elapsed time for attention_prob_times_values (96x2048x2048x797): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x797): 63.205

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1359.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x798x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x798x2048): 76.943
Elapsed time for attention_prob_times_values (96x2048x2048x798): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x798): 66.309

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1403.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x799x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x799x2048): 76.342
Elapsed time for attention_prob_times_values (96x2048x2048x799): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x799): 62.904

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1360.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x800x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x800x2048): 90.731
Elapsed time for attention_prob_times_values (96x2048x2048x800): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x800): 85.306

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1736.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x801x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x801x2048): 78.469
Elapsed time for attention_prob_times_values (96x2048x2048x801): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x801): 62.608

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1377.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x802x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x802x2048): 79.324
Elapsed time for attention_prob_times_values (96x2048x2048x802): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x802): 65.206

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1416.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateTransformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 65.905
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 65.297

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1689.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x397x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x397x2048): 64.698
Elapsed time for attention_prob_times_values (256x2048x2048x397): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x397): 60.617

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1615.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 66.689
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 65.223

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1706.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x399x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x399x2048): 62.991
Elapsed time for attention_prob_times_values (256x2048x2048x399): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x399): 61.532

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1614.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 69.033
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 79.862

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1925.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x401x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x401x2048): 65.630
Elapsed time for attention_prob_times_values (256x2048x2048x401): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x401): 62.032

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1662.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 67.103
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 64.917

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1724.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 66.308
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 61.437

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1670.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 67.747
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 66.039

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1755.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate

--------
Elapsed time for attention_key_query_prob (96x2048x803x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x803x2048): 78.259
Elapsed time for attention_prob_times_values (96x2048x2048x803): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x803): 63.811

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1393.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x804x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x804x2048): 79.938
Elapsed time for attention_prob_times_values (96x2048x2048x804): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x804): 64.511

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1416.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x805x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x805x2048): 75.399
Elapsed time for attention_prob_times_values (96x2048x2048x805): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x805): 63.948

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1374.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x806x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x806x2048): 78.943
Elapsed time for attention_prob_times_values (96x2048x2048x806): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x806): 64.692

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1414.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x807x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x807x2048): 77.800
Elapsed time for attention_prob_times_values (96x2048x2048x807): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x807): 64.128

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1400.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x808x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x808x2048): 79.946
Elapsed time for attention_prob_times_values (96x2048x2048x808): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x808): 85.581

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1648.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x809x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x809x2048): 74.408
Elapsed time for attention_prob_times_values (96x2048x2048x809): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x809): 63.999

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1373.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x810x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x810x2048): 75.136
Elapsed time for attention_prob_times_values (96x2048x2048x810): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x810): 65.380

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1397.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x811x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x811x2048): 77.305
Elapsed time for attention_prob_times_values (96x2048x2048x811): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x811): 64.159

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1402.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x812x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x812x2048): 79.140
Elapsed time for attention_prob_times_values (96x2048x2048x812): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x812): 64.537

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1424.153--------
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 65.710
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 62.494

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1685.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 67.628
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 65.973

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1761.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 67.026
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 62.835

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1714.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 68.389
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 78.900

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1941.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 65.072
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 63.094

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1701.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 65.939
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 64.720

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1739.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 65.156
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 61.586

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1689.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 65.704
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 67.253

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1778.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 64.722
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 61.445

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1690.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 67.347
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 65.550

Attention duration (in seconds): 0.0268

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x813x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x813x2048): 77.623
Elapsed time for attention_prob_times_values (96x2048x2048x813): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x813): 64.377

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1411.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x814x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x814x2048): 78.551
Elapsed time for attention_prob_times_values (96x2048x2048x814): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x814): 80.061

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1592.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x815x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x815x2048): 78.032
Elapsed time for attention_prob_times_values (96x2048x2048x815): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x815): 77.149

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1559.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x816x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x816x2048): 80.309
Elapsed time for attention_prob_times_values (96x2048x2048x816): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x816): 86.407

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1675.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x817x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x817x2048): 77.512
Elapsed time for attention_prob_times_values (96x2048x2048x817): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x817): 78.040

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1567.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x818x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x818x2048): 76.911
Elapsed time for attention_prob_times_values (96x2048x2048x818): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x818): 80.414

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1585.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x819x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x819x2048): 76.055
Elapsed time for attention_prob_times_values (96x2048x2048x819): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x819): 77.997

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1555.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x820x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x820x2048): 79.266
Elapsed time for attention_prob_times_values (96x2048x2048x820): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x820): 80.590

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1615.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x821x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x821x2048): 79.951
Elapsed time for attention_prob_times_values (96x2048x2048x821): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x821): 78.166

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1600.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 69.440
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 59.303

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1951.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 70.254
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 59.720

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1977.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 70.329
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 60.397

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1998.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.729
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 58.642

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 1966.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 54.389
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 58.799

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1751.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 54.080
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 60.580

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 1778.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0189
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 54.891
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 63.039

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1833.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0192
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 54.428
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 59.643

Attention duration (in seconds): 0.0367
Attention throughput (in TFLOP/s): 1785.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 54.816
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 63.811

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 1857.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0355
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0198
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 53.133
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 59.373

num_attention_heads: 24, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x822x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x822x2048): 78.476
Elapsed time for attention_prob_times_values (96x2048x2048x822): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x822): 80.711

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1612.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x823x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x823x2048): 77.751
Elapsed time for attention_prob_times_values (96x2048x2048x823): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x823): 78.303

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1583.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x824x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x824x2048): 79.589
Elapsed time for attention_prob_times_values (96x2048x2048x824): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x824): 87.147

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1689.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x825x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x825x2048): 76.741
Elapsed time for attention_prob_times_values (96x2048x2048x825): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x825): 78.613

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1579.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x826x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x826x2048): 77.640
Elapsed time for attention_prob_times_values (96x2048x2048x826): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x826): 81.083

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1614.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x827x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x827x2048): 70.833
Elapsed time for attention_prob_times_values (96x2048x2048x827): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x827): 78.810

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1520.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x828x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x828x2048): 74.534
Elapsed time for attention_prob_times_values (96x2048x2048x828): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x828): 74.725

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1522.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x829x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x829x2048): 77.074
Elapsed time for attention_prob_times_values (96x2048x2048x829): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x829): 78.981

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1593.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x830x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x830x2048): 73.217
Elapsed time for attention_prob_times_values (96x2048x2048x830): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x830): 74.648

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1512.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x831x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x831x2048): 75.148
Elapsed time for attention_prob_times_values (96x2048x2048x831): 0.0093
Attention throughput (in TFLOP/s): 1785.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 66.391
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 63.843

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1753.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 85.511
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 82.074

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2261.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 69.344
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 64.409

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1807.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 70.812
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 67.553

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1875.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 69.929
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 64.686

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1827.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 71.360
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 68.302

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1901.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 69.789
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 63.967

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1823.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 70.468
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 68.272

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1898.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 69.448
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 65.310

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1846.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x831): 72.071

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1506.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x832x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x832x2048): 90.961
Elapsed time for attention_prob_times_values (96x2048x2048x832): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x832): 88.971

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1844.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x833x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x833x2048): 75.112
Elapsed time for attention_prob_times_values (96x2048x2048x833): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x833): 76.455

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1555.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x834x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x834x2048): 79.660
Elapsed time for attention_prob_times_values (96x2048x2048x834): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x834): 81.852

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1658.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x835x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x835x2048): 78.716
Elapsed time for attention_prob_times_values (96x2048x2048x835): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x835): 79.624

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1628.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x836x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x836x2048): 77.994
Elapsed time for attention_prob_times_values (96x2048x2048x836): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x836): 78.632

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1612.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x837x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x837x2048): 75.709
Elapsed time for attention_prob_times_values (96x2048x2048x837): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x837): 79.820

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1602.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x838x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x838x2048): 79.471
Elapsed time for attention_prob_times_values (96x2048x2048x838): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x838): 82.211

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1668.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x839x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x839x2048): 76.592
Elapsed time for attention_prob_times_values (96x2048x2048x839): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x839): 76.862

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1585.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x840x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x840x2048): 80.378
Elapsed time for attention_prob_times_values (96x2048x2048x840): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x840): 85.713

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1716.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x841x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x841x2048): 77.571
Elapsed time for attention_prob_times_values (96x2048x2048x841): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x841): 78.115

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1612.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x842x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x842x2048): 78.510
Elapsed time for attention_prob_times_values (96x2048x2048x842): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x842): 80.544

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1648.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x843x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x843x2048): 77.911
Elapsed time for attention_prob_times_values (96x2048x2048x843): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x843): 80.410

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1642.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x844x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x844x2048): 79.524
Elapsed time for attention_prob_times_values (96x2048x2048x844): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x844): 82.852

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1686.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x845x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x845x2048): 77.948
Elapsed time for attention_prob_times_values (96x2048x2048x845): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x845): 80.564

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1648.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x846x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x846x2048): 78.090
Elapsed time for attention_prob_times_values (96x2048x2048x846): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x846): 82.808

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1674.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x847x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x847x2048): 78.005
Elapsed time for attention_prob_times_values (96x2048x2048x847): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x847): 77.797

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1624.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x848x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x848x2048): 79.734
Elapsed time for attention_prob_times_values (96x2048x2048x848): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x848): 89.613

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1761.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x849x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x849x2048): 77.327
Elapsed time for attention_prob_times_values (96x2048x2048x849): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x849): 79.859

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1642.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x850x2048): 0.0088
Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 1773.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 55.342
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 64.070

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1885.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 54.723
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 61.081

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1840.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0193
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 55.155
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 62.284

Attention duration (in seconds): 0.0364
Attention throughput (in TFLOP/s): 1872.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0364
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 52.996
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 62.919

Attention duration (in seconds): 0.0372
Attention throughput (in TFLOP/s): 1848.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0372
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0203
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 52.920
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 64.919

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1880.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0198
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 54.511
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 63.232

Attention duration (in seconds): 0.0368
Attention throughput (in TFLOP/s): 1895.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0197
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 54.817
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 64.320

Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 1923.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0200
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 54.448
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 63.700

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 1915.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 54.712
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 65.823

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1956.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 71.442
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 84.079

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2124.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 68.246
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 65.336

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1840.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 68.166
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 68.326

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1885.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 68.682
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 65.578

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1857.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 70.332
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 69.382

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1938.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 69.103
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 62.582

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1826.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 69.055
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 69.485

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1930.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 69.121
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 64.565

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1865.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 68.975
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 85.613

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2139.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x850x2048): 78.196
Elapsed time for attention_prob_times_values (96x2048x2048x850): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x850): 83.245

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1687.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x851x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x851x2048): 75.080
Elapsed time for attention_prob_times_values (96x2048x2048x851): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x851): 81.210

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1634.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x852x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x852x2048): 79.252
Elapsed time for attention_prob_times_values (96x2048x2048x852): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x852): 83.550

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1705.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x853x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x853x2048): 74.789
Elapsed time for attention_prob_times_values (96x2048x2048x853): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x853): 81.290

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1635.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x854x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x854x2048): 76.094
Elapsed time for attention_prob_times_values (96x2048x2048x854): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x854): 83.552

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1673.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x855x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x855x2048): 75.535
Elapsed time for attention_prob_times_values (96x2048x2048x855): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x855): 81.387

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1648.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x856x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x856x2048): 79.758
Elapsed time for attention_prob_times_values (96x2048x2048x856): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x856): 90.300

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1784.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x857x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x857x2048): 74.742
Elapsed time for attention_prob_times_values (96x2048x2048x857): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x857): 77.058

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1600.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x858x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x858x2048): 77.997
Elapsed time for attention_prob_times_values (96x2048x2048x858): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x858): 76.917

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1634.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x859x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x859x2048): 77.374
Elapsed time for attention_prob_times_values (96x2048x2048x859): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x859): 81.777

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1680.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x860x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x860x2048): 79.041
Elapsed time for attention_prob_times_values (96x2048x2048x860): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x860): 79.222

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1674.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x861x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x861x2048): 73.579
Elapsed time for attention_prob_times_values (96x2048x2048x861): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x861): 81.882

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1641.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x862x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x862x2048): 78.193
Elapsed time for attention_prob_times_values (96x2048x2048x862): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x862): 84.201

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1719.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x863x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x863x2048): 72.028
Elapsed time for attention_prob_times_values (96x2048x2048x863): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x863): 82.051

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1628.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x864x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x864x2048): 83.976
Elapsed time for attention_prob_times_values (96x2048x2048x864): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x864): 91.766

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1863.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x865x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x865x2048): 79.138
Elapsed time for attention_prob_times_values (96x2048x2048x865): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x865): 82.232

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1715.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x866x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x866x2048): 79.918
Elapsed time for attention_prob_times_values (96x2048x2048x866): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x866): 84.406

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1748.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x867x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x867x2048): 78.849
Elapsed time for attention_prob_times_values (96x2048x2048x867): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x867): 82.320

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1717.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x868x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x868x2048): 78.730
Elapsed time for attention_prob_times_values (96x2048x2048x868): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x868): 84.755

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1742.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateThroughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 68.291
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 65.585

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1877.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 69.165
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 70.001

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1956.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 66.997
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 66.793

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1885.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 70.245
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 68.923

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1965.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 66.203
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 67.164

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1887.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 69.556
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 67.494

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1943.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 69.035
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 67.496

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1941.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 70.903
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 86.738

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2223.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 68.260
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 64.980

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1901.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 66.866
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 69.740

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1954.304
MLP duration (in seconds): 0.0000

--------
Elapsed time for attention_key_query_prob (96x2048x869x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x869x2048): 78.688
Elapsed time for attention_prob_times_values (96x2048x2048x869): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x869): 82.441

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1720.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x870x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x870x2048): 79.323
Elapsed time for attention_prob_times_values (96x2048x2048x870): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x870): 84.676

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1752.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x871x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x871x2048): 78.440
Elapsed time for attention_prob_times_values (96x2048x2048x871): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x871): 82.593

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1723.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x872x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x872x2048): 77.577
Elapsed time for attention_prob_times_values (96x2048x2048x872): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x872): 91.767

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1802.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x873x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x873x2048): 77.521
Elapsed time for attention_prob_times_values (96x2048x2048x873): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x873): 82.637

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1716.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x874x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x874x2048): 78.614
Elapsed time for attention_prob_times_values (96x2048x2048x874): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x874): 85.144

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1756.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x875x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x875x2048): 76.553
Elapsed time for attention_prob_times_values (96x2048x2048x875): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x875): 82.775

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1710.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x876x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x876x2048): 79.350
Elapsed time for attention_prob_times_values (96x2048x2048x876): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x876): 85.373

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1770.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x877x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x877x2048): 78.046
Elapsed time for attention_prob_times_values (96x2048x2048x877): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x877): 82.950

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1733.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x878x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x878x2048): 78.898
Elapsed time for attention_prob_times_values (96x2048x2048x878): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x878): 85.194

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1767.798========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0211
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 52.012
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 66.099

Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 1913.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0376
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x1x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x1x2048): 0.686
Elapsed time for attention_prob_times_values (1024x2048x2048x1): 0.0393
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x1): 0.219

Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 0.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x2x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x2x2048): 2.006
Elapsed time for attention_prob_times_values (1024x2048x2048x2): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x2): 1.468

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x3x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x3x2048): 2.697
Elapsed time for attention_prob_times_values (1024x2048x2048x3): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x3): 2.082

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 4.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x4x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x4x2048): 3.521
Elapsed time for attention_prob_times_values (1024x2048x2048x4): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x4): 3.070

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 6.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x5x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x5x2048): 4.285
Elapsed time for attention_prob_times_values (1024x2048x2048x5): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x5): 3.627

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 8.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x6x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x6x2048): 5.135
Elapsed time for attention_prob_times_values (1024x2048x2048x6): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x6): 4.575

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 12.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x7x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x7x2048): 5.948
Elapsed time for attention_prob_times_values (1024x2048x2048x7): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x7): 5.283

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 15.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x8x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x8x2048): 6.522
Elapsed time for attention_prob_times_values (1024x2048x2048x8): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x8): 6.995

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 20.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x9x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x9x2048): 7.064

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x879x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x879x2048): 77.939
Elapsed time for attention_prob_times_values (96x2048x2048x879): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x879): 83.093

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1737.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x880x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x880x2048): 80.515
Elapsed time for attention_prob_times_values (96x2048x2048x880): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x880): 92.858

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1865.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x881x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x881x2048): 77.653
Elapsed time for attention_prob_times_values (96x2048x2048x881): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x881): 83.266

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1739.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x882x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x882x2048): 78.530
Elapsed time for attention_prob_times_values (96x2048x2048x882): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x882): 85.617

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1775.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x883x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x883x2048): 77.750
Elapsed time for attention_prob_times_values (96x2048x2048x883): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x883): 83.495

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1746.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x884x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x884x2048): 76.291
Elapsed time for attention_prob_times_values (96x2048x2048x884): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x884): 82.734

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1724.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x885x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x885x2048): 77.773
Elapsed time for attention_prob_times_values (96x2048x2048x885): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x885): 83.616

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1752.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x886x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x886x2048): 78.378
Elapsed time for attention_prob_times_values (96x2048x2048x886): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x886): 86.020

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1785.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x887x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x887x2048): 77.900
Elapsed time for attention_prob_times_values (96x2048x2048x887): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x887): 83.531

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1756.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 68.479
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 67.956

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1956.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 70.025
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 71.187

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2029.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 68.655
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 68.308

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1973.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 67.211
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 71.562

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2001.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 68.756
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 66.180

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1951.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 84.041
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 83.464

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2428.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 71.657
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 60.377

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 1904.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 72.736
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 65.545

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2008.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 70.201
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 62.805

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1935.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x888x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x888x2048): 79.733
Elapsed time for attention_prob_times_values (96x2048x2048x888): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x888): 93.445

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1876.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x889x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x889x2048): 77.635
Elapsed time for attention_prob_times_values (96x2048x2048x889): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x889): 83.480

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1756.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x890x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x890x2048): 78.234
Elapsed time for attention_prob_times_values (96x2048x2048x890): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x890): 86.543

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1796.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x891x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x891x2048): 77.599
Elapsed time for attention_prob_times_values (96x2048x2048x891): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x891): 84.131

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1766.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x892x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x892x2048): 78.872
Elapsed time for attention_prob_times_values (96x2048x2048x892): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x892): 86.747

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1809.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x893x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x893x2048): 77.036
Elapsed time for attention_prob_times_values (96x2048x2048x893): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x893): 84.315

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1765.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x894x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x894x2048): 78.360
Elapsed time for attention_prob_times_values (96x2048x2048x894): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x894): 86.784

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1807.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x895x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x895x2048): 77.506
Elapsed time for attention_prob_times_values (96x2048x2048x895): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x895): 84.320

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1775.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x896x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x896x2048): 88.018
Elapsed time for attention_prob_times_values (96x2048x2048x896): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x896): 95.585

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 2016.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x897x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x897x2048): 79.012
Elapsed time for attention_prob_times_values (96x2048x2048x897): 0.0095
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 71.304
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 63.867

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1970.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 71.200
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 62.480

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1950.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 71.699
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 65.712

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2014.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 71.233
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 62.969

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1967.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 73.460
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 85.430

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2330.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 69.655
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 61.516

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1931.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 69.148
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 66.305

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2005.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 69.736
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 63.506

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1973.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 72.119
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 66.200

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2053.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 70.540
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x897): 75.682

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1702.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x898x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x898x2048): 79.959
Elapsed time for attention_prob_times_values (96x2048x2048x898): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x898): 77.379

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1733.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x899x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x899x2048): 78.763
Elapsed time for attention_prob_times_values (96x2048x2048x899): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x899): 73.646

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1679.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x900x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x900x2048): 80.485
Elapsed time for attention_prob_times_values (96x2048x2048x900): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x900): 78.369

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1754.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x901x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x901x2048): 78.748
Elapsed time for attention_prob_times_values (96x2048x2048x901): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x901): 76.231

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1713.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x902x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x902x2048): 79.894
Elapsed time for attention_prob_times_values (96x2048x2048x902): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x902): 78.436

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1752.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x903x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x903x2048): 76.080
Elapsed time for attention_prob_times_values (96x2048x2048x903): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x903): 76.370

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1689.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x904x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x904x2048): 80.502
Elapsed time for attention_prob_times_values (96x2048x2048x904): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x904): 82.967

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1813.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x905x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x905x2048): 78.596
Elapsed time for attention_prob_times_values (96x2048x2048x905): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x905): 76.597

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1723.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x906x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x906x2048): 79.558
Elapsed time for attention_prob_times_values (96x2048x2048x906): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x906): 73.321

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1696.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x907x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x907x2048): 78.919
Elapsed time for attention_prob_times_values (96x2048x2048x907): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x907): 75.552

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1718.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x908x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x908x2048): 80.042
Elapsed time for attention_prob_times_values (96x2048x2048x908): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x908): 78.905

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1770.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x909x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x909x2048): 78.967
Elapsed time for attention_prob_times_values (96x2048x2048x909): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x909): 76.002

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1727.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x910x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x910x2048): 78.531
Elapsed time for attention_prob_times_values (96x2048x2048x910): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x910): 78.882

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1757.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x911x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x911x2048): 78.938
Elapsed time for attention_prob_times_values (96x2048x2048x911): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x911): 77.027

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1742.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x912x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x912x2048): 78.107
Elapsed time for attention_prob_times_values (96x2048x2048x912): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x912): 85.277

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1824.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x913x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x913x2048): 71.212
Elapsed time for attention_prob_times_values (96x2048x2048x913): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x913): 70.456

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1586.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x914x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x914x2048): 79.563
Elapsed time for attention_prob_times_values (96x2048x2048x914): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x914): 79.347

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1781.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x915x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x915x2048): 78.445
Elapsed time for attention_prob_times_values (96x2048x2048x915): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x915): 77.395

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1748.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x916x2048): 0.0092
Elapsed time for attention_prob_times_values (1024x2048x2048x9): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x9): 7.629

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 23.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x10x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x10x2048): 8.035
Elapsed time for attention_prob_times_values (1024x2048x2048x10): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x10): 8.404

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 28.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x11x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x11x2048): 8.829
Elapsed time for attention_prob_times_values (1024x2048x2048x11): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x11): 9.433

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 34.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x12x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x12x2048): 9.313
Elapsed time for attention_prob_times_values (1024x2048x2048x12): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x12): 10.516

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 39.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x13x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x13x2048): 10.199
Elapsed time for attention_prob_times_values (1024x2048x2048x13): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x13): 11.133

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 45.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x14x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x14x2048): 10.770
Elapsed time for attention_prob_times_values (1024x2048x2048x14): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x14): 12.454

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 51.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x15x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x15x2048): 11.947
Elapsed time for attention_prob_times_values (1024x2048x2048x15): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x15): 12.838

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 58.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x16x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x16x2048): 12.810
Elapsed time for attention_prob_times_values (1024x2048x2048x16): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x16): 13.883

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 66.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x17x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x17x2048): 13.122
Elapsed time for attention_prob_times_values (1024x2048x2048x17): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x17): 14.132

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 71.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x18x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x18x2048): 13.872
Elapsed time for attention_prob_times_values (1024x2048x2048x18): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x18): 15.299

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 80.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 63.962

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2000.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 68.458
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 67.145

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2025.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 68.383
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 62.410

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1953.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 70.609
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 87.177

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2340.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 69.773
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 61.797

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1970.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0208
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 48.021
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 67.612

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1691.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 67.873
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 64.514

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1996.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 71.667
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 67.980

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 2110.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 69.512
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 64.666

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2030.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 70.863
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 67.869

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2106.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x916x2048): 79.805
Elapsed time for attention_prob_times_values (96x2048x2048x916): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x916): 79.455

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1789.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x917x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x917x2048): 78.569
Elapsed time for attention_prob_times_values (96x2048x2048x917): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x917): 77.486

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1754.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x918x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x918x2048): 79.331
Elapsed time for attention_prob_times_values (96x2048x2048x918): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x918): 79.598

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1789.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x919x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x919x2048): 77.636
Elapsed time for attention_prob_times_values (96x2048x2048x919): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x919): 77.698

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1750.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x920x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x920x2048): 79.376
Elapsed time for attention_prob_times_values (96x2048x2048x920): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x920): 85.540

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1857.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x921x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x921x2048): 78.217
Elapsed time for attention_prob_times_values (96x2048x2048x921): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x921): 77.828

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1762.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x922x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x922x2048): 78.828
Elapsed time for attention_prob_times_values (96x2048x2048x922): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x922): 79.457

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1789.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x923x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x923x2048): 78.240
Elapsed time for attention_prob_times_values (96x2048x2048x923): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x923): 77.989

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1767.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x924x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x924x2048): 79.571
Elapsed time for attention_prob_times_values (96x2048x2048x924): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x924): 80.178

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1809.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x925x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x925x2048): 78.388
Elapsed time for attention_prob_times_values (96x2048x2048x925): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x925): 71.856

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1700.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x926x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x926x2048): 74.852
Elapsed time for attention_prob_times_values (96x2048x2048x926): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x926): 80.164

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1757.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x927x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x927x2048): 77.488
Elapsed time for attention_prob_times_values (96x2048x2048x927): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x927): 78.264

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1769.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x928x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x928x2048): 85.642
Elapsed time for attention_prob_times_values (96x2048x2048x928): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x928): 86.951

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1963.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x929x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x929x2048): 73.888
Elapsed time for attention_prob_times_values (96x2048x2048x929): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x929): 78.467

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1733.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x930x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x930x2048): 81.188
Elapsed time for attention_prob_times_values (96x2048x2048x930): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x930): 79.624

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1832.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x931x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x931x2048): 74.924
Elapsed time for attention_prob_times_values (96x2048x2048x931): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x931): 75.592

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1717.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x932x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x932x2048): 81.903
Elapsed time for attention_prob_times_values (96x2048x2048x932): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x932): 80.735

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1857.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x933x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x933x2048): 80.021
Elapsed time for attention_prob_times_values (96x2048x2048x933): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x933): 78.481

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1812.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x934x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x934x2048): 78.450
Elapsed time for attention_prob_times_values (96x2048x2048x934): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x934): 80.573

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1819.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


EstimateTransformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 70.492
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 64.752

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2054.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 71.879
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 85.105

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2376.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 67.394
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 64.886

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2020.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 66.450
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 68.081

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2059.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 69.828
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 63.632

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2043.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 70.673
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 69.070

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2148.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 68.701
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 64.132

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2044.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 70.622
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 69.066

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2156.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 68.921
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 65.677

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2080.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate

--------
Elapsed time for attention_key_query_prob (96x2048x935x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x935x2048): 79.769
Elapsed time for attention_prob_times_values (96x2048x2048x935): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x935): 78.526

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1813.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x936x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x936x2048): 81.820
Elapsed time for attention_prob_times_values (96x2048x2048x936): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x936): 86.934

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1933.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x937x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x937x2048): 79.125
Elapsed time for attention_prob_times_values (96x2048x2048x937): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x937): 76.427

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1785.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x938x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x938x2048): 79.000
Elapsed time for attention_prob_times_values (96x2048x2048x938): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x938): 80.970

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1838.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x939x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x939x2048): 79.254
Elapsed time for attention_prob_times_values (96x2048x2048x939): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x939): 78.970

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1820.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x940x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x940x2048): 81.209
Elapsed time for attention_prob_times_values (96x2048x2048x940): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x940): 81.257

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1870.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x941x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x941x2048): 76.929
Elapsed time for attention_prob_times_values (96x2048x2048x941): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x941): 78.945

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1796.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x942x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x942x2048): 80.356
Elapsed time for attention_prob_times_values (96x2048x2048x942): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x942): 81.280

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1865.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x943x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x943x2048): 79.578
Elapsed time for attention_prob_times_values (96x2048x2048x943): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x943): 79.160

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1833.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x944x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x944x2048): 81.651
Elapsed time for attention_prob_times_values (96x2048x2048x944): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x944): 85.162

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1927.919Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x19x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x19x2048): 14.569
Elapsed time for attention_prob_times_values (1024x2048x2048x19): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x19): 16.129

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 88.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x20x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x20x2048): 15.306
Elapsed time for attention_prob_times_values (1024x2048x2048x20): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x20): 17.395

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 97.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x21x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x21x2048): 15.482
Elapsed time for attention_prob_times_values (1024x2048x2048x21): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x21): 17.138

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 101.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x22x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x22x2048): 16.228
Elapsed time for attention_prob_times_values (1024x2048x2048x22): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x22): 18.904

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 113.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x23x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x23x2048): 17.452
Elapsed time for attention_prob_times_values (1024x2048x2048x23): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x23): 18.631

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 121.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x24x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x24x2048): 17.692
Elapsed time for attention_prob_times_values (1024x2048x2048x24): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x24): 20.003

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 131.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x25x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x25x2048): 18.155
Elapsed time for attention_prob_times_values (1024x2048x2048x25): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x25): 20.384

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 139.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x26x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x26x2048): 18.836
Elapsed time for attention_prob_times_values (1024x2048x2048x26): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x26): 21.339

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 150.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x27x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x27x2048): 19.127
Elapsed time for attention_prob_times_values (1024x2048x2048x27): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x27): 21.569

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 157.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------

MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x945x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x945x2048): 77.776
Elapsed time for attention_prob_times_values (96x2048x2048x945): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x945): 79.334

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1818.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x946x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x946x2048): 80.073
Elapsed time for attention_prob_times_values (96x2048x2048x946): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x946): 81.623

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1873.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x947x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x947x2048): 79.289
Elapsed time for attention_prob_times_values (96x2048x2048x947): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x947): 79.498

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1841.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x948x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x948x2048): 78.619
Elapsed time for attention_prob_times_values (96x2048x2048x948): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x948): 81.792

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1861.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x949x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x949x2048): 79.486
Elapsed time for attention_prob_times_values (96x2048x2048x949): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x949): 79.545

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1848.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x950x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x950x2048): 80.184
Elapsed time for attention_prob_times_values (96x2048x2048x950): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x950): 81.853

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1884.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x951x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x951x2048): 79.516
Elapsed time for attention_prob_times_values (96x2048x2048x951): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x951): 79.587

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1852.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x952x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x952x2048): 81.207
Elapsed time for attention_prob_times_values (96x2048x2048x952): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x952): 88.359

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1972.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x953x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x953x2048): 79.018
Elapsed time for attention_prob_times_values (96x2048x2048x953): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x953): 79.912

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1854.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 87.859
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 88.423

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2732.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 70.822
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 66.150

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2124.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 72.076
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 69.538

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2203.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 70.736
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 63.270

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2083.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 69.833
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 68.362

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2159.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 70.916
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 66.470

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2148.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 73.007
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 69.886

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2240.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 70.465
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 66.849

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2156.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 73.423
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 91.108

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2561.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 70.208
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 66.789

Attention duration (in seconds): 0.0307
num_attention_heads: 24, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x954x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x954x2048): 79.796
Elapsed time for attention_prob_times_values (96x2048x2048x954): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x954): 82.242

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1892.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x955x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x955x2048): 78.856
Elapsed time for attention_prob_times_values (96x2048x2048x955): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x955): 79.848

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1855.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x956x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x956x2048): 80.618
Elapsed time for attention_prob_times_values (96x2048x2048x956): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x956): 82.398

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1907.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x957x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x957x2048): 79.190
Elapsed time for attention_prob_times_values (96x2048x2048x957): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x957): 80.182

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1866.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x958x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x958x2048): 80.063
Elapsed time for attention_prob_times_values (96x2048x2048x958): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x958): 82.305

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1903.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x959x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x959x2048): 79.330
Elapsed time for attention_prob_times_values (96x2048x2048x959): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x959): 80.405

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1874.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x960x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x960x2048): 92.279
Elapsed time for attention_prob_times_values (96x2048x2048x960): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x960): 90.192

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2143.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x961x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x961x2048): 80.939
Elapsed time for attention_prob_times_values (96x2048x2048x961): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x961): 80.396

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1897.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x962x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x962x2048): 81.376
Elapsed time for attention_prob_times_values (96x2048x2048x962): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x962): 82.706

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1931.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x963x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x963x2048): 80.749
Elapsed time for attention_prob_times_values (96x2048x2048x963): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x963): 78.162

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1872.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x964x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x964x2048): 80.364
Elapsed time for attention_prob_times_values (96x2048x2048x964): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x964): 82.928

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1925.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x965x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x965x2048): 80.600
Elapsed time for attention_prob_times_values (96x2048x2048x965): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x965): 80.271

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1899.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x966x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x966x2048): 81.338
Elapsed time for attention_prob_times_values (96x2048x2048x966): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x966): 82.932

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1941.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x967x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x967x2048): 80.493
Elapsed time for attention_prob_times_values (96x2048x2048x967): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x967): 80.751

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1907.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x968x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x968x2048): 82.203
Elapsed time for attention_prob_times_values (96x2048x2048x968): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x968): 89.918

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2034.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x969x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x969x2048): 79.322
Elapsed time for attention_prob_times_values (96x2048x2048x969): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x969): 80.983

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1900.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x970x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x970x2048): 80.554
Elapsed time for attention_prob_times_values (96x2048x2048x970): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x970): 83.334

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1944.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x971x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x971x2048): 79.852
Elapsed time for attention_prob_times_values (96x2048x2048x971): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x971): 81.264

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1913.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x972x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x972x2048): 81.501
Elapsed time for attention_prob_times_values (96x2048x2048x972): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x972): 82.470

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1949.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 2160.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 71.495
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 70.295

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2241.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 71.302
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 66.805

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2185.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 72.470
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 68.519

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2236.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 71.142
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 67.145

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2197.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 70.623
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 69.738

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2236.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 71.481
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 65.183

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2177.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 73.489
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 92.831

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2625.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 67.559
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 66.450

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 2148.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 71.788
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 71.089

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2294.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x973x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x973x2048): 77.894
Elapsed time for attention_prob_times_values (96x2048x2048x973): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x973): 81.428

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1895.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x974x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x974x2048): 79.082
Elapsed time for attention_prob_times_values (96x2048x2048x974): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x974): 83.548

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1936.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x975x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x975x2048): 79.984
Elapsed time for attention_prob_times_values (96x2048x2048x975): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x975): 75.754

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1855.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x976x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x976x2048): 77.506
Elapsed time for attention_prob_times_values (96x2048x2048x976): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x976): 90.982

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1998.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x977x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x977x2048): 79.352
Elapsed time for attention_prob_times_values (96x2048x2048x977): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x977): 78.188

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1882.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x978x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x978x2048): 80.281
Elapsed time for attention_prob_times_values (96x2048x2048x978): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x978): 83.932

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1963.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x979x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x979x2048): 79.047
Elapsed time for attention_prob_times_values (96x2048x2048x979): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x979): 81.903

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1926.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x980x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x980x2048): 81.129
Elapsed time for attention_prob_times_values (96x2048x2048x980): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x980): 84.018

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1978.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x981x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x981x2048): 79.616
Elapsed time for attention_prob_times_values (96x2048x2048x981): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x981): 81.959

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1937.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x982x2048): 0.0098
Elapsed time for attention_key_query_prob (1024x2048x28x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x28x2048): 19.850
Elapsed time for attention_prob_times_values (1024x2048x2048x28): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x28): 22.831

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 169.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x29x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x29x2048): 20.561
Elapsed time for attention_prob_times_values (1024x2048x2048x29): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x29): 23.045

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 179.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x30x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x30x2048): 21.416
Elapsed time for attention_prob_times_values (1024x2048x2048x30): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x30): 14.132

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 144.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x31x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x31x2048): 22.652
Elapsed time for attention_prob_times_values (1024x2048x2048x31): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x31): 24.508

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 206.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x32x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x32x2048): 41.292
Elapsed time for attention_prob_times_values (1024x2048x2048x32): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x32): 25.973

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 286.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x33x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x33x2048): 27.205
Elapsed time for attention_prob_times_values (1024x2048x2048x33): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x33): 26.772

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 249.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x34x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x34x2048): 27.499
Elapsed time for attention_prob_times_values (1024x2048x2048x34): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x34): 27.716

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 262.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x35x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x35x2048): 26.280
Elapsed time for attention_prob_times_values (1024x2048x2048x35): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x35): 27.422

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 261.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x36x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x36x2048): 25.975
Elapsed time for attention_prob_times_values (1024x2048x2048x36): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x36): 28.369

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 271.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x37x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x37x2048): 26.355
Elapsed time for attention_prob_times_values (1024x2048x2048x37): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x37): 29.074

Attention duration (in seconds): 0.0230
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 70.794
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 67.909

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2231.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 72.125
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 71.460

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2315.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 68.017
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 68.069

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 2198.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 69.595
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 70.854

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2273.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 71.255
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 67.644

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2251.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 70.013
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 93.959

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2607.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 70.528
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 61.163

Attention duration (in seconds): 0.0331
Attention throughput (in TFLOP/s): 2133.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 70.904
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 69.460

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2289.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 70.489
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 61.552

Attention duration (in seconds): 0.0331
Attention throughput (in TFLOP/s): 2148.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x982x2048): 80.427
Elapsed time for attention_prob_times_values (96x2048x2048x982): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x982): 84.083

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1974.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x983x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x983x2048): 79.654
Elapsed time for attention_prob_times_values (96x2048x2048x983): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x983): 82.094

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1943.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x984x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x984x2048): 81.715
Elapsed time for attention_prob_times_values (96x2048x2048x984): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x984): 91.124

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2073.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x985x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x985x2048): 74.557
Elapsed time for attention_prob_times_values (96x2048x2048x985): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x985): 82.106

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1882.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x986x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x986x2048): 76.075
Elapsed time for attention_prob_times_values (96x2048x2048x986): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x986): 84.244

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1927.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x987x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x987x2048): 79.219
Elapsed time for attention_prob_times_values (96x2048x2048x987): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x987): 82.267

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1947.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x988x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x988x2048): 80.907
Elapsed time for attention_prob_times_values (96x2048x2048x988): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x988): 84.206

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1993.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x989x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x989x2048): 78.891
Elapsed time for attention_prob_times_values (96x2048x2048x989): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x989): 82.470

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1949.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x990x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x990x2048): 80.127
Elapsed time for attention_prob_times_values (96x2048x2048x990): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x990): 84.680

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1992.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x991x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x991x2048): 79.269
Elapsed time for attention_prob_times_values (96x2048x2048x991): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x991): 82.647

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1960.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x992x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x992x2048): 92.468
Elapsed time for attention_prob_times_values (96x2048x2048x992): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x992): 92.547

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2243.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x993x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x993x2048): 80.963
Elapsed time for attention_prob_times_values (96x2048x2048x993): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x993): 82.728

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1986.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x994x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x994x2048): 81.723
Elapsed time for attention_prob_times_values (96x2048x2048x994): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x994): 81.863

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1987.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x995x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x995x2048): 80.680
Elapsed time for attention_prob_times_values (96x2048x2048x995): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x995): 82.924

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1989.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x996x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x996x2048): 82.314
Elapsed time for attention_prob_times_values (96x2048x2048x996): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x996): 85.204

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2038.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x997x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x997x2048): 78.143
Elapsed time for attention_prob_times_values (96x2048x2048x997): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x997): 82.947

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1960.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x998x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x998x2048): 81.288
Elapsed time for attention_prob_times_values (96x2048x2048x998): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x998): 85.198

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2029.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x999x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x999x2048): 80.266
Elapsed time for attention_prob_times_values (96x2048x2048x999): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x999): 76.665

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1914.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1000x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1000x2048): 82.145
Elapsed time for attention_prob_times_values (96x2048x2048x1000): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1000): 92.527

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2126.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 70.114
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 68.758

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 2273.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 69.758
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 61.322

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2141.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 71.377
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 69.690

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2318.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 70.671
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 65.400

Attention duration (in seconds): 0.0323
Attention throughput (in TFLOP/s): 2237.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x1x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x1x2048): 1.044
Elapsed time for attention_prob_times_values (320x2048x2048x1): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x1): 0.216

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 0.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x2x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x2x2048): 1.939
Elapsed time for attention_prob_times_values (320x2048x2048x2): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x2): 1.416

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x3x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x3x2048): 2.666
Elapsed time for attention_prob_times_values (320x2048x2048x3): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x3): 2.013

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 2.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x4x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x4x2048): 3.420
Elapsed time for attention_prob_times_values (320x2048x2048x4): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x4): 2.913

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 4.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x5x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x5x2048): 4.252
Elapsed time for attention_prob_times_values (320x2048x2048x5): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x5): 3.604

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 5.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x6x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x6x2048): 5.086
Elapsed time for attention_prob_times_values (320x2048x2048x6): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x6): 4.552

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 7.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1001x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1001x2048): 79.641
Elapsed time for attention_prob_times_values (96x2048x2048x1001): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1001): 83.085

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1989.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1002x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1002x2048): 80.519
Elapsed time for attention_prob_times_values (96x2048x2048x1002): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1002): 85.475

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2030.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1003x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1003x2048): 79.697
Elapsed time for attention_prob_times_values (96x2048x2048x1003): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1003): 83.186

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1995.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1004x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1004x2048): 81.616
Elapsed time for attention_prob_times_values (96x2048x2048x1004): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1004): 85.715

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2051.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1005x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1005x2048): 79.955
Elapsed time for attention_prob_times_values (96x2048x2048x1005): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1005): 83.303

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2003.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1006x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1006x2048): 80.760
Elapsed time for attention_prob_times_values (96x2048x2048x1006): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1006): 85.812

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2045.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1007x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1007x2048): 79.975
Elapsed time for attention_prob_times_values (96x2048x2048x1007): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1007): 83.358

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2008.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1008x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1008x2048): 82.407
Elapsed time for attention_prob_times_values (96x2048x2048x1008): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1008): 93.696

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2159.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1009x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1009x2048): 79.349
Elapsed time for attention_prob_times_values (96x2048x2048x1009): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1009): 83.602

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2006.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1010x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1010x2048): 80.266
Elapsed time for attention_prob_times_values (96x2048x2048x1010): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1010): 86.007

Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x7x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x7x2048): 5.944
Elapsed time for attention_prob_times_values (320x2048x2048x7): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x7): 5.249

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 8.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x8x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x8x2048): 6.193
Elapsed time for attention_prob_times_values (320x2048x2048x8): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x8): 7.210

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 10.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x9x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x9x2048): 7.213
Elapsed time for attention_prob_times_values (320x2048x2048x9): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x9): 7.669

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 12.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x10x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x10x2048): 7.961
Elapsed time for attention_prob_times_values (320x2048x2048x10): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x10): 8.095

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 14.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x11x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x11x2048): 8.741
Elapsed time for attention_prob_times_values (320x2048x2048x11): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x11): 8.822

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 16.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x12x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x12x2048): 9.509
Elapsed time for attention_prob_times_values (320x2048x2048x12): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x12): 9.878

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 18.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x13x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x13x2048): 10.281
Elapsed time for attention_prob_times_values (320x2048x2048x13): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x13): 10.382

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 20.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x14x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x14x2048): 11.062
Elapsed time for attention_prob_times_values (320x2048x2048x14): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x14): 12.290

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 24.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x15x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x15x2048): 11.839
Elapsed time for attention_prob_times_values (320x2048x2048x15): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x15): 12.683

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 26.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x16x2048): 0.0034
Attention throughput (in TFLOP/s): 283.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x38x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x38x2048): 26.605
Elapsed time for attention_prob_times_values (1024x2048x2048x38): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x38): 30.725

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 299.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x39x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x39x2048): 27.774
Elapsed time for attention_prob_times_values (1024x2048x2048x39): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x39): 30.975

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 314.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x40x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x40x2048): 28.597
Elapsed time for attention_prob_times_values (1024x2048x2048x40): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x40): 33.077

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 337.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x41x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x41x2048): 27.277
Elapsed time for attention_prob_times_values (1024x2048x2048x41): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x41): 32.617

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 334.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x42x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x42x2048): 28.179
Elapsed time for attention_prob_times_values (1024x2048x2048x42): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x42): 33.529

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 352.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x43x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x43x2048): 29.135
Elapsed time for attention_prob_times_values (1024x2048x2048x43): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x43): 31.805

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 357.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x44x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x44x2048): 29.267
Elapsed time for attention_prob_times_values (1024x2048x2048x44): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x44): 35.144

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 383.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x45x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x45x2048): 30.752
Elapsed time for attention_prob_times_values (1024x2048x2048x45): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x45): 35.060

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 401.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x46x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x46x2048): 31.812
Elapsed time for attention_prob_times_values (1024x2048x2048x46): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x46): 35.540

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 419.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2048.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1011x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1011x2048): 79.580
Elapsed time for attention_prob_times_values (96x2048x2048x1011): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1011): 83.665

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2014.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1012x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1012x2048): 81.520
Elapsed time for attention_prob_times_values (96x2048x2048x1012): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1012): 86.199

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2071.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1013x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1013x2048): 79.695
Elapsed time for attention_prob_times_values (96x2048x2048x1013): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1013): 83.607

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2019.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1014x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1014x2048): 80.369
Elapsed time for attention_prob_times_values (96x2048x2048x1014): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1014): 86.230

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2060.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1015x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1015x2048): 79.609
Elapsed time for attention_prob_times_values (96x2048x2048x1015): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1015): 83.723

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2023.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1016x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1016x2048): 81.650
Elapsed time for attention_prob_times_values (96x2048x2048x1016): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1016): 93.899

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2167.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1017x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1017x2048): 79.278
Elapsed time for attention_prob_times_values (96x2048x2048x1017): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1017): 83.921

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2024.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1018x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1018x2048): 80.215
Elapsed time for attention_prob_times_values (96x2048x2048x1018): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1018): 86.490

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2069.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1019x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1019x2048): 80.068
Elapsed time for attention_prob_times_values (96x2048x2048x1019): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1019): 83.948

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2039.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x16x2048): 12.605
Elapsed time for attention_prob_times_values (320x2048x2048x16): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x16): 13.262

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 29.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x17x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x17x2048): 12.940
Elapsed time for attention_prob_times_values (320x2048x2048x17): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x17): 14.293

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 31.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x18x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x18x2048): 13.713
Elapsed time for attention_prob_times_values (320x2048x2048x18): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x18): 15.526

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 35.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x19x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x19x2048): 14.452
Elapsed time for attention_prob_times_values (320x2048x2048x19): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x19): 15.866

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 37.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x20x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x20x2048): 15.218
Elapsed time for attention_prob_times_values (320x2048x2048x20): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x20): 17.029

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 41.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x21x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x21x2048): 15.958
Elapsed time for attention_prob_times_values (320x2048x2048x21): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x21): 16.198

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 42.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x22x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x22x2048): 16.708
Elapsed time for attention_prob_times_values (320x2048x2048x22): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x22): 18.609

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 47.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x23x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x23x2048): 17.461
Elapsed time for attention_prob_times_values (320x2048x2048x23): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x23): 18.902

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 50.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x24x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x24x2048): 18.188
Elapsed time for attention_prob_times_values (320x2048x2048x24): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x24): 20.462

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 55.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x25x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x25x2048): 18.455
Elapsed time for attention_prob_times_values (320x2048x2048x25): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x25): 20.492

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 57.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1020x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1020x2048): 84.717
Elapsed time for attention_prob_times_values (96x2048x2048x1020): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1020): 86.198

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2128.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1021x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1021x2048): 79.564
Elapsed time for attention_prob_times_values (96x2048x2048x1021): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1021): 79.794

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1986.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1022x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1022x2048): 80.309
Elapsed time for attention_prob_times_values (96x2048x2048x1022): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1022): 83.097

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2038.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1023x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1023x2048): 79.478
Elapsed time for attention_prob_times_values (96x2048x2048x1023): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1023): 83.419

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2033.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1024x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1024x2048): 90.767
Elapsed time for attention_prob_times_values (96x2048x2048x1024): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1024): 96.296

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2336.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1025x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1025x2048): 81.033
Elapsed time for attention_prob_times_values (96x2048x2048x1025): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1025): 76.647

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1971.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1026x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1026x2048): 82.036
Elapsed time for attention_prob_times_values (96x2048x2048x1026): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1026): 79.123

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2017.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1027x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1027x2048): 77.990
Elapsed time for attention_prob_times_values (96x2048x2048x1027): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1027): 76.868

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1941.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1028x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1028x2048): 84.599
Elapsed time for attention_prob_times_values (96x2048x2048x1028): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1028): 79.203

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2052.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1029x2048): 0.0106
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x26x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x26x2048): 17.911
Elapsed time for attention_prob_times_values (320x2048x2048x26): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x26): 21.461

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 59.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x27x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x27x2048): 19.849
Elapsed time for attention_prob_times_values (320x2048x2048x27): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x27): 21.966

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 64.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x28x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x28x2048): 20.540
Elapsed time for attention_prob_times_values (320x2048x2048x28): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x28): 23.094

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 69.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x29x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x29x2048): 21.183
Elapsed time for attention_prob_times_values (320x2048x2048x29): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x29): 23.342

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 72.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x30x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x30x2048): 21.967
Elapsed time for attention_prob_times_values (320x2048x2048x30): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x30): 22.560

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 74.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x31x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x31x2048): 5.098
Elapsed time for attention_prob_times_values (320x2048x2048x31): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x31): 24.845

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 28.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x32x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x32x2048): 40.854
Elapsed time for attention_prob_times_values (320x2048x2048x32): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x32): 26.649

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 112.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x33x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x33x2048): 26.877
Elapsed time for attention_prob_times_values (320x2048x2048x33): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x33): 26.362

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 95.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x34x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x34x2048): 27.244
Elapsed time for attention_prob_times_values (320x2048x2048x34): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x34): 27.404

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 99.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1029x2048): 78.370
Elapsed time for attention_prob_times_values (96x2048x2048x1029): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1029): 76.700

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1947.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1030x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1030x2048): 78.260
Elapsed time for attention_prob_times_values (96x2048x2048x1030): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1030): 79.240

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1979.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1031x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1031x2048): 75.792
Elapsed time for attention_prob_times_values (96x2048x2048x1031): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1031): 77.244

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1925.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1032x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1032x2048): 78.045
Elapsed time for attention_prob_times_values (96x2048x2048x1032): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1032): 84.680

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2045.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1033x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1033x2048): 75.510
Elapsed time for attention_prob_times_values (96x2048x2048x1033): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1033): 73.726

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1880.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1034x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1034x2048): 77.047
Elapsed time for attention_prob_times_values (96x2048x2048x1034): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1034): 79.713

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1977.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1035x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1035x2048): 79.896
Elapsed time for attention_prob_times_values (96x2048x2048x1035): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1035): 77.666

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1989.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1036x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1036x2048): 81.920
Elapsed time for attention_prob_times_values (96x2048x2048x1036): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1036): 80.066

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2047.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1037x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1037x2048): 76.704
Elapsed time for attention_prob_times_values (96x2048x2048x1037): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1037): 76.596

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1939.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1038x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1038x2048): 80.977
Elapsed time for attention_prob_times_values (96x2048x2048x1038): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1038): 80.133

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2040.247
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (320x2048x35x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x35x2048): 26.220
Elapsed time for attention_prob_times_values (320x2048x2048x35): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x35): 27.948

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 101.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x36x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x36x2048): 26.889
Elapsed time for attention_prob_times_values (320x2048x2048x36): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x36): 29.141

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 106.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x37x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x37x2048): 25.340
Elapsed time for attention_prob_times_values (320x2048x2048x37): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x37): 29.386

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 105.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x38x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x38x2048): 27.649
Elapsed time for attention_prob_times_values (320x2048x2048x38): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x38): 30.541

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 115.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x39x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x39x2048): 26.129
Elapsed time for attention_prob_times_values (320x2048x2048x39): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x39): 30.853

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 114.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x40x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x40x2048): 28.836
Elapsed time for attention_prob_times_values (320x2048x2048x40): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x40): 32.571

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 126.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x41x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x41x2048): 28.298
Elapsed time for attention_prob_times_values (320x2048x2048x41): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x41): 32.166

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 126.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x42x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x42x2048): 29.174
Elapsed time for attention_prob_times_values (320x2048x2048x42): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x42): 33.108

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 132.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x43x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x43x2048): 29.618
Elapsed time for attention_prob_times_values (320x2048x2048x43): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x43): 33.313

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 136.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x44x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x44x2048): 30.465
Elapsed time for attention_prob_times_values (320x2048x2048x44): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x44): 34.684

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 143.944
========================================================================================================================
num_attention_heads: 256, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x47x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x47x2048): 32.213
Elapsed time for attention_prob_times_values (1024x2048x2048x47): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x47): 35.553

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 430.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x48x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x48x2048): 32.067
Elapsed time for attention_prob_times_values (1024x2048x2048x48): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x48): 39.080

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 457.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x49x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x49x2048): 33.354
Elapsed time for attention_prob_times_values (1024x2048x2048x49): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x49): 38.053

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 471.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x50x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x50x2048): 34.366
Elapsed time for attention_prob_times_values (1024x2048x2048x50): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x50): 39.073

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 493.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x51x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x51x2048): 34.098
Elapsed time for attention_prob_times_values (1024x2048x2048x51): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x51): 39.111

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 500.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x52x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x52x2048): 35.661
Elapsed time for attention_prob_times_values (1024x2048x2048x52): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x52): 40.923

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 533.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x53x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x53x2048): 35.141
Elapsed time for attention_prob_times_values (1024x2048x2048x53): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x53): 40.622

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 536.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x54x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x54x2048): 35.951
Elapsed time for attention_prob_times_values (1024x2048x2048x54): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x54): 41.288

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 557.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x55x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x55x2048): 35.468
Elapsed time for attention_prob_times_values (1024x2048x2048x55): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x55): 41.905

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 566.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x56x2048): 0.0125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1039x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1039x2048): 80.263
Elapsed time for attention_prob_times_values (96x2048x2048x1039): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1039): 78.003

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2005.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1040x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1040x2048): 82.518
Elapsed time for attention_prob_times_values (96x2048x2048x1040): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1040): 84.167

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2114.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1041x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1041x2048): 79.462
Elapsed time for attention_prob_times_values (96x2048x2048x1041): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1041): 78.235

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2002.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1042x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1042x2048): 80.539
Elapsed time for attention_prob_times_values (96x2048x2048x1042): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1042): 80.442

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2046.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1043x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1043x2048): 79.893
Elapsed time for attention_prob_times_values (96x2048x2048x1043): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1043): 78.319

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2012.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1044x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1044x2048): 80.605
Elapsed time for attention_prob_times_values (96x2048x2048x1044): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1044): 80.648

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2053.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1045x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1045x2048): 80.259
Elapsed time for attention_prob_times_values (96x2048x2048x1045): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1045): 78.506

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2023.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1046x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1046x2048): 80.839
Elapsed time for attention_prob_times_values (96x2048x2048x1046): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1046): 80.682

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2060.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1047x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1047x2048): 80.173
Elapsed time for attention_prob_times_values (96x2048x2048x1047): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1047): 77.675

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2015.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x45x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x45x2048): 30.899
Elapsed time for attention_prob_times_values (320x2048x2048x45): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x45): 34.429

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 147.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x46x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x46x2048): 31.836
Elapsed time for attention_prob_times_values (320x2048x2048x46): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x46): 36.046

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 155.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x47x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x47x2048): 32.280
Elapsed time for attention_prob_times_values (320x2048x2048x47): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x47): 36.161

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 159.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x48x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x48x2048): 33.354
Elapsed time for attention_prob_times_values (320x2048x2048x48): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x48): 38.660

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 170.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x49x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x49x2048): 33.391
Elapsed time for attention_prob_times_values (320x2048x2048x49): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x49): 37.418

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 170.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x50x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x50x2048): 34.048
Elapsed time for attention_prob_times_values (320x2048x2048x50): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x50): 38.571

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 177.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x51x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x51x2048): 33.487
Elapsed time for attention_prob_times_values (320x2048x2048x51): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x51): 38.571

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 178.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x52x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x52x2048): 35.554
Elapsed time for attention_prob_times_values (320x2048x2048x52): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x52): 40.324

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 191.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x53x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x53x2048): 34.234
Elapsed time for attention_prob_times_values (320x2048x2048x53): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x53): 40.119

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 189.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1048x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1048x2048): 81.848
Elapsed time for attention_prob_times_values (96x2048x2048x1048): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1048): 85.764

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2141.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1049x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1049x2048): 79.705
Elapsed time for attention_prob_times_values (96x2048x2048x1049): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1049): 78.579

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2024.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1050x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1050x2048): 80.454
Elapsed time for attention_prob_times_values (96x2048x2048x1050): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1050): 80.981

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2067.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1051x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1051x2048): 78.433
Elapsed time for attention_prob_times_values (96x2048x2048x1051): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1051): 78.946

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2017.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1052x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1052x2048): 81.281
Elapsed time for attention_prob_times_values (96x2048x2048x1052): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1052): 81.164

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2083.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1053x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1053x2048): 78.671
Elapsed time for attention_prob_times_values (96x2048x2048x1053): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1053): 79.054

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2025.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1054x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1054x2048): 79.458
Elapsed time for attention_prob_times_values (96x2048x2048x1054): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1054): 81.284

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2065.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1055x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1055x2048): 78.499
Elapsed time for attention_prob_times_values (96x2048x2048x1055): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1055): 79.225

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2028.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1056x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1056x2048): 90.396
Elapsed time for attention_prob_times_values (96x2048x2048x1056): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1056): 87.356

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2287.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1057x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1057x2048): 78.562
num_attention_heads: 80, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x54x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x54x2048): 35.420
Elapsed time for attention_prob_times_values (320x2048x2048x54): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x54): 41.649

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 199.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x55x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x55x2048): 37.523
Elapsed time for attention_prob_times_values (320x2048x2048x55): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x55): 41.327

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 208.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x56x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x56x2048): 38.294
Elapsed time for attention_prob_times_values (320x2048x2048x56): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x56): 44.568

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 221.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x57x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x57x2048): 37.753
Elapsed time for attention_prob_times_values (320x2048x2048x57): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x57): 43.141

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 219.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x58x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x58x2048): 38.458
Elapsed time for attention_prob_times_values (320x2048x2048x58): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x58): 44.487

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 228.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x59x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x59x2048): 38.876
Elapsed time for attention_prob_times_values (320x2048x2048x59): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x59): 44.027

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 231.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x60x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x60x2048): 39.478
Elapsed time for attention_prob_times_values (320x2048x2048x60): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x60): 46.099

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 241.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x61x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x61x2048): 40.143
Elapsed time for attention_prob_times_values (320x2048x2048x61): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x61): 45.468

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 245.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x62x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x62x2048): 40.877
Elapsed time for attention_prob_times_values (320x2048x2048x62): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x62): 47.342

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 256.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x63x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x63x2048): 41.328
Elapsed time for attention_prob_times_values (320x2048x2048x63): 0.0036
Elapsed time for attention_prob_times_values (96x2048x2048x1057): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1057): 79.368

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2035.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1058x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1058x2048): 82.472
Elapsed time for attention_prob_times_values (96x2048x2048x1058): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1058): 81.563

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2115.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1059x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1059x2048): 81.484
Elapsed time for attention_prob_times_values (96x2048x2048x1059): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1059): 79.434

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2077.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1060x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1060x2048): 83.206
Elapsed time for attention_prob_times_values (96x2048x2048x1060): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1060): 79.370

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2099.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1061x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1061x2048): 80.735
Elapsed time for attention_prob_times_values (96x2048x2048x1061): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1061): 79.309

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2069.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1062x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1062x2048): 82.217
Elapsed time for attention_prob_times_values (96x2048x2048x1062): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1062): 81.696

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2121.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1063x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1063x2048): 81.229
Elapsed time for attention_prob_times_values (96x2048x2048x1063): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1063): 79.624

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2083.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1064x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1064x2048): 83.105
Elapsed time for attention_prob_times_values (96x2048x2048x1064): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1064): 86.991

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2204.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1065x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1065x2048): 80.643
Elapsed time for attention_prob_times_values (96x2048x2048x1065): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1065): 79.735

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2081.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1066x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1066x2048): 81.363
Elapsed time for attention_prob_times_values (96x2048x2048x1066): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1066): 81.634

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2117.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x63): 46.841

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 260.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x64x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x64x2048): 57.646
Elapsed time for attention_prob_times_values (320x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x64): 50.795

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 324.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x65x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x65x2048): 44.174
Elapsed time for attention_prob_times_values (320x2048x2048x65): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x65): 34.042

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 233.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x66x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x66x2048): 44.892
Elapsed time for attention_prob_times_values (320x2048x2048x66): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x66): 35.325

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 243.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x67x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x67x2048): 44.442
Elapsed time for attention_prob_times_values (320x2048x2048x67): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x67): 34.840

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 243.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x68x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x68x2048): 45.329
Elapsed time for attention_prob_times_values (320x2048x2048x68): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x68): 36.165

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 253.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x69x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x69x2048): 45.026
Elapsed time for attention_prob_times_values (320x2048x2048x69): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x69): 35.030

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 251.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x70x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x70x2048): 46.125
Elapsed time for attention_prob_times_values (320x2048x2048x70): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x70): 35.749

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 260.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x71x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x71x2048): 45.760
Elapsed time for attention_prob_times_values (320x2048x2048x71): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x71): 36.842

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 267.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x72x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x72x2048): 47.210
Elapsed time for attention_prob_times_values (320x2048x2048x72): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x72): 35.359

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 267.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1067x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1067x2048): 80.873
Elapsed time for attention_prob_times_values (96x2048x2048x1067): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1067): 79.800

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2089.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1068x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1068x2048): 82.430
Elapsed time for attention_prob_times_values (96x2048x2048x1068): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1068): 82.225

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2143.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1069x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1069x2048): 80.493
Elapsed time for attention_prob_times_values (96x2048x2048x1069): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1069): 79.928

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2089.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1070x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1070x2048): 80.434
Elapsed time for attention_prob_times_values (96x2048x2048x1070): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1070): 75.342

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2028.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1071x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1071x2048): 75.093
Elapsed time for attention_prob_times_values (96x2048x2048x1071): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1071): 79.988

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2021.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1072x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1072x2048): 82.885
Elapsed time for attention_prob_times_values (96x2048x2048x1072): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1072): 88.078

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2231.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1073x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1073x2048): 80.561
Elapsed time for attention_prob_times_values (96x2048x2048x1073): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1073): 80.281

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2102.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1074x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1074x2048): 79.543
Elapsed time for attention_prob_times_values (96x2048x2048x1074): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1074): 82.559

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2120.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1075x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1075x2048): 80.698
Elapsed time for attention_prob_times_values (96x2048x2048x1075): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1075): 80.375

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2109.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x56x2048): 38.525
Elapsed time for attention_prob_times_values (1024x2048x2048x56): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x56): 43.888

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 615.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x57x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x57x2048): 37.880
Elapsed time for attention_prob_times_values (1024x2048x2048x57): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x57): 43.523

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 617.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x58x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x58x2048): 37.050
Elapsed time for attention_prob_times_values (1024x2048x2048x58): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x58): 45.067

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 630.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x59x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x59x2048): 39.004
Elapsed time for attention_prob_times_values (1024x2048x2048x59): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x59): 44.657

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 655.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x60x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x60x2048): 38.746
Elapsed time for attention_prob_times_values (1024x2048x2048x60): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x60): 45.764

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 671.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x61x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x61x2048): 39.025
Elapsed time for attention_prob_times_values (1024x2048x2048x61): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x61): 46.125

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 687.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x62x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x62x2048): 39.955
Elapsed time for attention_prob_times_values (1024x2048x2048x62): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x62): 48.093

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 720.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x63x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x63x2048): 41.359
Elapsed time for attention_prob_times_values (1024x2048x2048x63): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x63): 47.397

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 739.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x64x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x64x2048): 57.567
Elapsed time for attention_prob_times_values (1024x2048x2048x64): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x64): 51.359

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 922.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x65x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x65x2048): 43.290
Elapsed time for attention_prob_times_values (1024x2048x2048x65): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x65): 33.665

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 653.361
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x73x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x73x2048): 44.151
Elapsed time for attention_prob_times_values (320x2048x2048x73): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x73): 37.463

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 271.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x74x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x74x2048): 46.957
Elapsed time for attention_prob_times_values (320x2048x2048x74): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x74): 39.039

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 289.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x75x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x75x2048): 46.705
Elapsed time for attention_prob_times_values (320x2048x2048x75): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x75): 38.039

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 287.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x76x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x76x2048): 47.911
Elapsed time for attention_prob_times_values (320x2048x2048x76): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x76): 40.167

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 303.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x77x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x77x2048): 47.585
Elapsed time for attention_prob_times_values (320x2048x2048x77): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x77): 39.262

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 301.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x78x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x78x2048): 48.775
Elapsed time for attention_prob_times_values (320x2048x2048x78): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x78): 40.953

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 315.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x79x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x79x2048): 48.604
Elapsed time for attention_prob_times_values (320x2048x2048x79): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x79): 40.037

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 314.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x80x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x80x2048): 50.202
Elapsed time for attention_prob_times_values (320x2048x2048x80): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x80): 38.726

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 316.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x81x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x81x2048): 47.506
Elapsed time for attention_prob_times_values (320x2048x2048x81): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x81): 40.940

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 322.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x82x2048): 0.0046
--------
Elapsed time for attention_key_query_prob (96x2048x1076x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1076x2048): 82.396
Elapsed time for attention_prob_times_values (96x2048x2048x1076): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1076): 82.621

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2163.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1077x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1077x2048): 80.969
Elapsed time for attention_prob_times_values (96x2048x2048x1077): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1077): 79.530

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2105.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1078x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1078x2048): 81.651
Elapsed time for attention_prob_times_values (96x2048x2048x1078): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1078): 82.789

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2159.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1079x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1079x2048): 80.936
Elapsed time for attention_prob_times_values (96x2048x2048x1079): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1079): 78.917

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1080x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1080x2048): 82.853
Elapsed time for attention_prob_times_values (96x2048x2048x1080): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1080): 88.238

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2248.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1081x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1081x2048): 79.780
Elapsed time for attention_prob_times_values (96x2048x2048x1081): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1081): 80.724

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2113.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1082x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1082x2048): 80.310
Elapsed time for attention_prob_times_values (96x2048x2048x1082): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1082): 83.076

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2152.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1083x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1083x2048): 78.252
Elapsed time for attention_prob_times_values (96x2048x2048x1083): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1083): 80.861

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2098.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1084x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1084x2048): 80.150
Elapsed time for attention_prob_times_values (96x2048x2048x1084): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1084): 83.246

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2156.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1085x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1085x2048): 80.710
Elapsed time for attention_prob_times_values (96x2048x2048x1085): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1085): 80.942

Attention duration (in seconds): 0.0216
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x82x2048): 48.332
Elapsed time for attention_prob_times_values (320x2048x2048x82): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x82): 42.705

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 335.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x83x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x83x2048): 48.344
Elapsed time for attention_prob_times_values (320x2048x2048x83): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x83): 41.719

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 335.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x84x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x84x2048): 47.595
Elapsed time for attention_prob_times_values (320x2048x2048x84): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x84): 43.650

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 344.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x85x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x85x2048): 49.186
Elapsed time for attention_prob_times_values (320x2048x2048x85): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x85): 40.949

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 341.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x86x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x86x2048): 49.612
Elapsed time for attention_prob_times_values (320x2048x2048x86): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x86): 44.465

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 361.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x87x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x87x2048): 50.040
Elapsed time for attention_prob_times_values (320x2048x2048x87): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x87): 43.485

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 362.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x88x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x88x2048): 50.865
Elapsed time for attention_prob_times_values (320x2048x2048x88): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x88): 43.406

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 368.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x89x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x89x2048): 49.238
Elapsed time for attention_prob_times_values (320x2048x2048x89): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x89): 44.362

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 371.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x90x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x90x2048): 50.057
Elapsed time for attention_prob_times_values (320x2048x2048x90): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x90): 44.724

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 379.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x91x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x91x2048): 50.123
Elapsed time for attention_prob_times_values (320x2048x2048x91): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x91): 45.135

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 385.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Attention throughput (in TFLOP/s): 2136.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1086x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1086x2048): 81.425
Elapsed time for attention_prob_times_values (96x2048x2048x1086): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1086): 83.213

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2177.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1087x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1087x2048): 80.823
Elapsed time for attention_prob_times_values (96x2048x2048x1087): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1087): 80.891

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2140.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1088x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1088x2048): 93.230
Elapsed time for attention_prob_times_values (96x2048x2048x1088): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1088): 87.945

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2398.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1089x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1089x2048): 82.133
Elapsed time for attention_prob_times_values (96x2048x2048x1089): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1089): 81.115

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2164.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1090x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1090x2048): 82.970
Elapsed time for attention_prob_times_values (96x2048x2048x1090): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1090): 83.662

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2211.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1091x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1091x2048): 81.078
Elapsed time for attention_prob_times_values (96x2048x2048x1091): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1091): 81.330

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2157.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1092x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1092x2048): 83.394
Elapsed time for attention_prob_times_values (96x2048x2048x1092): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1092): 83.786

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2222.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1093x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1093x2048): 81.388
Elapsed time for attention_prob_times_values (96x2048x2048x1093): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1093): 81.391

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2166.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1094x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1094x2048): 82.602
Elapsed time for attention_prob_times_values (96x2048x2048x1094): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1094): 83.700

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2215.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x92x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x92x2048): 50.966
Elapsed time for attention_prob_times_values (320x2048x2048x92): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x92): 46.981

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 400.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x93x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x93x2048): 49.606
Elapsed time for attention_prob_times_values (320x2048x2048x93): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x93): 45.883

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 394.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x94x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x94x2048): 51.821
Elapsed time for attention_prob_times_values (320x2048x2048x94): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x94): 47.558

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 413.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x95x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x95x2048): 51.857
Elapsed time for attention_prob_times_values (320x2048x2048x95): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x95): 46.642

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 413.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x96x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x96x2048): 67.762
Elapsed time for attention_prob_times_values (320x2048x2048x96): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x96): 48.367

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 479.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x97x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x97x2048): 53.150
Elapsed time for attention_prob_times_values (320x2048x2048x97): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x97): 47.660

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 431.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x98x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x98x2048): 54.178
Elapsed time for attention_prob_times_values (320x2048x2048x98): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x98): 49.426

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 447.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x99x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x99x2048): 53.684
Elapsed time for attention_prob_times_values (320x2048x2048x99): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x99): 48.490

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 445.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x100x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x100x2048): 54.981
Elapsed time for attention_prob_times_values (320x2048x2048x100): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x100): 50.423

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 463.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 24, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1095x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1095x2048): 81.682
Elapsed time for attention_prob_times_values (96x2048x2048x1095): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1095): 81.459

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2175.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1096x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1096x2048): 83.777
Elapsed time for attention_prob_times_values (96x2048x2048x1096): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1096): 89.335

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2307.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1097x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1097x2048): 81.113
Elapsed time for attention_prob_times_values (96x2048x2048x1097): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1097): 78.033

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2124.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1098x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1098x2048): 81.881
Elapsed time for attention_prob_times_values (96x2048x2048x1098): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1098): 84.100

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2218.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1099x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1099x2048): 81.318
Elapsed time for attention_prob_times_values (96x2048x2048x1099): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1099): 81.849

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2182.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1100x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1100x2048): 82.916
Elapsed time for attention_prob_times_values (96x2048x2048x1100): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1100): 84.293

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2238.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1101x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1101x2048): 80.787
Elapsed time for attention_prob_times_values (96x2048x2048x1101): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1101): 82.034

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2182.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1102x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1102x2048): 77.622
Elapsed time for attention_prob_times_values (96x2048x2048x1102): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1102): 84.333

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2168.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1103x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1103x2048): 81.297
Elapsed time for attention_prob_times_values (96x2048x2048x1103): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1103): 82.268

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2195.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1104x2048): 0.0106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x66x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x66x2048): 44.477
Elapsed time for attention_prob_times_values (1024x2048x2048x66): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x66): 34.272

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 677.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x67x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x67x2048): 43.390
Elapsed time for attention_prob_times_values (1024x2048x2048x67): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x67): 34.121

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 678.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x68x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x68x2048): 45.071
Elapsed time for attention_prob_times_values (1024x2048x2048x68): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x68): 35.775

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 717.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x69x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x69x2048): 44.424
Elapsed time for attention_prob_times_values (1024x2048x2048x69): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x69): 35.547

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 720.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x70x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x70x2048): 45.790
Elapsed time for attention_prob_times_values (1024x2048x2048x70): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x70): 36.992

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 757.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x71x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x71x2048): 45.452
Elapsed time for attention_prob_times_values (1024x2048x2048x71): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x71): 36.551

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 759.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x72x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x72x2048): 46.747
Elapsed time for attention_prob_times_values (1024x2048x2048x72): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x72): 35.963

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 772.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x73x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x73x2048): 43.377
Elapsed time for attention_prob_times_values (1024x2048x2048x73): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x73): 36.286

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 760.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x74x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x74x2048): 44.753
Elapsed time for attention_prob_times_values (1024x2048x2048x74): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x74): 36.199

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 780.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (320x2048x101x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x101x2048): 52.252
Elapsed time for attention_prob_times_values (320x2048x2048x101): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x101): 47.588

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 442.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x102x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x102x2048): 55.218
Elapsed time for attention_prob_times_values (320x2048x2048x102): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x102): 48.420

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 462.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x103x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x103x2048): 54.725
Elapsed time for attention_prob_times_values (320x2048x2048x103): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x103): 50.248

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 473.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x104x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x104x2048): 54.712
Elapsed time for attention_prob_times_values (320x2048x2048x104): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x104): 50.683

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 480.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x105x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x105x2048): 53.004
Elapsed time for attention_prob_times_values (320x2048x2048x105): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x105): 49.371

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 470.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x106x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x106x2048): 55.440
Elapsed time for attention_prob_times_values (320x2048x2048x106): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x106): 52.652

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 501.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x107x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x107x2048): 54.937
Elapsed time for attention_prob_times_values (320x2048x2048x107): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x107): 51.380

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 496.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x108x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x108x2048): 56.326
Elapsed time for attention_prob_times_values (320x2048x2048x108): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x108): 53.647

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 518.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x109x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x109x2048): 55.637
Elapsed time for attention_prob_times_values (320x2048x2048x109): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x109): 50.539

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 503.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x110x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x110x2048): 56.754
Elapsed time for attention_prob_times_values (320x2048x2048x110): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x110): 52.465

Attention duration (in seconds): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1104x2048): 83.538
Elapsed time for attention_prob_times_values (96x2048x2048x1104): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1104): 90.035

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2329.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1105x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1105x2048): 78.892
Elapsed time for attention_prob_times_values (96x2048x2048x1105): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1105): 81.940

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2162.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1106x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1106x2048): 81.818
Elapsed time for attention_prob_times_values (96x2048x2048x1106): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1106): 84.338

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2236.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1107x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1107x2048): 80.974
Elapsed time for attention_prob_times_values (96x2048x2048x1107): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1107): 78.120

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2142.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1108x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1108x2048): 82.692
Elapsed time for attention_prob_times_values (96x2048x2048x1108): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1108): 84.612

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2255.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1109x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1109x2048): 81.130
Elapsed time for attention_prob_times_values (96x2048x2048x1109): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1109): 82.665

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2210.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1110x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1110x2048): 81.904
Elapsed time for attention_prob_times_values (96x2048x2048x1110): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1110): 84.850

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2251.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1111x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1111x2048): 76.999
Elapsed time for attention_prob_times_values (96x2048x2048x1111): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1111): 82.704

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2156.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1112x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1112x2048): 83.398
Elapsed time for attention_prob_times_values (96x2048x2048x1112): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1112): 90.940

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2354.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1113x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1113x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1113): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1113): 82.844

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2215.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1114x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1114x2048): 81.597
Elapsed time for attention_prob_times_values (96x2048x2048x1114): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1114): 84.827

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2254.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1115x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1115x2048): 80.884
Elapsed time for attention_prob_times_values (96x2048x2048x1115): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1115): 82.955

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2222.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1116x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1116x2048): 82.699
Elapsed time for attention_prob_times_values (96x2048x2048x1116): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1116): 85.135

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2278.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1117x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1117x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1117): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1117): 83.120

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2226.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1118x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1118x2048): 81.780
Elapsed time for attention_prob_times_values (96x2048x2048x1118): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1118): 85.068

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2268.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1119x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1119x2048): 78.998
Elapsed time for attention_prob_times_values (96x2048x2048x1119): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1119): 83.289

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2207.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1120x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1120x2048): 88.477
Elapsed time for attention_prob_times_values (96x2048x2048x1120): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1120): 92.307

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2462.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1121x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1121x2048): 82.345
Elapsed time for attention_prob_times_values (96x2048x2048x1121): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1121): 83.190

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2257.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1122x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1122x2048): 83.200
Elapsed time for attention_prob_times_values (96x2048x2048x1122): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1122): 85.492

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2301.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 523.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x111x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x111x2048): 56.275
Elapsed time for attention_prob_times_values (320x2048x2048x111): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x111): 52.699

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 526.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x112x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x112x2048): 56.535
Elapsed time for attention_prob_times_values (320x2048x2048x112): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x112): 54.765

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 542.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x113x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x113x2048): 56.198
Elapsed time for attention_prob_times_values (320x2048x2048x113): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x113): 53.644

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 539.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x114x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x114x2048): 57.490
Elapsed time for attention_prob_times_values (320x2048x2048x114): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x114): 55.820

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 561.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x115x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x115x2048): 55.401
Elapsed time for attention_prob_times_values (320x2048x2048x115): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x115): 54.100

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 546.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x116x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x116x2048): 58.285
Elapsed time for attention_prob_times_values (320x2048x2048x116): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x116): 56.551

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 577.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x117x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x117x2048): 57.632
Elapsed time for attention_prob_times_values (320x2048x2048x117): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x117): 53.130

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 560.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x118x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x118x2048): 56.847
Elapsed time for attention_prob_times_values (320x2048x2048x118): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x118): 57.130

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 582.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x119x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x119x2048): 58.316
Elapsed time for attention_prob_times_values (320x2048x2048x119): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x119): 55.484

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 585.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1123x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1123x2048): 82.178
Elapsed time for attention_prob_times_values (96x2048x2048x1123): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1123): 83.392

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2261.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1124x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1124x2048): 83.766
Elapsed time for attention_prob_times_values (96x2048x2048x1124): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1124): 85.448

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2313.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1125x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1125x2048): 80.760
Elapsed time for attention_prob_times_values (96x2048x2048x1125): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1125): 82.370

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2231.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1126x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1126x2048): 82.713
Elapsed time for attention_prob_times_values (96x2048x2048x1126): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1126): 85.650

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2305.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1127x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1127x2048): 81.630
Elapsed time for attention_prob_times_values (96x2048x2048x1127): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1127): 83.472

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2262.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1128x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1128x2048): 83.559
Elapsed time for attention_prob_times_values (96x2048x2048x1128): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1128): 92.130

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2404.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1129x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1129x2048): 76.826
Elapsed time for attention_prob_times_values (96x2048x2048x1129): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1129): 82.599

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2186.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1130x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1130x2048): 81.918
Elapsed time for attention_prob_times_values (96x2048x2048x1130): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1130): 83.889

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2278.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1131x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1131x2048): 81.369
Elapsed time for attention_prob_times_values (96x2048x2048x1131): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1131): 80.820

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2230.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1132x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1132x2048): 82.579
num_attention_heads: 80, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x120x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x120x2048): 58.745
Elapsed time for attention_prob_times_values (320x2048x2048x120): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x120): 56.328

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 596.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x121x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x121x2048): 58.175
Elapsed time for attention_prob_times_values (320x2048x2048x121): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x121): 42.960

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 516.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x122x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x122x2048): 59.079
Elapsed time for attention_prob_times_values (320x2048x2048x122): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x122): 58.663

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 619.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x123x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x123x2048): 57.717
Elapsed time for attention_prob_times_values (320x2048x2048x123): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x123): 44.887

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 535.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x124x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x124x2048): 60.126
Elapsed time for attention_prob_times_values (320x2048x2048x124): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x124): 57.662

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 629.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x125x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x125x2048): 58.928
Elapsed time for attention_prob_times_values (320x2048x2048x125): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x125): 43.403

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 538.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x126x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x126x2048): 59.919
Elapsed time for attention_prob_times_values (320x2048x2048x126): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x126): 56.540

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 630.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x127x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x127x2048): 58.473
Elapsed time for attention_prob_times_values (320x2048x2048x127): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x127): 43.532

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 545.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x128x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x128x2048): 69.725
Elapsed time for attention_prob_times_values (320x2048x2048x128): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x128): 63.495

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 731.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x129x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x129x2048): 57.197
Elapsed time for attention_prob_times_values (320x2048x2048x129): 0.0081
Elapsed time for attention_prob_times_values (96x2048x2048x1132): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1132): 86.099

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2320.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1133x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1133x2048): 81.174
Elapsed time for attention_prob_times_values (96x2048x2048x1133): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1133): 83.779

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2272.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1134x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1134x2048): 82.328
Elapsed time for attention_prob_times_values (96x2048x2048x1134): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1134): 86.171

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2322.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1135x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1135x2048): 77.286
Elapsed time for attention_prob_times_values (96x2048x2048x1135): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1135): 83.839

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2219.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1136x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1136x2048): 83.841
Elapsed time for attention_prob_times_values (96x2048x2048x1136): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1136): 92.945

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2435.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1137x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1137x2048): 80.914
Elapsed time for attention_prob_times_values (96x2048x2048x1137): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1137): 83.966

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2278.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1138x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1138x2048): 81.860
Elapsed time for attention_prob_times_values (96x2048x2048x1138): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1138): 86.128

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2322.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1139x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1139x2048): 80.989
Elapsed time for attention_prob_times_values (96x2048x2048x1139): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1139): 84.141

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2285.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1140x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1140x2048): 82.468
Elapsed time for attention_prob_times_values (96x2048x2048x1140): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1140): 86.594

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2341.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1141x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1141x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1141): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1141): 84.182

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2286.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
num_attention_heads: 256, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x75x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x75x2048): 44.599
Elapsed time for attention_prob_times_values (1024x2048x2048x75): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x75): 38.001

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 810.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x76x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x76x2048): 47.194
Elapsed time for attention_prob_times_values (1024x2048x2048x76): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x76): 39.846

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 864.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x77x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x77x2048): 45.425
Elapsed time for attention_prob_times_values (1024x2048x2048x77): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x77): 38.919

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 848.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x78x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x78x2048): 48.156
Elapsed time for attention_prob_times_values (1024x2048x2048x78): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x78): 40.576

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 902.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x79x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x79x2048): 48.125
Elapsed time for attention_prob_times_values (1024x2048x2048x79): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x79): 39.725

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 903.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x80x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x80x2048): 49.991
Elapsed time for attention_prob_times_values (1024x2048x2048x80): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x80): 39.392

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 925.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x81x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x81x2048): 46.293
Elapsed time for attention_prob_times_values (1024x2048x2048x81): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x81): 40.667

Attention duration (in seconds): 0.0321
Attention throughput (in TFLOP/s): 920.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x82x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x82x2048): 47.456
Elapsed time for attention_prob_times_values (1024x2048x2048x82): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x82): 42.355

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 962.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x83x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x83x2048): 48.737
Elapsed time for attention_prob_times_values (1024x2048x2048x83): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x83): 41.364

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 973.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x84x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x84x2048): 49.308
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x129): 42.862

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 542.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x130x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x130x2048): 59.119
Elapsed time for attention_prob_times_values (320x2048x2048x130): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x130): 47.978

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 590.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x131x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x131x2048): 59.251
Elapsed time for attention_prob_times_values (320x2048x2048x131): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x131): 46.341

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 584.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x132x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x132x2048): 60.889
Elapsed time for attention_prob_times_values (320x2048x2048x132): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x132): 48.877

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 613.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x133x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x133x2048): 59.522
Elapsed time for attention_prob_times_values (320x2048x2048x133): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x133): 47.013

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 598.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x134x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x134x2048): 60.948
Elapsed time for attention_prob_times_values (320x2048x2048x134): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x134): 49.306

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 625.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x135x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x135x2048): 59.383
Elapsed time for attention_prob_times_values (320x2048x2048x135): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x135): 46.926

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 605.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x136x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x136x2048): 60.406
Elapsed time for attention_prob_times_values (320x2048x2048x136): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x136): 46.442

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 610.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x137x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x137x2048): 59.624
Elapsed time for attention_prob_times_values (320x2048x2048x137): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x137): 47.865

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 621.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x138x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x138x2048): 61.265
Elapsed time for attention_prob_times_values (320x2048x2048x138): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x138): 50.657

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 653.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1142x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1142x2048): 79.336
Elapsed time for attention_prob_times_values (96x2048x2048x1142): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1142): 86.617

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2299.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1143x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1143x2048): 81.220
Elapsed time for attention_prob_times_values (96x2048x2048x1143): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1143): 84.293

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2298.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1144x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1144x2048): 83.133
Elapsed time for attention_prob_times_values (96x2048x2048x1144): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1144): 89.558

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2398.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1145x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1145x2048): 80.842
Elapsed time for attention_prob_times_values (96x2048x2048x1145): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1145): 84.428

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2299.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1146x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1146x2048): 81.569
Elapsed time for attention_prob_times_values (96x2048x2048x1146): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1146): 86.950

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2345.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1147x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1147x2048): 77.213
Elapsed time for attention_prob_times_values (96x2048x2048x1147): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1147): 84.608

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2251.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1148x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1148x2048): 82.215
Elapsed time for attention_prob_times_values (96x2048x2048x1148): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1148): 87.123

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2360.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1149x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1149x2048): 80.380
Elapsed time for attention_prob_times_values (96x2048x2048x1149): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1149): 84.645

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2303.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1150x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1150x2048): 81.484
Elapsed time for attention_prob_times_values (96x2048x2048x1150): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1150): 83.651

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2307.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x139x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x139x2048): 60.619
Elapsed time for attention_prob_times_values (320x2048x2048x139): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x139): 48.532

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 639.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x140x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x140x2048): 59.818
Elapsed time for attention_prob_times_values (320x2048x2048x140): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x140): 51.524

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 660.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x141x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x141x2048): 61.508
Elapsed time for attention_prob_times_values (320x2048x2048x141): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x141): 46.600

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 637.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x142x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x142x2048): 62.572
Elapsed time for attention_prob_times_values (320x2048x2048x142): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x142): 50.013

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 672.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x143x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x143x2048): 61.936
Elapsed time for attention_prob_times_values (320x2048x2048x143): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x143): 49.904

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 672.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x144x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x144x2048): 64.396
Elapsed time for attention_prob_times_values (320x2048x2048x144): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x144): 49.944

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 689.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x145x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x145x2048): 61.671
Elapsed time for attention_prob_times_values (320x2048x2048x145): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x145): 50.528

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 684.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x146x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x146x2048): 63.066
Elapsed time for attention_prob_times_values (320x2048x2048x146): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x146): 53.182

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 715.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x147x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x147x2048): 62.380
Elapsed time for attention_prob_times_values (320x2048x2048x147): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x147): 50.971

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 700.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
--------
Elapsed time for attention_key_query_prob (96x2048x1151x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1151x2048): 80.694
Elapsed time for attention_prob_times_values (96x2048x2048x1151): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1151): 84.800

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2313.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1152x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1152x2048): 88.610
Elapsed time for attention_prob_times_values (96x2048x2048x1152): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1152): 93.334

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2545.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1153x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1153x2048): 79.808
Elapsed time for attention_prob_times_values (96x2048x2048x1153): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1153): 77.971

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2210.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1154x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1154x2048): 83.179
Elapsed time for attention_prob_times_values (96x2048x2048x1154): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1154): 80.506

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2294.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1155x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1155x2048): 81.748
Elapsed time for attention_prob_times_values (96x2048x2048x1155): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1155): 76.651

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2220.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1156x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1156x2048): 83.857
Elapsed time for attention_prob_times_values (96x2048x2048x1156): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1156): 80.614

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2309.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1157x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1157x2048): 82.263
Elapsed time for attention_prob_times_values (96x2048x2048x1157): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1157): 75.851

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2219.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1158x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1158x2048): 83.009
Elapsed time for attention_prob_times_values (96x2048x2048x1158): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1158): 80.637

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2302.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1159x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1159x2048): 81.111
Elapsed time for attention_prob_times_values (96x2048x2048x1159): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1159): 78.272

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2243.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1160x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1160x2048): 81.368
Elapsed time for attention_prob_times_values (96x2048x2048x1160): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1160): 85.637

Attention duration (in seconds): 0.0224
slurmstepd: error: *** JOB 1507314 ON frontier08999 CANCELLED AT 2023-11-23T01:09:44 DUE TO TIME LIMIT ***
